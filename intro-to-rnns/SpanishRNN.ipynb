{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 50, 25, 18, 51, 79, 39, 38, 10, 77, 77, 77,  4, 25, 18, 18, 59,\n",
       "       38, 52, 25, 35, 16, 82, 16, 79, 29, 38, 25, 39, 79, 38, 25, 82, 82,\n",
       "       38, 25, 82, 16,  9, 79,  3, 38, 79, 81, 79, 39, 59, 38, 53,  5, 50,\n",
       "       25, 18, 18, 59, 38, 52, 25, 35, 16, 82, 59, 38, 16, 29, 38, 53,  5,\n",
       "       50, 25, 18, 18, 59, 38, 16,  5, 38, 16, 51, 29, 38, 26, 66,  5, 77,\n",
       "       66, 25, 59, 17, 77, 77, 20, 81, 79, 39, 59, 51, 50, 16,  5], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text.  Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30, 50, 25, 18, 51, 79, 39, 38, 10, 77, 77, 77,  4, 25, 18, 18, 59,\n",
       "        38, 52, 25, 35, 16, 82, 16, 79, 29, 38, 25, 39, 79, 38, 25, 82, 82,\n",
       "        38, 25, 82, 16,  9, 79,  3, 38, 79, 81, 79, 39, 59, 38, 53,  5],\n",
       "       [38, 25, 35, 38,  5, 26, 51, 38, 19, 26, 16,  5, 19, 38, 51, 26, 38,\n",
       "        29, 51, 25, 59, 73, 15, 38, 25,  5, 29, 66, 79, 39, 79, 68, 38,  1,\n",
       "         5,  5, 25, 73, 38, 29, 35, 16, 82, 16,  5, 19, 73, 38, 21, 53],\n",
       "       [81, 16,  5, 17, 77, 77, 15, 71, 79, 29, 73, 38, 16, 51, 27, 29, 38,\n",
       "        29, 79, 51, 51, 82, 79, 68, 17, 38, 14, 50, 79, 38, 18, 39, 16, 56,\n",
       "        79, 38, 16, 29, 38, 35, 25, 19,  5, 16, 52, 16, 56, 79,  5, 51],\n",
       "       [ 5, 38, 68, 53, 39, 16,  5, 19, 38, 50, 16, 29, 38, 56, 26,  5, 81,\n",
       "        79, 39, 29, 25, 51, 16, 26,  5, 38, 66, 16, 51, 50, 38, 50, 16, 29,\n",
       "        77, 21, 39, 26, 51, 50, 79, 39, 38, 66, 25, 29, 38, 51, 50, 16],\n",
       "       [38, 16, 51, 38, 16, 29, 73, 38, 29, 16, 39, 57, 15, 38, 29, 25, 16,\n",
       "        68, 38, 51, 50, 79, 38, 26, 82, 68, 38, 35, 25,  5, 73, 38, 19, 79,\n",
       "        51, 51, 16,  5, 19, 38, 53, 18, 73, 38, 25,  5, 68, 77, 56, 39],\n",
       "       [38, 80, 51, 38, 66, 25, 29, 77, 26,  5, 82, 59, 38, 66, 50, 79,  5,\n",
       "        38, 51, 50, 79, 38, 29, 25, 35, 79, 38, 79, 81, 79,  5, 16,  5, 19,\n",
       "        38, 50, 79, 38, 56, 25, 35, 79, 38, 51, 26, 38, 51, 50, 79, 16],\n",
       "       [50, 79,  5, 38, 56, 26, 35, 79, 38, 52, 26, 39, 38, 35, 79, 73, 15,\n",
       "        38, 29, 50, 79, 38, 29, 25, 16, 68, 73, 38, 25,  5, 68, 38, 66, 79,\n",
       "         5, 51, 38, 21, 25, 56,  9, 38, 16,  5, 51, 26, 38, 51, 50, 79],\n",
       "       [ 3, 38, 21, 53, 51, 38,  5, 26, 66, 38, 29, 50, 79, 38, 66, 26, 53,\n",
       "        82, 68, 38, 39, 79, 25, 68, 16, 82, 59, 38, 50, 25, 81, 79, 38, 29,\n",
       "        25, 56, 39, 16, 52, 16, 56, 79, 68, 73, 38,  5, 26, 51, 38, 35],\n",
       "       [51, 38, 16, 29,  5, 27, 51, 17, 38, 14, 50, 79, 59, 27, 39, 79, 38,\n",
       "        18, 39, 26, 18, 39, 16, 79, 51, 26, 39, 29, 38, 26, 52, 38, 25, 38,\n",
       "        29, 26, 39, 51, 73, 77, 21, 53, 51, 38, 66, 79, 27, 39, 79, 38],\n",
       "       [38, 29, 25, 16, 68, 38, 51, 26, 38, 50, 79, 39, 29, 79, 82, 52, 73,\n",
       "        38, 25,  5, 68, 38, 21, 79, 19, 25,  5, 38, 25, 19, 25, 16,  5, 38,\n",
       "        52, 39, 26, 35, 38, 51, 50, 79, 38, 21, 79, 19, 16,  5,  5, 16]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(lstm_size) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/35720 Training loss: 4.4208 1.3842 sec/batch\n",
      "Epoch 1/20  Iteration 2/35720 Training loss: 4.3823 0.2067 sec/batch\n",
      "Epoch 1/20  Iteration 3/35720 Training loss: 4.2444 0.2188 sec/batch\n",
      "Epoch 1/20  Iteration 4/35720 Training loss: 4.5646 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 5/35720 Training loss: 4.4444 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 6/35720 Training loss: 4.2743 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 7/35720 Training loss: 4.1526 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 8/35720 Training loss: 4.0538 0.2203 sec/batch\n",
      "Epoch 1/20  Iteration 9/35720 Training loss: 3.9771 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 10/35720 Training loss: 3.9067 0.2095 sec/batch\n",
      "Epoch 1/20  Iteration 11/35720 Training loss: 3.8500 0.2124 sec/batch\n",
      "Epoch 1/20  Iteration 12/35720 Training loss: 3.8004 0.2186 sec/batch\n",
      "Epoch 1/20  Iteration 13/35720 Training loss: 3.7504 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 14/35720 Training loss: 3.7061 0.2066 sec/batch\n",
      "Epoch 1/20  Iteration 15/35720 Training loss: 3.6703 0.2197 sec/batch\n",
      "Epoch 1/20  Iteration 16/35720 Training loss: 3.6395 0.2175 sec/batch\n",
      "Epoch 1/20  Iteration 17/35720 Training loss: 3.6130 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 18/35720 Training loss: 3.5884 0.2282 sec/batch\n",
      "Epoch 1/20  Iteration 19/35720 Training loss: 3.5686 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 20/35720 Training loss: 3.5448 0.2072 sec/batch\n",
      "Epoch 1/20  Iteration 21/35720 Training loss: 3.5267 0.2341 sec/batch\n",
      "Epoch 1/20  Iteration 22/35720 Training loss: 3.5097 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 23/35720 Training loss: 3.4932 0.2425 sec/batch\n",
      "Epoch 1/20  Iteration 24/35720 Training loss: 3.4771 0.2226 sec/batch\n",
      "Epoch 1/20  Iteration 25/35720 Training loss: 3.4639 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 26/35720 Training loss: 3.4505 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 27/35720 Training loss: 3.4384 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 28/35720 Training loss: 3.4264 0.2134 sec/batch\n",
      "Epoch 1/20  Iteration 29/35720 Training loss: 3.4151 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 30/35720 Training loss: 3.4042 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 31/35720 Training loss: 3.3950 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 32/35720 Training loss: 3.3851 0.2148 sec/batch\n",
      "Epoch 1/20  Iteration 33/35720 Training loss: 3.3756 0.2124 sec/batch\n",
      "Epoch 1/20  Iteration 34/35720 Training loss: 3.3673 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 35/35720 Training loss: 3.3591 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 36/35720 Training loss: 3.3523 0.2278 sec/batch\n",
      "Epoch 1/20  Iteration 37/35720 Training loss: 3.3450 0.2378 sec/batch\n",
      "Epoch 1/20  Iteration 38/35720 Training loss: 3.3392 0.2060 sec/batch\n",
      "Epoch 1/20  Iteration 39/35720 Training loss: 3.3336 0.2114 sec/batch\n",
      "Epoch 1/20  Iteration 40/35720 Training loss: 3.3279 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 41/35720 Training loss: 3.3238 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 42/35720 Training loss: 3.3185 0.2189 sec/batch\n",
      "Epoch 1/20  Iteration 43/35720 Training loss: 3.3135 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 44/35720 Training loss: 3.3071 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 45/35720 Training loss: 3.3015 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 46/35720 Training loss: 3.2976 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 47/35720 Training loss: 3.2941 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 48/35720 Training loss: 3.2903 0.2170 sec/batch\n",
      "Epoch 1/20  Iteration 49/35720 Training loss: 3.2867 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 50/35720 Training loss: 3.2830 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 51/35720 Training loss: 3.2796 0.2089 sec/batch\n",
      "Epoch 1/20  Iteration 52/35720 Training loss: 3.2752 0.2100 sec/batch\n",
      "Epoch 1/20  Iteration 53/35720 Training loss: 3.2734 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 54/35720 Training loss: 3.2707 0.2159 sec/batch\n",
      "Epoch 1/20  Iteration 55/35720 Training loss: 3.2683 0.2199 sec/batch\n",
      "Epoch 1/20  Iteration 56/35720 Training loss: 3.2651 0.2221 sec/batch\n",
      "Epoch 1/20  Iteration 57/35720 Training loss: 3.2626 0.2148 sec/batch\n",
      "Epoch 1/20  Iteration 58/35720 Training loss: 3.2610 0.2202 sec/batch\n",
      "Epoch 1/20  Iteration 59/35720 Training loss: 3.2577 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 60/35720 Training loss: 3.2549 0.2240 sec/batch\n",
      "Epoch 1/20  Iteration 61/35720 Training loss: 3.2527 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 62/35720 Training loss: 3.2501 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 63/35720 Training loss: 3.2475 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 64/35720 Training loss: 3.2448 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 65/35720 Training loss: 3.2417 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 66/35720 Training loss: 3.2385 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 67/35720 Training loss: 3.2359 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 68/35720 Training loss: 3.2342 0.2107 sec/batch\n",
      "Epoch 1/20  Iteration 69/35720 Training loss: 3.2326 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 70/35720 Training loss: 3.2305 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 71/35720 Training loss: 3.2286 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 72/35720 Training loss: 3.2269 0.2117 sec/batch\n",
      "Epoch 1/20  Iteration 73/35720 Training loss: 3.2242 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 74/35720 Training loss: 3.2220 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 75/35720 Training loss: 3.2198 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 76/35720 Training loss: 3.2182 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 77/35720 Training loss: 3.2160 0.2240 sec/batch\n",
      "Epoch 1/20  Iteration 78/35720 Training loss: 3.2151 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 79/35720 Training loss: 3.2131 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 80/35720 Training loss: 3.2121 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 81/35720 Training loss: 3.2097 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 82/35720 Training loss: 3.2075 0.2128 sec/batch\n",
      "Epoch 1/20  Iteration 83/35720 Training loss: 3.2054 0.2082 sec/batch\n",
      "Epoch 1/20  Iteration 84/35720 Training loss: 3.2040 0.2136 sec/batch\n",
      "Epoch 1/20  Iteration 85/35720 Training loss: 3.2034 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 86/35720 Training loss: 3.2017 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 87/35720 Training loss: 3.2008 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 88/35720 Training loss: 3.1992 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 89/35720 Training loss: 3.1975 0.2094 sec/batch\n",
      "Epoch 1/20  Iteration 90/35720 Training loss: 3.1960 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 91/35720 Training loss: 3.1939 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 92/35720 Training loss: 3.1920 0.2136 sec/batch\n",
      "Epoch 1/20  Iteration 93/35720 Training loss: 3.1906 0.2310 sec/batch\n",
      "Epoch 1/20  Iteration 94/35720 Training loss: 3.1889 0.2265 sec/batch\n",
      "Epoch 1/20  Iteration 95/35720 Training loss: 3.1878 0.2155 sec/batch\n",
      "Epoch 1/20  Iteration 96/35720 Training loss: 3.1863 0.2188 sec/batch\n",
      "Epoch 1/20  Iteration 97/35720 Training loss: 3.1861 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 98/35720 Training loss: 3.1855 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 99/35720 Training loss: 3.1840 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 100/35720 Training loss: 3.1835 0.2114 sec/batch\n",
      "Epoch 1/20  Iteration 101/35720 Training loss: 3.1830 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 102/35720 Training loss: 3.1822 0.2291 sec/batch\n",
      "Epoch 1/20  Iteration 103/35720 Training loss: 3.1805 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 104/35720 Training loss: 3.1795 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 105/35720 Training loss: 3.1780 0.2219 sec/batch\n",
      "Epoch 1/20  Iteration 106/35720 Training loss: 3.1766 0.2183 sec/batch\n",
      "Epoch 1/20  Iteration 107/35720 Training loss: 3.1753 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 108/35720 Training loss: 3.1738 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 109/35720 Training loss: 3.1723 0.2120 sec/batch\n",
      "Epoch 1/20  Iteration 110/35720 Training loss: 3.1707 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 111/35720 Training loss: 3.1697 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 112/35720 Training loss: 3.1681 0.2196 sec/batch\n",
      "Epoch 1/20  Iteration 113/35720 Training loss: 3.1675 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 114/35720 Training loss: 3.1667 0.2405 sec/batch\n",
      "Epoch 1/20  Iteration 115/35720 Training loss: 3.1652 0.2321 sec/batch\n",
      "Epoch 1/20  Iteration 116/35720 Training loss: 3.1636 0.2264 sec/batch\n",
      "Epoch 1/20  Iteration 117/35720 Training loss: 3.1623 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 118/35720 Training loss: 3.1621 0.2057 sec/batch\n",
      "Epoch 1/20  Iteration 119/35720 Training loss: 3.1617 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 120/35720 Training loss: 3.1611 0.2264 sec/batch\n",
      "Epoch 1/20  Iteration 121/35720 Training loss: 3.1607 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 122/35720 Training loss: 3.1607 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 123/35720 Training loss: 3.1597 0.2238 sec/batch\n",
      "Epoch 1/20  Iteration 124/35720 Training loss: 3.1581 0.2134 sec/batch\n",
      "Epoch 1/20  Iteration 125/35720 Training loss: 3.1570 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 126/35720 Training loss: 3.1561 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 127/35720 Training loss: 3.1550 0.2286 sec/batch\n",
      "Epoch 1/20  Iteration 128/35720 Training loss: 3.1537 0.2339 sec/batch\n",
      "Epoch 1/20  Iteration 129/35720 Training loss: 3.1526 0.2419 sec/batch\n",
      "Epoch 1/20  Iteration 130/35720 Training loss: 3.1517 0.2266 sec/batch\n",
      "Epoch 1/20  Iteration 131/35720 Training loss: 3.1503 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 132/35720 Training loss: 3.1492 0.2261 sec/batch\n",
      "Epoch 1/20  Iteration 133/35720 Training loss: 3.1476 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 134/35720 Training loss: 3.1463 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 135/35720 Training loss: 3.1449 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 136/35720 Training loss: 3.1434 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 137/35720 Training loss: 3.1422 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 138/35720 Training loss: 3.1404 0.2437 sec/batch\n",
      "Epoch 1/20  Iteration 139/35720 Training loss: 3.1391 0.2068 sec/batch\n",
      "Epoch 1/20  Iteration 140/35720 Training loss: 3.1371 0.2069 sec/batch\n",
      "Epoch 1/20  Iteration 141/35720 Training loss: 3.1355 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 142/35720 Training loss: 3.1337 0.2107 sec/batch\n",
      "Epoch 1/20  Iteration 143/35720 Training loss: 3.1315 0.2491 sec/batch\n",
      "Epoch 1/20  Iteration 144/35720 Training loss: 3.1300 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 145/35720 Training loss: 3.1279 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 146/35720 Training loss: 3.1258 0.2293 sec/batch\n",
      "Epoch 1/20  Iteration 147/35720 Training loss: 3.1240 0.2095 sec/batch\n",
      "Epoch 1/20  Iteration 148/35720 Training loss: 3.1221 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 149/35720 Training loss: 3.1197 0.2123 sec/batch\n",
      "Epoch 1/20  Iteration 150/35720 Training loss: 3.1176 0.2201 sec/batch\n",
      "Epoch 1/20  Iteration 151/35720 Training loss: 3.1157 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 152/35720 Training loss: 3.1133 0.2312 sec/batch\n",
      "Epoch 1/20  Iteration 153/35720 Training loss: 3.1112 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 154/35720 Training loss: 3.1094 0.2196 sec/batch\n",
      "Epoch 1/20  Iteration 155/35720 Training loss: 3.1071 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 156/35720 Training loss: 3.1050 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 157/35720 Training loss: 3.1028 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 158/35720 Training loss: 3.1016 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 159/35720 Training loss: 3.1002 0.2246 sec/batch\n",
      "Epoch 1/20  Iteration 160/35720 Training loss: 3.0988 0.2364 sec/batch\n",
      "Epoch 1/20  Iteration 161/35720 Training loss: 3.0968 0.2501 sec/batch\n",
      "Epoch 1/20  Iteration 162/35720 Training loss: 3.0948 0.2339 sec/batch\n",
      "Epoch 1/20  Iteration 163/35720 Training loss: 3.0927 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 164/35720 Training loss: 3.0909 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 165/35720 Training loss: 3.0890 0.2290 sec/batch\n",
      "Epoch 1/20  Iteration 166/35720 Training loss: 3.0873 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 167/35720 Training loss: 3.0853 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 168/35720 Training loss: 3.0837 0.2275 sec/batch\n",
      "Epoch 1/20  Iteration 169/35720 Training loss: 3.0815 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 170/35720 Training loss: 3.0796 0.2095 sec/batch\n",
      "Epoch 1/20  Iteration 171/35720 Training loss: 3.0779 0.2077 sec/batch\n",
      "Epoch 1/20  Iteration 172/35720 Training loss: 3.0762 0.2183 sec/batch\n",
      "Epoch 1/20  Iteration 173/35720 Training loss: 3.0743 0.2066 sec/batch\n",
      "Epoch 1/20  Iteration 174/35720 Training loss: 3.0722 0.2251 sec/batch\n",
      "Epoch 1/20  Iteration 175/35720 Training loss: 3.0697 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 176/35720 Training loss: 3.0676 0.2146 sec/batch\n",
      "Epoch 1/20  Iteration 177/35720 Training loss: 3.0658 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 178/35720 Training loss: 3.0639 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 179/35720 Training loss: 3.0618 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 180/35720 Training loss: 3.0594 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 181/35720 Training loss: 3.0571 0.2338 sec/batch\n",
      "Epoch 1/20  Iteration 182/35720 Training loss: 3.0549 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 183/35720 Training loss: 3.0525 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 184/35720 Training loss: 3.0502 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 185/35720 Training loss: 3.0476 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 186/35720 Training loss: 3.0451 0.2237 sec/batch\n",
      "Epoch 1/20  Iteration 187/35720 Training loss: 3.0423 0.2134 sec/batch\n",
      "Epoch 1/20  Iteration 188/35720 Training loss: 3.0399 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 189/35720 Training loss: 3.0375 0.2114 sec/batch\n",
      "Epoch 1/20  Iteration 190/35720 Training loss: 3.0347 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 191/35720 Training loss: 3.0320 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 192/35720 Training loss: 3.0299 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 193/35720 Training loss: 3.0275 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 194/35720 Training loss: 3.0252 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 195/35720 Training loss: 3.0223 0.2083 sec/batch\n",
      "Epoch 1/20  Iteration 196/35720 Training loss: 3.0195 0.2393 sec/batch\n",
      "Epoch 1/20  Iteration 197/35720 Training loss: 3.0174 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 198/35720 Training loss: 3.0148 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 199/35720 Training loss: 3.0125 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 200/35720 Training loss: 3.0099 0.2248 sec/batch\n",
      "Validation loss: 2.57946 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 201/35720 Training loss: 3.0072 0.2308 sec/batch\n",
      "Epoch 1/20  Iteration 202/35720 Training loss: 3.0048 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 203/35720 Training loss: 3.0024 0.2342 sec/batch\n",
      "Epoch 1/20  Iteration 204/35720 Training loss: 3.0000 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 205/35720 Training loss: 2.9975 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 206/35720 Training loss: 2.9953 0.2124 sec/batch\n",
      "Epoch 1/20  Iteration 207/35720 Training loss: 2.9929 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 208/35720 Training loss: 2.9902 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 209/35720 Training loss: 2.9875 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 210/35720 Training loss: 2.9848 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 211/35720 Training loss: 2.9823 0.2162 sec/batch\n",
      "Epoch 1/20  Iteration 212/35720 Training loss: 2.9801 0.2196 sec/batch\n",
      "Epoch 1/20  Iteration 213/35720 Training loss: 2.9777 0.2199 sec/batch\n",
      "Epoch 1/20  Iteration 214/35720 Training loss: 2.9749 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 215/35720 Training loss: 2.9722 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 216/35720 Training loss: 2.9695 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 217/35720 Training loss: 2.9669 0.2202 sec/batch\n",
      "Epoch 1/20  Iteration 218/35720 Training loss: 2.9642 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 219/35720 Training loss: 2.9616 0.2283 sec/batch\n",
      "Epoch 1/20  Iteration 220/35720 Training loss: 2.9593 0.2198 sec/batch\n",
      "Epoch 1/20  Iteration 221/35720 Training loss: 2.9570 0.2248 sec/batch\n",
      "Epoch 1/20  Iteration 222/35720 Training loss: 2.9544 0.2299 sec/batch\n",
      "Epoch 1/20  Iteration 223/35720 Training loss: 2.9519 0.2226 sec/batch\n",
      "Epoch 1/20  Iteration 224/35720 Training loss: 2.9494 0.2361 sec/batch\n",
      "Epoch 1/20  Iteration 225/35720 Training loss: 2.9471 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 226/35720 Training loss: 2.9446 0.2236 sec/batch\n",
      "Epoch 1/20  Iteration 227/35720 Training loss: 2.9423 0.2305 sec/batch\n",
      "Epoch 1/20  Iteration 228/35720 Training loss: 2.9400 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 229/35720 Training loss: 2.9380 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 230/35720 Training loss: 2.9358 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 231/35720 Training loss: 2.9332 0.2335 sec/batch\n",
      "Epoch 1/20  Iteration 232/35720 Training loss: 2.9309 0.2067 sec/batch\n",
      "Epoch 1/20  Iteration 233/35720 Training loss: 2.9287 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 234/35720 Training loss: 2.9261 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 235/35720 Training loss: 2.9234 0.2233 sec/batch\n",
      "Epoch 1/20  Iteration 236/35720 Training loss: 2.9212 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 237/35720 Training loss: 2.9190 0.2415 sec/batch\n",
      "Epoch 1/20  Iteration 238/35720 Training loss: 2.9168 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 239/35720 Training loss: 2.9145 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 240/35720 Training loss: 2.9122 0.2129 sec/batch\n",
      "Epoch 1/20  Iteration 241/35720 Training loss: 2.9099 0.2255 sec/batch\n",
      "Epoch 1/20  Iteration 242/35720 Training loss: 2.9078 0.2235 sec/batch\n",
      "Epoch 1/20  Iteration 243/35720 Training loss: 2.9059 0.2228 sec/batch\n",
      "Epoch 1/20  Iteration 244/35720 Training loss: 2.9040 0.2111 sec/batch\n",
      "Epoch 1/20  Iteration 245/35720 Training loss: 2.9022 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 246/35720 Training loss: 2.9001 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 247/35720 Training loss: 2.8983 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 248/35720 Training loss: 2.8961 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 249/35720 Training loss: 2.8938 0.2124 sec/batch\n",
      "Epoch 1/20  Iteration 250/35720 Training loss: 2.8920 0.2130 sec/batch\n",
      "Epoch 1/20  Iteration 251/35720 Training loss: 2.8898 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 252/35720 Training loss: 2.8877 0.2132 sec/batch\n",
      "Epoch 1/20  Iteration 253/35720 Training loss: 2.8854 0.2208 sec/batch\n",
      "Epoch 1/20  Iteration 254/35720 Training loss: 2.8835 0.2218 sec/batch\n",
      "Epoch 1/20  Iteration 255/35720 Training loss: 2.8816 0.2240 sec/batch\n",
      "Epoch 1/20  Iteration 256/35720 Training loss: 2.8799 0.2242 sec/batch\n",
      "Epoch 1/20  Iteration 257/35720 Training loss: 2.8778 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 258/35720 Training loss: 2.8761 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 259/35720 Training loss: 2.8740 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 260/35720 Training loss: 2.8721 0.2266 sec/batch\n",
      "Epoch 1/20  Iteration 261/35720 Training loss: 2.8701 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 262/35720 Training loss: 2.8681 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 263/35720 Training loss: 2.8660 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 264/35720 Training loss: 2.8639 0.2142 sec/batch\n",
      "Epoch 1/20  Iteration 265/35720 Training loss: 2.8618 0.2229 sec/batch\n",
      "Epoch 1/20  Iteration 266/35720 Training loss: 2.8598 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 267/35720 Training loss: 2.8576 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 268/35720 Training loss: 2.8557 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 269/35720 Training loss: 2.8532 0.2084 sec/batch\n",
      "Epoch 1/20  Iteration 270/35720 Training loss: 2.8508 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 271/35720 Training loss: 2.8489 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 272/35720 Training loss: 2.8469 0.2139 sec/batch\n",
      "Epoch 1/20  Iteration 273/35720 Training loss: 2.8448 0.2352 sec/batch\n",
      "Epoch 1/20  Iteration 274/35720 Training loss: 2.8431 0.2248 sec/batch\n",
      "Epoch 1/20  Iteration 275/35720 Training loss: 2.8410 0.2258 sec/batch\n",
      "Epoch 1/20  Iteration 276/35720 Training loss: 2.8399 0.2337 sec/batch\n",
      "Epoch 1/20  Iteration 277/35720 Training loss: 2.8379 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 278/35720 Training loss: 2.8361 0.2247 sec/batch\n",
      "Epoch 1/20  Iteration 279/35720 Training loss: 2.8341 0.2246 sec/batch\n",
      "Epoch 1/20  Iteration 280/35720 Training loss: 2.8325 0.2072 sec/batch\n",
      "Epoch 1/20  Iteration 281/35720 Training loss: 2.8310 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 282/35720 Training loss: 2.8291 0.2191 sec/batch\n",
      "Epoch 1/20  Iteration 283/35720 Training loss: 2.8273 0.2363 sec/batch\n",
      "Epoch 1/20  Iteration 284/35720 Training loss: 2.8256 0.2126 sec/batch\n",
      "Epoch 1/20  Iteration 285/35720 Training loss: 2.8244 0.2255 sec/batch\n",
      "Epoch 1/20  Iteration 286/35720 Training loss: 2.8228 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 287/35720 Training loss: 2.8209 0.2243 sec/batch\n",
      "Epoch 1/20  Iteration 288/35720 Training loss: 2.8189 0.2063 sec/batch\n",
      "Epoch 1/20  Iteration 289/35720 Training loss: 2.8173 0.2100 sec/batch\n",
      "Epoch 1/20  Iteration 290/35720 Training loss: 2.8160 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 291/35720 Training loss: 2.8143 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 292/35720 Training loss: 2.8128 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 293/35720 Training loss: 2.8111 0.2134 sec/batch\n",
      "Epoch 1/20  Iteration 294/35720 Training loss: 2.8092 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 295/35720 Training loss: 2.8081 0.2129 sec/batch\n",
      "Epoch 1/20  Iteration 296/35720 Training loss: 2.8064 0.2130 sec/batch\n",
      "Epoch 1/20  Iteration 297/35720 Training loss: 2.8051 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 298/35720 Training loss: 2.8039 0.2193 sec/batch\n",
      "Epoch 1/20  Iteration 299/35720 Training loss: 2.8022 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 300/35720 Training loss: 2.8003 0.2186 sec/batch\n",
      "Epoch 1/20  Iteration 301/35720 Training loss: 2.7988 0.2117 sec/batch\n",
      "Epoch 1/20  Iteration 302/35720 Training loss: 2.7971 0.2126 sec/batch\n",
      "Epoch 1/20  Iteration 303/35720 Training loss: 2.7953 0.2139 sec/batch\n",
      "Epoch 1/20  Iteration 304/35720 Training loss: 2.7938 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 305/35720 Training loss: 2.7923 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 306/35720 Training loss: 2.7907 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 307/35720 Training loss: 2.7892 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 308/35720 Training loss: 2.7873 0.2069 sec/batch\n",
      "Epoch 1/20  Iteration 309/35720 Training loss: 2.7857 0.2329 sec/batch\n",
      "Epoch 1/20  Iteration 310/35720 Training loss: 2.7838 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 311/35720 Training loss: 2.7819 0.2219 sec/batch\n",
      "Epoch 1/20  Iteration 312/35720 Training loss: 2.7802 0.2270 sec/batch\n",
      "Epoch 1/20  Iteration 313/35720 Training loss: 2.7784 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 314/35720 Training loss: 2.7766 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 315/35720 Training loss: 2.7753 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 316/35720 Training loss: 2.7736 0.2169 sec/batch\n",
      "Epoch 1/20  Iteration 317/35720 Training loss: 2.7721 0.2236 sec/batch\n",
      "Epoch 1/20  Iteration 318/35720 Training loss: 2.7709 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 319/35720 Training loss: 2.7692 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 320/35720 Training loss: 2.7673 0.2146 sec/batch\n",
      "Epoch 1/20  Iteration 321/35720 Training loss: 2.7658 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 322/35720 Training loss: 2.7640 0.2326 sec/batch\n",
      "Epoch 1/20  Iteration 323/35720 Training loss: 2.7624 0.2319 sec/batch\n",
      "Epoch 1/20  Iteration 324/35720 Training loss: 2.7608 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 325/35720 Training loss: 2.7592 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 326/35720 Training loss: 2.7575 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 327/35720 Training loss: 2.7559 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 328/35720 Training loss: 2.7543 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 329/35720 Training loss: 2.7529 0.2150 sec/batch\n",
      "Epoch 1/20  Iteration 330/35720 Training loss: 2.7513 0.2245 sec/batch\n",
      "Epoch 1/20  Iteration 331/35720 Training loss: 2.7498 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 332/35720 Training loss: 2.7485 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 333/35720 Training loss: 2.7474 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 334/35720 Training loss: 2.7459 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 335/35720 Training loss: 2.7446 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 336/35720 Training loss: 2.7432 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 337/35720 Training loss: 2.7420 0.2228 sec/batch\n",
      "Epoch 1/20  Iteration 338/35720 Training loss: 2.7403 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 339/35720 Training loss: 2.7390 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 340/35720 Training loss: 2.7376 0.2246 sec/batch\n",
      "Epoch 1/20  Iteration 341/35720 Training loss: 2.7361 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 342/35720 Training loss: 2.7344 0.2218 sec/batch\n",
      "Epoch 1/20  Iteration 343/35720 Training loss: 2.7329 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 344/35720 Training loss: 2.7313 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 345/35720 Training loss: 2.7298 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 346/35720 Training loss: 2.7284 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 347/35720 Training loss: 2.7271 0.2253 sec/batch\n",
      "Epoch 1/20  Iteration 348/35720 Training loss: 2.7256 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 349/35720 Training loss: 2.7244 0.2064 sec/batch\n",
      "Epoch 1/20  Iteration 350/35720 Training loss: 2.7229 0.2191 sec/batch\n",
      "Epoch 1/20  Iteration 351/35720 Training loss: 2.7216 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 352/35720 Training loss: 2.7203 0.2268 sec/batch\n",
      "Epoch 1/20  Iteration 353/35720 Training loss: 2.7190 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 354/35720 Training loss: 2.7175 0.2256 sec/batch\n",
      "Epoch 1/20  Iteration 355/35720 Training loss: 2.7162 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 356/35720 Training loss: 2.7148 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 357/35720 Training loss: 2.7134 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 358/35720 Training loss: 2.7117 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 359/35720 Training loss: 2.7102 0.2267 sec/batch\n",
      "Epoch 1/20  Iteration 360/35720 Training loss: 2.7088 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 361/35720 Training loss: 2.7074 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 362/35720 Training loss: 2.7060 0.2282 sec/batch\n",
      "Epoch 1/20  Iteration 363/35720 Training loss: 2.7046 0.2282 sec/batch\n",
      "Epoch 1/20  Iteration 364/35720 Training loss: 2.7031 0.2241 sec/batch\n",
      "Epoch 1/20  Iteration 365/35720 Training loss: 2.7016 0.2067 sec/batch\n",
      "Epoch 1/20  Iteration 366/35720 Training loss: 2.7002 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 367/35720 Training loss: 2.6988 0.2095 sec/batch\n",
      "Epoch 1/20  Iteration 368/35720 Training loss: 2.6974 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 369/35720 Training loss: 2.6962 0.2313 sec/batch\n",
      "Epoch 1/20  Iteration 370/35720 Training loss: 2.6950 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 371/35720 Training loss: 2.6935 0.2362 sec/batch\n",
      "Epoch 1/20  Iteration 372/35720 Training loss: 2.6920 0.2230 sec/batch\n",
      "Epoch 1/20  Iteration 373/35720 Training loss: 2.6910 0.2128 sec/batch\n",
      "Epoch 1/20  Iteration 374/35720 Training loss: 2.6897 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 375/35720 Training loss: 2.6883 0.2191 sec/batch\n",
      "Epoch 1/20  Iteration 376/35720 Training loss: 2.6873 0.2251 sec/batch\n",
      "Epoch 1/20  Iteration 377/35720 Training loss: 2.6859 0.2280 sec/batch\n",
      "Epoch 1/20  Iteration 378/35720 Training loss: 2.6842 0.2438 sec/batch\n",
      "Epoch 1/20  Iteration 379/35720 Training loss: 2.6829 0.2631 sec/batch\n",
      "Epoch 1/20  Iteration 380/35720 Training loss: 2.6817 0.2756 sec/batch\n",
      "Epoch 1/20  Iteration 381/35720 Training loss: 2.6804 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 382/35720 Training loss: 2.6791 0.2235 sec/batch\n",
      "Epoch 1/20  Iteration 383/35720 Training loss: 2.6781 0.2198 sec/batch\n",
      "Epoch 1/20  Iteration 384/35720 Training loss: 2.6768 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 385/35720 Training loss: 2.6756 0.2276 sec/batch\n",
      "Epoch 1/20  Iteration 386/35720 Training loss: 2.6742 0.2199 sec/batch\n",
      "Epoch 1/20  Iteration 387/35720 Training loss: 2.6732 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 388/35720 Training loss: 2.6719 0.2464 sec/batch\n",
      "Epoch 1/20  Iteration 389/35720 Training loss: 2.6708 0.2202 sec/batch\n",
      "Epoch 1/20  Iteration 390/35720 Training loss: 2.6697 0.2137 sec/batch\n",
      "Epoch 1/20  Iteration 391/35720 Training loss: 2.6683 0.2236 sec/batch\n",
      "Epoch 1/20  Iteration 392/35720 Training loss: 2.6670 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 393/35720 Training loss: 2.6659 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 394/35720 Training loss: 2.6645 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 395/35720 Training loss: 2.6631 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 396/35720 Training loss: 2.6616 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 397/35720 Training loss: 2.6602 0.2267 sec/batch\n",
      "Epoch 1/20  Iteration 398/35720 Training loss: 2.6588 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 399/35720 Training loss: 2.6574 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 400/35720 Training loss: 2.6561 0.2167 sec/batch\n",
      "Validation loss: 2.24435 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 401/35720 Training loss: 2.6546 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 402/35720 Training loss: 2.6534 0.2317 sec/batch\n",
      "Epoch 1/20  Iteration 403/35720 Training loss: 2.6523 0.2235 sec/batch\n",
      "Epoch 1/20  Iteration 404/35720 Training loss: 2.6508 0.2256 sec/batch\n",
      "Epoch 1/20  Iteration 405/35720 Training loss: 2.6496 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 406/35720 Training loss: 2.6484 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 407/35720 Training loss: 2.6470 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 408/35720 Training loss: 2.6455 0.2197 sec/batch\n",
      "Epoch 1/20  Iteration 409/35720 Training loss: 2.6441 0.2175 sec/batch\n",
      "Epoch 1/20  Iteration 410/35720 Training loss: 2.6429 0.2256 sec/batch\n",
      "Epoch 1/20  Iteration 411/35720 Training loss: 2.6416 0.2065 sec/batch\n",
      "Epoch 1/20  Iteration 412/35720 Training loss: 2.6402 0.2224 sec/batch\n",
      "Epoch 1/20  Iteration 413/35720 Training loss: 2.6389 0.2229 sec/batch\n",
      "Epoch 1/20  Iteration 414/35720 Training loss: 2.6377 0.2085 sec/batch\n",
      "Epoch 1/20  Iteration 415/35720 Training loss: 2.6363 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 416/35720 Training loss: 2.6350 0.2221 sec/batch\n",
      "Epoch 1/20  Iteration 417/35720 Training loss: 2.6338 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 418/35720 Training loss: 2.6326 0.2267 sec/batch\n",
      "Epoch 1/20  Iteration 419/35720 Training loss: 2.6311 0.2305 sec/batch\n",
      "Epoch 1/20  Iteration 420/35720 Training loss: 2.6300 0.2199 sec/batch\n",
      "Epoch 1/20  Iteration 421/35720 Training loss: 2.6289 0.2273 sec/batch\n",
      "Epoch 1/20  Iteration 422/35720 Training loss: 2.6277 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 423/35720 Training loss: 2.6263 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 424/35720 Training loss: 2.6251 0.2089 sec/batch\n",
      "Epoch 1/20  Iteration 425/35720 Training loss: 2.6238 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 426/35720 Training loss: 2.6227 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 427/35720 Training loss: 2.6214 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 428/35720 Training loss: 2.6203 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 429/35720 Training loss: 2.6193 0.2241 sec/batch\n",
      "Epoch 1/20  Iteration 430/35720 Training loss: 2.6183 0.2088 sec/batch\n",
      "Epoch 1/20  Iteration 431/35720 Training loss: 2.6172 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 432/35720 Training loss: 2.6162 0.2325 sec/batch\n",
      "Epoch 1/20  Iteration 433/35720 Training loss: 2.6153 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 434/35720 Training loss: 2.6143 0.2097 sec/batch\n",
      "Epoch 1/20  Iteration 435/35720 Training loss: 2.6133 0.2247 sec/batch\n",
      "Epoch 1/20  Iteration 436/35720 Training loss: 2.6121 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 437/35720 Training loss: 2.6109 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 438/35720 Training loss: 2.6098 0.2127 sec/batch\n",
      "Epoch 1/20  Iteration 439/35720 Training loss: 2.6087 0.2279 sec/batch\n",
      "Epoch 1/20  Iteration 440/35720 Training loss: 2.6074 0.2280 sec/batch\n",
      "Epoch 1/20  Iteration 441/35720 Training loss: 2.6064 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 442/35720 Training loss: 2.6053 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 443/35720 Training loss: 2.6040 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 444/35720 Training loss: 2.6028 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 445/35720 Training loss: 2.6016 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 446/35720 Training loss: 2.6004 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 447/35720 Training loss: 2.5994 0.2190 sec/batch\n",
      "Epoch 1/20  Iteration 448/35720 Training loss: 2.5981 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 449/35720 Training loss: 2.5970 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 450/35720 Training loss: 2.5959 0.2161 sec/batch\n",
      "Epoch 1/20  Iteration 451/35720 Training loss: 2.5946 0.2130 sec/batch\n",
      "Epoch 1/20  Iteration 452/35720 Training loss: 2.5934 0.2250 sec/batch\n",
      "Epoch 1/20  Iteration 453/35720 Training loss: 2.5923 0.2219 sec/batch\n",
      "Epoch 1/20  Iteration 454/35720 Training loss: 2.5915 0.2307 sec/batch\n",
      "Epoch 1/20  Iteration 455/35720 Training loss: 2.5904 0.2162 sec/batch\n",
      "Epoch 1/20  Iteration 456/35720 Training loss: 2.5892 0.2267 sec/batch\n",
      "Epoch 1/20  Iteration 457/35720 Training loss: 2.5881 0.2126 sec/batch\n",
      "Epoch 1/20  Iteration 458/35720 Training loss: 2.5870 0.2375 sec/batch\n",
      "Epoch 1/20  Iteration 459/35720 Training loss: 2.5858 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 460/35720 Training loss: 2.5846 0.2094 sec/batch\n",
      "Epoch 1/20  Iteration 461/35720 Training loss: 2.5834 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 462/35720 Training loss: 2.5823 0.2273 sec/batch\n",
      "Epoch 1/20  Iteration 463/35720 Training loss: 2.5810 0.2225 sec/batch\n",
      "Epoch 1/20  Iteration 464/35720 Training loss: 2.5799 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 465/35720 Training loss: 2.5787 0.2107 sec/batch\n",
      "Epoch 1/20  Iteration 466/35720 Training loss: 2.5777 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 467/35720 Training loss: 2.5764 0.2202 sec/batch\n",
      "Epoch 1/20  Iteration 468/35720 Training loss: 2.5753 0.2192 sec/batch\n",
      "Epoch 1/20  Iteration 469/35720 Training loss: 2.5741 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 470/35720 Training loss: 2.5728 0.2289 sec/batch\n",
      "Epoch 1/20  Iteration 471/35720 Training loss: 2.5716 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 472/35720 Training loss: 2.5705 0.2126 sec/batch\n",
      "Epoch 1/20  Iteration 473/35720 Training loss: 2.5694 0.2132 sec/batch\n",
      "Epoch 1/20  Iteration 474/35720 Training loss: 2.5681 0.2279 sec/batch\n",
      "Epoch 1/20  Iteration 475/35720 Training loss: 2.5671 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 476/35720 Training loss: 2.5660 0.2270 sec/batch\n",
      "Epoch 1/20  Iteration 477/35720 Training loss: 2.5649 0.2058 sec/batch\n",
      "Epoch 1/20  Iteration 478/35720 Training loss: 2.5638 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 479/35720 Training loss: 2.5628 0.2255 sec/batch\n",
      "Epoch 1/20  Iteration 480/35720 Training loss: 2.5617 0.2085 sec/batch\n",
      "Epoch 1/20  Iteration 481/35720 Training loss: 2.5604 0.2083 sec/batch\n",
      "Epoch 1/20  Iteration 482/35720 Training loss: 2.5592 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 483/35720 Training loss: 2.5583 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 484/35720 Training loss: 2.5572 0.2320 sec/batch\n",
      "Epoch 1/20  Iteration 485/35720 Training loss: 2.5560 0.2185 sec/batch\n",
      "Epoch 1/20  Iteration 486/35720 Training loss: 2.5550 0.2176 sec/batch\n",
      "Epoch 1/20  Iteration 487/35720 Training loss: 2.5538 0.2189 sec/batch\n",
      "Epoch 1/20  Iteration 488/35720 Training loss: 2.5528 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 489/35720 Training loss: 2.5520 0.2199 sec/batch\n",
      "Epoch 1/20  Iteration 490/35720 Training loss: 2.5509 0.2142 sec/batch\n",
      "Epoch 1/20  Iteration 491/35720 Training loss: 2.5497 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 492/35720 Training loss: 2.5486 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 493/35720 Training loss: 2.5474 0.2159 sec/batch\n",
      "Epoch 1/20  Iteration 494/35720 Training loss: 2.5463 0.2261 sec/batch\n",
      "Epoch 1/20  Iteration 495/35720 Training loss: 2.5451 0.2251 sec/batch\n",
      "Epoch 1/20  Iteration 496/35720 Training loss: 2.5440 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 497/35720 Training loss: 2.5430 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 498/35720 Training loss: 2.5421 0.2247 sec/batch\n",
      "Epoch 1/20  Iteration 499/35720 Training loss: 2.5410 0.2291 sec/batch\n",
      "Epoch 1/20  Iteration 500/35720 Training loss: 2.5399 0.2085 sec/batch\n",
      "Epoch 1/20  Iteration 501/35720 Training loss: 2.5387 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 502/35720 Training loss: 2.5376 0.2114 sec/batch\n",
      "Epoch 1/20  Iteration 503/35720 Training loss: 2.5365 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 504/35720 Training loss: 2.5355 0.2237 sec/batch\n",
      "Epoch 1/20  Iteration 505/35720 Training loss: 2.5346 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 506/35720 Training loss: 2.5337 0.2291 sec/batch\n",
      "Epoch 1/20  Iteration 507/35720 Training loss: 2.5327 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 508/35720 Training loss: 2.5316 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 509/35720 Training loss: 2.5308 0.2148 sec/batch\n",
      "Epoch 1/20  Iteration 510/35720 Training loss: 2.5296 0.2270 sec/batch\n",
      "Epoch 1/20  Iteration 511/35720 Training loss: 2.5287 0.2169 sec/batch\n",
      "Epoch 1/20  Iteration 512/35720 Training loss: 2.5276 0.2198 sec/batch\n",
      "Epoch 1/20  Iteration 513/35720 Training loss: 2.5267 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 514/35720 Training loss: 2.5255 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 515/35720 Training loss: 2.5245 0.2178 sec/batch\n",
      "Epoch 1/20  Iteration 516/35720 Training loss: 2.5233 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 517/35720 Training loss: 2.5221 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 518/35720 Training loss: 2.5213 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 519/35720 Training loss: 2.5201 0.2237 sec/batch\n",
      "Epoch 1/20  Iteration 520/35720 Training loss: 2.5190 0.2252 sec/batch\n",
      "Epoch 1/20  Iteration 521/35720 Training loss: 2.5180 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 522/35720 Training loss: 2.5171 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 523/35720 Training loss: 2.5162 0.2080 sec/batch\n",
      "Epoch 1/20  Iteration 524/35720 Training loss: 2.5151 0.2176 sec/batch\n",
      "Epoch 1/20  Iteration 525/35720 Training loss: 2.5139 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 526/35720 Training loss: 2.5130 0.2146 sec/batch\n",
      "Epoch 1/20  Iteration 527/35720 Training loss: 2.5119 0.2395 sec/batch\n",
      "Epoch 1/20  Iteration 528/35720 Training loss: 2.5109 0.2249 sec/batch\n",
      "Epoch 1/20  Iteration 529/35720 Training loss: 2.5097 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 530/35720 Training loss: 2.5089 0.2077 sec/batch\n",
      "Epoch 1/20  Iteration 531/35720 Training loss: 2.5078 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 532/35720 Training loss: 2.5068 0.2199 sec/batch\n",
      "Epoch 1/20  Iteration 533/35720 Training loss: 2.5059 0.2203 sec/batch\n",
      "Epoch 1/20  Iteration 534/35720 Training loss: 2.5049 0.2142 sec/batch\n",
      "Epoch 1/20  Iteration 535/35720 Training loss: 2.5040 0.2203 sec/batch\n",
      "Epoch 1/20  Iteration 536/35720 Training loss: 2.5028 0.2252 sec/batch\n",
      "Epoch 1/20  Iteration 537/35720 Training loss: 2.5017 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 538/35720 Training loss: 2.5006 0.2208 sec/batch\n",
      "Epoch 1/20  Iteration 539/35720 Training loss: 2.4996 0.2252 sec/batch\n",
      "Epoch 1/20  Iteration 540/35720 Training loss: 2.4985 0.2287 sec/batch\n",
      "Epoch 1/20  Iteration 541/35720 Training loss: 2.4974 0.2271 sec/batch\n",
      "Epoch 1/20  Iteration 542/35720 Training loss: 2.4963 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 543/35720 Training loss: 2.4951 0.2189 sec/batch\n",
      "Epoch 1/20  Iteration 544/35720 Training loss: 2.4939 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 545/35720 Training loss: 2.4930 0.2138 sec/batch\n",
      "Epoch 1/20  Iteration 546/35720 Training loss: 2.4921 0.2360 sec/batch\n",
      "Epoch 1/20  Iteration 547/35720 Training loss: 2.4912 0.2068 sec/batch\n",
      "Epoch 1/20  Iteration 548/35720 Training loss: 2.4902 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 549/35720 Training loss: 2.4891 0.2086 sec/batch\n",
      "Epoch 1/20  Iteration 550/35720 Training loss: 2.4882 0.2148 sec/batch\n",
      "Epoch 1/20  Iteration 551/35720 Training loss: 2.4872 0.2089 sec/batch\n",
      "Epoch 1/20  Iteration 552/35720 Training loss: 2.4861 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 553/35720 Training loss: 2.4851 0.2199 sec/batch\n",
      "Epoch 1/20  Iteration 554/35720 Training loss: 2.4842 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 555/35720 Training loss: 2.4832 0.2226 sec/batch\n",
      "Epoch 1/20  Iteration 556/35720 Training loss: 2.4824 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 557/35720 Training loss: 2.4814 0.2159 sec/batch\n",
      "Epoch 1/20  Iteration 558/35720 Training loss: 2.4806 0.2062 sec/batch\n",
      "Epoch 1/20  Iteration 559/35720 Training loss: 2.4797 0.2360 sec/batch\n",
      "Epoch 1/20  Iteration 560/35720 Training loss: 2.4786 0.2285 sec/batch\n",
      "Epoch 1/20  Iteration 561/35720 Training loss: 2.4776 0.2238 sec/batch\n",
      "Epoch 1/20  Iteration 562/35720 Training loss: 2.4767 0.2182 sec/batch\n",
      "Epoch 1/20  Iteration 563/35720 Training loss: 2.4757 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 564/35720 Training loss: 2.4748 0.2242 sec/batch\n",
      "Epoch 1/20  Iteration 565/35720 Training loss: 2.4738 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 566/35720 Training loss: 2.4727 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 567/35720 Training loss: 2.4718 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 568/35720 Training loss: 2.4709 0.2201 sec/batch\n",
      "Epoch 1/20  Iteration 569/35720 Training loss: 2.4701 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 570/35720 Training loss: 2.4694 0.2282 sec/batch\n",
      "Epoch 1/20  Iteration 571/35720 Training loss: 2.4683 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 572/35720 Training loss: 2.4675 0.2067 sec/batch\n",
      "Epoch 1/20  Iteration 573/35720 Training loss: 2.4667 0.2070 sec/batch\n",
      "Epoch 1/20  Iteration 574/35720 Training loss: 2.4660 0.2139 sec/batch\n",
      "Epoch 1/20  Iteration 575/35720 Training loss: 2.4653 0.2084 sec/batch\n",
      "Epoch 1/20  Iteration 576/35720 Training loss: 2.4645 0.2090 sec/batch\n",
      "Epoch 1/20  Iteration 577/35720 Training loss: 2.4639 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 578/35720 Training loss: 2.4630 0.2089 sec/batch\n",
      "Epoch 1/20  Iteration 579/35720 Training loss: 2.4620 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 580/35720 Training loss: 2.4611 0.2080 sec/batch\n",
      "Epoch 1/20  Iteration 581/35720 Training loss: 2.4602 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 582/35720 Training loss: 2.4594 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 583/35720 Training loss: 2.4587 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 584/35720 Training loss: 2.4579 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 585/35720 Training loss: 2.4572 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 586/35720 Training loss: 2.4567 0.2083 sec/batch\n",
      "Epoch 1/20  Iteration 587/35720 Training loss: 2.4557 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 588/35720 Training loss: 2.4549 0.2269 sec/batch\n",
      "Epoch 1/20  Iteration 589/35720 Training loss: 2.4538 0.2065 sec/batch\n",
      "Epoch 1/20  Iteration 590/35720 Training loss: 2.4528 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 591/35720 Training loss: 2.4520 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 592/35720 Training loss: 2.4512 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 593/35720 Training loss: 2.4504 0.2338 sec/batch\n",
      "Epoch 1/20  Iteration 594/35720 Training loss: 2.4497 0.2218 sec/batch\n",
      "Epoch 1/20  Iteration 595/35720 Training loss: 2.4488 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 596/35720 Training loss: 2.4478 0.2236 sec/batch\n",
      "Epoch 1/20  Iteration 597/35720 Training loss: 2.4470 0.2284 sec/batch\n",
      "Epoch 1/20  Iteration 598/35720 Training loss: 2.4463 0.2185 sec/batch\n",
      "Epoch 1/20  Iteration 599/35720 Training loss: 2.4454 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 600/35720 Training loss: 2.4445 0.2195 sec/batch\n",
      "Validation loss: 2.03929 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 601/35720 Training loss: 2.4437 0.2197 sec/batch\n",
      "Epoch 1/20  Iteration 602/35720 Training loss: 2.4428 0.2090 sec/batch\n",
      "Epoch 1/20  Iteration 603/35720 Training loss: 2.4419 0.2246 sec/batch\n",
      "Epoch 1/20  Iteration 604/35720 Training loss: 2.4410 0.2192 sec/batch\n",
      "Epoch 1/20  Iteration 605/35720 Training loss: 2.4403 0.2170 sec/batch\n",
      "Epoch 1/20  Iteration 606/35720 Training loss: 2.4395 0.2230 sec/batch\n",
      "Epoch 1/20  Iteration 607/35720 Training loss: 2.4388 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 608/35720 Training loss: 2.4379 0.2085 sec/batch\n",
      "Epoch 1/20  Iteration 609/35720 Training loss: 2.4372 0.2182 sec/batch\n",
      "Epoch 1/20  Iteration 610/35720 Training loss: 2.4363 0.2129 sec/batch\n",
      "Epoch 1/20  Iteration 611/35720 Training loss: 2.4355 0.2123 sec/batch\n",
      "Epoch 1/20  Iteration 612/35720 Training loss: 2.4345 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 613/35720 Training loss: 2.4337 0.2205 sec/batch\n",
      "Epoch 1/20  Iteration 614/35720 Training loss: 2.4329 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 615/35720 Training loss: 2.4321 0.2137 sec/batch\n",
      "Epoch 1/20  Iteration 616/35720 Training loss: 2.4312 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 617/35720 Training loss: 2.4304 0.2379 sec/batch\n",
      "Epoch 1/20  Iteration 618/35720 Training loss: 2.4294 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 619/35720 Training loss: 2.4284 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 620/35720 Training loss: 2.4274 0.2225 sec/batch\n",
      "Epoch 1/20  Iteration 621/35720 Training loss: 2.4264 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 622/35720 Training loss: 2.4255 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 623/35720 Training loss: 2.4246 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 624/35720 Training loss: 2.4238 0.2078 sec/batch\n",
      "Epoch 1/20  Iteration 625/35720 Training loss: 2.4228 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 626/35720 Training loss: 2.4219 0.2290 sec/batch\n",
      "Epoch 1/20  Iteration 627/35720 Training loss: 2.4210 0.2409 sec/batch\n",
      "Epoch 1/20  Iteration 628/35720 Training loss: 2.4201 0.2352 sec/batch\n",
      "Epoch 1/20  Iteration 629/35720 Training loss: 2.4192 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 630/35720 Training loss: 2.4183 0.2065 sec/batch\n",
      "Epoch 1/20  Iteration 631/35720 Training loss: 2.4175 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 632/35720 Training loss: 2.4165 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 633/35720 Training loss: 2.4155 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 634/35720 Training loss: 2.4148 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 635/35720 Training loss: 2.4139 0.2233 sec/batch\n",
      "Epoch 1/20  Iteration 636/35720 Training loss: 2.4130 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 637/35720 Training loss: 2.4121 0.2355 sec/batch\n",
      "Epoch 1/20  Iteration 638/35720 Training loss: 2.4113 0.2245 sec/batch\n",
      "Epoch 1/20  Iteration 639/35720 Training loss: 2.4106 0.2190 sec/batch\n",
      "Epoch 1/20  Iteration 640/35720 Training loss: 2.4097 0.2289 sec/batch\n",
      "Epoch 1/20  Iteration 641/35720 Training loss: 2.4089 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 642/35720 Training loss: 2.4081 0.2203 sec/batch\n",
      "Epoch 1/20  Iteration 643/35720 Training loss: 2.4074 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 644/35720 Training loss: 2.4067 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 645/35720 Training loss: 2.4060 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 646/35720 Training loss: 2.4054 0.2203 sec/batch\n",
      "Epoch 1/20  Iteration 647/35720 Training loss: 2.4046 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 648/35720 Training loss: 2.4039 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 649/35720 Training loss: 2.4030 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 650/35720 Training loss: 2.4022 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 651/35720 Training loss: 2.4014 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 652/35720 Training loss: 2.4007 0.2181 sec/batch\n",
      "Epoch 1/20  Iteration 653/35720 Training loss: 2.4001 0.2169 sec/batch\n",
      "Epoch 1/20  Iteration 654/35720 Training loss: 2.3995 0.2201 sec/batch\n",
      "Epoch 1/20  Iteration 655/35720 Training loss: 2.3988 0.2178 sec/batch\n",
      "Epoch 1/20  Iteration 656/35720 Training loss: 2.3980 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 657/35720 Training loss: 2.3972 0.2277 sec/batch\n",
      "Epoch 1/20  Iteration 658/35720 Training loss: 2.3965 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 659/35720 Training loss: 2.3957 0.2264 sec/batch\n",
      "Epoch 1/20  Iteration 660/35720 Training loss: 2.3949 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 661/35720 Training loss: 2.3942 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 662/35720 Training loss: 2.3935 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 663/35720 Training loss: 2.3927 0.2413 sec/batch\n",
      "Epoch 1/20  Iteration 664/35720 Training loss: 2.3920 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 665/35720 Training loss: 2.3913 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 666/35720 Training loss: 2.3906 0.2255 sec/batch\n",
      "Epoch 1/20  Iteration 667/35720 Training loss: 2.3898 0.2626 sec/batch\n",
      "Epoch 1/20  Iteration 668/35720 Training loss: 2.3889 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 669/35720 Training loss: 2.3881 0.2247 sec/batch\n",
      "Epoch 1/20  Iteration 670/35720 Training loss: 2.3872 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 671/35720 Training loss: 2.3863 0.2127 sec/batch\n",
      "Epoch 1/20  Iteration 672/35720 Training loss: 2.3856 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 673/35720 Training loss: 2.3849 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 674/35720 Training loss: 2.3840 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 675/35720 Training loss: 2.3831 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 676/35720 Training loss: 2.3821 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 677/35720 Training loss: 2.3813 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 678/35720 Training loss: 2.3804 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 679/35720 Training loss: 2.3795 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 680/35720 Training loss: 2.3788 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 681/35720 Training loss: 2.3779 0.2340 sec/batch\n",
      "Epoch 1/20  Iteration 682/35720 Training loss: 2.3770 0.2371 sec/batch\n",
      "Epoch 1/20  Iteration 683/35720 Training loss: 2.3761 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 684/35720 Training loss: 2.3753 0.2137 sec/batch\n",
      "Epoch 1/20  Iteration 685/35720 Training loss: 2.3747 0.2193 sec/batch\n",
      "Epoch 1/20  Iteration 686/35720 Training loss: 2.3739 0.2107 sec/batch\n",
      "Epoch 1/20  Iteration 687/35720 Training loss: 2.3731 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 688/35720 Training loss: 2.3722 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 689/35720 Training loss: 2.3714 0.2066 sec/batch\n",
      "Epoch 1/20  Iteration 690/35720 Training loss: 2.3705 0.2229 sec/batch\n",
      "Epoch 1/20  Iteration 691/35720 Training loss: 2.3699 0.2223 sec/batch\n",
      "Epoch 1/20  Iteration 692/35720 Training loss: 2.3693 0.2139 sec/batch\n",
      "Epoch 1/20  Iteration 693/35720 Training loss: 2.3686 0.2268 sec/batch\n",
      "Epoch 1/20  Iteration 694/35720 Training loss: 2.3679 0.2146 sec/batch\n",
      "Epoch 1/20  Iteration 695/35720 Training loss: 2.3672 0.2138 sec/batch\n",
      "Epoch 1/20  Iteration 696/35720 Training loss: 2.3665 0.2176 sec/batch\n",
      "Epoch 1/20  Iteration 697/35720 Training loss: 2.3657 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 698/35720 Training loss: 2.3649 0.2059 sec/batch\n",
      "Epoch 1/20  Iteration 699/35720 Training loss: 2.3641 0.2090 sec/batch\n",
      "Epoch 1/20  Iteration 700/35720 Training loss: 2.3635 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 701/35720 Training loss: 2.3628 0.2094 sec/batch\n",
      "Epoch 1/20  Iteration 702/35720 Training loss: 2.3621 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 703/35720 Training loss: 2.3614 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 704/35720 Training loss: 2.3607 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 705/35720 Training loss: 2.3600 0.2310 sec/batch\n",
      "Epoch 1/20  Iteration 706/35720 Training loss: 2.3592 0.2079 sec/batch\n",
      "Epoch 1/20  Iteration 707/35720 Training loss: 2.3586 0.2317 sec/batch\n",
      "Epoch 1/20  Iteration 708/35720 Training loss: 2.3579 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 709/35720 Training loss: 2.3573 0.2117 sec/batch\n",
      "Epoch 1/20  Iteration 710/35720 Training loss: 2.3566 0.2271 sec/batch\n",
      "Epoch 1/20  Iteration 711/35720 Training loss: 2.3559 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 712/35720 Training loss: 2.3552 0.2183 sec/batch\n",
      "Epoch 1/20  Iteration 713/35720 Training loss: 2.3543 0.2071 sec/batch\n",
      "Epoch 1/20  Iteration 714/35720 Training loss: 2.3536 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 715/35720 Training loss: 2.3529 0.2181 sec/batch\n",
      "Epoch 1/20  Iteration 716/35720 Training loss: 2.3521 0.2197 sec/batch\n",
      "Epoch 1/20  Iteration 717/35720 Training loss: 2.3516 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 718/35720 Training loss: 2.3509 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 719/35720 Training loss: 2.3501 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 720/35720 Training loss: 2.3495 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 721/35720 Training loss: 2.3488 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 722/35720 Training loss: 2.3482 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 723/35720 Training loss: 2.3477 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 724/35720 Training loss: 2.3472 0.2136 sec/batch\n",
      "Epoch 1/20  Iteration 725/35720 Training loss: 2.3465 0.2243 sec/batch\n",
      "Epoch 1/20  Iteration 726/35720 Training loss: 2.3457 0.2193 sec/batch\n",
      "Epoch 1/20  Iteration 727/35720 Training loss: 2.3451 0.2142 sec/batch\n",
      "Epoch 1/20  Iteration 728/35720 Training loss: 2.3444 0.2170 sec/batch\n",
      "Epoch 1/20  Iteration 729/35720 Training loss: 2.3436 0.2315 sec/batch\n",
      "Epoch 1/20  Iteration 730/35720 Training loss: 2.3429 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 731/35720 Training loss: 2.3421 0.2137 sec/batch\n",
      "Epoch 1/20  Iteration 732/35720 Training loss: 2.3414 0.2226 sec/batch\n",
      "Epoch 1/20  Iteration 733/35720 Training loss: 2.3407 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 734/35720 Training loss: 2.3399 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 735/35720 Training loss: 2.3392 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 736/35720 Training loss: 2.3386 0.2400 sec/batch\n",
      "Epoch 1/20  Iteration 737/35720 Training loss: 2.3380 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 738/35720 Training loss: 2.3372 0.2062 sec/batch\n",
      "Epoch 1/20  Iteration 739/35720 Training loss: 2.3366 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 740/35720 Training loss: 2.3360 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 741/35720 Training loss: 2.3353 0.2182 sec/batch\n",
      "Epoch 1/20  Iteration 742/35720 Training loss: 2.3347 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 743/35720 Training loss: 2.3340 0.2246 sec/batch\n",
      "Epoch 1/20  Iteration 744/35720 Training loss: 2.3333 0.2223 sec/batch\n",
      "Epoch 1/20  Iteration 745/35720 Training loss: 2.3326 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 746/35720 Training loss: 2.3319 0.2138 sec/batch\n",
      "Epoch 1/20  Iteration 747/35720 Training loss: 2.3313 0.2178 sec/batch\n",
      "Epoch 1/20  Iteration 748/35720 Training loss: 2.3305 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 749/35720 Training loss: 2.3298 0.2082 sec/batch\n",
      "Epoch 1/20  Iteration 750/35720 Training loss: 2.3291 0.2559 sec/batch\n",
      "Epoch 1/20  Iteration 751/35720 Training loss: 2.3284 0.2248 sec/batch\n",
      "Epoch 1/20  Iteration 752/35720 Training loss: 2.3279 0.2117 sec/batch\n",
      "Epoch 1/20  Iteration 753/35720 Training loss: 2.3271 0.2052 sec/batch\n",
      "Epoch 1/20  Iteration 754/35720 Training loss: 2.3265 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 755/35720 Training loss: 2.3257 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 756/35720 Training loss: 2.3249 0.2219 sec/batch\n",
      "Epoch 1/20  Iteration 757/35720 Training loss: 2.3242 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 758/35720 Training loss: 2.3235 0.2114 sec/batch\n",
      "Epoch 1/20  Iteration 759/35720 Training loss: 2.3228 0.2075 sec/batch\n",
      "Epoch 1/20  Iteration 760/35720 Training loss: 2.3222 0.2139 sec/batch\n",
      "Epoch 1/20  Iteration 761/35720 Training loss: 2.3216 0.2312 sec/batch\n",
      "Epoch 1/20  Iteration 762/35720 Training loss: 2.3210 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 763/35720 Training loss: 2.3203 0.2137 sec/batch\n",
      "Epoch 1/20  Iteration 764/35720 Training loss: 2.3196 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 765/35720 Training loss: 2.3189 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 766/35720 Training loss: 2.3184 0.2190 sec/batch\n",
      "Epoch 1/20  Iteration 767/35720 Training loss: 2.3177 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 768/35720 Training loss: 2.3171 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 769/35720 Training loss: 2.3164 0.2064 sec/batch\n",
      "Epoch 1/20  Iteration 770/35720 Training loss: 2.3159 0.2230 sec/batch\n",
      "Epoch 1/20  Iteration 771/35720 Training loss: 2.3153 0.2128 sec/batch\n",
      "Epoch 1/20  Iteration 772/35720 Training loss: 2.3147 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 773/35720 Training loss: 2.3139 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 774/35720 Training loss: 2.3131 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 775/35720 Training loss: 2.3123 0.2317 sec/batch\n",
      "Epoch 1/20  Iteration 776/35720 Training loss: 2.3115 0.2068 sec/batch\n",
      "Epoch 1/20  Iteration 777/35720 Training loss: 2.3109 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 778/35720 Training loss: 2.3103 0.2223 sec/batch\n",
      "Epoch 1/20  Iteration 779/35720 Training loss: 2.3097 0.2295 sec/batch\n",
      "Epoch 1/20  Iteration 780/35720 Training loss: 2.3091 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 781/35720 Training loss: 2.3084 0.2258 sec/batch\n",
      "Epoch 1/20  Iteration 782/35720 Training loss: 2.3078 0.2253 sec/batch\n",
      "Epoch 1/20  Iteration 783/35720 Training loss: 2.3072 0.2074 sec/batch\n",
      "Epoch 1/20  Iteration 784/35720 Training loss: 2.3066 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 785/35720 Training loss: 2.3058 0.2297 sec/batch\n",
      "Epoch 1/20  Iteration 786/35720 Training loss: 2.3052 0.2225 sec/batch\n",
      "Epoch 1/20  Iteration 787/35720 Training loss: 2.3044 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 788/35720 Training loss: 2.3037 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 789/35720 Training loss: 2.3031 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 790/35720 Training loss: 2.3024 0.2229 sec/batch\n",
      "Epoch 1/20  Iteration 791/35720 Training loss: 2.3020 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 792/35720 Training loss: 2.3013 0.2074 sec/batch\n",
      "Epoch 1/20  Iteration 793/35720 Training loss: 2.3007 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 794/35720 Training loss: 2.2999 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 795/35720 Training loss: 2.2993 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 796/35720 Training loss: 2.2987 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 797/35720 Training loss: 2.2981 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 798/35720 Training loss: 2.2975 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 799/35720 Training loss: 2.2967 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 800/35720 Training loss: 2.2961 0.2105 sec/batch\n",
      "Validation loss: 1.89928 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 801/35720 Training loss: 2.2957 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 802/35720 Training loss: 2.2951 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 803/35720 Training loss: 2.2946 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 804/35720 Training loss: 2.2942 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 805/35720 Training loss: 2.2937 0.2134 sec/batch\n",
      "Epoch 1/20  Iteration 806/35720 Training loss: 2.2932 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 807/35720 Training loss: 2.2926 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 808/35720 Training loss: 2.2921 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 809/35720 Training loss: 2.2916 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 810/35720 Training loss: 2.2910 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 811/35720 Training loss: 2.2904 0.2295 sec/batch\n",
      "Epoch 1/20  Iteration 812/35720 Training loss: 2.2898 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 813/35720 Training loss: 2.2893 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 814/35720 Training loss: 2.2886 0.2095 sec/batch\n",
      "Epoch 1/20  Iteration 815/35720 Training loss: 2.2880 0.2236 sec/batch\n",
      "Epoch 1/20  Iteration 816/35720 Training loss: 2.2872 0.2283 sec/batch\n",
      "Epoch 1/20  Iteration 817/35720 Training loss: 2.2866 0.2205 sec/batch\n",
      "Epoch 1/20  Iteration 818/35720 Training loss: 2.2860 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 819/35720 Training loss: 2.2854 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 820/35720 Training loss: 2.2847 0.2123 sec/batch\n",
      "Epoch 1/20  Iteration 821/35720 Training loss: 2.2840 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 822/35720 Training loss: 2.2832 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 823/35720 Training loss: 2.2825 0.2355 sec/batch\n",
      "Epoch 1/20  Iteration 824/35720 Training loss: 2.2819 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 825/35720 Training loss: 2.2812 0.2228 sec/batch\n",
      "Epoch 1/20  Iteration 826/35720 Training loss: 2.2806 0.2262 sec/batch\n",
      "Epoch 1/20  Iteration 827/35720 Training loss: 2.2799 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 828/35720 Training loss: 2.2792 0.2228 sec/batch\n",
      "Epoch 1/20  Iteration 829/35720 Training loss: 2.2786 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 830/35720 Training loss: 2.2780 0.2332 sec/batch\n",
      "Epoch 1/20  Iteration 831/35720 Training loss: 2.2773 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 832/35720 Training loss: 2.2768 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 833/35720 Training loss: 2.2764 0.2142 sec/batch\n",
      "Epoch 1/20  Iteration 834/35720 Training loss: 2.2758 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 835/35720 Training loss: 2.2753 0.2120 sec/batch\n",
      "Epoch 1/20  Iteration 836/35720 Training loss: 2.2746 0.2262 sec/batch\n",
      "Epoch 1/20  Iteration 837/35720 Training loss: 2.2740 0.2155 sec/batch\n",
      "Epoch 1/20  Iteration 838/35720 Training loss: 2.2735 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 839/35720 Training loss: 2.2730 0.2281 sec/batch\n",
      "Epoch 1/20  Iteration 840/35720 Training loss: 2.2724 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 841/35720 Training loss: 2.2718 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 842/35720 Training loss: 2.2713 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 843/35720 Training loss: 2.2706 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 844/35720 Training loss: 2.2701 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 845/35720 Training loss: 2.2695 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 846/35720 Training loss: 2.2689 0.2299 sec/batch\n",
      "Epoch 1/20  Iteration 847/35720 Training loss: 2.2682 0.2136 sec/batch\n",
      "Epoch 1/20  Iteration 848/35720 Training loss: 2.2677 0.2083 sec/batch\n",
      "Epoch 1/20  Iteration 849/35720 Training loss: 2.2670 0.2086 sec/batch\n",
      "Epoch 1/20  Iteration 850/35720 Training loss: 2.2664 0.2137 sec/batch\n",
      "Epoch 1/20  Iteration 851/35720 Training loss: 2.2658 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 852/35720 Training loss: 2.2653 0.2266 sec/batch\n",
      "Epoch 1/20  Iteration 853/35720 Training loss: 2.2647 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 854/35720 Training loss: 2.2641 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 855/35720 Training loss: 2.2635 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 856/35720 Training loss: 2.2629 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 857/35720 Training loss: 2.2623 0.2320 sec/batch\n",
      "Epoch 1/20  Iteration 858/35720 Training loss: 2.2618 0.2127 sec/batch\n",
      "Epoch 1/20  Iteration 859/35720 Training loss: 2.2612 0.2114 sec/batch\n",
      "Epoch 1/20  Iteration 860/35720 Training loss: 2.2605 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 861/35720 Training loss: 2.2600 0.2197 sec/batch\n",
      "Epoch 1/20  Iteration 862/35720 Training loss: 2.2593 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 863/35720 Training loss: 2.2586 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 864/35720 Training loss: 2.2580 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 865/35720 Training loss: 2.2573 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 866/35720 Training loss: 2.2568 0.2205 sec/batch\n",
      "Epoch 1/20  Iteration 867/35720 Training loss: 2.2564 0.2243 sec/batch\n",
      "Epoch 1/20  Iteration 868/35720 Training loss: 2.2558 0.2219 sec/batch\n",
      "Epoch 1/20  Iteration 869/35720 Training loss: 2.2552 0.2139 sec/batch\n",
      "Epoch 1/20  Iteration 870/35720 Training loss: 2.2546 0.2155 sec/batch\n",
      "Epoch 1/20  Iteration 871/35720 Training loss: 2.2540 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 872/35720 Training loss: 2.2534 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 873/35720 Training loss: 2.2527 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 874/35720 Training loss: 2.2521 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 875/35720 Training loss: 2.2514 0.2127 sec/batch\n",
      "Epoch 1/20  Iteration 876/35720 Training loss: 2.2508 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 877/35720 Training loss: 2.2502 0.2155 sec/batch\n",
      "Epoch 1/20  Iteration 878/35720 Training loss: 2.2496 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 879/35720 Training loss: 2.2489 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 880/35720 Training loss: 2.2483 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 881/35720 Training loss: 2.2479 0.2241 sec/batch\n",
      "Epoch 1/20  Iteration 882/35720 Training loss: 2.2473 0.2178 sec/batch\n",
      "Epoch 1/20  Iteration 883/35720 Training loss: 2.2467 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 884/35720 Training loss: 2.2460 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 885/35720 Training loss: 2.2454 0.2257 sec/batch\n",
      "Epoch 1/20  Iteration 886/35720 Training loss: 2.2447 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 887/35720 Training loss: 2.2442 0.2358 sec/batch\n",
      "Epoch 1/20  Iteration 888/35720 Training loss: 2.2436 0.2117 sec/batch\n",
      "Epoch 1/20  Iteration 889/35720 Training loss: 2.2429 0.2233 sec/batch\n",
      "Epoch 1/20  Iteration 890/35720 Training loss: 2.2423 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 891/35720 Training loss: 2.2417 0.2246 sec/batch\n",
      "Epoch 1/20  Iteration 892/35720 Training loss: 2.2410 0.2143 sec/batch\n",
      "Epoch 1/20  Iteration 893/35720 Training loss: 2.2404 0.2269 sec/batch\n",
      "Epoch 1/20  Iteration 894/35720 Training loss: 2.2398 0.2274 sec/batch\n",
      "Epoch 1/20  Iteration 895/35720 Training loss: 2.2392 0.2221 sec/batch\n",
      "Epoch 1/20  Iteration 896/35720 Training loss: 2.2386 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 897/35720 Training loss: 2.2379 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 898/35720 Training loss: 2.2372 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 899/35720 Training loss: 2.2365 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 900/35720 Training loss: 2.2359 0.2189 sec/batch\n",
      "Epoch 1/20  Iteration 901/35720 Training loss: 2.2353 0.2383 sec/batch\n",
      "Epoch 1/20  Iteration 902/35720 Training loss: 2.2346 0.2075 sec/batch\n",
      "Epoch 1/20  Iteration 903/35720 Training loss: 2.2339 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 904/35720 Training loss: 2.2333 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 905/35720 Training loss: 2.2326 0.2178 sec/batch\n",
      "Epoch 1/20  Iteration 906/35720 Training loss: 2.2320 0.2297 sec/batch\n",
      "Epoch 1/20  Iteration 907/35720 Training loss: 2.2315 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 908/35720 Training loss: 2.2309 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 909/35720 Training loss: 2.2303 0.2126 sec/batch\n",
      "Epoch 1/20  Iteration 910/35720 Training loss: 2.2297 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 911/35720 Training loss: 2.2291 0.2228 sec/batch\n",
      "Epoch 1/20  Iteration 912/35720 Training loss: 2.2285 0.2082 sec/batch\n",
      "Epoch 1/20  Iteration 913/35720 Training loss: 2.2279 0.2390 sec/batch\n",
      "Epoch 1/20  Iteration 914/35720 Training loss: 2.2274 0.2306 sec/batch\n",
      "Epoch 1/20  Iteration 915/35720 Training loss: 2.2269 0.2123 sec/batch\n",
      "Epoch 1/20  Iteration 916/35720 Training loss: 2.2264 0.2248 sec/batch\n",
      "Epoch 1/20  Iteration 917/35720 Training loss: 2.2259 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 918/35720 Training loss: 2.2254 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 919/35720 Training loss: 2.2248 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 920/35720 Training loss: 2.2243 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 921/35720 Training loss: 2.2237 0.2225 sec/batch\n",
      "Epoch 1/20  Iteration 922/35720 Training loss: 2.2230 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 923/35720 Training loss: 2.2226 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 924/35720 Training loss: 2.2220 0.2252 sec/batch\n",
      "Epoch 1/20  Iteration 925/35720 Training loss: 2.2215 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 926/35720 Training loss: 2.2210 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 927/35720 Training loss: 2.2207 0.2138 sec/batch\n",
      "Epoch 1/20  Iteration 928/35720 Training loss: 2.2201 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 929/35720 Training loss: 2.2197 0.2380 sec/batch\n",
      "Epoch 1/20  Iteration 930/35720 Training loss: 2.2192 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 931/35720 Training loss: 2.2187 0.2203 sec/batch\n",
      "Epoch 1/20  Iteration 932/35720 Training loss: 2.2182 0.2176 sec/batch\n",
      "Epoch 1/20  Iteration 933/35720 Training loss: 2.2176 0.2290 sec/batch\n",
      "Epoch 1/20  Iteration 934/35720 Training loss: 2.2171 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 935/35720 Training loss: 2.2165 0.2395 sec/batch\n",
      "Epoch 1/20  Iteration 936/35720 Training loss: 2.2158 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 937/35720 Training loss: 2.2152 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 938/35720 Training loss: 2.2147 0.2286 sec/batch\n",
      "Epoch 1/20  Iteration 939/35720 Training loss: 2.2142 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 940/35720 Training loss: 2.2136 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 941/35720 Training loss: 2.2130 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 942/35720 Training loss: 2.2126 0.2148 sec/batch\n",
      "Epoch 1/20  Iteration 943/35720 Training loss: 2.2119 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 944/35720 Training loss: 2.2115 0.2238 sec/batch\n",
      "Epoch 1/20  Iteration 945/35720 Training loss: 2.2109 0.2058 sec/batch\n",
      "Epoch 1/20  Iteration 946/35720 Training loss: 2.2105 0.2376 sec/batch\n",
      "Epoch 1/20  Iteration 947/35720 Training loss: 2.2099 0.2107 sec/batch\n",
      "Epoch 1/20  Iteration 948/35720 Training loss: 2.2093 0.2161 sec/batch\n",
      "Epoch 1/20  Iteration 949/35720 Training loss: 2.2088 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 950/35720 Training loss: 2.2082 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 951/35720 Training loss: 2.2077 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 952/35720 Training loss: 2.2071 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 953/35720 Training loss: 2.2065 0.2264 sec/batch\n",
      "Epoch 1/20  Iteration 954/35720 Training loss: 2.2060 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 955/35720 Training loss: 2.2055 0.2255 sec/batch\n",
      "Epoch 1/20  Iteration 956/35720 Training loss: 2.2049 0.2146 sec/batch\n",
      "Epoch 1/20  Iteration 957/35720 Training loss: 2.2044 0.2253 sec/batch\n",
      "Epoch 1/20  Iteration 958/35720 Training loss: 2.2038 0.2139 sec/batch\n",
      "Epoch 1/20  Iteration 959/35720 Training loss: 2.2033 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 960/35720 Training loss: 2.2027 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 961/35720 Training loss: 2.2021 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 962/35720 Training loss: 2.2015 0.2100 sec/batch\n",
      "Epoch 1/20  Iteration 963/35720 Training loss: 2.2008 0.2097 sec/batch\n",
      "Epoch 1/20  Iteration 964/35720 Training loss: 2.2003 0.2192 sec/batch\n",
      "Epoch 1/20  Iteration 965/35720 Training loss: 2.1997 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 966/35720 Training loss: 2.1992 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 967/35720 Training loss: 2.1988 0.2065 sec/batch\n",
      "Epoch 1/20  Iteration 968/35720 Training loss: 2.1983 0.2278 sec/batch\n",
      "Epoch 1/20  Iteration 969/35720 Training loss: 2.1980 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 970/35720 Training loss: 2.1977 0.2218 sec/batch\n",
      "Epoch 1/20  Iteration 971/35720 Training loss: 2.1971 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 972/35720 Training loss: 2.1966 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 973/35720 Training loss: 2.1960 0.2191 sec/batch\n",
      "Epoch 1/20  Iteration 974/35720 Training loss: 2.1955 0.2268 sec/batch\n",
      "Epoch 1/20  Iteration 975/35720 Training loss: 2.1949 0.2267 sec/batch\n",
      "Epoch 1/20  Iteration 976/35720 Training loss: 2.1944 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 977/35720 Training loss: 2.1938 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 978/35720 Training loss: 2.1934 0.2071 sec/batch\n",
      "Epoch 1/20  Iteration 979/35720 Training loss: 2.1929 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 980/35720 Training loss: 2.1924 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 981/35720 Training loss: 2.1918 0.2342 sec/batch\n",
      "Epoch 1/20  Iteration 982/35720 Training loss: 2.1912 0.2309 sec/batch\n",
      "Epoch 1/20  Iteration 983/35720 Training loss: 2.1906 0.2282 sec/batch\n",
      "Epoch 1/20  Iteration 984/35720 Training loss: 2.1901 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 985/35720 Training loss: 2.1897 0.2192 sec/batch\n",
      "Epoch 1/20  Iteration 986/35720 Training loss: 2.1892 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 987/35720 Training loss: 2.1887 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 988/35720 Training loss: 2.1882 0.2065 sec/batch\n",
      "Epoch 1/20  Iteration 989/35720 Training loss: 2.1877 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 990/35720 Training loss: 2.1871 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 991/35720 Training loss: 2.1866 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 992/35720 Training loss: 2.1861 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 993/35720 Training loss: 2.1857 0.2228 sec/batch\n",
      "Epoch 1/20  Iteration 994/35720 Training loss: 2.1852 0.2326 sec/batch\n",
      "Epoch 1/20  Iteration 995/35720 Training loss: 2.1847 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 996/35720 Training loss: 2.1841 0.2190 sec/batch\n",
      "Epoch 1/20  Iteration 997/35720 Training loss: 2.1838 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 998/35720 Training loss: 2.1833 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 999/35720 Training loss: 2.1828 0.2288 sec/batch\n",
      "Epoch 1/20  Iteration 1000/35720 Training loss: 2.1823 0.2340 sec/batch\n",
      "Validation loss: 1.79809 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 1001/35720 Training loss: 2.1817 0.2424 sec/batch\n",
      "Epoch 1/20  Iteration 1002/35720 Training loss: 2.1811 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 1003/35720 Training loss: 2.1805 0.2107 sec/batch\n",
      "Epoch 1/20  Iteration 1004/35720 Training loss: 2.1799 0.2288 sec/batch\n",
      "Epoch 1/20  Iteration 1005/35720 Training loss: 2.1794 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 1006/35720 Training loss: 2.1788 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 1007/35720 Training loss: 2.1783 0.2175 sec/batch\n",
      "Epoch 1/20  Iteration 1008/35720 Training loss: 2.1777 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 1009/35720 Training loss: 2.1771 0.2273 sec/batch\n",
      "Epoch 1/20  Iteration 1010/35720 Training loss: 2.1765 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 1011/35720 Training loss: 2.1760 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 1012/35720 Training loss: 2.1755 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 1013/35720 Training loss: 2.1749 0.2261 sec/batch\n",
      "Epoch 1/20  Iteration 1014/35720 Training loss: 2.1743 0.2128 sec/batch\n",
      "Epoch 1/20  Iteration 1015/35720 Training loss: 2.1738 0.2150 sec/batch\n",
      "Epoch 1/20  Iteration 1016/35720 Training loss: 2.1733 0.2261 sec/batch\n",
      "Epoch 1/20  Iteration 1017/35720 Training loss: 2.1728 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 1018/35720 Training loss: 2.1723 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 1019/35720 Training loss: 2.1717 0.2228 sec/batch\n",
      "Epoch 1/20  Iteration 1020/35720 Training loss: 2.1712 0.2191 sec/batch\n",
      "Epoch 1/20  Iteration 1021/35720 Training loss: 2.1707 0.2288 sec/batch\n",
      "Epoch 1/20  Iteration 1022/35720 Training loss: 2.1702 0.2072 sec/batch\n",
      "Epoch 1/20  Iteration 1023/35720 Training loss: 2.1698 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 1024/35720 Training loss: 2.1692 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 1025/35720 Training loss: 2.1688 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 1026/35720 Training loss: 2.1683 0.2399 sec/batch\n",
      "Epoch 1/20  Iteration 1027/35720 Training loss: 2.1678 0.2074 sec/batch\n",
      "Epoch 1/20  Iteration 1028/35720 Training loss: 2.1672 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 1029/35720 Training loss: 2.1666 0.2095 sec/batch\n",
      "Epoch 1/20  Iteration 1030/35720 Training loss: 2.1661 0.2273 sec/batch\n",
      "Epoch 1/20  Iteration 1031/35720 Training loss: 2.1655 0.2090 sec/batch\n",
      "Epoch 1/20  Iteration 1032/35720 Training loss: 2.1651 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 1033/35720 Training loss: 2.1645 0.2292 sec/batch\n",
      "Epoch 1/20  Iteration 1034/35720 Training loss: 2.1641 0.2229 sec/batch\n",
      "Epoch 1/20  Iteration 1035/35720 Training loss: 2.1636 0.2159 sec/batch\n",
      "Epoch 1/20  Iteration 1036/35720 Training loss: 2.1632 0.2254 sec/batch\n",
      "Epoch 1/20  Iteration 1037/35720 Training loss: 2.1627 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 1038/35720 Training loss: 2.1623 0.2226 sec/batch\n",
      "Epoch 1/20  Iteration 1039/35720 Training loss: 2.1618 0.2223 sec/batch\n",
      "Epoch 1/20  Iteration 1040/35720 Training loss: 2.1613 0.2235 sec/batch\n",
      "Epoch 1/20  Iteration 1041/35720 Training loss: 2.1608 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 1042/35720 Training loss: 2.1603 0.2075 sec/batch\n",
      "Epoch 1/20  Iteration 1043/35720 Training loss: 2.1598 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 1044/35720 Training loss: 2.1595 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 1045/35720 Training loss: 2.1590 0.2231 sec/batch\n",
      "Epoch 1/20  Iteration 1046/35720 Training loss: 2.1585 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 1047/35720 Training loss: 2.1580 0.2270 sec/batch\n",
      "Epoch 1/20  Iteration 1048/35720 Training loss: 2.1576 0.2069 sec/batch\n",
      "Epoch 1/20  Iteration 1049/35720 Training loss: 2.1571 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 1050/35720 Training loss: 2.1565 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 1051/35720 Training loss: 2.1559 0.2235 sec/batch\n",
      "Epoch 1/20  Iteration 1052/35720 Training loss: 2.1556 0.2183 sec/batch\n",
      "Epoch 1/20  Iteration 1053/35720 Training loss: 2.1551 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 1054/35720 Training loss: 2.1547 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 1055/35720 Training loss: 2.1542 0.2251 sec/batch\n",
      "Epoch 1/20  Iteration 1056/35720 Training loss: 2.1538 0.2138 sec/batch\n",
      "Epoch 1/20  Iteration 1057/35720 Training loss: 2.1533 0.2219 sec/batch\n",
      "Epoch 1/20  Iteration 1058/35720 Training loss: 2.1527 0.2181 sec/batch\n",
      "Epoch 1/20  Iteration 1059/35720 Training loss: 2.1523 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 1060/35720 Training loss: 2.1517 0.2298 sec/batch\n",
      "Epoch 1/20  Iteration 1061/35720 Training loss: 2.1512 0.2293 sec/batch\n",
      "Epoch 1/20  Iteration 1062/35720 Training loss: 2.1507 0.2219 sec/batch\n",
      "Epoch 1/20  Iteration 1063/35720 Training loss: 2.1503 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 1064/35720 Training loss: 2.1498 0.2097 sec/batch\n",
      "Epoch 1/20  Iteration 1065/35720 Training loss: 2.1493 0.2315 sec/batch\n",
      "Epoch 1/20  Iteration 1066/35720 Training loss: 2.1488 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 1067/35720 Training loss: 2.1483 0.2295 sec/batch\n",
      "Epoch 1/20  Iteration 1068/35720 Training loss: 2.1478 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 1069/35720 Training loss: 2.1473 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1070/35720 Training loss: 2.1469 0.2089 sec/batch\n",
      "Epoch 1/20  Iteration 1071/35720 Training loss: 2.1464 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 1072/35720 Training loss: 2.1460 0.2212 sec/batch\n",
      "Epoch 1/20  Iteration 1073/35720 Training loss: 2.1455 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 1074/35720 Training loss: 2.1451 0.2277 sec/batch\n",
      "Epoch 1/20  Iteration 1075/35720 Training loss: 2.1446 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 1076/35720 Training loss: 2.1442 0.2221 sec/batch\n",
      "Epoch 1/20  Iteration 1077/35720 Training loss: 2.1437 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 1078/35720 Training loss: 2.1432 0.2254 sec/batch\n",
      "Epoch 1/20  Iteration 1079/35720 Training loss: 2.1427 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 1080/35720 Training loss: 2.1423 0.2067 sec/batch\n",
      "Epoch 1/20  Iteration 1081/35720 Training loss: 2.1418 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 1082/35720 Training loss: 2.1412 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 1083/35720 Training loss: 2.1407 0.2169 sec/batch\n",
      "Epoch 1/20  Iteration 1084/35720 Training loss: 2.1402 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 1085/35720 Training loss: 2.1397 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 1086/35720 Training loss: 2.1392 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 1087/35720 Training loss: 2.1387 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 1088/35720 Training loss: 2.1383 0.2142 sec/batch\n",
      "Epoch 1/20  Iteration 1089/35720 Training loss: 2.1379 0.2084 sec/batch\n",
      "Epoch 1/20  Iteration 1090/35720 Training loss: 2.1375 0.2170 sec/batch\n",
      "Epoch 1/20  Iteration 1091/35720 Training loss: 2.1371 0.2281 sec/batch\n",
      "Epoch 1/20  Iteration 1092/35720 Training loss: 2.1367 0.2434 sec/batch\n",
      "Epoch 1/20  Iteration 1093/35720 Training loss: 2.1362 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 1094/35720 Training loss: 2.1358 0.2269 sec/batch\n",
      "Epoch 1/20  Iteration 1095/35720 Training loss: 2.1353 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 1096/35720 Training loss: 2.1348 0.2232 sec/batch\n",
      "Epoch 1/20  Iteration 1097/35720 Training loss: 2.1343 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 1098/35720 Training loss: 2.1339 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 1099/35720 Training loss: 2.1335 0.2117 sec/batch\n",
      "Epoch 1/20  Iteration 1100/35720 Training loss: 2.1330 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 1101/35720 Training loss: 2.1326 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 1102/35720 Training loss: 2.1322 0.2258 sec/batch\n",
      "Epoch 1/20  Iteration 1103/35720 Training loss: 2.1318 0.2298 sec/batch\n",
      "Epoch 1/20  Iteration 1104/35720 Training loss: 2.1313 0.2520 sec/batch\n",
      "Epoch 1/20  Iteration 1105/35720 Training loss: 2.1309 0.2460 sec/batch\n",
      "Epoch 1/20  Iteration 1106/35720 Training loss: 2.1304 0.2307 sec/batch\n",
      "Epoch 1/20  Iteration 1107/35720 Training loss: 2.1300 0.2313 sec/batch\n",
      "Epoch 1/20  Iteration 1108/35720 Training loss: 2.1298 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 1109/35720 Training loss: 2.1293 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 1110/35720 Training loss: 2.1290 0.2283 sec/batch\n",
      "Epoch 1/20  Iteration 1111/35720 Training loss: 2.1286 0.2077 sec/batch\n",
      "Epoch 1/20  Iteration 1112/35720 Training loss: 2.1280 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 1113/35720 Training loss: 2.1275 0.2277 sec/batch\n",
      "Epoch 1/20  Iteration 1114/35720 Training loss: 2.1271 0.2090 sec/batch\n",
      "Epoch 1/20  Iteration 1115/35720 Training loss: 2.1266 0.2271 sec/batch\n",
      "Epoch 1/20  Iteration 1116/35720 Training loss: 2.1262 0.2218 sec/batch\n",
      "Epoch 1/20  Iteration 1117/35720 Training loss: 2.1257 0.2492 sec/batch\n",
      "Epoch 1/20  Iteration 1118/35720 Training loss: 2.1251 0.2057 sec/batch\n",
      "Epoch 1/20  Iteration 1119/35720 Training loss: 2.1246 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 1120/35720 Training loss: 2.1243 0.2254 sec/batch\n",
      "Epoch 1/20  Iteration 1121/35720 Training loss: 2.1239 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 1122/35720 Training loss: 2.1236 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1123/35720 Training loss: 2.1233 0.2223 sec/batch\n",
      "Epoch 1/20  Iteration 1124/35720 Training loss: 2.1229 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 1125/35720 Training loss: 2.1224 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 1126/35720 Training loss: 2.1219 0.2198 sec/batch\n",
      "Epoch 1/20  Iteration 1127/35720 Training loss: 2.1214 0.2249 sec/batch\n",
      "Epoch 1/20  Iteration 1128/35720 Training loss: 2.1209 0.2053 sec/batch\n",
      "Epoch 1/20  Iteration 1129/35720 Training loss: 2.1205 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 1130/35720 Training loss: 2.1201 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 1131/35720 Training loss: 2.1196 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 1132/35720 Training loss: 2.1192 0.2327 sec/batch\n",
      "Epoch 1/20  Iteration 1133/35720 Training loss: 2.1187 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 1134/35720 Training loss: 2.1183 0.2169 sec/batch\n",
      "Epoch 1/20  Iteration 1135/35720 Training loss: 2.1178 0.2252 sec/batch\n",
      "Epoch 1/20  Iteration 1136/35720 Training loss: 2.1174 0.2071 sec/batch\n",
      "Epoch 1/20  Iteration 1137/35720 Training loss: 2.1168 0.2150 sec/batch\n",
      "Epoch 1/20  Iteration 1138/35720 Training loss: 2.1162 0.2316 sec/batch\n",
      "Epoch 1/20  Iteration 1139/35720 Training loss: 2.1158 0.2219 sec/batch\n",
      "Epoch 1/20  Iteration 1140/35720 Training loss: 2.1154 0.2150 sec/batch\n",
      "Epoch 1/20  Iteration 1141/35720 Training loss: 2.1149 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 1142/35720 Training loss: 2.1143 0.2148 sec/batch\n",
      "Epoch 1/20  Iteration 1143/35720 Training loss: 2.1138 0.2252 sec/batch\n",
      "Epoch 1/20  Iteration 1144/35720 Training loss: 2.1133 0.2312 sec/batch\n",
      "Epoch 1/20  Iteration 1145/35720 Training loss: 2.1129 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 1146/35720 Training loss: 2.1124 0.2241 sec/batch\n",
      "Epoch 1/20  Iteration 1147/35720 Training loss: 2.1119 0.2202 sec/batch\n",
      "Epoch 1/20  Iteration 1148/35720 Training loss: 2.1114 0.2218 sec/batch\n",
      "Epoch 1/20  Iteration 1149/35720 Training loss: 2.1109 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 1150/35720 Training loss: 2.1104 0.2253 sec/batch\n",
      "Epoch 1/20  Iteration 1151/35720 Training loss: 2.1100 0.2250 sec/batch\n",
      "Epoch 1/20  Iteration 1152/35720 Training loss: 2.1096 0.2068 sec/batch\n",
      "Epoch 1/20  Iteration 1153/35720 Training loss: 2.1092 0.2193 sec/batch\n",
      "Epoch 1/20  Iteration 1154/35720 Training loss: 2.1088 0.2290 sec/batch\n",
      "Epoch 1/20  Iteration 1155/35720 Training loss: 2.1084 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 1156/35720 Training loss: 2.1079 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 1157/35720 Training loss: 2.1075 0.2250 sec/batch\n",
      "Epoch 1/20  Iteration 1158/35720 Training loss: 2.1070 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 1159/35720 Training loss: 2.1066 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 1160/35720 Training loss: 2.1061 0.2189 sec/batch\n",
      "Epoch 1/20  Iteration 1161/35720 Training loss: 2.1056 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 1162/35720 Training loss: 2.1051 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 1163/35720 Training loss: 2.1048 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 1164/35720 Training loss: 2.1044 0.2277 sec/batch\n",
      "Epoch 1/20  Iteration 1165/35720 Training loss: 2.1040 0.2394 sec/batch\n",
      "Epoch 1/20  Iteration 1166/35720 Training loss: 2.1035 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 1167/35720 Training loss: 2.1030 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 1168/35720 Training loss: 2.1026 0.2337 sec/batch\n",
      "Epoch 1/20  Iteration 1169/35720 Training loss: 2.1023 0.2272 sec/batch\n",
      "Epoch 1/20  Iteration 1170/35720 Training loss: 2.1019 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 1171/35720 Training loss: 2.1015 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 1172/35720 Training loss: 2.1010 0.2084 sec/batch\n",
      "Epoch 1/20  Iteration 1173/35720 Training loss: 2.1005 0.2155 sec/batch\n",
      "Epoch 1/20  Iteration 1174/35720 Training loss: 2.1001 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 1175/35720 Training loss: 2.0997 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 1176/35720 Training loss: 2.0994 0.2097 sec/batch\n",
      "Epoch 1/20  Iteration 1177/35720 Training loss: 2.0991 0.2233 sec/batch\n",
      "Epoch 1/20  Iteration 1178/35720 Training loss: 2.0987 0.2282 sec/batch\n",
      "Epoch 1/20  Iteration 1179/35720 Training loss: 2.0982 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 1180/35720 Training loss: 2.0978 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 1181/35720 Training loss: 2.0975 0.2271 sec/batch\n",
      "Epoch 1/20  Iteration 1182/35720 Training loss: 2.0971 0.2078 sec/batch\n",
      "Epoch 1/20  Iteration 1183/35720 Training loss: 2.0967 0.2166 sec/batch\n",
      "Epoch 1/20  Iteration 1184/35720 Training loss: 2.0962 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 1185/35720 Training loss: 2.0958 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1186/35720 Training loss: 2.0954 0.2161 sec/batch\n",
      "Epoch 1/20  Iteration 1187/35720 Training loss: 2.0950 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 1188/35720 Training loss: 2.0946 0.2070 sec/batch\n",
      "Epoch 1/20  Iteration 1189/35720 Training loss: 2.0942 0.2128 sec/batch\n",
      "Epoch 1/20  Iteration 1190/35720 Training loss: 2.0939 0.2232 sec/batch\n",
      "Epoch 1/20  Iteration 1191/35720 Training loss: 2.0935 0.2109 sec/batch\n",
      "Epoch 1/20  Iteration 1192/35720 Training loss: 2.0931 0.2111 sec/batch\n",
      "Epoch 1/20  Iteration 1193/35720 Training loss: 2.0927 0.2130 sec/batch\n",
      "Epoch 1/20  Iteration 1194/35720 Training loss: 2.0923 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 1195/35720 Training loss: 2.0919 0.2263 sec/batch\n",
      "Epoch 1/20  Iteration 1196/35720 Training loss: 2.0916 0.2321 sec/batch\n",
      "Epoch 1/20  Iteration 1197/35720 Training loss: 2.0913 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 1198/35720 Training loss: 2.0909 0.2188 sec/batch\n",
      "Epoch 1/20  Iteration 1199/35720 Training loss: 2.0905 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 1200/35720 Training loss: 2.0901 0.2231 sec/batch\n",
      "Validation loss: 1.73339 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 1201/35720 Training loss: 2.0897 0.2341 sec/batch\n",
      "Epoch 1/20  Iteration 1202/35720 Training loss: 2.0892 0.2778 sec/batch\n",
      "Epoch 1/20  Iteration 1203/35720 Training loss: 2.0888 0.2584 sec/batch\n",
      "Epoch 1/20  Iteration 1204/35720 Training loss: 2.0883 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 1205/35720 Training loss: 2.0878 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 1206/35720 Training loss: 2.0874 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 1207/35720 Training loss: 2.0870 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1208/35720 Training loss: 2.0866 0.2125 sec/batch\n",
      "Epoch 1/20  Iteration 1209/35720 Training loss: 2.0862 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 1210/35720 Training loss: 2.0858 0.2808 sec/batch\n",
      "Epoch 1/20  Iteration 1211/35720 Training loss: 2.0854 0.2175 sec/batch\n",
      "Epoch 1/20  Iteration 1212/35720 Training loss: 2.0850 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 1213/35720 Training loss: 2.0846 0.2198 sec/batch\n",
      "Epoch 1/20  Iteration 1214/35720 Training loss: 2.0842 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1215/35720 Training loss: 2.0838 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 1216/35720 Training loss: 2.0834 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 1217/35720 Training loss: 2.0831 0.2283 sec/batch\n",
      "Epoch 1/20  Iteration 1218/35720 Training loss: 2.0827 0.2111 sec/batch\n",
      "Epoch 1/20  Iteration 1219/35720 Training loss: 2.0823 0.2054 sec/batch\n",
      "Epoch 1/20  Iteration 1220/35720 Training loss: 2.0819 0.2242 sec/batch\n",
      "Epoch 1/20  Iteration 1221/35720 Training loss: 2.0814 0.2094 sec/batch\n",
      "Epoch 1/20  Iteration 1222/35720 Training loss: 2.0810 0.2100 sec/batch\n",
      "Epoch 1/20  Iteration 1223/35720 Training loss: 2.0806 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 1224/35720 Training loss: 2.0801 0.2246 sec/batch\n",
      "Epoch 1/20  Iteration 1225/35720 Training loss: 2.0798 0.2077 sec/batch\n",
      "Epoch 1/20  Iteration 1226/35720 Training loss: 2.0794 0.2129 sec/batch\n",
      "Epoch 1/20  Iteration 1227/35720 Training loss: 2.0790 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 1228/35720 Training loss: 2.0786 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 1229/35720 Training loss: 2.0783 0.2183 sec/batch\n",
      "Epoch 1/20  Iteration 1230/35720 Training loss: 2.0780 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 1231/35720 Training loss: 2.0776 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 1232/35720 Training loss: 2.0771 0.2182 sec/batch\n",
      "Epoch 1/20  Iteration 1233/35720 Training loss: 2.0767 0.2208 sec/batch\n",
      "Epoch 1/20  Iteration 1234/35720 Training loss: 2.0763 0.2068 sec/batch\n",
      "Epoch 1/20  Iteration 1235/35720 Training loss: 2.0759 0.2308 sec/batch\n",
      "Epoch 1/20  Iteration 1236/35720 Training loss: 2.0755 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 1237/35720 Training loss: 2.0750 0.2331 sec/batch\n",
      "Epoch 1/20  Iteration 1238/35720 Training loss: 2.0745 0.2185 sec/batch\n",
      "Epoch 1/20  Iteration 1239/35720 Training loss: 2.0741 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 1240/35720 Training loss: 2.0736 0.2212 sec/batch\n",
      "Epoch 1/20  Iteration 1241/35720 Training loss: 2.0732 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 1242/35720 Training loss: 2.0728 0.2155 sec/batch\n",
      "Epoch 1/20  Iteration 1243/35720 Training loss: 2.0724 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 1244/35720 Training loss: 2.0719 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 1245/35720 Training loss: 2.0715 0.2334 sec/batch\n",
      "Epoch 1/20  Iteration 1246/35720 Training loss: 2.0712 0.2072 sec/batch\n",
      "Epoch 1/20  Iteration 1247/35720 Training loss: 2.0708 0.2162 sec/batch\n",
      "Epoch 1/20  Iteration 1248/35720 Training loss: 2.0704 0.2313 sec/batch\n",
      "Epoch 1/20  Iteration 1249/35720 Training loss: 2.0699 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 1250/35720 Training loss: 2.0695 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 1251/35720 Training loss: 2.0691 0.2245 sec/batch\n",
      "Epoch 1/20  Iteration 1252/35720 Training loss: 2.0686 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 1253/35720 Training loss: 2.0682 0.2259 sec/batch\n",
      "Epoch 1/20  Iteration 1254/35720 Training loss: 2.0678 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1255/35720 Training loss: 2.0675 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1256/35720 Training loss: 2.0671 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 1257/35720 Training loss: 2.0667 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 1258/35720 Training loss: 2.0662 0.2358 sec/batch\n",
      "Epoch 1/20  Iteration 1259/35720 Training loss: 2.0659 0.2192 sec/batch\n",
      "Epoch 1/20  Iteration 1260/35720 Training loss: 2.0655 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1261/35720 Training loss: 2.0651 0.2377 sec/batch\n",
      "Epoch 1/20  Iteration 1262/35720 Training loss: 2.0647 0.2319 sec/batch\n",
      "Epoch 1/20  Iteration 1263/35720 Training loss: 2.0643 0.2150 sec/batch\n",
      "Epoch 1/20  Iteration 1264/35720 Training loss: 2.0639 0.2305 sec/batch\n",
      "Epoch 1/20  Iteration 1265/35720 Training loss: 2.0636 0.2226 sec/batch\n",
      "Epoch 1/20  Iteration 1266/35720 Training loss: 2.0631 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 1267/35720 Training loss: 2.0627 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 1268/35720 Training loss: 2.0623 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1269/35720 Training loss: 2.0619 0.2201 sec/batch\n",
      "Epoch 1/20  Iteration 1270/35720 Training loss: 2.0616 0.2170 sec/batch\n",
      "Epoch 1/20  Iteration 1271/35720 Training loss: 2.0612 0.2132 sec/batch\n",
      "Epoch 1/20  Iteration 1272/35720 Training loss: 2.0608 0.2237 sec/batch\n",
      "Epoch 1/20  Iteration 1273/35720 Training loss: 2.0604 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 1274/35720 Training loss: 2.0600 0.2228 sec/batch\n",
      "Epoch 1/20  Iteration 1275/35720 Training loss: 2.0596 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 1276/35720 Training loss: 2.0592 0.2111 sec/batch\n",
      "Epoch 1/20  Iteration 1277/35720 Training loss: 2.0587 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 1278/35720 Training loss: 2.0583 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1279/35720 Training loss: 2.0579 0.2212 sec/batch\n",
      "Epoch 1/20  Iteration 1280/35720 Training loss: 2.0574 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 1281/35720 Training loss: 2.0569 0.2161 sec/batch\n",
      "Epoch 1/20  Iteration 1282/35720 Training loss: 2.0565 0.2181 sec/batch\n",
      "Epoch 1/20  Iteration 1283/35720 Training loss: 2.0560 0.2172 sec/batch\n",
      "Epoch 1/20  Iteration 1284/35720 Training loss: 2.0556 0.2294 sec/batch\n",
      "Epoch 1/20  Iteration 1285/35720 Training loss: 2.0551 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 1286/35720 Training loss: 2.0547 0.2161 sec/batch\n",
      "Epoch 1/20  Iteration 1287/35720 Training loss: 2.0543 0.2124 sec/batch\n",
      "Epoch 1/20  Iteration 1288/35720 Training loss: 2.0538 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 1289/35720 Training loss: 2.0535 0.2388 sec/batch\n",
      "Epoch 1/20  Iteration 1290/35720 Training loss: 2.0531 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 1291/35720 Training loss: 2.0527 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 1292/35720 Training loss: 2.0523 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 1293/35720 Training loss: 2.0519 0.2323 sec/batch\n",
      "Epoch 1/20  Iteration 1294/35720 Training loss: 2.0517 0.2268 sec/batch\n",
      "Epoch 1/20  Iteration 1295/35720 Training loss: 2.0513 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 1296/35720 Training loss: 2.0509 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 1297/35720 Training loss: 2.0506 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 1298/35720 Training loss: 2.0502 0.2100 sec/batch\n",
      "Epoch 1/20  Iteration 1299/35720 Training loss: 2.0497 0.2286 sec/batch\n",
      "Epoch 1/20  Iteration 1300/35720 Training loss: 2.0492 0.2188 sec/batch\n",
      "Epoch 1/20  Iteration 1301/35720 Training loss: 2.0488 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 1302/35720 Training loss: 2.0484 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 1303/35720 Training loss: 2.0480 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 1304/35720 Training loss: 2.0477 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 1305/35720 Training loss: 2.0473 0.2085 sec/batch\n",
      "Epoch 1/20  Iteration 1306/35720 Training loss: 2.0469 0.2063 sec/batch\n",
      "Epoch 1/20  Iteration 1307/35720 Training loss: 2.0465 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 1308/35720 Training loss: 2.0460 0.2162 sec/batch\n",
      "Epoch 1/20  Iteration 1309/35720 Training loss: 2.0456 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 1310/35720 Training loss: 2.0452 0.2308 sec/batch\n",
      "Epoch 1/20  Iteration 1311/35720 Training loss: 2.0448 0.2185 sec/batch\n",
      "Epoch 1/20  Iteration 1312/35720 Training loss: 2.0444 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 1313/35720 Training loss: 2.0440 0.2265 sec/batch\n",
      "Epoch 1/20  Iteration 1314/35720 Training loss: 2.0437 0.2100 sec/batch\n",
      "Epoch 1/20  Iteration 1315/35720 Training loss: 2.0433 0.2317 sec/batch\n",
      "Epoch 1/20  Iteration 1316/35720 Training loss: 2.0430 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 1317/35720 Training loss: 2.0426 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 1318/35720 Training loss: 2.0422 0.2277 sec/batch\n",
      "Epoch 1/20  Iteration 1319/35720 Training loss: 2.0418 0.2202 sec/batch\n",
      "Epoch 1/20  Iteration 1320/35720 Training loss: 2.0413 0.2079 sec/batch\n",
      "Epoch 1/20  Iteration 1321/35720 Training loss: 2.0410 0.2089 sec/batch\n",
      "Epoch 1/20  Iteration 1322/35720 Training loss: 2.0406 0.2237 sec/batch\n",
      "Epoch 1/20  Iteration 1323/35720 Training loss: 2.0402 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 1324/35720 Training loss: 2.0399 0.2345 sec/batch\n",
      "Epoch 1/20  Iteration 1325/35720 Training loss: 2.0395 0.2106 sec/batch\n",
      "Epoch 1/20  Iteration 1326/35720 Training loss: 2.0392 0.2271 sec/batch\n",
      "Epoch 1/20  Iteration 1327/35720 Training loss: 2.0389 0.2075 sec/batch\n",
      "Epoch 1/20  Iteration 1328/35720 Training loss: 2.0385 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 1329/35720 Training loss: 2.0382 0.2212 sec/batch\n",
      "Epoch 1/20  Iteration 1330/35720 Training loss: 2.0378 0.2061 sec/batch\n",
      "Epoch 1/20  Iteration 1331/35720 Training loss: 2.0373 0.2340 sec/batch\n",
      "Epoch 1/20  Iteration 1332/35720 Training loss: 2.0369 0.2325 sec/batch\n",
      "Epoch 1/20  Iteration 1333/35720 Training loss: 2.0366 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1334/35720 Training loss: 2.0362 0.2181 sec/batch\n",
      "Epoch 1/20  Iteration 1335/35720 Training loss: 2.0358 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 1336/35720 Training loss: 2.0356 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 1337/35720 Training loss: 2.0353 0.2251 sec/batch\n",
      "Epoch 1/20  Iteration 1338/35720 Training loss: 2.0349 0.2292 sec/batch\n",
      "Epoch 1/20  Iteration 1339/35720 Training loss: 2.0345 0.2126 sec/batch\n",
      "Epoch 1/20  Iteration 1340/35720 Training loss: 2.0341 0.2232 sec/batch\n",
      "Epoch 1/20  Iteration 1341/35720 Training loss: 2.0337 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 1342/35720 Training loss: 2.0334 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 1343/35720 Training loss: 2.0330 0.2176 sec/batch\n",
      "Epoch 1/20  Iteration 1344/35720 Training loss: 2.0326 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 1345/35720 Training loss: 2.0322 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 1346/35720 Training loss: 2.0319 0.2417 sec/batch\n",
      "Epoch 1/20  Iteration 1347/35720 Training loss: 2.0316 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 1348/35720 Training loss: 2.0313 0.2233 sec/batch\n",
      "Epoch 1/20  Iteration 1349/35720 Training loss: 2.0309 0.2371 sec/batch\n",
      "Epoch 1/20  Iteration 1350/35720 Training loss: 2.0307 0.2072 sec/batch\n",
      "Epoch 1/20  Iteration 1351/35720 Training loss: 2.0303 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 1352/35720 Training loss: 2.0301 0.2275 sec/batch\n",
      "Epoch 1/20  Iteration 1353/35720 Training loss: 2.0298 0.2129 sec/batch\n",
      "Epoch 1/20  Iteration 1354/35720 Training loss: 2.0294 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 1355/35720 Training loss: 2.0290 0.2125 sec/batch\n",
      "Epoch 1/20  Iteration 1356/35720 Training loss: 2.0286 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 1357/35720 Training loss: 2.0282 0.2095 sec/batch\n",
      "Epoch 1/20  Iteration 1358/35720 Training loss: 2.0278 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 1359/35720 Training loss: 2.0274 0.2301 sec/batch\n",
      "Epoch 1/20  Iteration 1360/35720 Training loss: 2.0270 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 1361/35720 Training loss: 2.0266 0.2097 sec/batch\n",
      "Epoch 1/20  Iteration 1362/35720 Training loss: 2.0262 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 1363/35720 Training loss: 2.0260 0.2291 sec/batch\n",
      "Epoch 1/20  Iteration 1364/35720 Training loss: 2.0257 0.2245 sec/batch\n",
      "Epoch 1/20  Iteration 1365/35720 Training loss: 2.0253 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 1366/35720 Training loss: 2.0249 0.2264 sec/batch\n",
      "Epoch 1/20  Iteration 1367/35720 Training loss: 2.0246 0.2074 sec/batch\n",
      "Epoch 1/20  Iteration 1368/35720 Training loss: 2.0243 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 1369/35720 Training loss: 2.0239 0.2123 sec/batch\n",
      "Epoch 1/20  Iteration 1370/35720 Training loss: 2.0236 0.2221 sec/batch\n",
      "Epoch 1/20  Iteration 1371/35720 Training loss: 2.0233 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 1372/35720 Training loss: 2.0230 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 1373/35720 Training loss: 2.0226 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 1374/35720 Training loss: 2.0223 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 1375/35720 Training loss: 2.0219 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1376/35720 Training loss: 2.0216 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 1377/35720 Training loss: 2.0212 0.2183 sec/batch\n",
      "Epoch 1/20  Iteration 1378/35720 Training loss: 2.0209 0.2271 sec/batch\n",
      "Epoch 1/20  Iteration 1379/35720 Training loss: 2.0205 0.2205 sec/batch\n",
      "Epoch 1/20  Iteration 1380/35720 Training loss: 2.0202 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 1381/35720 Training loss: 2.0198 0.2321 sec/batch\n",
      "Epoch 1/20  Iteration 1382/35720 Training loss: 2.0194 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 1383/35720 Training loss: 2.0191 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 1384/35720 Training loss: 2.0187 0.2058 sec/batch\n",
      "Epoch 1/20  Iteration 1385/35720 Training loss: 2.0184 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 1386/35720 Training loss: 2.0181 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 1387/35720 Training loss: 2.0178 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 1388/35720 Training loss: 2.0174 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 1389/35720 Training loss: 2.0171 0.2201 sec/batch\n",
      "Epoch 1/20  Iteration 1390/35720 Training loss: 2.0167 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 1391/35720 Training loss: 2.0163 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 1392/35720 Training loss: 2.0159 0.2409 sec/batch\n",
      "Epoch 1/20  Iteration 1393/35720 Training loss: 2.0156 0.2260 sec/batch\n",
      "Epoch 1/20  Iteration 1394/35720 Training loss: 2.0151 0.2159 sec/batch\n",
      "Epoch 1/20  Iteration 1395/35720 Training loss: 2.0147 0.2097 sec/batch\n",
      "Epoch 1/20  Iteration 1396/35720 Training loss: 2.0143 0.2114 sec/batch\n",
      "Epoch 1/20  Iteration 1397/35720 Training loss: 2.0139 0.2340 sec/batch\n",
      "Epoch 1/20  Iteration 1398/35720 Training loss: 2.0135 0.2199 sec/batch\n",
      "Epoch 1/20  Iteration 1399/35720 Training loss: 2.0131 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 1400/35720 Training loss: 2.0127 0.2238 sec/batch\n",
      "Validation loss: 1.67741 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 1401/35720 Training loss: 2.0124 0.2186 sec/batch\n",
      "Epoch 1/20  Iteration 1402/35720 Training loss: 2.0121 0.2262 sec/batch\n",
      "Epoch 1/20  Iteration 1403/35720 Training loss: 2.0117 0.2390 sec/batch\n",
      "Epoch 1/20  Iteration 1404/35720 Training loss: 2.0114 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 1405/35720 Training loss: 2.0111 0.2284 sec/batch\n",
      "Epoch 1/20  Iteration 1406/35720 Training loss: 2.0107 0.2237 sec/batch\n",
      "Epoch 1/20  Iteration 1407/35720 Training loss: 2.0104 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 1408/35720 Training loss: 2.0102 0.2121 sec/batch\n",
      "Epoch 1/20  Iteration 1409/35720 Training loss: 2.0098 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 1410/35720 Training loss: 2.0095 0.2061 sec/batch\n",
      "Epoch 1/20  Iteration 1411/35720 Training loss: 2.0091 0.2303 sec/batch\n",
      "Epoch 1/20  Iteration 1412/35720 Training loss: 2.0088 0.2325 sec/batch\n",
      "Epoch 1/20  Iteration 1413/35720 Training loss: 2.0085 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 1414/35720 Training loss: 2.0082 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 1415/35720 Training loss: 2.0079 0.2247 sec/batch\n",
      "Epoch 1/20  Iteration 1416/35720 Training loss: 2.0076 0.2281 sec/batch\n",
      "Epoch 1/20  Iteration 1417/35720 Training loss: 2.0073 0.2178 sec/batch\n",
      "Epoch 1/20  Iteration 1418/35720 Training loss: 2.0070 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 1419/35720 Training loss: 2.0067 0.2181 sec/batch\n",
      "Epoch 1/20  Iteration 1420/35720 Training loss: 2.0064 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 1421/35720 Training loss: 2.0061 0.2531 sec/batch\n",
      "Epoch 1/20  Iteration 1422/35720 Training loss: 2.0056 0.2230 sec/batch\n",
      "Epoch 1/20  Iteration 1423/35720 Training loss: 2.0053 0.2257 sec/batch\n",
      "Epoch 1/20  Iteration 1424/35720 Training loss: 2.0050 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 1425/35720 Training loss: 2.0047 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 1426/35720 Training loss: 2.0044 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 1427/35720 Training loss: 2.0041 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 1428/35720 Training loss: 2.0038 0.2191 sec/batch\n",
      "Epoch 1/20  Iteration 1429/35720 Training loss: 2.0035 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 1430/35720 Training loss: 2.0031 0.2246 sec/batch\n",
      "Epoch 1/20  Iteration 1431/35720 Training loss: 2.0027 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 1432/35720 Training loss: 2.0024 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 1433/35720 Training loss: 2.0020 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 1434/35720 Training loss: 2.0016 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 1435/35720 Training loss: 2.0013 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 1436/35720 Training loss: 2.0009 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 1437/35720 Training loss: 2.0005 0.2181 sec/batch\n",
      "Epoch 1/20  Iteration 1438/35720 Training loss: 2.0003 0.2055 sec/batch\n",
      "Epoch 1/20  Iteration 1439/35720 Training loss: 2.0000 0.2226 sec/batch\n",
      "Epoch 1/20  Iteration 1440/35720 Training loss: 1.9997 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 1441/35720 Training loss: 1.9994 0.2170 sec/batch\n",
      "Epoch 1/20  Iteration 1442/35720 Training loss: 1.9991 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 1443/35720 Training loss: 1.9988 0.2289 sec/batch\n",
      "Epoch 1/20  Iteration 1444/35720 Training loss: 1.9984 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 1445/35720 Training loss: 1.9981 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 1446/35720 Training loss: 1.9977 0.2146 sec/batch\n",
      "Epoch 1/20  Iteration 1447/35720 Training loss: 1.9974 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 1448/35720 Training loss: 1.9970 0.2296 sec/batch\n",
      "Epoch 1/20  Iteration 1449/35720 Training loss: 1.9967 0.2317 sec/batch\n",
      "Epoch 1/20  Iteration 1450/35720 Training loss: 1.9963 0.2190 sec/batch\n",
      "Epoch 1/20  Iteration 1451/35720 Training loss: 1.9959 0.2392 sec/batch\n",
      "Epoch 1/20  Iteration 1452/35720 Training loss: 1.9956 0.2253 sec/batch\n",
      "Epoch 1/20  Iteration 1453/35720 Training loss: 1.9952 0.2162 sec/batch\n",
      "Epoch 1/20  Iteration 1454/35720 Training loss: 1.9949 0.2217 sec/batch\n",
      "Epoch 1/20  Iteration 1455/35720 Training loss: 1.9945 0.2275 sec/batch\n",
      "Epoch 1/20  Iteration 1456/35720 Training loss: 1.9942 0.2159 sec/batch\n",
      "Epoch 1/20  Iteration 1457/35720 Training loss: 1.9939 0.2270 sec/batch\n",
      "Epoch 1/20  Iteration 1458/35720 Training loss: 1.9936 0.2289 sec/batch\n",
      "Epoch 1/20  Iteration 1459/35720 Training loss: 1.9933 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 1460/35720 Training loss: 1.9931 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 1461/35720 Training loss: 1.9927 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 1462/35720 Training loss: 1.9924 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 1463/35720 Training loss: 1.9920 0.2183 sec/batch\n",
      "Epoch 1/20  Iteration 1464/35720 Training loss: 1.9917 0.2243 sec/batch\n",
      "Epoch 1/20  Iteration 1465/35720 Training loss: 1.9913 0.2170 sec/batch\n",
      "Epoch 1/20  Iteration 1466/35720 Training loss: 1.9909 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 1467/35720 Training loss: 1.9905 0.2068 sec/batch\n",
      "Epoch 1/20  Iteration 1468/35720 Training loss: 1.9902 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 1469/35720 Training loss: 1.9899 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 1470/35720 Training loss: 1.9895 0.2256 sec/batch\n",
      "Epoch 1/20  Iteration 1471/35720 Training loss: 1.9893 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1472/35720 Training loss: 1.9889 0.2291 sec/batch\n",
      "Epoch 1/20  Iteration 1473/35720 Training loss: 1.9886 0.2137 sec/batch\n",
      "Epoch 1/20  Iteration 1474/35720 Training loss: 1.9882 0.2156 sec/batch\n",
      "Epoch 1/20  Iteration 1475/35720 Training loss: 1.9877 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 1476/35720 Training loss: 1.9873 0.2302 sec/batch\n",
      "Epoch 1/20  Iteration 1477/35720 Training loss: 1.9869 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 1478/35720 Training loss: 1.9865 0.2130 sec/batch\n",
      "Epoch 1/20  Iteration 1479/35720 Training loss: 1.9861 0.2243 sec/batch\n",
      "Epoch 1/20  Iteration 1480/35720 Training loss: 1.9858 0.2303 sec/batch\n",
      "Epoch 1/20  Iteration 1481/35720 Training loss: 1.9855 0.2233 sec/batch\n",
      "Epoch 1/20  Iteration 1482/35720 Training loss: 1.9850 0.2083 sec/batch\n",
      "Epoch 1/20  Iteration 1483/35720 Training loss: 1.9847 0.2188 sec/batch\n",
      "Epoch 1/20  Iteration 1484/35720 Training loss: 1.9843 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 1485/35720 Training loss: 1.9839 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 1486/35720 Training loss: 1.9836 0.2266 sec/batch\n",
      "Epoch 1/20  Iteration 1487/35720 Training loss: 1.9833 0.2175 sec/batch\n",
      "Epoch 1/20  Iteration 1488/35720 Training loss: 1.9829 0.2088 sec/batch\n",
      "Epoch 1/20  Iteration 1489/35720 Training loss: 1.9826 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 1490/35720 Training loss: 1.9824 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 1491/35720 Training loss: 1.9821 0.2164 sec/batch\n",
      "Epoch 1/20  Iteration 1492/35720 Training loss: 1.9817 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 1493/35720 Training loss: 1.9814 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 1494/35720 Training loss: 1.9810 0.2053 sec/batch\n",
      "Epoch 1/20  Iteration 1495/35720 Training loss: 1.9807 0.2275 sec/batch\n",
      "Epoch 1/20  Iteration 1496/35720 Training loss: 1.9804 0.2220 sec/batch\n",
      "Epoch 1/20  Iteration 1497/35720 Training loss: 1.9800 0.2154 sec/batch\n",
      "Epoch 1/20  Iteration 1498/35720 Training loss: 1.9798 0.2369 sec/batch\n",
      "Epoch 1/20  Iteration 1499/35720 Training loss: 1.9795 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 1500/35720 Training loss: 1.9791 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 1501/35720 Training loss: 1.9787 0.2378 sec/batch\n",
      "Epoch 1/20  Iteration 1502/35720 Training loss: 1.9784 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 1503/35720 Training loss: 1.9781 0.2332 sec/batch\n",
      "Epoch 1/20  Iteration 1504/35720 Training loss: 1.9778 0.2413 sec/batch\n",
      "Epoch 1/20  Iteration 1505/35720 Training loss: 1.9775 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 1506/35720 Training loss: 1.9772 0.2208 sec/batch\n",
      "Epoch 1/20  Iteration 1507/35720 Training loss: 1.9769 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 1508/35720 Training loss: 1.9766 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 1509/35720 Training loss: 1.9763 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 1510/35720 Training loss: 1.9760 0.2173 sec/batch\n",
      "Epoch 1/20  Iteration 1511/35720 Training loss: 1.9758 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 1512/35720 Training loss: 1.9754 0.2253 sec/batch\n",
      "Epoch 1/20  Iteration 1513/35720 Training loss: 1.9751 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 1514/35720 Training loss: 1.9748 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 1515/35720 Training loss: 1.9745 0.2119 sec/batch\n",
      "Epoch 1/20  Iteration 1516/35720 Training loss: 1.9742 0.2138 sec/batch\n",
      "Epoch 1/20  Iteration 1517/35720 Training loss: 1.9740 0.2123 sec/batch\n",
      "Epoch 1/20  Iteration 1518/35720 Training loss: 1.9736 0.2147 sec/batch\n",
      "Epoch 1/20  Iteration 1519/35720 Training loss: 1.9734 0.2250 sec/batch\n",
      "Epoch 1/20  Iteration 1520/35720 Training loss: 1.9731 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 1521/35720 Training loss: 1.9727 0.2122 sec/batch\n",
      "Epoch 1/20  Iteration 1522/35720 Training loss: 1.9724 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 1523/35720 Training loss: 1.9721 0.2193 sec/batch\n",
      "Epoch 1/20  Iteration 1524/35720 Training loss: 1.9719 0.2071 sec/batch\n",
      "Epoch 1/20  Iteration 1525/35720 Training loss: 1.9716 0.2259 sec/batch\n",
      "Epoch 1/20  Iteration 1526/35720 Training loss: 1.9714 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 1527/35720 Training loss: 1.9712 0.2151 sec/batch\n",
      "Epoch 1/20  Iteration 1528/35720 Training loss: 1.9708 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 1529/35720 Training loss: 1.9706 0.2184 sec/batch\n",
      "Epoch 1/20  Iteration 1530/35720 Training loss: 1.9703 0.2203 sec/batch\n",
      "Epoch 1/20  Iteration 1531/35720 Training loss: 1.9700 0.2223 sec/batch\n",
      "Epoch 1/20  Iteration 1532/35720 Training loss: 1.9697 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 1533/35720 Training loss: 1.9694 0.2242 sec/batch\n",
      "Epoch 1/20  Iteration 1534/35720 Training loss: 1.9691 0.2178 sec/batch\n",
      "Epoch 1/20  Iteration 1535/35720 Training loss: 1.9688 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 1536/35720 Training loss: 1.9685 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 1537/35720 Training loss: 1.9681 0.2347 sec/batch\n",
      "Epoch 1/20  Iteration 1538/35720 Training loss: 1.9678 0.2299 sec/batch\n",
      "Epoch 1/20  Iteration 1539/35720 Training loss: 1.9675 0.2155 sec/batch\n",
      "Epoch 1/20  Iteration 1540/35720 Training loss: 1.9672 0.2227 sec/batch\n",
      "Epoch 1/20  Iteration 1541/35720 Training loss: 1.9668 0.2130 sec/batch\n",
      "Epoch 1/20  Iteration 1542/35720 Training loss: 1.9665 0.2301 sec/batch\n",
      "Epoch 1/20  Iteration 1543/35720 Training loss: 1.9662 0.2183 sec/batch\n",
      "Epoch 1/20  Iteration 1544/35720 Training loss: 1.9658 0.2189 sec/batch\n",
      "Epoch 1/20  Iteration 1545/35720 Training loss: 1.9655 0.2197 sec/batch\n",
      "Epoch 1/20  Iteration 1546/35720 Training loss: 1.9651 0.2212 sec/batch\n",
      "Epoch 1/20  Iteration 1547/35720 Training loss: 1.9648 0.2117 sec/batch\n",
      "Epoch 1/20  Iteration 1548/35720 Training loss: 1.9645 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 1549/35720 Training loss: 1.9643 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 1550/35720 Training loss: 1.9639 0.2056 sec/batch\n",
      "Epoch 1/20  Iteration 1551/35720 Training loss: 1.9636 0.2346 sec/batch\n",
      "Epoch 1/20  Iteration 1552/35720 Training loss: 1.9632 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 1553/35720 Training loss: 1.9628 0.2194 sec/batch\n",
      "Epoch 1/20  Iteration 1554/35720 Training loss: 1.9625 0.2337 sec/batch\n",
      "Epoch 1/20  Iteration 1555/35720 Training loss: 1.9621 0.2294 sec/batch\n",
      "Epoch 1/20  Iteration 1556/35720 Training loss: 1.9618 0.2090 sec/batch\n",
      "Epoch 1/20  Iteration 1557/35720 Training loss: 1.9615 0.2082 sec/batch\n",
      "Epoch 1/20  Iteration 1558/35720 Training loss: 1.9611 0.2066 sec/batch\n",
      "Epoch 1/20  Iteration 1559/35720 Training loss: 1.9608 0.2225 sec/batch\n",
      "Epoch 1/20  Iteration 1560/35720 Training loss: 1.9604 0.2251 sec/batch\n",
      "Epoch 1/20  Iteration 1561/35720 Training loss: 1.9601 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 1562/35720 Training loss: 1.9598 0.2265 sec/batch\n",
      "Epoch 1/20  Iteration 1563/35720 Training loss: 1.9594 0.2153 sec/batch\n",
      "Epoch 1/20  Iteration 1564/35720 Training loss: 1.9591 0.2296 sec/batch\n",
      "Epoch 1/20  Iteration 1565/35720 Training loss: 1.9587 0.2256 sec/batch\n",
      "Epoch 1/20  Iteration 1566/35720 Training loss: 1.9584 0.2079 sec/batch\n",
      "Epoch 1/20  Iteration 1567/35720 Training loss: 1.9581 0.2101 sec/batch\n",
      "Epoch 1/20  Iteration 1568/35720 Training loss: 1.9578 0.2340 sec/batch\n",
      "Epoch 1/20  Iteration 1569/35720 Training loss: 1.9575 0.2165 sec/batch\n",
      "Epoch 1/20  Iteration 1570/35720 Training loss: 1.9572 0.2263 sec/batch\n",
      "Epoch 1/20  Iteration 1571/35720 Training loss: 1.9569 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 1572/35720 Training loss: 1.9565 0.2062 sec/batch\n",
      "Epoch 1/20  Iteration 1573/35720 Training loss: 1.9562 0.2236 sec/batch\n",
      "Epoch 1/20  Iteration 1574/35720 Training loss: 1.9559 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 1575/35720 Training loss: 1.9555 0.2255 sec/batch\n",
      "Epoch 1/20  Iteration 1576/35720 Training loss: 1.9552 0.2126 sec/batch\n",
      "Epoch 1/20  Iteration 1577/35720 Training loss: 1.9548 0.2201 sec/batch\n",
      "Epoch 1/20  Iteration 1578/35720 Training loss: 1.9545 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 1579/35720 Training loss: 1.9542 0.2182 sec/batch\n",
      "Epoch 1/20  Iteration 1580/35720 Training loss: 1.9538 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 1581/35720 Training loss: 1.9535 0.2144 sec/batch\n",
      "Epoch 1/20  Iteration 1582/35720 Training loss: 1.9532 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 1583/35720 Training loss: 1.9528 0.2176 sec/batch\n",
      "Epoch 1/20  Iteration 1584/35720 Training loss: 1.9525 0.2233 sec/batch\n",
      "Epoch 1/20  Iteration 1585/35720 Training loss: 1.9522 0.2120 sec/batch\n",
      "Epoch 1/20  Iteration 1586/35720 Training loss: 1.9518 0.2181 sec/batch\n",
      "Epoch 1/20  Iteration 1587/35720 Training loss: 1.9515 0.2224 sec/batch\n",
      "Epoch 1/20  Iteration 1588/35720 Training loss: 1.9511 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 1589/35720 Training loss: 1.9508 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 1590/35720 Training loss: 1.9505 0.2333 sec/batch\n",
      "Epoch 1/20  Iteration 1591/35720 Training loss: 1.9502 0.2221 sec/batch\n",
      "Epoch 1/20  Iteration 1592/35720 Training loss: 1.9498 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 1593/35720 Training loss: 1.9496 0.2108 sec/batch\n",
      "Epoch 1/20  Iteration 1594/35720 Training loss: 1.9493 0.2158 sec/batch\n",
      "Epoch 1/20  Iteration 1595/35720 Training loss: 1.9489 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 1596/35720 Training loss: 1.9485 0.2308 sec/batch\n",
      "Epoch 1/20  Iteration 1597/35720 Training loss: 1.9482 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 1598/35720 Training loss: 1.9479 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 1599/35720 Training loss: 1.9476 0.2223 sec/batch\n",
      "Epoch 1/20  Iteration 1600/35720 Training loss: 1.9474 0.2087 sec/batch\n",
      "Validation loss: 1.63827 Saving checkpoint!\n",
      "Epoch 1/20  Iteration 1601/35720 Training loss: 1.9471 0.2135 sec/batch\n",
      "Epoch 1/20  Iteration 1602/35720 Training loss: 1.9468 0.2086 sec/batch\n",
      "Epoch 1/20  Iteration 1603/35720 Training loss: 1.9465 0.2089 sec/batch\n",
      "Epoch 1/20  Iteration 1604/35720 Training loss: 1.9462 0.2256 sec/batch\n",
      "Epoch 1/20  Iteration 1605/35720 Training loss: 1.9458 0.2240 sec/batch\n",
      "Epoch 1/20  Iteration 1606/35720 Training loss: 1.9456 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 1607/35720 Training loss: 1.9453 0.2212 sec/batch\n",
      "Epoch 1/20  Iteration 1608/35720 Training loss: 1.9450 0.2222 sec/batch\n",
      "Epoch 1/20  Iteration 1609/35720 Training loss: 1.9447 0.2069 sec/batch\n",
      "Epoch 1/20  Iteration 1610/35720 Training loss: 1.9443 0.2063 sec/batch\n",
      "Epoch 1/20  Iteration 1611/35720 Training loss: 1.9440 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1612/35720 Training loss: 1.9437 0.2297 sec/batch\n",
      "Epoch 1/20  Iteration 1613/35720 Training loss: 1.9433 0.2238 sec/batch\n",
      "Epoch 1/20  Iteration 1614/35720 Training loss: 1.9430 0.2100 sec/batch\n",
      "Epoch 1/20  Iteration 1615/35720 Training loss: 1.9427 0.2116 sec/batch\n",
      "Epoch 1/20  Iteration 1616/35720 Training loss: 1.9424 0.2208 sec/batch\n",
      "Epoch 1/20  Iteration 1617/35720 Training loss: 1.9421 0.2236 sec/batch\n",
      "Epoch 1/20  Iteration 1618/35720 Training loss: 1.9418 0.2093 sec/batch\n",
      "Epoch 1/20  Iteration 1619/35720 Training loss: 1.9414 0.2244 sec/batch\n",
      "Epoch 1/20  Iteration 1620/35720 Training loss: 1.9411 0.2216 sec/batch\n",
      "Epoch 1/20  Iteration 1621/35720 Training loss: 1.9408 0.2111 sec/batch\n",
      "Epoch 1/20  Iteration 1622/35720 Training loss: 1.9405 0.2110 sec/batch\n",
      "Epoch 1/20  Iteration 1623/35720 Training loss: 1.9402 0.2113 sec/batch\n",
      "Epoch 1/20  Iteration 1624/35720 Training loss: 1.9400 0.2111 sec/batch\n",
      "Epoch 1/20  Iteration 1625/35720 Training loss: 1.9397 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 1626/35720 Training loss: 1.9395 0.2163 sec/batch\n",
      "Epoch 1/20  Iteration 1627/35720 Training loss: 1.9393 0.2136 sec/batch\n",
      "Epoch 1/20  Iteration 1628/35720 Training loss: 1.9390 0.2176 sec/batch\n",
      "Epoch 1/20  Iteration 1629/35720 Training loss: 1.9386 0.2107 sec/batch\n",
      "Epoch 1/20  Iteration 1630/35720 Training loss: 1.9384 0.2254 sec/batch\n",
      "Epoch 1/20  Iteration 1631/35720 Training loss: 1.9381 0.2278 sec/batch\n",
      "Epoch 1/20  Iteration 1632/35720 Training loss: 1.9377 0.2129 sec/batch\n",
      "Epoch 1/20  Iteration 1633/35720 Training loss: 1.9374 0.2129 sec/batch\n",
      "Epoch 1/20  Iteration 1634/35720 Training loss: 1.9371 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 1635/35720 Training loss: 1.9368 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 1636/35720 Training loss: 1.9365 0.2322 sec/batch\n",
      "Epoch 1/20  Iteration 1637/35720 Training loss: 1.9362 0.2078 sec/batch\n",
      "Epoch 1/20  Iteration 1638/35720 Training loss: 1.9359 0.2115 sec/batch\n",
      "Epoch 1/20  Iteration 1639/35720 Training loss: 1.9356 0.2133 sec/batch\n",
      "Epoch 1/20  Iteration 1640/35720 Training loss: 1.9352 0.2160 sec/batch\n",
      "Epoch 1/20  Iteration 1641/35720 Training loss: 1.9349 0.2797 sec/batch\n",
      "Epoch 1/20  Iteration 1642/35720 Training loss: 1.9347 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 1643/35720 Training loss: 1.9343 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 1644/35720 Training loss: 1.9340 0.2136 sec/batch\n",
      "Epoch 1/20  Iteration 1645/35720 Training loss: 1.9337 0.2162 sec/batch\n",
      "Epoch 1/20  Iteration 1646/35720 Training loss: 1.9335 0.2069 sec/batch\n",
      "Epoch 1/20  Iteration 1647/35720 Training loss: 1.9333 0.2197 sec/batch\n",
      "Epoch 1/20  Iteration 1648/35720 Training loss: 1.9330 0.2072 sec/batch\n",
      "Epoch 1/20  Iteration 1649/35720 Training loss: 1.9328 0.2210 sec/batch\n",
      "Epoch 1/20  Iteration 1650/35720 Training loss: 1.9325 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 1651/35720 Training loss: 1.9323 0.2359 sec/batch\n",
      "Epoch 1/20  Iteration 1652/35720 Training loss: 1.9320 0.2161 sec/batch\n",
      "Epoch 1/20  Iteration 1653/35720 Training loss: 1.9318 0.2268 sec/batch\n",
      "Epoch 1/20  Iteration 1654/35720 Training loss: 1.9315 0.2263 sec/batch\n",
      "Epoch 1/20  Iteration 1655/35720 Training loss: 1.9312 0.2284 sec/batch\n",
      "Epoch 1/20  Iteration 1656/35720 Training loss: 1.9309 0.2168 sec/batch\n",
      "Epoch 1/20  Iteration 1657/35720 Training loss: 1.9306 0.2200 sec/batch\n",
      "Epoch 1/20  Iteration 1658/35720 Training loss: 1.9304 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1659/35720 Training loss: 1.9301 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 1660/35720 Training loss: 1.9299 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 1661/35720 Training loss: 1.9295 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 1662/35720 Training loss: 1.9292 0.2174 sec/batch\n",
      "Epoch 1/20  Iteration 1663/35720 Training loss: 1.9290 0.2088 sec/batch\n",
      "Epoch 1/20  Iteration 1664/35720 Training loss: 1.9287 0.2102 sec/batch\n",
      "Epoch 1/20  Iteration 1665/35720 Training loss: 1.9285 0.2180 sec/batch\n",
      "Epoch 1/20  Iteration 1666/35720 Training loss: 1.9282 0.2203 sec/batch\n",
      "Epoch 1/20  Iteration 1667/35720 Training loss: 1.9279 0.2320 sec/batch\n",
      "Epoch 1/20  Iteration 1668/35720 Training loss: 1.9277 0.2062 sec/batch\n",
      "Epoch 1/20  Iteration 1669/35720 Training loss: 1.9275 0.2085 sec/batch\n",
      "Epoch 1/20  Iteration 1670/35720 Training loss: 1.9272 0.2212 sec/batch\n",
      "Epoch 1/20  Iteration 1671/35720 Training loss: 1.9269 0.2261 sec/batch\n",
      "Epoch 1/20  Iteration 1672/35720 Training loss: 1.9267 0.2187 sec/batch\n",
      "Epoch 1/20  Iteration 1673/35720 Training loss: 1.9264 0.2293 sec/batch\n",
      "Epoch 1/20  Iteration 1674/35720 Training loss: 1.9262 0.2077 sec/batch\n",
      "Epoch 1/20  Iteration 1675/35720 Training loss: 1.9259 0.2214 sec/batch\n",
      "Epoch 1/20  Iteration 1676/35720 Training loss: 1.9257 0.2218 sec/batch\n",
      "Epoch 1/20  Iteration 1677/35720 Training loss: 1.9253 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 1678/35720 Training loss: 1.9250 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 1679/35720 Training loss: 1.9248 0.2232 sec/batch\n",
      "Epoch 1/20  Iteration 1680/35720 Training loss: 1.9245 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 1681/35720 Training loss: 1.9242 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 1682/35720 Training loss: 1.9239 0.2204 sec/batch\n",
      "Epoch 1/20  Iteration 1683/35720 Training loss: 1.9236 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 1684/35720 Training loss: 1.9233 0.2071 sec/batch\n",
      "Epoch 1/20  Iteration 1685/35720 Training loss: 1.9231 0.2239 sec/batch\n",
      "Epoch 1/20  Iteration 1686/35720 Training loss: 1.9228 0.2167 sec/batch\n",
      "Epoch 1/20  Iteration 1687/35720 Training loss: 1.9225 0.2295 sec/batch\n",
      "Epoch 1/20  Iteration 1688/35720 Training loss: 1.9223 0.2092 sec/batch\n",
      "Epoch 1/20  Iteration 1689/35720 Training loss: 1.9220 0.2081 sec/batch\n",
      "Epoch 1/20  Iteration 1690/35720 Training loss: 1.9217 0.2094 sec/batch\n",
      "Epoch 1/20  Iteration 1691/35720 Training loss: 1.9213 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 1692/35720 Training loss: 1.9210 0.2285 sec/batch\n",
      "Epoch 1/20  Iteration 1693/35720 Training loss: 1.9207 0.2096 sec/batch\n",
      "Epoch 1/20  Iteration 1694/35720 Training loss: 1.9204 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 1695/35720 Training loss: 1.9201 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 1696/35720 Training loss: 1.9198 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 1697/35720 Training loss: 1.9195 0.2223 sec/batch\n",
      "Epoch 1/20  Iteration 1698/35720 Training loss: 1.9192 0.2145 sec/batch\n",
      "Epoch 1/20  Iteration 1699/35720 Training loss: 1.9189 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1700/35720 Training loss: 1.9187 0.2378 sec/batch\n",
      "Epoch 1/20  Iteration 1701/35720 Training loss: 1.9184 0.2208 sec/batch\n",
      "Epoch 1/20  Iteration 1702/35720 Training loss: 1.9181 0.2157 sec/batch\n",
      "Epoch 1/20  Iteration 1703/35720 Training loss: 1.9178 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 1704/35720 Training loss: 1.9174 0.2383 sec/batch\n",
      "Epoch 1/20  Iteration 1705/35720 Training loss: 1.9171 0.2068 sec/batch\n",
      "Epoch 1/20  Iteration 1706/35720 Training loss: 1.9168 0.2274 sec/batch\n",
      "Epoch 1/20  Iteration 1707/35720 Training loss: 1.9165 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1708/35720 Training loss: 1.9162 0.2198 sec/batch\n",
      "Epoch 1/20  Iteration 1709/35720 Training loss: 1.9160 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 1710/35720 Training loss: 1.9157 0.2072 sec/batch\n",
      "Epoch 1/20  Iteration 1711/35720 Training loss: 1.9155 0.2140 sec/batch\n",
      "Epoch 1/20  Iteration 1712/35720 Training loss: 1.9152 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 1713/35720 Training loss: 1.9150 0.2088 sec/batch\n",
      "Epoch 1/20  Iteration 1714/35720 Training loss: 1.9147 0.2587 sec/batch\n",
      "Epoch 1/20  Iteration 1715/35720 Training loss: 1.9144 0.2073 sec/batch\n",
      "Epoch 1/20  Iteration 1716/35720 Training loss: 1.9142 0.2141 sec/batch\n",
      "Epoch 1/20  Iteration 1717/35720 Training loss: 1.9139 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 1718/35720 Training loss: 1.9136 0.2103 sec/batch\n",
      "Epoch 1/20  Iteration 1719/35720 Training loss: 1.9133 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 1720/35720 Training loss: 1.9131 0.2304 sec/batch\n",
      "Epoch 1/20  Iteration 1721/35720 Training loss: 1.9128 0.2087 sec/batch\n",
      "Epoch 1/20  Iteration 1722/35720 Training loss: 1.9125 0.2105 sec/batch\n",
      "Epoch 1/20  Iteration 1723/35720 Training loss: 1.9123 0.2236 sec/batch\n",
      "Epoch 1/20  Iteration 1724/35720 Training loss: 1.9119 0.2185 sec/batch\n",
      "Epoch 1/20  Iteration 1725/35720 Training loss: 1.9116 0.2192 sec/batch\n",
      "Epoch 1/20  Iteration 1726/35720 Training loss: 1.9114 0.2195 sec/batch\n",
      "Epoch 1/20  Iteration 1727/35720 Training loss: 1.9110 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 1728/35720 Training loss: 1.9108 0.2191 sec/batch\n",
      "Epoch 1/20  Iteration 1729/35720 Training loss: 1.9104 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1730/35720 Training loss: 1.9101 0.2264 sec/batch\n",
      "Epoch 1/20  Iteration 1731/35720 Training loss: 1.9099 0.2329 sec/batch\n",
      "Epoch 1/20  Iteration 1732/35720 Training loss: 1.9096 0.2066 sec/batch\n",
      "Epoch 1/20  Iteration 1733/35720 Training loss: 1.9093 0.2146 sec/batch\n",
      "Epoch 1/20  Iteration 1734/35720 Training loss: 1.9089 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 1735/35720 Training loss: 1.9087 0.2237 sec/batch\n",
      "Epoch 1/20  Iteration 1736/35720 Training loss: 1.9084 0.2091 sec/batch\n",
      "Epoch 1/20  Iteration 1737/35720 Training loss: 1.9082 0.2206 sec/batch\n",
      "Epoch 1/20  Iteration 1738/35720 Training loss: 1.9079 0.2182 sec/batch\n",
      "Epoch 1/20  Iteration 1739/35720 Training loss: 1.9077 0.2415 sec/batch\n",
      "Epoch 1/20  Iteration 1740/35720 Training loss: 1.9074 0.2266 sec/batch\n",
      "Epoch 1/20  Iteration 1741/35720 Training loss: 1.9071 0.2112 sec/batch\n",
      "Epoch 1/20  Iteration 1742/35720 Training loss: 1.9069 0.2177 sec/batch\n",
      "Epoch 1/20  Iteration 1743/35720 Training loss: 1.9066 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 1744/35720 Training loss: 1.9064 0.2099 sec/batch\n",
      "Epoch 1/20  Iteration 1745/35720 Training loss: 1.9062 0.2127 sec/batch\n",
      "Epoch 1/20  Iteration 1746/35720 Training loss: 1.9060 0.2213 sec/batch\n",
      "Epoch 1/20  Iteration 1747/35720 Training loss: 1.9058 0.2221 sec/batch\n",
      "Epoch 1/20  Iteration 1748/35720 Training loss: 1.9055 0.2309 sec/batch\n",
      "Epoch 1/20  Iteration 1749/35720 Training loss: 1.9052 0.2179 sec/batch\n",
      "Epoch 1/20  Iteration 1750/35720 Training loss: 1.9049 0.2215 sec/batch\n",
      "Epoch 1/20  Iteration 1751/35720 Training loss: 1.9046 0.2098 sec/batch\n",
      "Epoch 1/20  Iteration 1752/35720 Training loss: 1.9044 0.2209 sec/batch\n",
      "Epoch 1/20  Iteration 1753/35720 Training loss: 1.9041 0.2514 sec/batch\n",
      "Epoch 1/20  Iteration 1754/35720 Training loss: 1.9038 0.2155 sec/batch\n",
      "Epoch 1/20  Iteration 1755/35720 Training loss: 1.9035 0.2175 sec/batch\n",
      "Epoch 1/20  Iteration 1756/35720 Training loss: 1.9033 0.2245 sec/batch\n",
      "Epoch 1/20  Iteration 1757/35720 Training loss: 1.9030 0.2393 sec/batch\n",
      "Epoch 1/20  Iteration 1758/35720 Training loss: 1.9026 0.2198 sec/batch\n",
      "Epoch 1/20  Iteration 1759/35720 Training loss: 1.9024 0.2243 sec/batch\n",
      "Epoch 1/20  Iteration 1760/35720 Training loss: 1.9021 0.2186 sec/batch\n",
      "Epoch 1/20  Iteration 1761/35720 Training loss: 1.9019 0.2211 sec/batch\n",
      "Epoch 1/20  Iteration 1762/35720 Training loss: 1.9016 0.2078 sec/batch\n",
      "Epoch 1/20  Iteration 1763/35720 Training loss: 1.9013 0.2224 sec/batch\n",
      "Epoch 1/20  Iteration 1764/35720 Training loss: 1.9010 0.2190 sec/batch\n",
      "Epoch 1/20  Iteration 1765/35720 Training loss: 1.9007 0.2132 sec/batch\n",
      "Epoch 1/20  Iteration 1766/35720 Training loss: 1.9004 0.2086 sec/batch\n",
      "Epoch 1/20  Iteration 1767/35720 Training loss: 1.9002 0.2152 sec/batch\n",
      "Epoch 1/20  Iteration 1768/35720 Training loss: 1.8999 0.2070 sec/batch\n",
      "Epoch 1/20  Iteration 1769/35720 Training loss: 1.8996 0.2234 sec/batch\n",
      "Epoch 1/20  Iteration 1770/35720 Training loss: 1.8994 0.2225 sec/batch\n",
      "Epoch 1/20  Iteration 1771/35720 Training loss: 1.8992 0.2131 sec/batch\n",
      "Epoch 1/20  Iteration 1772/35720 Training loss: 1.8989 0.2241 sec/batch\n",
      "Epoch 1/20  Iteration 1773/35720 Training loss: 1.8985 0.2071 sec/batch\n",
      "Epoch 1/20  Iteration 1774/35720 Training loss: 1.8982 0.2197 sec/batch\n",
      "Epoch 1/20  Iteration 1775/35720 Training loss: 1.8980 0.2118 sec/batch\n",
      "Epoch 1/20  Iteration 1776/35720 Training loss: 1.8977 0.2225 sec/batch\n",
      "Epoch 1/20  Iteration 1777/35720 Training loss: 1.8975 0.2171 sec/batch\n",
      "Epoch 1/20  Iteration 1778/35720 Training loss: 1.8971 0.2192 sec/batch\n",
      "Epoch 1/20  Iteration 1779/35720 Training loss: 1.8968 0.2076 sec/batch\n",
      "Epoch 1/20  Iteration 1780/35720 Training loss: 1.8965 0.2104 sec/batch\n",
      "Epoch 1/20  Iteration 1781/35720 Training loss: 1.8962 0.2196 sec/batch\n",
      "Epoch 1/20  Iteration 1782/35720 Training loss: 1.8959 0.2149 sec/batch\n",
      "Epoch 1/20  Iteration 1783/35720 Training loss: 1.8957 0.2189 sec/batch\n",
      "Epoch 1/20  Iteration 1784/35720 Training loss: 1.8954 0.2207 sec/batch\n",
      "Epoch 1/20  Iteration 1785/35720 Training loss: 1.8951 0.2326 sec/batch\n",
      "Epoch 1/20  Iteration 1786/35720 Training loss: 1.8949 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 1787/35720 Training loss: 1.4370 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 1788/35720 Training loss: 1.4086 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 1789/35720 Training loss: 1.4197 0.2322 sec/batch\n",
      "Epoch 2/20  Iteration 1790/35720 Training loss: 1.4124 0.2307 sec/batch\n",
      "Epoch 2/20  Iteration 1791/35720 Training loss: 1.4306 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 1792/35720 Training loss: 1.4125 0.2142 sec/batch\n",
      "Epoch 2/20  Iteration 1793/35720 Training loss: 1.4094 0.2109 sec/batch\n",
      "Epoch 2/20  Iteration 1794/35720 Training loss: 1.3953 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 1795/35720 Training loss: 1.3885 0.2227 sec/batch\n",
      "Epoch 2/20  Iteration 1796/35720 Training loss: 1.3969 0.2150 sec/batch\n",
      "Epoch 2/20  Iteration 1797/35720 Training loss: 1.4002 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 1798/35720 Training loss: 1.3986 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 1799/35720 Training loss: 1.4004 0.2214 sec/batch\n",
      "Epoch 2/20  Iteration 1800/35720 Training loss: 1.4148 0.2079 sec/batch\n",
      "Validation loss: 1.57432 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 1801/35720 Training loss: 1.4285 0.2090 sec/batch\n",
      "Epoch 2/20  Iteration 1802/35720 Training loss: 1.4347 0.2107 sec/batch\n",
      "Epoch 2/20  Iteration 1803/35720 Training loss: 1.4364 0.2119 sec/batch\n",
      "Epoch 2/20  Iteration 1804/35720 Training loss: 1.4324 0.2113 sec/batch\n",
      "Epoch 2/20  Iteration 1805/35720 Training loss: 1.4344 0.2254 sec/batch\n",
      "Epoch 2/20  Iteration 1806/35720 Training loss: 1.4386 0.2127 sec/batch\n",
      "Epoch 2/20  Iteration 1807/35720 Training loss: 1.4426 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 1808/35720 Training loss: 1.4404 0.2220 sec/batch\n",
      "Epoch 2/20  Iteration 1809/35720 Training loss: 1.4430 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 1810/35720 Training loss: 1.4447 0.2202 sec/batch\n",
      "Epoch 2/20  Iteration 1811/35720 Training loss: 1.4510 0.2145 sec/batch\n",
      "Epoch 2/20  Iteration 1812/35720 Training loss: 1.4509 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 1813/35720 Training loss: 1.4569 0.2117 sec/batch\n",
      "Epoch 2/20  Iteration 1814/35720 Training loss: 1.4577 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 1815/35720 Training loss: 1.4575 0.2129 sec/batch\n",
      "Epoch 2/20  Iteration 1816/35720 Training loss: 1.4566 0.2122 sec/batch\n",
      "Epoch 2/20  Iteration 1817/35720 Training loss: 1.4643 0.2063 sec/batch\n",
      "Epoch 2/20  Iteration 1818/35720 Training loss: 1.4630 0.2081 sec/batch\n",
      "Epoch 2/20  Iteration 1819/35720 Training loss: 1.4661 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 1820/35720 Training loss: 1.4696 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 1821/35720 Training loss: 1.4723 0.2254 sec/batch\n",
      "Epoch 2/20  Iteration 1822/35720 Training loss: 1.4721 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 1823/35720 Training loss: 1.4714 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 1824/35720 Training loss: 1.4683 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 1825/35720 Training loss: 1.4652 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 1826/35720 Training loss: 1.4645 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 1827/35720 Training loss: 1.4621 0.2189 sec/batch\n",
      "Epoch 2/20  Iteration 1828/35720 Training loss: 1.4609 0.2243 sec/batch\n",
      "Epoch 2/20  Iteration 1829/35720 Training loss: 1.4567 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 1830/35720 Training loss: 1.4550 0.2116 sec/batch\n",
      "Epoch 2/20  Iteration 1831/35720 Training loss: 1.4534 0.2071 sec/batch\n",
      "Epoch 2/20  Iteration 1832/35720 Training loss: 1.4514 0.2097 sec/batch\n",
      "Epoch 2/20  Iteration 1833/35720 Training loss: 1.4509 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 1834/35720 Training loss: 1.4497 0.2268 sec/batch\n",
      "Epoch 2/20  Iteration 1835/35720 Training loss: 1.4507 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 1836/35720 Training loss: 1.4497 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 1837/35720 Training loss: 1.4504 0.2156 sec/batch\n",
      "Epoch 2/20  Iteration 1838/35720 Training loss: 1.4487 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 1839/35720 Training loss: 1.4480 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 1840/35720 Training loss: 1.4461 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 1841/35720 Training loss: 1.4451 0.2308 sec/batch\n",
      "Epoch 2/20  Iteration 1842/35720 Training loss: 1.4437 0.2109 sec/batch\n",
      "Epoch 2/20  Iteration 1843/35720 Training loss: 1.4437 0.2312 sec/batch\n",
      "Epoch 2/20  Iteration 1844/35720 Training loss: 1.4430 0.2068 sec/batch\n",
      "Epoch 2/20  Iteration 1845/35720 Training loss: 1.4402 0.2176 sec/batch\n",
      "Epoch 2/20  Iteration 1846/35720 Training loss: 1.4386 0.2268 sec/batch\n",
      "Epoch 2/20  Iteration 1847/35720 Training loss: 1.4369 0.2160 sec/batch\n",
      "Epoch 2/20  Iteration 1848/35720 Training loss: 1.4352 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 1849/35720 Training loss: 1.4355 0.2289 sec/batch\n",
      "Epoch 2/20  Iteration 1850/35720 Training loss: 1.4347 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 1851/35720 Training loss: 1.4357 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 1852/35720 Training loss: 1.4359 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 1853/35720 Training loss: 1.4358 0.2299 sec/batch\n",
      "Epoch 2/20  Iteration 1854/35720 Training loss: 1.4346 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 1855/35720 Training loss: 1.4346 0.2138 sec/batch\n",
      "Epoch 2/20  Iteration 1856/35720 Training loss: 1.4342 0.2147 sec/batch\n",
      "Epoch 2/20  Iteration 1857/35720 Training loss: 1.4341 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 1858/35720 Training loss: 1.4345 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 1859/35720 Training loss: 1.4340 0.2247 sec/batch\n",
      "Epoch 2/20  Iteration 1860/35720 Training loss: 1.4337 0.2229 sec/batch\n",
      "Epoch 2/20  Iteration 1861/35720 Training loss: 1.4314 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 1862/35720 Training loss: 1.4307 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 1863/35720 Training loss: 1.4292 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 1864/35720 Training loss: 1.4300 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 1865/35720 Training loss: 1.4295 0.2440 sec/batch\n",
      "Epoch 2/20  Iteration 1866/35720 Training loss: 1.4319 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 1867/35720 Training loss: 1.4322 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 1868/35720 Training loss: 1.4318 0.2208 sec/batch\n",
      "Epoch 2/20  Iteration 1869/35720 Training loss: 1.4327 0.2155 sec/batch\n",
      "Epoch 2/20  Iteration 1870/35720 Training loss: 1.4332 0.2075 sec/batch\n",
      "Epoch 2/20  Iteration 1871/35720 Training loss: 1.4331 0.2255 sec/batch\n",
      "Epoch 2/20  Iteration 1872/35720 Training loss: 1.4336 0.2166 sec/batch\n",
      "Epoch 2/20  Iteration 1873/35720 Training loss: 1.4338 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 1874/35720 Training loss: 1.4336 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 1875/35720 Training loss: 1.4324 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 1876/35720 Training loss: 1.4313 0.2082 sec/batch\n",
      "Epoch 2/20  Iteration 1877/35720 Training loss: 1.4311 0.2064 sec/batch\n",
      "Epoch 2/20  Iteration 1878/35720 Training loss: 1.4298 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 1879/35720 Training loss: 1.4288 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 1880/35720 Training loss: 1.4278 0.2151 sec/batch\n",
      "Epoch 2/20  Iteration 1881/35720 Training loss: 1.4273 0.2347 sec/batch\n",
      "Epoch 2/20  Iteration 1882/35720 Training loss: 1.4264 0.2119 sec/batch\n",
      "Epoch 2/20  Iteration 1883/35720 Training loss: 1.4268 0.2306 sec/batch\n",
      "Epoch 2/20  Iteration 1884/35720 Training loss: 1.4263 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 1885/35720 Training loss: 1.4262 0.2110 sec/batch\n",
      "Epoch 2/20  Iteration 1886/35720 Training loss: 1.4257 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 1887/35720 Training loss: 1.4251 0.2259 sec/batch\n",
      "Epoch 2/20  Iteration 1888/35720 Training loss: 1.4253 0.2318 sec/batch\n",
      "Epoch 2/20  Iteration 1889/35720 Training loss: 1.4262 0.2125 sec/batch\n",
      "Epoch 2/20  Iteration 1890/35720 Training loss: 1.4264 0.2080 sec/batch\n",
      "Epoch 2/20  Iteration 1891/35720 Training loss: 1.4260 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 1892/35720 Training loss: 1.4252 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 1893/35720 Training loss: 1.4252 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 1894/35720 Training loss: 1.4250 0.2107 sec/batch\n",
      "Epoch 2/20  Iteration 1895/35720 Training loss: 1.4259 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 1896/35720 Training loss: 1.4256 0.2075 sec/batch\n",
      "Epoch 2/20  Iteration 1897/35720 Training loss: 1.4256 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 1898/35720 Training loss: 1.4270 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 1899/35720 Training loss: 1.4273 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 1900/35720 Training loss: 1.4288 0.2068 sec/batch\n",
      "Epoch 2/20  Iteration 1901/35720 Training loss: 1.4280 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 1902/35720 Training loss: 1.4279 0.2292 sec/batch\n",
      "Epoch 2/20  Iteration 1903/35720 Training loss: 1.4275 0.2244 sec/batch\n",
      "Epoch 2/20  Iteration 1904/35720 Training loss: 1.4284 0.2102 sec/batch\n",
      "Epoch 2/20  Iteration 1905/35720 Training loss: 1.4290 0.2124 sec/batch\n",
      "Epoch 2/20  Iteration 1906/35720 Training loss: 1.4296 0.2059 sec/batch\n",
      "Epoch 2/20  Iteration 1907/35720 Training loss: 1.4303 0.2102 sec/batch\n",
      "Epoch 2/20  Iteration 1908/35720 Training loss: 1.4301 0.2181 sec/batch\n",
      "Epoch 2/20  Iteration 1909/35720 Training loss: 1.4310 0.2166 sec/batch\n",
      "Epoch 2/20  Iteration 1910/35720 Training loss: 1.4314 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 1911/35720 Training loss: 1.4304 0.2202 sec/batch\n",
      "Epoch 2/20  Iteration 1912/35720 Training loss: 1.4308 0.2256 sec/batch\n",
      "Epoch 2/20  Iteration 1913/35720 Training loss: 1.4305 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 1914/35720 Training loss: 1.4307 0.2112 sec/batch\n",
      "Epoch 2/20  Iteration 1915/35720 Training loss: 1.4300 0.2428 sec/batch\n",
      "Epoch 2/20  Iteration 1916/35720 Training loss: 1.4296 0.2073 sec/batch\n",
      "Epoch 2/20  Iteration 1917/35720 Training loss: 1.4291 0.2065 sec/batch\n",
      "Epoch 2/20  Iteration 1918/35720 Training loss: 1.4288 0.2228 sec/batch\n",
      "Epoch 2/20  Iteration 1919/35720 Training loss: 1.4282 0.2105 sec/batch\n",
      "Epoch 2/20  Iteration 1920/35720 Training loss: 1.4280 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 1921/35720 Training loss: 1.4279 0.2271 sec/batch\n",
      "Epoch 2/20  Iteration 1922/35720 Training loss: 1.4278 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 1923/35720 Training loss: 1.4289 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 1924/35720 Training loss: 1.4292 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 1925/35720 Training loss: 1.4296 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 1926/35720 Training loss: 1.4296 0.2157 sec/batch\n",
      "Epoch 2/20  Iteration 1927/35720 Training loss: 1.4288 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 1928/35720 Training loss: 1.4280 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 1929/35720 Training loss: 1.4274 0.2226 sec/batch\n",
      "Epoch 2/20  Iteration 1930/35720 Training loss: 1.4268 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 1931/35720 Training loss: 1.4263 0.2108 sec/batch\n",
      "Epoch 2/20  Iteration 1932/35720 Training loss: 1.4262 0.2125 sec/batch\n",
      "Epoch 2/20  Iteration 1933/35720 Training loss: 1.4253 0.2129 sec/batch\n",
      "Epoch 2/20  Iteration 1934/35720 Training loss: 1.4255 0.2144 sec/batch\n",
      "Epoch 2/20  Iteration 1935/35720 Training loss: 1.4251 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 1936/35720 Training loss: 1.4239 0.2131 sec/batch\n",
      "Epoch 2/20  Iteration 1937/35720 Training loss: 1.4236 0.2300 sec/batch\n",
      "Epoch 2/20  Iteration 1938/35720 Training loss: 1.4235 0.2281 sec/batch\n",
      "Epoch 2/20  Iteration 1939/35720 Training loss: 1.4234 0.2110 sec/batch\n",
      "Epoch 2/20  Iteration 1940/35720 Training loss: 1.4240 0.2322 sec/batch\n",
      "Epoch 2/20  Iteration 1941/35720 Training loss: 1.4245 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 1942/35720 Training loss: 1.4247 0.2100 sec/batch\n",
      "Epoch 2/20  Iteration 1943/35720 Training loss: 1.4247 0.2278 sec/batch\n",
      "Epoch 2/20  Iteration 1944/35720 Training loss: 1.4251 0.2065 sec/batch\n",
      "Epoch 2/20  Iteration 1945/35720 Training loss: 1.4247 0.2121 sec/batch\n",
      "Epoch 2/20  Iteration 1946/35720 Training loss: 1.4252 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 1947/35720 Training loss: 1.4245 0.2111 sec/batch\n",
      "Epoch 2/20  Iteration 1948/35720 Training loss: 1.4243 0.2164 sec/batch\n",
      "Epoch 2/20  Iteration 1949/35720 Training loss: 1.4247 0.2227 sec/batch\n",
      "Epoch 2/20  Iteration 1950/35720 Training loss: 1.4251 0.2118 sec/batch\n",
      "Epoch 2/20  Iteration 1951/35720 Training loss: 1.4254 0.2155 sec/batch\n",
      "Epoch 2/20  Iteration 1952/35720 Training loss: 1.4259 0.2235 sec/batch\n",
      "Epoch 2/20  Iteration 1953/35720 Training loss: 1.4262 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 1954/35720 Training loss: 1.4267 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 1955/35720 Training loss: 1.4265 0.2248 sec/batch\n",
      "Epoch 2/20  Iteration 1956/35720 Training loss: 1.4266 0.2317 sec/batch\n",
      "Epoch 2/20  Iteration 1957/35720 Training loss: 1.4283 0.2103 sec/batch\n",
      "Epoch 2/20  Iteration 1958/35720 Training loss: 1.4288 0.2229 sec/batch\n",
      "Epoch 2/20  Iteration 1959/35720 Training loss: 1.4288 0.2165 sec/batch\n",
      "Epoch 2/20  Iteration 1960/35720 Training loss: 1.4296 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 1961/35720 Training loss: 1.4295 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 1962/35720 Training loss: 1.4292 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 1963/35720 Training loss: 1.4292 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 1964/35720 Training loss: 1.4289 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 1965/35720 Training loss: 1.4288 0.2132 sec/batch\n",
      "Epoch 2/20  Iteration 1966/35720 Training loss: 1.4287 0.2221 sec/batch\n",
      "Epoch 2/20  Iteration 1967/35720 Training loss: 1.4290 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 1968/35720 Training loss: 1.4291 0.2380 sec/batch\n",
      "Epoch 2/20  Iteration 1969/35720 Training loss: 1.4291 0.2302 sec/batch\n",
      "Epoch 2/20  Iteration 1970/35720 Training loss: 1.4295 0.2130 sec/batch\n",
      "Epoch 2/20  Iteration 1971/35720 Training loss: 1.4293 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 1972/35720 Training loss: 1.4292 0.2081 sec/batch\n",
      "Epoch 2/20  Iteration 1973/35720 Training loss: 1.4288 0.2234 sec/batch\n",
      "Epoch 2/20  Iteration 1974/35720 Training loss: 1.4289 0.2247 sec/batch\n",
      "Epoch 2/20  Iteration 1975/35720 Training loss: 1.4291 0.2158 sec/batch\n",
      "Epoch 2/20  Iteration 1976/35720 Training loss: 1.4292 0.2062 sec/batch\n",
      "Epoch 2/20  Iteration 1977/35720 Training loss: 1.4292 0.2115 sec/batch\n",
      "Epoch 2/20  Iteration 1978/35720 Training loss: 1.4300 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 1979/35720 Training loss: 1.4302 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 1980/35720 Training loss: 1.4305 0.2288 sec/batch\n",
      "Epoch 2/20  Iteration 1981/35720 Training loss: 1.4301 0.2287 sec/batch\n",
      "Epoch 2/20  Iteration 1982/35720 Training loss: 1.4302 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 1983/35720 Training loss: 1.4297 0.2060 sec/batch\n",
      "Epoch 2/20  Iteration 1984/35720 Training loss: 1.4298 0.2093 sec/batch\n",
      "Epoch 2/20  Iteration 1985/35720 Training loss: 1.4299 0.2311 sec/batch\n",
      "Epoch 2/20  Iteration 1986/35720 Training loss: 1.4301 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 1987/35720 Training loss: 1.4296 0.2181 sec/batch\n",
      "Epoch 2/20  Iteration 1988/35720 Training loss: 1.4292 0.2252 sec/batch\n",
      "Epoch 2/20  Iteration 1989/35720 Training loss: 1.4293 0.2159 sec/batch\n",
      "Epoch 2/20  Iteration 1990/35720 Training loss: 1.4293 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 1991/35720 Training loss: 1.4291 0.2110 sec/batch\n",
      "Epoch 2/20  Iteration 1992/35720 Training loss: 1.4289 0.2130 sec/batch\n",
      "Epoch 2/20  Iteration 1993/35720 Training loss: 1.4294 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 1994/35720 Training loss: 1.4295 0.2271 sec/batch\n",
      "Epoch 2/20  Iteration 1995/35720 Training loss: 1.4297 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 1996/35720 Training loss: 1.4297 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 1997/35720 Training loss: 1.4296 0.2179 sec/batch\n",
      "Epoch 2/20  Iteration 1998/35720 Training loss: 1.4294 0.2149 sec/batch\n",
      "Epoch 2/20  Iteration 1999/35720 Training loss: 1.4295 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 2000/35720 Training loss: 1.4294 0.2152 sec/batch\n",
      "Validation loss: 1.53963 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 2001/35720 Training loss: 1.4295 0.2119 sec/batch\n",
      "Epoch 2/20  Iteration 2002/35720 Training loss: 1.4293 0.2081 sec/batch\n",
      "Epoch 2/20  Iteration 2003/35720 Training loss: 1.4289 0.2138 sec/batch\n",
      "Epoch 2/20  Iteration 2004/35720 Training loss: 1.4287 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 2005/35720 Training loss: 1.4286 0.2155 sec/batch\n",
      "Epoch 2/20  Iteration 2006/35720 Training loss: 1.4284 0.2240 sec/batch\n",
      "Epoch 2/20  Iteration 2007/35720 Training loss: 1.4285 0.2056 sec/batch\n",
      "Epoch 2/20  Iteration 2008/35720 Training loss: 1.4285 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2009/35720 Training loss: 1.4287 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 2010/35720 Training loss: 1.4287 0.2254 sec/batch\n",
      "Epoch 2/20  Iteration 2011/35720 Training loss: 1.4287 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 2012/35720 Training loss: 1.4285 0.2113 sec/batch\n",
      "Epoch 2/20  Iteration 2013/35720 Training loss: 1.4283 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 2014/35720 Training loss: 1.4277 0.2125 sec/batch\n",
      "Epoch 2/20  Iteration 2015/35720 Training loss: 1.4275 0.2059 sec/batch\n",
      "Epoch 2/20  Iteration 2016/35720 Training loss: 1.4281 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 2017/35720 Training loss: 1.4281 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 2018/35720 Training loss: 1.4278 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 2019/35720 Training loss: 1.4277 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2020/35720 Training loss: 1.4276 0.2157 sec/batch\n",
      "Epoch 2/20  Iteration 2021/35720 Training loss: 1.4275 0.2066 sec/batch\n",
      "Epoch 2/20  Iteration 2022/35720 Training loss: 1.4272 0.2277 sec/batch\n",
      "Epoch 2/20  Iteration 2023/35720 Training loss: 1.4274 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2024/35720 Training loss: 1.4272 0.2347 sec/batch\n",
      "Epoch 2/20  Iteration 2025/35720 Training loss: 1.4268 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 2026/35720 Training loss: 1.4268 0.2064 sec/batch\n",
      "Epoch 2/20  Iteration 2027/35720 Training loss: 1.4264 0.2342 sec/batch\n",
      "Epoch 2/20  Iteration 2028/35720 Training loss: 1.4264 0.2139 sec/batch\n",
      "Epoch 2/20  Iteration 2029/35720 Training loss: 1.4267 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 2030/35720 Training loss: 1.4268 0.2182 sec/batch\n",
      "Epoch 2/20  Iteration 2031/35720 Training loss: 1.4267 0.2144 sec/batch\n",
      "Epoch 2/20  Iteration 2032/35720 Training loss: 1.4266 0.2096 sec/batch\n",
      "Epoch 2/20  Iteration 2033/35720 Training loss: 1.4270 0.2275 sec/batch\n",
      "Epoch 2/20  Iteration 2034/35720 Training loss: 1.4266 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2035/35720 Training loss: 1.4261 0.2150 sec/batch\n",
      "Epoch 2/20  Iteration 2036/35720 Training loss: 1.4260 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 2037/35720 Training loss: 1.4260 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 2038/35720 Training loss: 1.4257 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 2039/35720 Training loss: 1.4255 0.2103 sec/batch\n",
      "Epoch 2/20  Iteration 2040/35720 Training loss: 1.4258 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 2041/35720 Training loss: 1.4261 0.2241 sec/batch\n",
      "Epoch 2/20  Iteration 2042/35720 Training loss: 1.4262 0.2167 sec/batch\n",
      "Epoch 2/20  Iteration 2043/35720 Training loss: 1.4261 0.2308 sec/batch\n",
      "Epoch 2/20  Iteration 2044/35720 Training loss: 1.4263 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2045/35720 Training loss: 1.4265 0.2058 sec/batch\n",
      "Epoch 2/20  Iteration 2046/35720 Training loss: 1.4262 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 2047/35720 Training loss: 1.4260 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2048/35720 Training loss: 1.4258 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 2049/35720 Training loss: 1.4256 0.2144 sec/batch\n",
      "Epoch 2/20  Iteration 2050/35720 Training loss: 1.4257 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2051/35720 Training loss: 1.4257 0.2068 sec/batch\n",
      "Epoch 2/20  Iteration 2052/35720 Training loss: 1.4259 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 2053/35720 Training loss: 1.4255 0.2060 sec/batch\n",
      "Epoch 2/20  Iteration 2054/35720 Training loss: 1.4253 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 2055/35720 Training loss: 1.4247 0.2159 sec/batch\n",
      "Epoch 2/20  Iteration 2056/35720 Training loss: 1.4238 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 2057/35720 Training loss: 1.4232 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 2058/35720 Training loss: 1.4229 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 2059/35720 Training loss: 1.4227 0.2157 sec/batch\n",
      "Epoch 2/20  Iteration 2060/35720 Training loss: 1.4224 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 2061/35720 Training loss: 1.4219 0.2284 sec/batch\n",
      "Epoch 2/20  Iteration 2062/35720 Training loss: 1.4216 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 2063/35720 Training loss: 1.4213 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 2064/35720 Training loss: 1.4211 0.2285 sec/batch\n",
      "Epoch 2/20  Iteration 2065/35720 Training loss: 1.4207 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 2066/35720 Training loss: 1.4206 0.2122 sec/batch\n",
      "Epoch 2/20  Iteration 2067/35720 Training loss: 1.4206 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 2068/35720 Training loss: 1.4200 0.2066 sec/batch\n",
      "Epoch 2/20  Iteration 2069/35720 Training loss: 1.4196 0.2059 sec/batch\n",
      "Epoch 2/20  Iteration 2070/35720 Training loss: 1.4192 0.2108 sec/batch\n",
      "Epoch 2/20  Iteration 2071/35720 Training loss: 1.4193 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 2072/35720 Training loss: 1.4190 0.2190 sec/batch\n",
      "Epoch 2/20  Iteration 2073/35720 Training loss: 1.4184 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 2074/35720 Training loss: 1.4182 0.2187 sec/batch\n",
      "Epoch 2/20  Iteration 2075/35720 Training loss: 1.4182 0.2189 sec/batch\n",
      "Epoch 2/20  Iteration 2076/35720 Training loss: 1.4183 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 2077/35720 Training loss: 1.4182 0.2281 sec/batch\n",
      "Epoch 2/20  Iteration 2078/35720 Training loss: 1.4183 0.2081 sec/batch\n",
      "Epoch 2/20  Iteration 2079/35720 Training loss: 1.4181 0.2241 sec/batch\n",
      "Epoch 2/20  Iteration 2080/35720 Training loss: 1.4179 0.2106 sec/batch\n",
      "Epoch 2/20  Iteration 2081/35720 Training loss: 1.4188 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2082/35720 Training loss: 1.4188 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2083/35720 Training loss: 1.4188 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 2084/35720 Training loss: 1.4190 0.2243 sec/batch\n",
      "Epoch 2/20  Iteration 2085/35720 Training loss: 1.4188 0.2156 sec/batch\n",
      "Epoch 2/20  Iteration 2086/35720 Training loss: 1.4187 0.2303 sec/batch\n",
      "Epoch 2/20  Iteration 2087/35720 Training loss: 1.4188 0.2142 sec/batch\n",
      "Epoch 2/20  Iteration 2088/35720 Training loss: 1.4185 0.2240 sec/batch\n",
      "Epoch 2/20  Iteration 2089/35720 Training loss: 1.4183 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 2090/35720 Training loss: 1.4182 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 2091/35720 Training loss: 1.4181 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 2092/35720 Training loss: 1.4180 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 2093/35720 Training loss: 1.4181 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2094/35720 Training loss: 1.4180 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 2095/35720 Training loss: 1.4176 0.2100 sec/batch\n",
      "Epoch 2/20  Iteration 2096/35720 Training loss: 1.4172 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 2097/35720 Training loss: 1.4170 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 2098/35720 Training loss: 1.4169 0.2103 sec/batch\n",
      "Epoch 2/20  Iteration 2099/35720 Training loss: 1.4166 0.2073 sec/batch\n",
      "Epoch 2/20  Iteration 2100/35720 Training loss: 1.4162 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 2101/35720 Training loss: 1.4162 0.2108 sec/batch\n",
      "Epoch 2/20  Iteration 2102/35720 Training loss: 1.4161 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2103/35720 Training loss: 1.4158 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 2104/35720 Training loss: 1.4160 0.2080 sec/batch\n",
      "Epoch 2/20  Iteration 2105/35720 Training loss: 1.4159 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 2106/35720 Training loss: 1.4155 0.2305 sec/batch\n",
      "Epoch 2/20  Iteration 2107/35720 Training loss: 1.4154 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 2108/35720 Training loss: 1.4150 0.2068 sec/batch\n",
      "Epoch 2/20  Iteration 2109/35720 Training loss: 1.4151 0.2083 sec/batch\n",
      "Epoch 2/20  Iteration 2110/35720 Training loss: 1.4151 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 2111/35720 Training loss: 1.4148 0.2110 sec/batch\n",
      "Epoch 2/20  Iteration 2112/35720 Training loss: 1.4150 0.2062 sec/batch\n",
      "Epoch 2/20  Iteration 2113/35720 Training loss: 1.4149 0.2189 sec/batch\n",
      "Epoch 2/20  Iteration 2114/35720 Training loss: 1.4149 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 2115/35720 Training loss: 1.4149 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 2116/35720 Training loss: 1.4149 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 2117/35720 Training loss: 1.4149 0.2248 sec/batch\n",
      "Epoch 2/20  Iteration 2118/35720 Training loss: 1.4151 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 2119/35720 Training loss: 1.4151 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 2120/35720 Training loss: 1.4149 0.2113 sec/batch\n",
      "Epoch 2/20  Iteration 2121/35720 Training loss: 1.4146 0.2267 sec/batch\n",
      "Epoch 2/20  Iteration 2122/35720 Training loss: 1.4144 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 2123/35720 Training loss: 1.4143 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 2124/35720 Training loss: 1.4141 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 2125/35720 Training loss: 1.4142 0.2062 sec/batch\n",
      "Epoch 2/20  Iteration 2126/35720 Training loss: 1.4142 0.2102 sec/batch\n",
      "Epoch 2/20  Iteration 2127/35720 Training loss: 1.4139 0.2182 sec/batch\n",
      "Epoch 2/20  Iteration 2128/35720 Training loss: 1.4137 0.2087 sec/batch\n",
      "Epoch 2/20  Iteration 2129/35720 Training loss: 1.4136 0.2098 sec/batch\n",
      "Epoch 2/20  Iteration 2130/35720 Training loss: 1.4134 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 2131/35720 Training loss: 1.4130 0.2285 sec/batch\n",
      "Epoch 2/20  Iteration 2132/35720 Training loss: 1.4133 0.2245 sec/batch\n",
      "Epoch 2/20  Iteration 2133/35720 Training loss: 1.4134 0.2097 sec/batch\n",
      "Epoch 2/20  Iteration 2134/35720 Training loss: 1.4135 0.2055 sec/batch\n",
      "Epoch 2/20  Iteration 2135/35720 Training loss: 1.4135 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2136/35720 Training loss: 1.4133 0.2081 sec/batch\n",
      "Epoch 2/20  Iteration 2137/35720 Training loss: 1.4134 0.2079 sec/batch\n",
      "Epoch 2/20  Iteration 2138/35720 Training loss: 1.4133 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 2139/35720 Training loss: 1.4131 0.2253 sec/batch\n",
      "Epoch 2/20  Iteration 2140/35720 Training loss: 1.4129 0.2331 sec/batch\n",
      "Epoch 2/20  Iteration 2141/35720 Training loss: 1.4129 0.2099 sec/batch\n",
      "Epoch 2/20  Iteration 2142/35720 Training loss: 1.4129 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 2143/35720 Training loss: 1.4129 0.2166 sec/batch\n",
      "Epoch 2/20  Iteration 2144/35720 Training loss: 1.4127 0.2290 sec/batch\n",
      "Epoch 2/20  Iteration 2145/35720 Training loss: 1.4124 0.2305 sec/batch\n",
      "Epoch 2/20  Iteration 2146/35720 Training loss: 1.4122 0.2087 sec/batch\n",
      "Epoch 2/20  Iteration 2147/35720 Training loss: 1.4120 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 2148/35720 Training loss: 1.4120 0.2119 sec/batch\n",
      "Epoch 2/20  Iteration 2149/35720 Training loss: 1.4117 0.2226 sec/batch\n",
      "Epoch 2/20  Iteration 2150/35720 Training loss: 1.4116 0.2192 sec/batch\n",
      "Epoch 2/20  Iteration 2151/35720 Training loss: 1.4113 0.2074 sec/batch\n",
      "Epoch 2/20  Iteration 2152/35720 Training loss: 1.4112 0.2179 sec/batch\n",
      "Epoch 2/20  Iteration 2153/35720 Training loss: 1.4110 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 2154/35720 Training loss: 1.4106 0.2202 sec/batch\n",
      "Epoch 2/20  Iteration 2155/35720 Training loss: 1.4104 0.2167 sec/batch\n",
      "Epoch 2/20  Iteration 2156/35720 Training loss: 1.4102 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 2157/35720 Training loss: 1.4099 0.2158 sec/batch\n",
      "Epoch 2/20  Iteration 2158/35720 Training loss: 1.4096 0.2061 sec/batch\n",
      "Epoch 2/20  Iteration 2159/35720 Training loss: 1.4096 0.2100 sec/batch\n",
      "Epoch 2/20  Iteration 2160/35720 Training loss: 1.4095 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 2161/35720 Training loss: 1.4094 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 2162/35720 Training loss: 1.4096 0.2074 sec/batch\n",
      "Epoch 2/20  Iteration 2163/35720 Training loss: 1.4094 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2164/35720 Training loss: 1.4091 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 2165/35720 Training loss: 1.4088 0.2403 sec/batch\n",
      "Epoch 2/20  Iteration 2166/35720 Training loss: 1.4085 0.2261 sec/batch\n",
      "Epoch 2/20  Iteration 2167/35720 Training loss: 1.4084 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 2168/35720 Training loss: 1.4082 0.2267 sec/batch\n",
      "Epoch 2/20  Iteration 2169/35720 Training loss: 1.4081 0.2182 sec/batch\n",
      "Epoch 2/20  Iteration 2170/35720 Training loss: 1.4080 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 2171/35720 Training loss: 1.4080 0.2078 sec/batch\n",
      "Epoch 2/20  Iteration 2172/35720 Training loss: 1.4079 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 2173/35720 Training loss: 1.4079 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 2174/35720 Training loss: 1.4079 0.2144 sec/batch\n",
      "Epoch 2/20  Iteration 2175/35720 Training loss: 1.4081 0.2157 sec/batch\n",
      "Epoch 2/20  Iteration 2176/35720 Training loss: 1.4082 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 2177/35720 Training loss: 1.4081 0.2063 sec/batch\n",
      "Epoch 2/20  Iteration 2178/35720 Training loss: 1.4079 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2179/35720 Training loss: 1.4080 0.2087 sec/batch\n",
      "Epoch 2/20  Iteration 2180/35720 Training loss: 1.4077 0.2108 sec/batch\n",
      "Epoch 2/20  Iteration 2181/35720 Training loss: 1.4077 0.2293 sec/batch\n",
      "Epoch 2/20  Iteration 2182/35720 Training loss: 1.4074 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 2183/35720 Training loss: 1.4073 0.2068 sec/batch\n",
      "Epoch 2/20  Iteration 2184/35720 Training loss: 1.4070 0.2095 sec/batch\n",
      "Epoch 2/20  Iteration 2185/35720 Training loss: 1.4068 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2186/35720 Training loss: 1.4066 0.2120 sec/batch\n",
      "Epoch 2/20  Iteration 2187/35720 Training loss: 1.4064 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 2188/35720 Training loss: 1.4066 0.2179 sec/batch\n",
      "Epoch 2/20  Iteration 2189/35720 Training loss: 1.4068 0.2269 sec/batch\n",
      "Epoch 2/20  Iteration 2190/35720 Training loss: 1.4067 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 2191/35720 Training loss: 1.4065 0.2229 sec/batch\n",
      "Epoch 2/20  Iteration 2192/35720 Training loss: 1.4064 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 2193/35720 Training loss: 1.4063 0.2225 sec/batch\n",
      "Epoch 2/20  Iteration 2194/35720 Training loss: 1.4061 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 2195/35720 Training loss: 1.4057 0.2292 sec/batch\n",
      "Epoch 2/20  Iteration 2196/35720 Training loss: 1.4056 0.2058 sec/batch\n",
      "Epoch 2/20  Iteration 2197/35720 Training loss: 1.4056 0.2079 sec/batch\n",
      "Epoch 2/20  Iteration 2198/35720 Training loss: 1.4052 0.2226 sec/batch\n",
      "Epoch 2/20  Iteration 2199/35720 Training loss: 1.4048 0.2277 sec/batch\n",
      "Epoch 2/20  Iteration 2200/35720 Training loss: 1.4048 0.2345 sec/batch\n",
      "Validation loss: 1.52528 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 2201/35720 Training loss: 1.4048 0.2117 sec/batch\n",
      "Epoch 2/20  Iteration 2202/35720 Training loss: 1.4047 0.2111 sec/batch\n",
      "Epoch 2/20  Iteration 2203/35720 Training loss: 1.4045 0.2150 sec/batch\n",
      "Epoch 2/20  Iteration 2204/35720 Training loss: 1.4043 0.2059 sec/batch\n",
      "Epoch 2/20  Iteration 2205/35720 Training loss: 1.4039 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 2206/35720 Training loss: 1.4038 0.2113 sec/batch\n",
      "Epoch 2/20  Iteration 2207/35720 Training loss: 1.4039 0.2082 sec/batch\n",
      "Epoch 2/20  Iteration 2208/35720 Training loss: 1.4037 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 2209/35720 Training loss: 1.4037 0.2116 sec/batch\n",
      "Epoch 2/20  Iteration 2210/35720 Training loss: 1.4037 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 2211/35720 Training loss: 1.4037 0.2247 sec/batch\n",
      "Epoch 2/20  Iteration 2212/35720 Training loss: 1.4035 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 2213/35720 Training loss: 1.4032 0.2063 sec/batch\n",
      "Epoch 2/20  Iteration 2214/35720 Training loss: 1.4033 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 2215/35720 Training loss: 1.4033 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 2216/35720 Training loss: 1.4035 0.2147 sec/batch\n",
      "Epoch 2/20  Iteration 2217/35720 Training loss: 1.4035 0.2292 sec/batch\n",
      "Epoch 2/20  Iteration 2218/35720 Training loss: 1.4037 0.2115 sec/batch\n",
      "Epoch 2/20  Iteration 2219/35720 Training loss: 1.4041 0.2241 sec/batch\n",
      "Epoch 2/20  Iteration 2220/35720 Training loss: 1.4043 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 2221/35720 Training loss: 1.4044 0.2156 sec/batch\n",
      "Epoch 2/20  Iteration 2222/35720 Training loss: 1.4043 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 2223/35720 Training loss: 1.4043 0.2067 sec/batch\n",
      "Epoch 2/20  Iteration 2224/35720 Training loss: 1.4044 0.2116 sec/batch\n",
      "Epoch 2/20  Iteration 2225/35720 Training loss: 1.4044 0.2275 sec/batch\n",
      "Epoch 2/20  Iteration 2226/35720 Training loss: 1.4043 0.2241 sec/batch\n",
      "Epoch 2/20  Iteration 2227/35720 Training loss: 1.4047 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 2228/35720 Training loss: 1.4049 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2229/35720 Training loss: 1.4047 0.2170 sec/batch\n",
      "Epoch 2/20  Iteration 2230/35720 Training loss: 1.4046 0.2137 sec/batch\n",
      "Epoch 2/20  Iteration 2231/35720 Training loss: 1.4045 0.2310 sec/batch\n",
      "Epoch 2/20  Iteration 2232/35720 Training loss: 1.4045 0.2138 sec/batch\n",
      "Epoch 2/20  Iteration 2233/35720 Training loss: 1.4048 0.2141 sec/batch\n",
      "Epoch 2/20  Iteration 2234/35720 Training loss: 1.4048 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 2235/35720 Training loss: 1.4049 0.2255 sec/batch\n",
      "Epoch 2/20  Iteration 2236/35720 Training loss: 1.4049 0.2317 sec/batch\n",
      "Epoch 2/20  Iteration 2237/35720 Training loss: 1.4045 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 2238/35720 Training loss: 1.4044 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 2239/35720 Training loss: 1.4044 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 2240/35720 Training loss: 1.4045 0.2269 sec/batch\n",
      "Epoch 2/20  Iteration 2241/35720 Training loss: 1.4047 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 2242/35720 Training loss: 1.4048 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 2243/35720 Training loss: 1.4049 0.2223 sec/batch\n",
      "Epoch 2/20  Iteration 2244/35720 Training loss: 1.4049 0.2256 sec/batch\n",
      "Epoch 2/20  Iteration 2245/35720 Training loss: 1.4046 0.2267 sec/batch\n",
      "Epoch 2/20  Iteration 2246/35720 Training loss: 1.4044 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 2247/35720 Training loss: 1.4043 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 2248/35720 Training loss: 1.4042 0.2138 sec/batch\n",
      "Epoch 2/20  Iteration 2249/35720 Training loss: 1.4044 0.2080 sec/batch\n",
      "Epoch 2/20  Iteration 2250/35720 Training loss: 1.4041 0.2098 sec/batch\n",
      "Epoch 2/20  Iteration 2251/35720 Training loss: 1.4040 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2252/35720 Training loss: 1.4038 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 2253/35720 Training loss: 1.4036 0.2081 sec/batch\n",
      "Epoch 2/20  Iteration 2254/35720 Training loss: 1.4035 0.2061 sec/batch\n",
      "Epoch 2/20  Iteration 2255/35720 Training loss: 1.4032 0.2281 sec/batch\n",
      "Epoch 2/20  Iteration 2256/35720 Training loss: 1.4029 0.2176 sec/batch\n",
      "Epoch 2/20  Iteration 2257/35720 Training loss: 1.4025 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 2258/35720 Training loss: 1.4024 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 2259/35720 Training loss: 1.4023 0.2122 sec/batch\n",
      "Epoch 2/20  Iteration 2260/35720 Training loss: 1.4020 0.2078 sec/batch\n",
      "Epoch 2/20  Iteration 2261/35720 Training loss: 1.4020 0.2131 sec/batch\n",
      "Epoch 2/20  Iteration 2262/35720 Training loss: 1.4019 0.2296 sec/batch\n",
      "Epoch 2/20  Iteration 2263/35720 Training loss: 1.4019 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 2264/35720 Training loss: 1.4018 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 2265/35720 Training loss: 1.4019 0.2071 sec/batch\n",
      "Epoch 2/20  Iteration 2266/35720 Training loss: 1.4017 0.2087 sec/batch\n",
      "Epoch 2/20  Iteration 2267/35720 Training loss: 1.4014 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 2268/35720 Training loss: 1.4011 0.2164 sec/batch\n",
      "Epoch 2/20  Iteration 2269/35720 Training loss: 1.4011 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 2270/35720 Training loss: 1.4010 0.2139 sec/batch\n",
      "Epoch 2/20  Iteration 2271/35720 Training loss: 1.4007 0.2076 sec/batch\n",
      "Epoch 2/20  Iteration 2272/35720 Training loss: 1.4006 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 2273/35720 Training loss: 1.4004 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 2274/35720 Training loss: 1.4003 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 2275/35720 Training loss: 1.4003 0.2076 sec/batch\n",
      "Epoch 2/20  Iteration 2276/35720 Training loss: 1.4001 0.2078 sec/batch\n",
      "Epoch 2/20  Iteration 2277/35720 Training loss: 1.3998 0.2114 sec/batch\n",
      "Epoch 2/20  Iteration 2278/35720 Training loss: 1.3998 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 2279/35720 Training loss: 1.3996 0.2097 sec/batch\n",
      "Epoch 2/20  Iteration 2280/35720 Training loss: 1.3993 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 2281/35720 Training loss: 1.3991 0.2339 sec/batch\n",
      "Epoch 2/20  Iteration 2282/35720 Training loss: 1.3988 0.2074 sec/batch\n",
      "Epoch 2/20  Iteration 2283/35720 Training loss: 1.3989 0.2151 sec/batch\n",
      "Epoch 2/20  Iteration 2284/35720 Training loss: 1.3990 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 2285/35720 Training loss: 1.3989 0.2325 sec/batch\n",
      "Epoch 2/20  Iteration 2286/35720 Training loss: 1.3988 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 2287/35720 Training loss: 1.3985 0.2273 sec/batch\n",
      "Epoch 2/20  Iteration 2288/35720 Training loss: 1.3982 0.2123 sec/batch\n",
      "Epoch 2/20  Iteration 2289/35720 Training loss: 1.3979 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 2290/35720 Training loss: 1.3978 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2291/35720 Training loss: 1.3978 0.2126 sec/batch\n",
      "Epoch 2/20  Iteration 2292/35720 Training loss: 1.3977 0.2258 sec/batch\n",
      "Epoch 2/20  Iteration 2293/35720 Training loss: 1.3976 0.2219 sec/batch\n",
      "Epoch 2/20  Iteration 2294/35720 Training loss: 1.3974 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 2295/35720 Training loss: 1.3975 0.2115 sec/batch\n",
      "Epoch 2/20  Iteration 2296/35720 Training loss: 1.3972 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 2297/35720 Training loss: 1.3970 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 2298/35720 Training loss: 1.3967 0.2156 sec/batch\n",
      "Epoch 2/20  Iteration 2299/35720 Training loss: 1.3966 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 2300/35720 Training loss: 1.3964 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 2301/35720 Training loss: 1.3964 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 2302/35720 Training loss: 1.3963 0.2287 sec/batch\n",
      "Epoch 2/20  Iteration 2303/35720 Training loss: 1.3961 0.2286 sec/batch\n",
      "Epoch 2/20  Iteration 2304/35720 Training loss: 1.3961 0.2244 sec/batch\n",
      "Epoch 2/20  Iteration 2305/35720 Training loss: 1.3958 0.2303 sec/batch\n",
      "Epoch 2/20  Iteration 2306/35720 Training loss: 1.3958 0.2130 sec/batch\n",
      "Epoch 2/20  Iteration 2307/35720 Training loss: 1.3956 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2308/35720 Training loss: 1.3955 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 2309/35720 Training loss: 1.3955 0.2250 sec/batch\n",
      "Epoch 2/20  Iteration 2310/35720 Training loss: 1.3952 0.2558 sec/batch\n",
      "Epoch 2/20  Iteration 2311/35720 Training loss: 1.3950 0.2707 sec/batch\n",
      "Epoch 2/20  Iteration 2312/35720 Training loss: 1.3950 0.2303 sec/batch\n",
      "Epoch 2/20  Iteration 2313/35720 Training loss: 1.3948 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 2314/35720 Training loss: 1.3946 0.2164 sec/batch\n",
      "Epoch 2/20  Iteration 2315/35720 Training loss: 1.3944 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2316/35720 Training loss: 1.3942 0.2158 sec/batch\n",
      "Epoch 2/20  Iteration 2317/35720 Training loss: 1.3942 0.2250 sec/batch\n",
      "Epoch 2/20  Iteration 2318/35720 Training loss: 1.3940 0.2105 sec/batch\n",
      "Epoch 2/20  Iteration 2319/35720 Training loss: 1.3939 0.2076 sec/batch\n",
      "Epoch 2/20  Iteration 2320/35720 Training loss: 1.3939 0.2285 sec/batch\n",
      "Epoch 2/20  Iteration 2321/35720 Training loss: 1.3937 0.2248 sec/batch\n",
      "Epoch 2/20  Iteration 2322/35720 Training loss: 1.3935 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2323/35720 Training loss: 1.3931 0.2259 sec/batch\n",
      "Epoch 2/20  Iteration 2324/35720 Training loss: 1.3929 0.2099 sec/batch\n",
      "Epoch 2/20  Iteration 2325/35720 Training loss: 1.3928 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 2326/35720 Training loss: 1.3925 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2327/35720 Training loss: 1.3923 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 2328/35720 Training loss: 1.3921 0.2451 sec/batch\n",
      "Epoch 2/20  Iteration 2329/35720 Training loss: 1.3918 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 2330/35720 Training loss: 1.3916 0.2125 sec/batch\n",
      "Epoch 2/20  Iteration 2331/35720 Training loss: 1.3917 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 2332/35720 Training loss: 1.3917 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 2333/35720 Training loss: 1.3916 0.2225 sec/batch\n",
      "Epoch 2/20  Iteration 2334/35720 Training loss: 1.3915 0.2190 sec/batch\n",
      "Epoch 2/20  Iteration 2335/35720 Training loss: 1.3911 0.2259 sec/batch\n",
      "Epoch 2/20  Iteration 2336/35720 Training loss: 1.3911 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 2337/35720 Training loss: 1.3909 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 2338/35720 Training loss: 1.3907 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 2339/35720 Training loss: 1.3904 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 2340/35720 Training loss: 1.3904 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 2341/35720 Training loss: 1.3901 0.2062 sec/batch\n",
      "Epoch 2/20  Iteration 2342/35720 Training loss: 1.3901 0.2094 sec/batch\n",
      "Epoch 2/20  Iteration 2343/35720 Training loss: 1.3899 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 2344/35720 Training loss: 1.3899 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 2345/35720 Training loss: 1.3898 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2346/35720 Training loss: 1.3894 0.2160 sec/batch\n",
      "Epoch 2/20  Iteration 2347/35720 Training loss: 1.3895 0.2170 sec/batch\n",
      "Epoch 2/20  Iteration 2348/35720 Training loss: 1.3893 0.2082 sec/batch\n",
      "Epoch 2/20  Iteration 2349/35720 Training loss: 1.3891 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 2350/35720 Training loss: 1.3889 0.2250 sec/batch\n",
      "Epoch 2/20  Iteration 2351/35720 Training loss: 1.3886 0.2291 sec/batch\n",
      "Epoch 2/20  Iteration 2352/35720 Training loss: 1.3884 0.2066 sec/batch\n",
      "Epoch 2/20  Iteration 2353/35720 Training loss: 1.3883 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 2354/35720 Training loss: 1.3882 0.2271 sec/batch\n",
      "Epoch 2/20  Iteration 2355/35720 Training loss: 1.3883 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2356/35720 Training loss: 1.3883 0.2273 sec/batch\n",
      "Epoch 2/20  Iteration 2357/35720 Training loss: 1.3882 0.2260 sec/batch\n",
      "Epoch 2/20  Iteration 2358/35720 Training loss: 1.3882 0.2129 sec/batch\n",
      "Epoch 2/20  Iteration 2359/35720 Training loss: 1.3883 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 2360/35720 Training loss: 1.3882 0.2223 sec/batch\n",
      "Epoch 2/20  Iteration 2361/35720 Training loss: 1.3882 0.2073 sec/batch\n",
      "Epoch 2/20  Iteration 2362/35720 Training loss: 1.3883 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 2363/35720 Training loss: 1.3883 0.2149 sec/batch\n",
      "Epoch 2/20  Iteration 2364/35720 Training loss: 1.3882 0.2208 sec/batch\n",
      "Epoch 2/20  Iteration 2365/35720 Training loss: 1.3880 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 2366/35720 Training loss: 1.3878 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 2367/35720 Training loss: 1.3878 0.2285 sec/batch\n",
      "Epoch 2/20  Iteration 2368/35720 Training loss: 1.3877 0.2234 sec/batch\n",
      "Epoch 2/20  Iteration 2369/35720 Training loss: 1.3878 0.2190 sec/batch\n",
      "Epoch 2/20  Iteration 2370/35720 Training loss: 1.3877 0.2139 sec/batch\n",
      "Epoch 2/20  Iteration 2371/35720 Training loss: 1.3876 0.2108 sec/batch\n",
      "Epoch 2/20  Iteration 2372/35720 Training loss: 1.3873 0.2067 sec/batch\n",
      "Epoch 2/20  Iteration 2373/35720 Training loss: 1.3871 0.2231 sec/batch\n",
      "Epoch 2/20  Iteration 2374/35720 Training loss: 1.3869 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 2375/35720 Training loss: 1.3866 0.2150 sec/batch\n",
      "Epoch 2/20  Iteration 2376/35720 Training loss: 1.3864 0.2175 sec/batch\n",
      "Epoch 2/20  Iteration 2377/35720 Training loss: 1.3864 0.2250 sec/batch\n",
      "Epoch 2/20  Iteration 2378/35720 Training loss: 1.3863 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 2379/35720 Training loss: 1.3863 0.2243 sec/batch\n",
      "Epoch 2/20  Iteration 2380/35720 Training loss: 1.3863 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 2381/35720 Training loss: 1.3860 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 2382/35720 Training loss: 1.3859 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 2383/35720 Training loss: 1.3857 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 2384/35720 Training loss: 1.3858 0.2300 sec/batch\n",
      "Epoch 2/20  Iteration 2385/35720 Training loss: 1.3856 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2386/35720 Training loss: 1.3853 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2387/35720 Training loss: 1.3851 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 2388/35720 Training loss: 1.3849 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 2389/35720 Training loss: 1.3848 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 2390/35720 Training loss: 1.3846 0.2057 sec/batch\n",
      "Epoch 2/20  Iteration 2391/35720 Training loss: 1.3846 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 2392/35720 Training loss: 1.3846 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 2393/35720 Training loss: 1.3846 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 2394/35720 Training loss: 1.3845 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 2395/35720 Training loss: 1.3847 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 2396/35720 Training loss: 1.3844 0.2145 sec/batch\n",
      "Epoch 2/20  Iteration 2397/35720 Training loss: 1.3844 0.2094 sec/batch\n",
      "Epoch 2/20  Iteration 2398/35720 Training loss: 1.3841 0.2157 sec/batch\n",
      "Epoch 2/20  Iteration 2399/35720 Training loss: 1.3841 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 2400/35720 Training loss: 1.3842 0.2187 sec/batch\n",
      "Validation loss: 1.4925 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 2401/35720 Training loss: 1.3843 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 2402/35720 Training loss: 1.3842 0.2103 sec/batch\n",
      "Epoch 2/20  Iteration 2403/35720 Training loss: 1.3841 0.2253 sec/batch\n",
      "Epoch 2/20  Iteration 2404/35720 Training loss: 1.3840 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 2405/35720 Training loss: 1.3837 0.2300 sec/batch\n",
      "Epoch 2/20  Iteration 2406/35720 Training loss: 1.3835 0.2288 sec/batch\n",
      "Epoch 2/20  Iteration 2407/35720 Training loss: 1.3832 0.2273 sec/batch\n",
      "Epoch 2/20  Iteration 2408/35720 Training loss: 1.3832 0.2062 sec/batch\n",
      "Epoch 2/20  Iteration 2409/35720 Training loss: 1.3830 0.2125 sec/batch\n",
      "Epoch 2/20  Iteration 2410/35720 Training loss: 1.3829 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 2411/35720 Training loss: 1.3826 0.2333 sec/batch\n",
      "Epoch 2/20  Iteration 2412/35720 Training loss: 1.3825 0.2164 sec/batch\n",
      "Epoch 2/20  Iteration 2413/35720 Training loss: 1.3825 0.2124 sec/batch\n",
      "Epoch 2/20  Iteration 2414/35720 Training loss: 1.3822 0.2241 sec/batch\n",
      "Epoch 2/20  Iteration 2415/35720 Training loss: 1.3821 0.2288 sec/batch\n",
      "Epoch 2/20  Iteration 2416/35720 Training loss: 1.3819 0.2245 sec/batch\n",
      "Epoch 2/20  Iteration 2417/35720 Training loss: 1.3820 0.2114 sec/batch\n",
      "Epoch 2/20  Iteration 2418/35720 Training loss: 1.3818 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 2419/35720 Training loss: 1.3815 0.2222 sec/batch\n",
      "Epoch 2/20  Iteration 2420/35720 Training loss: 1.3814 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 2421/35720 Training loss: 1.3814 0.2120 sec/batch\n",
      "Epoch 2/20  Iteration 2422/35720 Training loss: 1.3813 0.2275 sec/batch\n",
      "Epoch 2/20  Iteration 2423/35720 Training loss: 1.3812 0.2268 sec/batch\n",
      "Epoch 2/20  Iteration 2424/35720 Training loss: 1.3811 0.2262 sec/batch\n",
      "Epoch 2/20  Iteration 2425/35720 Training loss: 1.3811 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 2426/35720 Training loss: 1.3810 0.2128 sec/batch\n",
      "Epoch 2/20  Iteration 2427/35720 Training loss: 1.3811 0.2121 sec/batch\n",
      "Epoch 2/20  Iteration 2428/35720 Training loss: 1.3810 0.2059 sec/batch\n",
      "Epoch 2/20  Iteration 2429/35720 Training loss: 1.3811 0.2100 sec/batch\n",
      "Epoch 2/20  Iteration 2430/35720 Training loss: 1.3810 0.2238 sec/batch\n",
      "Epoch 2/20  Iteration 2431/35720 Training loss: 1.3810 0.2243 sec/batch\n",
      "Epoch 2/20  Iteration 2432/35720 Training loss: 1.3811 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 2433/35720 Training loss: 1.3811 0.2212 sec/batch\n",
      "Epoch 2/20  Iteration 2434/35720 Training loss: 1.3811 0.2065 sec/batch\n",
      "Epoch 2/20  Iteration 2435/35720 Training loss: 1.3809 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 2436/35720 Training loss: 1.3807 0.2259 sec/batch\n",
      "Epoch 2/20  Iteration 2437/35720 Training loss: 1.3807 0.2164 sec/batch\n",
      "Epoch 2/20  Iteration 2438/35720 Training loss: 1.3806 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 2439/35720 Training loss: 1.3806 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 2440/35720 Training loss: 1.3808 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 2441/35720 Training loss: 1.3809 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 2442/35720 Training loss: 1.3808 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 2443/35720 Training loss: 1.3809 0.2212 sec/batch\n",
      "Epoch 2/20  Iteration 2444/35720 Training loss: 1.3810 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 2445/35720 Training loss: 1.3809 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 2446/35720 Training loss: 1.3809 0.2124 sec/batch\n",
      "Epoch 2/20  Iteration 2447/35720 Training loss: 1.3809 0.2290 sec/batch\n",
      "Epoch 2/20  Iteration 2448/35720 Training loss: 1.3809 0.2248 sec/batch\n",
      "Epoch 2/20  Iteration 2449/35720 Training loss: 1.3809 0.2126 sec/batch\n",
      "Epoch 2/20  Iteration 2450/35720 Training loss: 1.3809 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 2451/35720 Training loss: 1.3809 0.2090 sec/batch\n",
      "Epoch 2/20  Iteration 2452/35720 Training loss: 1.3809 0.2318 sec/batch\n",
      "Epoch 2/20  Iteration 2453/35720 Training loss: 1.3810 0.2267 sec/batch\n",
      "Epoch 2/20  Iteration 2454/35720 Training loss: 1.3808 0.2306 sec/batch\n",
      "Epoch 2/20  Iteration 2455/35720 Training loss: 1.3807 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2456/35720 Training loss: 1.3806 0.2229 sec/batch\n",
      "Epoch 2/20  Iteration 2457/35720 Training loss: 1.3803 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 2458/35720 Training loss: 1.3803 0.2120 sec/batch\n",
      "Epoch 2/20  Iteration 2459/35720 Training loss: 1.3803 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 2460/35720 Training loss: 1.3801 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2461/35720 Training loss: 1.3799 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2462/35720 Training loss: 1.3798 0.2110 sec/batch\n",
      "Epoch 2/20  Iteration 2463/35720 Training loss: 1.3797 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2464/35720 Training loss: 1.3795 0.2164 sec/batch\n",
      "Epoch 2/20  Iteration 2465/35720 Training loss: 1.3795 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2466/35720 Training loss: 1.3794 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2467/35720 Training loss: 1.3793 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 2468/35720 Training loss: 1.3791 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 2469/35720 Training loss: 1.3789 0.2235 sec/batch\n",
      "Epoch 2/20  Iteration 2470/35720 Training loss: 1.3787 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 2471/35720 Training loss: 1.3788 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 2472/35720 Training loss: 1.3787 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 2473/35720 Training loss: 1.3787 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 2474/35720 Training loss: 1.3786 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 2475/35720 Training loss: 1.3785 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 2476/35720 Training loss: 1.3784 0.2332 sec/batch\n",
      "Epoch 2/20  Iteration 2477/35720 Training loss: 1.3785 0.2144 sec/batch\n",
      "Epoch 2/20  Iteration 2478/35720 Training loss: 1.3786 0.2067 sec/batch\n",
      "Epoch 2/20  Iteration 2479/35720 Training loss: 1.3788 0.2096 sec/batch\n",
      "Epoch 2/20  Iteration 2480/35720 Training loss: 1.3787 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 2481/35720 Training loss: 1.3786 0.2220 sec/batch\n",
      "Epoch 2/20  Iteration 2482/35720 Training loss: 1.3786 0.2111 sec/batch\n",
      "Epoch 2/20  Iteration 2483/35720 Training loss: 1.3786 0.2059 sec/batch\n",
      "Epoch 2/20  Iteration 2484/35720 Training loss: 1.3785 0.2093 sec/batch\n",
      "Epoch 2/20  Iteration 2485/35720 Training loss: 1.3784 0.2107 sec/batch\n",
      "Epoch 2/20  Iteration 2486/35720 Training loss: 1.3784 0.2110 sec/batch\n",
      "Epoch 2/20  Iteration 2487/35720 Training loss: 1.3783 0.2118 sec/batch\n",
      "Epoch 2/20  Iteration 2488/35720 Training loss: 1.3782 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2489/35720 Training loss: 1.3781 0.2235 sec/batch\n",
      "Epoch 2/20  Iteration 2490/35720 Training loss: 1.3780 0.2206 sec/batch\n",
      "Epoch 2/20  Iteration 2491/35720 Training loss: 1.3780 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 2492/35720 Training loss: 1.3780 0.2121 sec/batch\n",
      "Epoch 2/20  Iteration 2493/35720 Training loss: 1.3781 0.2141 sec/batch\n",
      "Epoch 2/20  Iteration 2494/35720 Training loss: 1.3781 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 2495/35720 Training loss: 1.3781 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 2496/35720 Training loss: 1.3781 0.2277 sec/batch\n",
      "Epoch 2/20  Iteration 2497/35720 Training loss: 1.3782 0.2200 sec/batch\n",
      "Epoch 2/20  Iteration 2498/35720 Training loss: 1.3782 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 2499/35720 Training loss: 1.3781 0.2263 sec/batch\n",
      "Epoch 2/20  Iteration 2500/35720 Training loss: 1.3780 0.2268 sec/batch\n",
      "Epoch 2/20  Iteration 2501/35720 Training loss: 1.3780 0.2064 sec/batch\n",
      "Epoch 2/20  Iteration 2502/35720 Training loss: 1.3780 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 2503/35720 Training loss: 1.3782 0.2253 sec/batch\n",
      "Epoch 2/20  Iteration 2504/35720 Training loss: 1.3782 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 2505/35720 Training loss: 1.3781 0.2264 sec/batch\n",
      "Epoch 2/20  Iteration 2506/35720 Training loss: 1.3781 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 2507/35720 Training loss: 1.3780 0.2095 sec/batch\n",
      "Epoch 2/20  Iteration 2508/35720 Training loss: 1.3780 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2509/35720 Training loss: 1.3781 0.2282 sec/batch\n",
      "Epoch 2/20  Iteration 2510/35720 Training loss: 1.3780 0.2151 sec/batch\n",
      "Epoch 2/20  Iteration 2511/35720 Training loss: 1.3779 0.2257 sec/batch\n",
      "Epoch 2/20  Iteration 2512/35720 Training loss: 1.3778 0.2056 sec/batch\n",
      "Epoch 2/20  Iteration 2513/35720 Training loss: 1.3780 0.2094 sec/batch\n",
      "Epoch 2/20  Iteration 2514/35720 Training loss: 1.3779 0.2108 sec/batch\n",
      "Epoch 2/20  Iteration 2515/35720 Training loss: 1.3779 0.2285 sec/batch\n",
      "Epoch 2/20  Iteration 2516/35720 Training loss: 1.3778 0.2120 sec/batch\n",
      "Epoch 2/20  Iteration 2517/35720 Training loss: 1.3777 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 2518/35720 Training loss: 1.3777 0.2220 sec/batch\n",
      "Epoch 2/20  Iteration 2519/35720 Training loss: 1.3777 0.2079 sec/batch\n",
      "Epoch 2/20  Iteration 2520/35720 Training loss: 1.3775 0.2090 sec/batch\n",
      "Epoch 2/20  Iteration 2521/35720 Training loss: 1.3775 0.2214 sec/batch\n",
      "Epoch 2/20  Iteration 2522/35720 Training loss: 1.3774 0.2271 sec/batch\n",
      "Epoch 2/20  Iteration 2523/35720 Training loss: 1.3775 0.2123 sec/batch\n",
      "Epoch 2/20  Iteration 2524/35720 Training loss: 1.3773 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 2525/35720 Training loss: 1.3774 0.2256 sec/batch\n",
      "Epoch 2/20  Iteration 2526/35720 Training loss: 1.3774 0.2147 sec/batch\n",
      "Epoch 2/20  Iteration 2527/35720 Training loss: 1.3772 0.2167 sec/batch\n",
      "Epoch 2/20  Iteration 2528/35720 Training loss: 1.3772 0.2139 sec/batch\n",
      "Epoch 2/20  Iteration 2529/35720 Training loss: 1.3772 0.2524 sec/batch\n",
      "Epoch 2/20  Iteration 2530/35720 Training loss: 1.3772 0.2715 sec/batch\n",
      "Epoch 2/20  Iteration 2531/35720 Training loss: 1.3772 0.2444 sec/batch\n",
      "Epoch 2/20  Iteration 2532/35720 Training loss: 1.3771 0.2245 sec/batch\n",
      "Epoch 2/20  Iteration 2533/35720 Training loss: 1.3771 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 2534/35720 Training loss: 1.3769 0.2239 sec/batch\n",
      "Epoch 2/20  Iteration 2535/35720 Training loss: 1.3768 0.2118 sec/batch\n",
      "Epoch 2/20  Iteration 2536/35720 Training loss: 1.3768 0.2166 sec/batch\n",
      "Epoch 2/20  Iteration 2537/35720 Training loss: 1.3768 0.2244 sec/batch\n",
      "Epoch 2/20  Iteration 2538/35720 Training loss: 1.3770 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 2539/35720 Training loss: 1.3767 0.2075 sec/batch\n",
      "Epoch 2/20  Iteration 2540/35720 Training loss: 1.3767 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 2541/35720 Training loss: 1.3765 0.2097 sec/batch\n",
      "Epoch 2/20  Iteration 2542/35720 Training loss: 1.3764 0.2282 sec/batch\n",
      "Epoch 2/20  Iteration 2543/35720 Training loss: 1.3762 0.2117 sec/batch\n",
      "Epoch 2/20  Iteration 2544/35720 Training loss: 1.3762 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 2545/35720 Training loss: 1.3762 0.2278 sec/batch\n",
      "Epoch 2/20  Iteration 2546/35720 Training loss: 1.3761 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2547/35720 Training loss: 1.3761 0.2299 sec/batch\n",
      "Epoch 2/20  Iteration 2548/35720 Training loss: 1.3761 0.2279 sec/batch\n",
      "Epoch 2/20  Iteration 2549/35720 Training loss: 1.3761 0.2156 sec/batch\n",
      "Epoch 2/20  Iteration 2550/35720 Training loss: 1.3759 0.2208 sec/batch\n",
      "Epoch 2/20  Iteration 2551/35720 Training loss: 1.3759 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 2552/35720 Training loss: 1.3759 0.2126 sec/batch\n",
      "Epoch 2/20  Iteration 2553/35720 Training loss: 1.3759 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 2554/35720 Training loss: 1.3758 0.2051 sec/batch\n",
      "Epoch 2/20  Iteration 2555/35720 Training loss: 1.3758 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2556/35720 Training loss: 1.3759 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 2557/35720 Training loss: 1.3759 0.2105 sec/batch\n",
      "Epoch 2/20  Iteration 2558/35720 Training loss: 1.3760 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 2559/35720 Training loss: 1.3759 0.2113 sec/batch\n",
      "Epoch 2/20  Iteration 2560/35720 Training loss: 1.3756 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 2561/35720 Training loss: 1.3754 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 2562/35720 Training loss: 1.3752 0.2192 sec/batch\n",
      "Epoch 2/20  Iteration 2563/35720 Training loss: 1.3753 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 2564/35720 Training loss: 1.3753 0.2130 sec/batch\n",
      "Epoch 2/20  Iteration 2565/35720 Training loss: 1.3753 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 2566/35720 Training loss: 1.3753 0.2245 sec/batch\n",
      "Epoch 2/20  Iteration 2567/35720 Training loss: 1.3752 0.2061 sec/batch\n",
      "Epoch 2/20  Iteration 2568/35720 Training loss: 1.3752 0.2111 sec/batch\n",
      "Epoch 2/20  Iteration 2569/35720 Training loss: 1.3753 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 2570/35720 Training loss: 1.3752 0.2309 sec/batch\n",
      "Epoch 2/20  Iteration 2571/35720 Training loss: 1.3751 0.2311 sec/batch\n",
      "Epoch 2/20  Iteration 2572/35720 Training loss: 1.3749 0.2170 sec/batch\n",
      "Epoch 2/20  Iteration 2573/35720 Training loss: 1.3749 0.2078 sec/batch\n",
      "Epoch 2/20  Iteration 2574/35720 Training loss: 1.3748 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 2575/35720 Training loss: 1.3749 0.2083 sec/batch\n",
      "Epoch 2/20  Iteration 2576/35720 Training loss: 1.3748 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 2577/35720 Training loss: 1.3749 0.2254 sec/batch\n",
      "Epoch 2/20  Iteration 2578/35720 Training loss: 1.3749 0.2226 sec/batch\n",
      "Epoch 2/20  Iteration 2579/35720 Training loss: 1.3748 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 2580/35720 Training loss: 1.3747 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 2581/35720 Training loss: 1.3747 0.2284 sec/batch\n",
      "Epoch 2/20  Iteration 2582/35720 Training loss: 1.3746 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 2583/35720 Training loss: 1.3745 0.2212 sec/batch\n",
      "Epoch 2/20  Iteration 2584/35720 Training loss: 1.3744 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 2585/35720 Training loss: 1.3743 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 2586/35720 Training loss: 1.3743 0.2074 sec/batch\n",
      "Epoch 2/20  Iteration 2587/35720 Training loss: 1.3742 0.2097 sec/batch\n",
      "Epoch 2/20  Iteration 2588/35720 Training loss: 1.3742 0.2150 sec/batch\n",
      "Epoch 2/20  Iteration 2589/35720 Training loss: 1.3743 0.2125 sec/batch\n",
      "Epoch 2/20  Iteration 2590/35720 Training loss: 1.3745 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 2591/35720 Training loss: 1.3746 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 2592/35720 Training loss: 1.3747 0.2151 sec/batch\n",
      "Epoch 2/20  Iteration 2593/35720 Training loss: 1.3747 0.2165 sec/batch\n",
      "Epoch 2/20  Iteration 2594/35720 Training loss: 1.3747 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 2595/35720 Training loss: 1.3748 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 2596/35720 Training loss: 1.3748 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 2597/35720 Training loss: 1.3749 0.2071 sec/batch\n",
      "Epoch 2/20  Iteration 2598/35720 Training loss: 1.3748 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2599/35720 Training loss: 1.3748 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2600/35720 Training loss: 1.3748 0.2112 sec/batch\n",
      "Validation loss: 1.47692 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 2601/35720 Training loss: 1.3749 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2602/35720 Training loss: 1.3748 0.2120 sec/batch\n",
      "Epoch 2/20  Iteration 2603/35720 Training loss: 1.3747 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 2604/35720 Training loss: 1.3747 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 2605/35720 Training loss: 1.3746 0.2115 sec/batch\n",
      "Epoch 2/20  Iteration 2606/35720 Training loss: 1.3746 0.2083 sec/batch\n",
      "Epoch 2/20  Iteration 2607/35720 Training loss: 1.3745 0.2061 sec/batch\n",
      "Epoch 2/20  Iteration 2608/35720 Training loss: 1.3744 0.2149 sec/batch\n",
      "Epoch 2/20  Iteration 2609/35720 Training loss: 1.3742 0.2149 sec/batch\n",
      "Epoch 2/20  Iteration 2610/35720 Training loss: 1.3741 0.2256 sec/batch\n",
      "Epoch 2/20  Iteration 2611/35720 Training loss: 1.3739 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 2612/35720 Training loss: 1.3738 0.2137 sec/batch\n",
      "Epoch 2/20  Iteration 2613/35720 Training loss: 1.3738 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 2614/35720 Training loss: 1.3735 0.2288 sec/batch\n",
      "Epoch 2/20  Iteration 2615/35720 Training loss: 1.3735 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 2616/35720 Training loss: 1.3734 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 2617/35720 Training loss: 1.3732 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 2618/35720 Training loss: 1.3733 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2619/35720 Training loss: 1.3735 0.2121 sec/batch\n",
      "Epoch 2/20  Iteration 2620/35720 Training loss: 1.3735 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 2621/35720 Training loss: 1.3735 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 2622/35720 Training loss: 1.3734 0.2139 sec/batch\n",
      "Epoch 2/20  Iteration 2623/35720 Training loss: 1.3734 0.2105 sec/batch\n",
      "Epoch 2/20  Iteration 2624/35720 Training loss: 1.3734 0.2240 sec/batch\n",
      "Epoch 2/20  Iteration 2625/35720 Training loss: 1.3735 0.2284 sec/batch\n",
      "Epoch 2/20  Iteration 2626/35720 Training loss: 1.3734 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 2627/35720 Training loss: 1.3735 0.2226 sec/batch\n",
      "Epoch 2/20  Iteration 2628/35720 Training loss: 1.3734 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2629/35720 Training loss: 1.3733 0.2141 sec/batch\n",
      "Epoch 2/20  Iteration 2630/35720 Training loss: 1.3734 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 2631/35720 Training loss: 1.3733 0.2052 sec/batch\n",
      "Epoch 2/20  Iteration 2632/35720 Training loss: 1.3732 0.2310 sec/batch\n",
      "Epoch 2/20  Iteration 2633/35720 Training loss: 1.3731 0.2116 sec/batch\n",
      "Epoch 2/20  Iteration 2634/35720 Training loss: 1.3731 0.2155 sec/batch\n",
      "Epoch 2/20  Iteration 2635/35720 Training loss: 1.3730 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 2636/35720 Training loss: 1.3729 0.2271 sec/batch\n",
      "Epoch 2/20  Iteration 2637/35720 Training loss: 1.3729 0.2157 sec/batch\n",
      "Epoch 2/20  Iteration 2638/35720 Training loss: 1.3728 0.2283 sec/batch\n",
      "Epoch 2/20  Iteration 2639/35720 Training loss: 1.3728 0.2103 sec/batch\n",
      "Epoch 2/20  Iteration 2640/35720 Training loss: 1.3728 0.2115 sec/batch\n",
      "Epoch 2/20  Iteration 2641/35720 Training loss: 1.3727 0.2075 sec/batch\n",
      "Epoch 2/20  Iteration 2642/35720 Training loss: 1.3726 0.2059 sec/batch\n",
      "Epoch 2/20  Iteration 2643/35720 Training loss: 1.3725 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 2644/35720 Training loss: 1.3725 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 2645/35720 Training loss: 1.3724 0.2081 sec/batch\n",
      "Epoch 2/20  Iteration 2646/35720 Training loss: 1.3723 0.2127 sec/batch\n",
      "Epoch 2/20  Iteration 2647/35720 Training loss: 1.3723 0.2150 sec/batch\n",
      "Epoch 2/20  Iteration 2648/35720 Training loss: 1.3721 0.2064 sec/batch\n",
      "Epoch 2/20  Iteration 2649/35720 Training loss: 1.3721 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2650/35720 Training loss: 1.3720 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 2651/35720 Training loss: 1.3719 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 2652/35720 Training loss: 1.3719 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 2653/35720 Training loss: 1.3720 0.2066 sec/batch\n",
      "Epoch 2/20  Iteration 2654/35720 Training loss: 1.3719 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2655/35720 Training loss: 1.3718 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 2656/35720 Training loss: 1.3718 0.2222 sec/batch\n",
      "Epoch 2/20  Iteration 2657/35720 Training loss: 1.3716 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2658/35720 Training loss: 1.3715 0.2181 sec/batch\n",
      "Epoch 2/20  Iteration 2659/35720 Training loss: 1.3713 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 2660/35720 Training loss: 1.3712 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 2661/35720 Training loss: 1.3711 0.2189 sec/batch\n",
      "Epoch 2/20  Iteration 2662/35720 Training loss: 1.3710 0.2282 sec/batch\n",
      "Epoch 2/20  Iteration 2663/35720 Training loss: 1.3708 0.2182 sec/batch\n",
      "Epoch 2/20  Iteration 2664/35720 Training loss: 1.3708 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 2665/35720 Training loss: 1.3707 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 2666/35720 Training loss: 1.3706 0.2159 sec/batch\n",
      "Epoch 2/20  Iteration 2667/35720 Training loss: 1.3706 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 2668/35720 Training loss: 1.3705 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 2669/35720 Training loss: 1.3706 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 2670/35720 Training loss: 1.3704 0.2300 sec/batch\n",
      "Epoch 2/20  Iteration 2671/35720 Training loss: 1.3704 0.2253 sec/batch\n",
      "Epoch 2/20  Iteration 2672/35720 Training loss: 1.3703 0.2295 sec/batch\n",
      "Epoch 2/20  Iteration 2673/35720 Training loss: 1.3702 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2674/35720 Training loss: 1.3701 0.2126 sec/batch\n",
      "Epoch 2/20  Iteration 2675/35720 Training loss: 1.3700 0.2129 sec/batch\n",
      "Epoch 2/20  Iteration 2676/35720 Training loss: 1.3698 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 2677/35720 Training loss: 1.3697 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 2678/35720 Training loss: 1.3696 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2679/35720 Training loss: 1.3695 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 2680/35720 Training loss: 1.3693 0.2252 sec/batch\n",
      "Epoch 2/20  Iteration 2681/35720 Training loss: 1.3693 0.2190 sec/batch\n",
      "Epoch 2/20  Iteration 2682/35720 Training loss: 1.3692 0.2216 sec/batch\n",
      "Epoch 2/20  Iteration 2683/35720 Training loss: 1.3690 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 2684/35720 Training loss: 1.3688 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2685/35720 Training loss: 1.3685 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2686/35720 Training loss: 1.3684 0.2120 sec/batch\n",
      "Epoch 2/20  Iteration 2687/35720 Training loss: 1.3683 0.2179 sec/batch\n",
      "Epoch 2/20  Iteration 2688/35720 Training loss: 1.3682 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 2689/35720 Training loss: 1.3680 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 2690/35720 Training loss: 1.3679 0.2259 sec/batch\n",
      "Epoch 2/20  Iteration 2691/35720 Training loss: 1.3677 0.2050 sec/batch\n",
      "Epoch 2/20  Iteration 2692/35720 Training loss: 1.3676 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2693/35720 Training loss: 1.3675 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 2694/35720 Training loss: 1.3675 0.2094 sec/batch\n",
      "Epoch 2/20  Iteration 2695/35720 Training loss: 1.3674 0.2109 sec/batch\n",
      "Epoch 2/20  Iteration 2696/35720 Training loss: 1.3674 0.2087 sec/batch\n",
      "Epoch 2/20  Iteration 2697/35720 Training loss: 1.3674 0.2105 sec/batch\n",
      "Epoch 2/20  Iteration 2698/35720 Training loss: 1.3673 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 2699/35720 Training loss: 1.3672 0.2220 sec/batch\n",
      "Epoch 2/20  Iteration 2700/35720 Training loss: 1.3672 0.2231 sec/batch\n",
      "Epoch 2/20  Iteration 2701/35720 Training loss: 1.3672 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 2702/35720 Training loss: 1.3672 0.2158 sec/batch\n",
      "Epoch 2/20  Iteration 2703/35720 Training loss: 1.3671 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 2704/35720 Training loss: 1.3671 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 2705/35720 Training loss: 1.3669 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 2706/35720 Training loss: 1.3669 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 2707/35720 Training loss: 1.3668 0.2292 sec/batch\n",
      "Epoch 2/20  Iteration 2708/35720 Training loss: 1.3666 0.2200 sec/batch\n",
      "Epoch 2/20  Iteration 2709/35720 Training loss: 1.3666 0.2124 sec/batch\n",
      "Epoch 2/20  Iteration 2710/35720 Training loss: 1.3666 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 2711/35720 Training loss: 1.3665 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 2712/35720 Training loss: 1.3665 0.2080 sec/batch\n",
      "Epoch 2/20  Iteration 2713/35720 Training loss: 1.3666 0.2318 sec/batch\n",
      "Epoch 2/20  Iteration 2714/35720 Training loss: 1.3664 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 2715/35720 Training loss: 1.3666 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 2716/35720 Training loss: 1.3666 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 2717/35720 Training loss: 1.3666 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 2718/35720 Training loss: 1.3666 0.2190 sec/batch\n",
      "Epoch 2/20  Iteration 2719/35720 Training loss: 1.3665 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 2720/35720 Training loss: 1.3665 0.2122 sec/batch\n",
      "Epoch 2/20  Iteration 2721/35720 Training loss: 1.3663 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 2722/35720 Training loss: 1.3661 0.2222 sec/batch\n",
      "Epoch 2/20  Iteration 2723/35720 Training loss: 1.3660 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 2724/35720 Training loss: 1.3659 0.2279 sec/batch\n",
      "Epoch 2/20  Iteration 2725/35720 Training loss: 1.3658 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 2726/35720 Training loss: 1.3657 0.2219 sec/batch\n",
      "Epoch 2/20  Iteration 2727/35720 Training loss: 1.3656 0.2138 sec/batch\n",
      "Epoch 2/20  Iteration 2728/35720 Training loss: 1.3656 0.2121 sec/batch\n",
      "Epoch 2/20  Iteration 2729/35720 Training loss: 1.3655 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 2730/35720 Training loss: 1.3656 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 2731/35720 Training loss: 1.3654 0.2064 sec/batch\n",
      "Epoch 2/20  Iteration 2732/35720 Training loss: 1.3654 0.2256 sec/batch\n",
      "Epoch 2/20  Iteration 2733/35720 Training loss: 1.3654 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 2734/35720 Training loss: 1.3651 0.2234 sec/batch\n",
      "Epoch 2/20  Iteration 2735/35720 Training loss: 1.3652 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2736/35720 Training loss: 1.3650 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 2737/35720 Training loss: 1.3649 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2738/35720 Training loss: 1.3647 0.2147 sec/batch\n",
      "Epoch 2/20  Iteration 2739/35720 Training loss: 1.3646 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2740/35720 Training loss: 1.3646 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 2741/35720 Training loss: 1.3645 0.2296 sec/batch\n",
      "Epoch 2/20  Iteration 2742/35720 Training loss: 1.3644 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 2743/35720 Training loss: 1.3643 0.2049 sec/batch\n",
      "Epoch 2/20  Iteration 2744/35720 Training loss: 1.3642 0.2117 sec/batch\n",
      "Epoch 2/20  Iteration 2745/35720 Training loss: 1.3642 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2746/35720 Training loss: 1.3641 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2747/35720 Training loss: 1.3640 0.2160 sec/batch\n",
      "Epoch 2/20  Iteration 2748/35720 Training loss: 1.3638 0.2291 sec/batch\n",
      "Epoch 2/20  Iteration 2749/35720 Training loss: 1.3637 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 2750/35720 Training loss: 1.3636 0.2061 sec/batch\n",
      "Epoch 2/20  Iteration 2751/35720 Training loss: 1.3636 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 2752/35720 Training loss: 1.3634 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 2753/35720 Training loss: 1.3635 0.2652 sec/batch\n",
      "Epoch 2/20  Iteration 2754/35720 Training loss: 1.3634 0.2284 sec/batch\n",
      "Epoch 2/20  Iteration 2755/35720 Training loss: 1.3636 0.2393 sec/batch\n",
      "Epoch 2/20  Iteration 2756/35720 Training loss: 1.3638 0.2261 sec/batch\n",
      "Epoch 2/20  Iteration 2757/35720 Training loss: 1.3636 0.2219 sec/batch\n",
      "Epoch 2/20  Iteration 2758/35720 Training loss: 1.3635 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2759/35720 Training loss: 1.3634 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 2760/35720 Training loss: 1.3633 0.2288 sec/batch\n",
      "Epoch 2/20  Iteration 2761/35720 Training loss: 1.3632 0.2061 sec/batch\n",
      "Epoch 2/20  Iteration 2762/35720 Training loss: 1.3632 0.2265 sec/batch\n",
      "Epoch 2/20  Iteration 2763/35720 Training loss: 1.3631 0.2276 sec/batch\n",
      "Epoch 2/20  Iteration 2764/35720 Training loss: 1.3632 0.2052 sec/batch\n",
      "Epoch 2/20  Iteration 2765/35720 Training loss: 1.3631 0.2238 sec/batch\n",
      "Epoch 2/20  Iteration 2766/35720 Training loss: 1.3630 0.2202 sec/batch\n",
      "Epoch 2/20  Iteration 2767/35720 Training loss: 1.3629 0.2134 sec/batch\n",
      "Epoch 2/20  Iteration 2768/35720 Training loss: 1.3627 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 2769/35720 Training loss: 1.3626 0.2175 sec/batch\n",
      "Epoch 2/20  Iteration 2770/35720 Training loss: 1.3625 0.2240 sec/batch\n",
      "Epoch 2/20  Iteration 2771/35720 Training loss: 1.3626 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 2772/35720 Training loss: 1.3625 0.2093 sec/batch\n",
      "Epoch 2/20  Iteration 2773/35720 Training loss: 1.3625 0.2234 sec/batch\n",
      "Epoch 2/20  Iteration 2774/35720 Training loss: 1.3625 0.2218 sec/batch\n",
      "Epoch 2/20  Iteration 2775/35720 Training loss: 1.3624 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 2776/35720 Training loss: 1.3624 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 2777/35720 Training loss: 1.3623 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 2778/35720 Training loss: 1.3623 0.2141 sec/batch\n",
      "Epoch 2/20  Iteration 2779/35720 Training loss: 1.3623 0.2244 sec/batch\n",
      "Epoch 2/20  Iteration 2780/35720 Training loss: 1.3622 0.2297 sec/batch\n",
      "Epoch 2/20  Iteration 2781/35720 Training loss: 1.3622 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2782/35720 Training loss: 1.3621 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2783/35720 Training loss: 1.3623 0.2087 sec/batch\n",
      "Epoch 2/20  Iteration 2784/35720 Training loss: 1.3623 0.2291 sec/batch\n",
      "Epoch 2/20  Iteration 2785/35720 Training loss: 1.3623 0.2114 sec/batch\n",
      "Epoch 2/20  Iteration 2786/35720 Training loss: 1.3622 0.2282 sec/batch\n",
      "Epoch 2/20  Iteration 2787/35720 Training loss: 1.3620 0.2182 sec/batch\n",
      "Epoch 2/20  Iteration 2788/35720 Training loss: 1.3618 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 2789/35720 Training loss: 1.3616 0.2254 sec/batch\n",
      "Epoch 2/20  Iteration 2790/35720 Training loss: 1.3615 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 2791/35720 Training loss: 1.3614 0.2227 sec/batch\n",
      "Epoch 2/20  Iteration 2792/35720 Training loss: 1.3612 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 2793/35720 Training loss: 1.3611 0.2170 sec/batch\n",
      "Epoch 2/20  Iteration 2794/35720 Training loss: 1.3610 0.2061 sec/batch\n",
      "Epoch 2/20  Iteration 2795/35720 Training loss: 1.3608 0.2090 sec/batch\n",
      "Epoch 2/20  Iteration 2796/35720 Training loss: 1.3606 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2797/35720 Training loss: 1.3605 0.2165 sec/batch\n",
      "Epoch 2/20  Iteration 2798/35720 Training loss: 1.3605 0.2200 sec/batch\n",
      "Epoch 2/20  Iteration 2799/35720 Training loss: 1.3604 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 2800/35720 Training loss: 1.3603 0.2180 sec/batch\n",
      "Validation loss: 1.45567 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 2801/35720 Training loss: 1.3603 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 2802/35720 Training loss: 1.3602 0.2142 sec/batch\n",
      "Epoch 2/20  Iteration 2803/35720 Training loss: 1.3602 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 2804/35720 Training loss: 1.3601 0.2285 sec/batch\n",
      "Epoch 2/20  Iteration 2805/35720 Training loss: 1.3600 0.2114 sec/batch\n",
      "Epoch 2/20  Iteration 2806/35720 Training loss: 1.3599 0.2100 sec/batch\n",
      "Epoch 2/20  Iteration 2807/35720 Training loss: 1.3598 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2808/35720 Training loss: 1.3599 0.2118 sec/batch\n",
      "Epoch 2/20  Iteration 2809/35720 Training loss: 1.3599 0.2287 sec/batch\n",
      "Epoch 2/20  Iteration 2810/35720 Training loss: 1.3599 0.2098 sec/batch\n",
      "Epoch 2/20  Iteration 2811/35720 Training loss: 1.3599 0.2323 sec/batch\n",
      "Epoch 2/20  Iteration 2812/35720 Training loss: 1.3598 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 2813/35720 Training loss: 1.3596 0.2279 sec/batch\n",
      "Epoch 2/20  Iteration 2814/35720 Training loss: 1.3595 0.2222 sec/batch\n",
      "Epoch 2/20  Iteration 2815/35720 Training loss: 1.3593 0.2309 sec/batch\n",
      "Epoch 2/20  Iteration 2816/35720 Training loss: 1.3592 0.2274 sec/batch\n",
      "Epoch 2/20  Iteration 2817/35720 Training loss: 1.3592 0.2157 sec/batch\n",
      "Epoch 2/20  Iteration 2818/35720 Training loss: 1.3592 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2819/35720 Training loss: 1.3591 0.2098 sec/batch\n",
      "Epoch 2/20  Iteration 2820/35720 Training loss: 1.3591 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 2821/35720 Training loss: 1.3591 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 2822/35720 Training loss: 1.3591 0.2160 sec/batch\n",
      "Epoch 2/20  Iteration 2823/35720 Training loss: 1.3591 0.2056 sec/batch\n",
      "Epoch 2/20  Iteration 2824/35720 Training loss: 1.3590 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 2825/35720 Training loss: 1.3589 0.2094 sec/batch\n",
      "Epoch 2/20  Iteration 2826/35720 Training loss: 1.3589 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2827/35720 Training loss: 1.3588 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2828/35720 Training loss: 1.3588 0.2112 sec/batch\n",
      "Epoch 2/20  Iteration 2829/35720 Training loss: 1.3587 0.2109 sec/batch\n",
      "Epoch 2/20  Iteration 2830/35720 Training loss: 1.3587 0.2299 sec/batch\n",
      "Epoch 2/20  Iteration 2831/35720 Training loss: 1.3587 0.2071 sec/batch\n",
      "Epoch 2/20  Iteration 2832/35720 Training loss: 1.3586 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 2833/35720 Training loss: 1.3584 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 2834/35720 Training loss: 1.3584 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2835/35720 Training loss: 1.3583 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 2836/35720 Training loss: 1.3582 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 2837/35720 Training loss: 1.3581 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 2838/35720 Training loss: 1.3581 0.2222 sec/batch\n",
      "Epoch 2/20  Iteration 2839/35720 Training loss: 1.3581 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2840/35720 Training loss: 1.3581 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2841/35720 Training loss: 1.3581 0.2055 sec/batch\n",
      "Epoch 2/20  Iteration 2842/35720 Training loss: 1.3580 0.2119 sec/batch\n",
      "Epoch 2/20  Iteration 2843/35720 Training loss: 1.3580 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2844/35720 Training loss: 1.3579 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 2845/35720 Training loss: 1.3578 0.2112 sec/batch\n",
      "Epoch 2/20  Iteration 2846/35720 Training loss: 1.3576 0.2212 sec/batch\n",
      "Epoch 2/20  Iteration 2847/35720 Training loss: 1.3576 0.2229 sec/batch\n",
      "Epoch 2/20  Iteration 2848/35720 Training loss: 1.3575 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2849/35720 Training loss: 1.3575 0.2121 sec/batch\n",
      "Epoch 2/20  Iteration 2850/35720 Training loss: 1.3574 0.2216 sec/batch\n",
      "Epoch 2/20  Iteration 2851/35720 Training loss: 1.3574 0.2055 sec/batch\n",
      "Epoch 2/20  Iteration 2852/35720 Training loss: 1.3572 0.2214 sec/batch\n",
      "Epoch 2/20  Iteration 2853/35720 Training loss: 1.3572 0.2075 sec/batch\n",
      "Epoch 2/20  Iteration 2854/35720 Training loss: 1.3571 0.2292 sec/batch\n",
      "Epoch 2/20  Iteration 2855/35720 Training loss: 1.3571 0.2316 sec/batch\n",
      "Epoch 2/20  Iteration 2856/35720 Training loss: 1.3570 0.2076 sec/batch\n",
      "Epoch 2/20  Iteration 2857/35720 Training loss: 1.3570 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 2858/35720 Training loss: 1.3569 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 2859/35720 Training loss: 1.3568 0.2190 sec/batch\n",
      "Epoch 2/20  Iteration 2860/35720 Training loss: 1.3568 0.2156 sec/batch\n",
      "Epoch 2/20  Iteration 2861/35720 Training loss: 1.3567 0.2062 sec/batch\n",
      "Epoch 2/20  Iteration 2862/35720 Training loss: 1.3568 0.2228 sec/batch\n",
      "Epoch 2/20  Iteration 2863/35720 Training loss: 1.3567 0.2100 sec/batch\n",
      "Epoch 2/20  Iteration 2864/35720 Training loss: 1.3566 0.2099 sec/batch\n",
      "Epoch 2/20  Iteration 2865/35720 Training loss: 1.3565 0.2165 sec/batch\n",
      "Epoch 2/20  Iteration 2866/35720 Training loss: 1.3565 0.2216 sec/batch\n",
      "Epoch 2/20  Iteration 2867/35720 Training loss: 1.3564 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 2868/35720 Training loss: 1.3563 0.2192 sec/batch\n",
      "Epoch 2/20  Iteration 2869/35720 Training loss: 1.3562 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 2870/35720 Training loss: 1.3560 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 2871/35720 Training loss: 1.3560 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 2872/35720 Training loss: 1.3559 0.2077 sec/batch\n",
      "Epoch 2/20  Iteration 2873/35720 Training loss: 1.3559 0.2128 sec/batch\n",
      "Epoch 2/20  Iteration 2874/35720 Training loss: 1.3559 0.2093 sec/batch\n",
      "Epoch 2/20  Iteration 2875/35720 Training loss: 1.3559 0.2098 sec/batch\n",
      "Epoch 2/20  Iteration 2876/35720 Training loss: 1.3558 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 2877/35720 Training loss: 1.3558 0.2082 sec/batch\n",
      "Epoch 2/20  Iteration 2878/35720 Training loss: 1.3557 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 2879/35720 Training loss: 1.3557 0.2097 sec/batch\n",
      "Epoch 2/20  Iteration 2880/35720 Training loss: 1.3556 0.2159 sec/batch\n",
      "Epoch 2/20  Iteration 2881/35720 Training loss: 1.3556 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 2882/35720 Training loss: 1.3555 0.2150 sec/batch\n",
      "Epoch 2/20  Iteration 2883/35720 Training loss: 1.3553 0.2266 sec/batch\n",
      "Epoch 2/20  Iteration 2884/35720 Training loss: 1.3553 0.2081 sec/batch\n",
      "Epoch 2/20  Iteration 2885/35720 Training loss: 1.3553 0.2227 sec/batch\n",
      "Epoch 2/20  Iteration 2886/35720 Training loss: 1.3552 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 2887/35720 Training loss: 1.3553 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 2888/35720 Training loss: 1.3552 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 2889/35720 Training loss: 1.3552 0.2187 sec/batch\n",
      "Epoch 2/20  Iteration 2890/35720 Training loss: 1.3552 0.2051 sec/batch\n",
      "Epoch 2/20  Iteration 2891/35720 Training loss: 1.3552 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 2892/35720 Training loss: 1.3550 0.2170 sec/batch\n",
      "Epoch 2/20  Iteration 2893/35720 Training loss: 1.3550 0.2094 sec/batch\n",
      "Epoch 2/20  Iteration 2894/35720 Training loss: 1.3551 0.2228 sec/batch\n",
      "Epoch 2/20  Iteration 2895/35720 Training loss: 1.3551 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 2896/35720 Training loss: 1.3550 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 2897/35720 Training loss: 1.3549 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2898/35720 Training loss: 1.3548 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 2899/35720 Training loss: 1.3547 0.2116 sec/batch\n",
      "Epoch 2/20  Iteration 2900/35720 Training loss: 1.3546 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 2901/35720 Training loss: 1.3546 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 2902/35720 Training loss: 1.3546 0.2423 sec/batch\n",
      "Epoch 2/20  Iteration 2903/35720 Training loss: 1.3544 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 2904/35720 Training loss: 1.3543 0.2112 sec/batch\n",
      "Epoch 2/20  Iteration 2905/35720 Training loss: 1.3542 0.2273 sec/batch\n",
      "Epoch 2/20  Iteration 2906/35720 Training loss: 1.3542 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 2907/35720 Training loss: 1.3543 0.2155 sec/batch\n",
      "Epoch 2/20  Iteration 2908/35720 Training loss: 1.3543 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 2909/35720 Training loss: 1.3545 0.2298 sec/batch\n",
      "Epoch 2/20  Iteration 2910/35720 Training loss: 1.3544 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 2911/35720 Training loss: 1.3543 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 2912/35720 Training loss: 1.3543 0.2079 sec/batch\n",
      "Epoch 2/20  Iteration 2913/35720 Training loss: 1.3542 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 2914/35720 Training loss: 1.3541 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 2915/35720 Training loss: 1.3540 0.2175 sec/batch\n",
      "Epoch 2/20  Iteration 2916/35720 Training loss: 1.3540 0.2139 sec/batch\n",
      "Epoch 2/20  Iteration 2917/35720 Training loss: 1.3540 0.2166 sec/batch\n",
      "Epoch 2/20  Iteration 2918/35720 Training loss: 1.3538 0.2138 sec/batch\n",
      "Epoch 2/20  Iteration 2919/35720 Training loss: 1.3537 0.2264 sec/batch\n",
      "Epoch 2/20  Iteration 2920/35720 Training loss: 1.3537 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 2921/35720 Training loss: 1.3536 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2922/35720 Training loss: 1.3535 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 2923/35720 Training loss: 1.3534 0.2221 sec/batch\n",
      "Epoch 2/20  Iteration 2924/35720 Training loss: 1.3532 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 2925/35720 Training loss: 1.3531 0.2119 sec/batch\n",
      "Epoch 2/20  Iteration 2926/35720 Training loss: 1.3531 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 2927/35720 Training loss: 1.3530 0.2074 sec/batch\n",
      "Epoch 2/20  Iteration 2928/35720 Training loss: 1.3529 0.2058 sec/batch\n",
      "Epoch 2/20  Iteration 2929/35720 Training loss: 1.3527 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 2930/35720 Training loss: 1.3526 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 2931/35720 Training loss: 1.3525 0.2250 sec/batch\n",
      "Epoch 2/20  Iteration 2932/35720 Training loss: 1.3524 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 2933/35720 Training loss: 1.3522 0.2220 sec/batch\n",
      "Epoch 2/20  Iteration 2934/35720 Training loss: 1.3521 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 2935/35720 Training loss: 1.3520 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 2936/35720 Training loss: 1.3519 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 2937/35720 Training loss: 1.3519 0.2157 sec/batch\n",
      "Epoch 2/20  Iteration 2938/35720 Training loss: 1.3518 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 2939/35720 Training loss: 1.3518 0.2127 sec/batch\n",
      "Epoch 2/20  Iteration 2940/35720 Training loss: 1.3518 0.2243 sec/batch\n",
      "Epoch 2/20  Iteration 2941/35720 Training loss: 1.3517 0.2077 sec/batch\n",
      "Epoch 2/20  Iteration 2942/35720 Training loss: 1.3516 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 2943/35720 Training loss: 1.3516 0.2310 sec/batch\n",
      "Epoch 2/20  Iteration 2944/35720 Training loss: 1.3515 0.2057 sec/batch\n",
      "Epoch 2/20  Iteration 2945/35720 Training loss: 1.3514 0.2106 sec/batch\n",
      "Epoch 2/20  Iteration 2946/35720 Training loss: 1.3513 0.2212 sec/batch\n",
      "Epoch 2/20  Iteration 2947/35720 Training loss: 1.3511 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 2948/35720 Training loss: 1.3511 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 2949/35720 Training loss: 1.3512 0.2252 sec/batch\n",
      "Epoch 2/20  Iteration 2950/35720 Training loss: 1.3512 0.2250 sec/batch\n",
      "Epoch 2/20  Iteration 2951/35720 Training loss: 1.3511 0.2181 sec/batch\n",
      "Epoch 2/20  Iteration 2952/35720 Training loss: 1.3510 0.2066 sec/batch\n",
      "Epoch 2/20  Iteration 2953/35720 Training loss: 1.3508 0.2121 sec/batch\n",
      "Epoch 2/20  Iteration 2954/35720 Training loss: 1.3508 0.2261 sec/batch\n",
      "Epoch 2/20  Iteration 2955/35720 Training loss: 1.3509 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 2956/35720 Training loss: 1.3509 0.2066 sec/batch\n",
      "Epoch 2/20  Iteration 2957/35720 Training loss: 1.3508 0.2103 sec/batch\n",
      "Epoch 2/20  Iteration 2958/35720 Training loss: 1.3508 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 2959/35720 Training loss: 1.3506 0.2072 sec/batch\n",
      "Epoch 2/20  Iteration 2960/35720 Training loss: 1.3505 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 2961/35720 Training loss: 1.3505 0.2062 sec/batch\n",
      "Epoch 2/20  Iteration 2962/35720 Training loss: 1.3506 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 2963/35720 Training loss: 1.3506 0.2167 sec/batch\n",
      "Epoch 2/20  Iteration 2964/35720 Training loss: 1.3506 0.2239 sec/batch\n",
      "Epoch 2/20  Iteration 2965/35720 Training loss: 1.3505 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2966/35720 Training loss: 1.3505 0.2305 sec/batch\n",
      "Epoch 2/20  Iteration 2967/35720 Training loss: 1.3505 0.2149 sec/batch\n",
      "Epoch 2/20  Iteration 2968/35720 Training loss: 1.3505 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2969/35720 Training loss: 1.3504 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 2970/35720 Training loss: 1.3503 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 2971/35720 Training loss: 1.3502 0.2165 sec/batch\n",
      "Epoch 2/20  Iteration 2972/35720 Training loss: 1.3501 0.2241 sec/batch\n",
      "Epoch 2/20  Iteration 2973/35720 Training loss: 1.3501 0.2417 sec/batch\n",
      "Epoch 2/20  Iteration 2974/35720 Training loss: 1.3501 0.2414 sec/batch\n",
      "Epoch 2/20  Iteration 2975/35720 Training loss: 1.3500 0.2190 sec/batch\n",
      "Epoch 2/20  Iteration 2976/35720 Training loss: 1.3500 0.2170 sec/batch\n",
      "Epoch 2/20  Iteration 2977/35720 Training loss: 1.3500 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 2978/35720 Training loss: 1.3500 0.2137 sec/batch\n",
      "Epoch 2/20  Iteration 2979/35720 Training loss: 1.3499 0.2294 sec/batch\n",
      "Epoch 2/20  Iteration 2980/35720 Training loss: 1.3498 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 2981/35720 Training loss: 1.3498 0.2278 sec/batch\n",
      "Epoch 2/20  Iteration 2982/35720 Training loss: 1.3498 0.2075 sec/batch\n",
      "Epoch 2/20  Iteration 2983/35720 Training loss: 1.3498 0.2167 sec/batch\n",
      "Epoch 2/20  Iteration 2984/35720 Training loss: 1.3497 0.2230 sec/batch\n",
      "Epoch 2/20  Iteration 2985/35720 Training loss: 1.3497 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 2986/35720 Training loss: 1.3497 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 2987/35720 Training loss: 1.3496 0.2264 sec/batch\n",
      "Epoch 2/20  Iteration 2988/35720 Training loss: 1.3494 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 2989/35720 Training loss: 1.3493 0.2384 sec/batch\n",
      "Epoch 2/20  Iteration 2990/35720 Training loss: 1.3492 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 2991/35720 Training loss: 1.3491 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 2992/35720 Training loss: 1.3490 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 2993/35720 Training loss: 1.3490 0.2258 sec/batch\n",
      "Epoch 2/20  Iteration 2994/35720 Training loss: 1.3489 0.2128 sec/batch\n",
      "Epoch 2/20  Iteration 2995/35720 Training loss: 1.3488 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 2996/35720 Training loss: 1.3488 0.2267 sec/batch\n",
      "Epoch 2/20  Iteration 2997/35720 Training loss: 1.3487 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 2998/35720 Training loss: 1.3487 0.2262 sec/batch\n",
      "Epoch 2/20  Iteration 2999/35720 Training loss: 1.3486 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 3000/35720 Training loss: 1.3485 0.2188 sec/batch\n",
      "Validation loss: 1.44866 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 3001/35720 Training loss: 1.3485 0.2118 sec/batch\n",
      "Epoch 2/20  Iteration 3002/35720 Training loss: 1.3485 0.2189 sec/batch\n",
      "Epoch 2/20  Iteration 3003/35720 Training loss: 1.3484 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 3004/35720 Training loss: 1.3484 0.2093 sec/batch\n",
      "Epoch 2/20  Iteration 3005/35720 Training loss: 1.3483 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 3006/35720 Training loss: 1.3482 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 3007/35720 Training loss: 1.3481 0.2253 sec/batch\n",
      "Epoch 2/20  Iteration 3008/35720 Training loss: 1.3481 0.2388 sec/batch\n",
      "Epoch 2/20  Iteration 3009/35720 Training loss: 1.3480 0.2357 sec/batch\n",
      "Epoch 2/20  Iteration 3010/35720 Training loss: 1.3479 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 3011/35720 Training loss: 1.3478 0.2080 sec/batch\n",
      "Epoch 2/20  Iteration 3012/35720 Training loss: 1.3478 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 3013/35720 Training loss: 1.3478 0.2257 sec/batch\n",
      "Epoch 2/20  Iteration 3014/35720 Training loss: 1.3477 0.2098 sec/batch\n",
      "Epoch 2/20  Iteration 3015/35720 Training loss: 1.3477 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 3016/35720 Training loss: 1.3478 0.2151 sec/batch\n",
      "Epoch 2/20  Iteration 3017/35720 Training loss: 1.3477 0.2111 sec/batch\n",
      "Epoch 2/20  Iteration 3018/35720 Training loss: 1.3475 0.2316 sec/batch\n",
      "Epoch 2/20  Iteration 3019/35720 Training loss: 1.3474 0.2067 sec/batch\n",
      "Epoch 2/20  Iteration 3020/35720 Training loss: 1.3474 0.2255 sec/batch\n",
      "Epoch 2/20  Iteration 3021/35720 Training loss: 1.3474 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 3022/35720 Training loss: 1.3473 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 3023/35720 Training loss: 1.3471 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 3024/35720 Training loss: 1.3470 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 3025/35720 Training loss: 1.3468 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 3026/35720 Training loss: 1.3467 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 3027/35720 Training loss: 1.3465 0.2277 sec/batch\n",
      "Epoch 2/20  Iteration 3028/35720 Training loss: 1.3465 0.2083 sec/batch\n",
      "Epoch 2/20  Iteration 3029/35720 Training loss: 1.3464 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 3030/35720 Training loss: 1.3462 0.2284 sec/batch\n",
      "Epoch 2/20  Iteration 3031/35720 Training loss: 1.3462 0.2264 sec/batch\n",
      "Epoch 2/20  Iteration 3032/35720 Training loss: 1.3461 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 3033/35720 Training loss: 1.3460 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 3034/35720 Training loss: 1.3460 0.2113 sec/batch\n",
      "Epoch 2/20  Iteration 3035/35720 Training loss: 1.3458 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 3036/35720 Training loss: 1.3457 0.2130 sec/batch\n",
      "Epoch 2/20  Iteration 3037/35720 Training loss: 1.3456 0.2344 sec/batch\n",
      "Epoch 2/20  Iteration 3038/35720 Training loss: 1.3455 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 3039/35720 Training loss: 1.3455 0.2151 sec/batch\n",
      "Epoch 2/20  Iteration 3040/35720 Training loss: 1.3454 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 3041/35720 Training loss: 1.3454 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 3042/35720 Training loss: 1.3452 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 3043/35720 Training loss: 1.3451 0.2235 sec/batch\n",
      "Epoch 2/20  Iteration 3044/35720 Training loss: 1.3450 0.2230 sec/batch\n",
      "Epoch 2/20  Iteration 3045/35720 Training loss: 1.3450 0.2261 sec/batch\n",
      "Epoch 2/20  Iteration 3046/35720 Training loss: 1.3450 0.2286 sec/batch\n",
      "Epoch 2/20  Iteration 3047/35720 Training loss: 1.3449 0.2061 sec/batch\n",
      "Epoch 2/20  Iteration 3048/35720 Training loss: 1.3448 0.2094 sec/batch\n",
      "Epoch 2/20  Iteration 3049/35720 Training loss: 1.3447 0.2155 sec/batch\n",
      "Epoch 2/20  Iteration 3050/35720 Training loss: 1.3446 0.2160 sec/batch\n",
      "Epoch 2/20  Iteration 3051/35720 Training loss: 1.3445 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 3052/35720 Training loss: 1.3444 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 3053/35720 Training loss: 1.3443 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 3054/35720 Training loss: 1.3442 0.2225 sec/batch\n",
      "Epoch 2/20  Iteration 3055/35720 Training loss: 1.3442 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 3056/35720 Training loss: 1.3442 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 3057/35720 Training loss: 1.3441 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 3058/35720 Training loss: 1.3440 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 3059/35720 Training loss: 1.3439 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3060/35720 Training loss: 1.3438 0.2047 sec/batch\n",
      "Epoch 2/20  Iteration 3061/35720 Training loss: 1.3437 0.2095 sec/batch\n",
      "Epoch 2/20  Iteration 3062/35720 Training loss: 1.3437 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 3063/35720 Training loss: 1.3435 0.2239 sec/batch\n",
      "Epoch 2/20  Iteration 3064/35720 Training loss: 1.3434 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 3065/35720 Training loss: 1.3433 0.2223 sec/batch\n",
      "Epoch 2/20  Iteration 3066/35720 Training loss: 1.3432 0.2225 sec/batch\n",
      "Epoch 2/20  Iteration 3067/35720 Training loss: 1.3430 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 3068/35720 Training loss: 1.3429 0.2072 sec/batch\n",
      "Epoch 2/20  Iteration 3069/35720 Training loss: 1.3428 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 3070/35720 Training loss: 1.3427 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 3071/35720 Training loss: 1.3425 0.2175 sec/batch\n",
      "Epoch 2/20  Iteration 3072/35720 Training loss: 1.3424 0.2122 sec/batch\n",
      "Epoch 2/20  Iteration 3073/35720 Training loss: 1.3424 0.2266 sec/batch\n",
      "Epoch 2/20  Iteration 3074/35720 Training loss: 1.3423 0.2102 sec/batch\n",
      "Epoch 2/20  Iteration 3075/35720 Training loss: 1.3423 0.2285 sec/batch\n",
      "Epoch 2/20  Iteration 3076/35720 Training loss: 1.3422 0.2099 sec/batch\n",
      "Epoch 2/20  Iteration 3077/35720 Training loss: 1.3421 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 3078/35720 Training loss: 1.3420 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 3079/35720 Training loss: 1.3420 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 3080/35720 Training loss: 1.3421 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 3081/35720 Training loss: 1.3420 0.2233 sec/batch\n",
      "Epoch 2/20  Iteration 3082/35720 Training loss: 1.3420 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 3083/35720 Training loss: 1.3420 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3084/35720 Training loss: 1.3420 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 3085/35720 Training loss: 1.3418 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3086/35720 Training loss: 1.3417 0.2175 sec/batch\n",
      "Epoch 2/20  Iteration 3087/35720 Training loss: 1.3415 0.2181 sec/batch\n",
      "Epoch 2/20  Iteration 3088/35720 Training loss: 1.3415 0.2206 sec/batch\n",
      "Epoch 2/20  Iteration 3089/35720 Training loss: 1.3415 0.2259 sec/batch\n",
      "Epoch 2/20  Iteration 3090/35720 Training loss: 1.3414 0.2127 sec/batch\n",
      "Epoch 2/20  Iteration 3091/35720 Training loss: 1.3413 0.2231 sec/batch\n",
      "Epoch 2/20  Iteration 3092/35720 Training loss: 1.3412 0.2250 sec/batch\n",
      "Epoch 2/20  Iteration 3093/35720 Training loss: 1.3412 0.2258 sec/batch\n",
      "Epoch 2/20  Iteration 3094/35720 Training loss: 1.3410 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 3095/35720 Training loss: 1.3409 0.2104 sec/batch\n",
      "Epoch 2/20  Iteration 3096/35720 Training loss: 1.3408 0.2060 sec/batch\n",
      "Epoch 2/20  Iteration 3097/35720 Training loss: 1.3407 0.2248 sec/batch\n",
      "Epoch 2/20  Iteration 3098/35720 Training loss: 1.3407 0.2335 sec/batch\n",
      "Epoch 2/20  Iteration 3099/35720 Training loss: 1.3406 0.2156 sec/batch\n",
      "Epoch 2/20  Iteration 3100/35720 Training loss: 1.3405 0.2220 sec/batch\n",
      "Epoch 2/20  Iteration 3101/35720 Training loss: 1.3405 0.2307 sec/batch\n",
      "Epoch 2/20  Iteration 3102/35720 Training loss: 1.3405 0.2291 sec/batch\n",
      "Epoch 2/20  Iteration 3103/35720 Training loss: 1.3404 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3104/35720 Training loss: 1.3403 0.2218 sec/batch\n",
      "Epoch 2/20  Iteration 3105/35720 Training loss: 1.3402 0.2292 sec/batch\n",
      "Epoch 2/20  Iteration 3106/35720 Training loss: 1.3401 0.2070 sec/batch\n",
      "Epoch 2/20  Iteration 3107/35720 Training loss: 1.3401 0.2099 sec/batch\n",
      "Epoch 2/20  Iteration 3108/35720 Training loss: 1.3400 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 3109/35720 Training loss: 1.3399 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 3110/35720 Training loss: 1.3399 0.2151 sec/batch\n",
      "Epoch 2/20  Iteration 3111/35720 Training loss: 1.3399 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 3112/35720 Training loss: 1.3399 0.2095 sec/batch\n",
      "Epoch 2/20  Iteration 3113/35720 Training loss: 1.3398 0.2116 sec/batch\n",
      "Epoch 2/20  Iteration 3114/35720 Training loss: 1.3398 0.2247 sec/batch\n",
      "Epoch 2/20  Iteration 3115/35720 Training loss: 1.3397 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 3116/35720 Training loss: 1.3396 0.2126 sec/batch\n",
      "Epoch 2/20  Iteration 3117/35720 Training loss: 1.3395 0.2141 sec/batch\n",
      "Epoch 2/20  Iteration 3118/35720 Training loss: 1.3394 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 3119/35720 Training loss: 1.3393 0.2257 sec/batch\n",
      "Epoch 2/20  Iteration 3120/35720 Training loss: 1.3392 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 3121/35720 Training loss: 1.3392 0.2164 sec/batch\n",
      "Epoch 2/20  Iteration 3122/35720 Training loss: 1.3393 0.2222 sec/batch\n",
      "Epoch 2/20  Iteration 3123/35720 Training loss: 1.3393 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 3124/35720 Training loss: 1.3392 0.2120 sec/batch\n",
      "Epoch 2/20  Iteration 3125/35720 Training loss: 1.3392 0.2295 sec/batch\n",
      "Epoch 2/20  Iteration 3126/35720 Training loss: 1.3391 0.2271 sec/batch\n",
      "Epoch 2/20  Iteration 3127/35720 Training loss: 1.3390 0.2095 sec/batch\n",
      "Epoch 2/20  Iteration 3128/35720 Training loss: 1.3389 0.2074 sec/batch\n",
      "Epoch 2/20  Iteration 3129/35720 Training loss: 1.3388 0.2159 sec/batch\n",
      "Epoch 2/20  Iteration 3130/35720 Training loss: 1.3387 0.2067 sec/batch\n",
      "Epoch 2/20  Iteration 3131/35720 Training loss: 1.3387 0.2301 sec/batch\n",
      "Epoch 2/20  Iteration 3132/35720 Training loss: 1.3387 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 3133/35720 Training loss: 1.3386 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 3134/35720 Training loss: 1.3386 0.2269 sec/batch\n",
      "Epoch 2/20  Iteration 3135/35720 Training loss: 1.3385 0.2244 sec/batch\n",
      "Epoch 2/20  Iteration 3136/35720 Training loss: 1.3386 0.2311 sec/batch\n",
      "Epoch 2/20  Iteration 3137/35720 Training loss: 1.3386 0.2208 sec/batch\n",
      "Epoch 2/20  Iteration 3138/35720 Training loss: 1.3386 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 3139/35720 Training loss: 1.3386 0.2303 sec/batch\n",
      "Epoch 2/20  Iteration 3140/35720 Training loss: 1.3385 0.2214 sec/batch\n",
      "Epoch 2/20  Iteration 3141/35720 Training loss: 1.3384 0.2155 sec/batch\n",
      "Epoch 2/20  Iteration 3142/35720 Training loss: 1.3383 0.2239 sec/batch\n",
      "Epoch 2/20  Iteration 3143/35720 Training loss: 1.3382 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 3144/35720 Training loss: 1.3381 0.2153 sec/batch\n",
      "Epoch 2/20  Iteration 3145/35720 Training loss: 1.3381 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 3146/35720 Training loss: 1.3380 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 3147/35720 Training loss: 1.3379 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 3148/35720 Training loss: 1.3378 0.2200 sec/batch\n",
      "Epoch 2/20  Iteration 3149/35720 Training loss: 1.3378 0.2353 sec/batch\n",
      "Epoch 2/20  Iteration 3150/35720 Training loss: 1.3378 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 3151/35720 Training loss: 1.3377 0.2221 sec/batch\n",
      "Epoch 2/20  Iteration 3152/35720 Training loss: 1.3376 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 3153/35720 Training loss: 1.3376 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 3154/35720 Training loss: 1.3376 0.2166 sec/batch\n",
      "Epoch 2/20  Iteration 3155/35720 Training loss: 1.3376 0.2305 sec/batch\n",
      "Epoch 2/20  Iteration 3156/35720 Training loss: 1.3376 0.2260 sec/batch\n",
      "Epoch 2/20  Iteration 3157/35720 Training loss: 1.3375 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 3158/35720 Training loss: 1.3376 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 3159/35720 Training loss: 1.3375 0.2175 sec/batch\n",
      "Epoch 2/20  Iteration 3160/35720 Training loss: 1.3375 0.2240 sec/batch\n",
      "Epoch 2/20  Iteration 3161/35720 Training loss: 1.3374 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 3162/35720 Training loss: 1.3374 0.2159 sec/batch\n",
      "Epoch 2/20  Iteration 3163/35720 Training loss: 1.3373 0.2057 sec/batch\n",
      "Epoch 2/20  Iteration 3164/35720 Training loss: 1.3373 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 3165/35720 Training loss: 1.3372 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 3166/35720 Training loss: 1.3372 0.2130 sec/batch\n",
      "Epoch 2/20  Iteration 3167/35720 Training loss: 1.3371 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 3168/35720 Training loss: 1.3371 0.2083 sec/batch\n",
      "Epoch 2/20  Iteration 3169/35720 Training loss: 1.3370 0.2219 sec/batch\n",
      "Epoch 2/20  Iteration 3170/35720 Training loss: 1.3369 0.2216 sec/batch\n",
      "Epoch 2/20  Iteration 3171/35720 Training loss: 1.3369 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 3172/35720 Training loss: 1.3369 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 3173/35720 Training loss: 1.3369 0.2200 sec/batch\n",
      "Epoch 2/20  Iteration 3174/35720 Training loss: 1.3368 0.2288 sec/batch\n",
      "Epoch 2/20  Iteration 3175/35720 Training loss: 1.3367 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 3176/35720 Training loss: 1.3366 0.2202 sec/batch\n",
      "Epoch 2/20  Iteration 3177/35720 Training loss: 1.3366 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 3178/35720 Training loss: 1.3364 0.2054 sec/batch\n",
      "Epoch 2/20  Iteration 3179/35720 Training loss: 1.3364 0.2086 sec/batch\n",
      "Epoch 2/20  Iteration 3180/35720 Training loss: 1.3362 0.2122 sec/batch\n",
      "Epoch 2/20  Iteration 3181/35720 Training loss: 1.3361 0.2293 sec/batch\n",
      "Epoch 2/20  Iteration 3182/35720 Training loss: 1.3360 0.2110 sec/batch\n",
      "Epoch 2/20  Iteration 3183/35720 Training loss: 1.3359 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 3184/35720 Training loss: 1.3358 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 3185/35720 Training loss: 1.3357 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 3186/35720 Training loss: 1.3356 0.2119 sec/batch\n",
      "Epoch 2/20  Iteration 3187/35720 Training loss: 1.3355 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 3188/35720 Training loss: 1.3355 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 3189/35720 Training loss: 1.3354 0.2245 sec/batch\n",
      "Epoch 2/20  Iteration 3190/35720 Training loss: 1.3354 0.2222 sec/batch\n",
      "Epoch 2/20  Iteration 3191/35720 Training loss: 1.3354 0.2345 sec/batch\n",
      "Epoch 2/20  Iteration 3192/35720 Training loss: 1.3353 0.2765 sec/batch\n",
      "Epoch 2/20  Iteration 3193/35720 Training loss: 1.3353 0.2398 sec/batch\n",
      "Epoch 2/20  Iteration 3194/35720 Training loss: 1.3353 0.2134 sec/batch\n",
      "Epoch 2/20  Iteration 3195/35720 Training loss: 1.3353 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 3196/35720 Training loss: 1.3352 0.2278 sec/batch\n",
      "Epoch 2/20  Iteration 3197/35720 Training loss: 1.3352 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 3198/35720 Training loss: 1.3351 0.2357 sec/batch\n",
      "Epoch 2/20  Iteration 3199/35720 Training loss: 1.3351 0.2231 sec/batch\n",
      "Epoch 2/20  Iteration 3200/35720 Training loss: 1.3350 0.2245 sec/batch\n",
      "Validation loss: 1.43751 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 3201/35720 Training loss: 1.3351 0.2074 sec/batch\n",
      "Epoch 2/20  Iteration 3202/35720 Training loss: 1.3350 0.2096 sec/batch\n",
      "Epoch 2/20  Iteration 3203/35720 Training loss: 1.3350 0.2109 sec/batch\n",
      "Epoch 2/20  Iteration 3204/35720 Training loss: 1.3350 0.2076 sec/batch\n",
      "Epoch 2/20  Iteration 3205/35720 Training loss: 1.3350 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 3206/35720 Training loss: 1.3350 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 3207/35720 Training loss: 1.3350 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 3208/35720 Training loss: 1.3348 0.2263 sec/batch\n",
      "Epoch 2/20  Iteration 3209/35720 Training loss: 1.3348 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 3210/35720 Training loss: 1.3347 0.2187 sec/batch\n",
      "Epoch 2/20  Iteration 3211/35720 Training loss: 1.3347 0.2280 sec/batch\n",
      "Epoch 2/20  Iteration 3212/35720 Training loss: 1.3347 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 3213/35720 Training loss: 1.3347 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 3214/35720 Training loss: 1.3347 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 3215/35720 Training loss: 1.3347 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 3216/35720 Training loss: 1.3346 0.2147 sec/batch\n",
      "Epoch 2/20  Iteration 3217/35720 Training loss: 1.3345 0.2192 sec/batch\n",
      "Epoch 2/20  Iteration 3218/35720 Training loss: 1.3344 0.2063 sec/batch\n",
      "Epoch 2/20  Iteration 3219/35720 Training loss: 1.3342 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 3220/35720 Training loss: 1.3342 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 3221/35720 Training loss: 1.3341 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 3222/35720 Training loss: 1.3340 0.2218 sec/batch\n",
      "Epoch 2/20  Iteration 3223/35720 Training loss: 1.3340 0.2114 sec/batch\n",
      "Epoch 2/20  Iteration 3224/35720 Training loss: 1.3340 0.2117 sec/batch\n",
      "Epoch 2/20  Iteration 3225/35720 Training loss: 1.3340 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 3226/35720 Training loss: 1.3340 0.2126 sec/batch\n",
      "Epoch 2/20  Iteration 3227/35720 Training loss: 1.3340 0.2095 sec/batch\n",
      "Epoch 2/20  Iteration 3228/35720 Training loss: 1.3340 0.2083 sec/batch\n",
      "Epoch 2/20  Iteration 3229/35720 Training loss: 1.3339 0.2095 sec/batch\n",
      "Epoch 2/20  Iteration 3230/35720 Training loss: 1.3338 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 3231/35720 Training loss: 1.3337 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 3232/35720 Training loss: 1.3337 0.2256 sec/batch\n",
      "Epoch 2/20  Iteration 3233/35720 Training loss: 1.3337 0.2127 sec/batch\n",
      "Epoch 2/20  Iteration 3234/35720 Training loss: 1.3336 0.2209 sec/batch\n",
      "Epoch 2/20  Iteration 3235/35720 Training loss: 1.3335 0.2166 sec/batch\n",
      "Epoch 2/20  Iteration 3236/35720 Training loss: 1.3334 0.2118 sec/batch\n",
      "Epoch 2/20  Iteration 3237/35720 Training loss: 1.3333 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 3238/35720 Training loss: 1.3332 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 3239/35720 Training loss: 1.3331 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 3240/35720 Training loss: 1.3331 0.2247 sec/batch\n",
      "Epoch 2/20  Iteration 3241/35720 Training loss: 1.3330 0.2238 sec/batch\n",
      "Epoch 2/20  Iteration 3242/35720 Training loss: 1.3330 0.2264 sec/batch\n",
      "Epoch 2/20  Iteration 3243/35720 Training loss: 1.3329 0.2088 sec/batch\n",
      "Epoch 2/20  Iteration 3244/35720 Training loss: 1.3330 0.2158 sec/batch\n",
      "Epoch 2/20  Iteration 3245/35720 Training loss: 1.3330 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 3246/35720 Training loss: 1.3330 0.2222 sec/batch\n",
      "Epoch 2/20  Iteration 3247/35720 Training loss: 1.3330 0.2128 sec/batch\n",
      "Epoch 2/20  Iteration 3248/35720 Training loss: 1.3330 0.2231 sec/batch\n",
      "Epoch 2/20  Iteration 3249/35720 Training loss: 1.3329 0.2193 sec/batch\n",
      "Epoch 2/20  Iteration 3250/35720 Training loss: 1.3329 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 3251/35720 Training loss: 1.3328 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 3252/35720 Training loss: 1.3327 0.2260 sec/batch\n",
      "Epoch 2/20  Iteration 3253/35720 Training loss: 1.3326 0.2312 sec/batch\n",
      "Epoch 2/20  Iteration 3254/35720 Training loss: 1.3326 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 3255/35720 Training loss: 1.3325 0.2167 sec/batch\n",
      "Epoch 2/20  Iteration 3256/35720 Training loss: 1.3324 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 3257/35720 Training loss: 1.3325 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 3258/35720 Training loss: 1.3324 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 3259/35720 Training loss: 1.3323 0.2122 sec/batch\n",
      "Epoch 2/20  Iteration 3260/35720 Training loss: 1.3321 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 3261/35720 Training loss: 1.3320 0.2115 sec/batch\n",
      "Epoch 2/20  Iteration 3262/35720 Training loss: 1.3318 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3263/35720 Training loss: 1.3317 0.2276 sec/batch\n",
      "Epoch 2/20  Iteration 3264/35720 Training loss: 1.3316 0.2248 sec/batch\n",
      "Epoch 2/20  Iteration 3265/35720 Training loss: 1.3315 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 3266/35720 Training loss: 1.3314 0.2066 sec/batch\n",
      "Epoch 2/20  Iteration 3267/35720 Training loss: 1.3314 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 3268/35720 Training loss: 1.3313 0.2298 sec/batch\n",
      "Epoch 2/20  Iteration 3269/35720 Training loss: 1.3312 0.2090 sec/batch\n",
      "Epoch 2/20  Iteration 3270/35720 Training loss: 1.3311 0.2311 sec/batch\n",
      "Epoch 2/20  Iteration 3271/35720 Training loss: 1.3309 0.2291 sec/batch\n",
      "Epoch 2/20  Iteration 3272/35720 Training loss: 1.3309 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 3273/35720 Training loss: 1.3308 0.2114 sec/batch\n",
      "Epoch 2/20  Iteration 3274/35720 Training loss: 1.3308 0.2244 sec/batch\n",
      "Epoch 2/20  Iteration 3275/35720 Training loss: 1.3307 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3276/35720 Training loss: 1.3307 0.2134 sec/batch\n",
      "Epoch 2/20  Iteration 3277/35720 Training loss: 1.3307 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 3278/35720 Training loss: 1.3305 0.2208 sec/batch\n",
      "Epoch 2/20  Iteration 3279/35720 Training loss: 1.3305 0.2159 sec/batch\n",
      "Epoch 2/20  Iteration 3280/35720 Training loss: 1.3304 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 3281/35720 Training loss: 1.3304 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 3282/35720 Training loss: 1.3303 0.2192 sec/batch\n",
      "Epoch 2/20  Iteration 3283/35720 Training loss: 1.3302 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 3284/35720 Training loss: 1.3303 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 3285/35720 Training loss: 1.3303 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 3286/35720 Training loss: 1.3302 0.2227 sec/batch\n",
      "Epoch 2/20  Iteration 3287/35720 Training loss: 1.3301 0.2317 sec/batch\n",
      "Epoch 2/20  Iteration 3288/35720 Training loss: 1.3300 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 3289/35720 Training loss: 1.3300 0.2269 sec/batch\n",
      "Epoch 2/20  Iteration 3290/35720 Training loss: 1.3299 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 3291/35720 Training loss: 1.3299 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 3292/35720 Training loss: 1.3299 0.2096 sec/batch\n",
      "Epoch 2/20  Iteration 3293/35720 Training loss: 1.3298 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 3294/35720 Training loss: 1.3298 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 3295/35720 Training loss: 1.3297 0.2078 sec/batch\n",
      "Epoch 2/20  Iteration 3296/35720 Training loss: 1.3297 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 3297/35720 Training loss: 1.3297 0.2187 sec/batch\n",
      "Epoch 2/20  Iteration 3298/35720 Training loss: 1.3296 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 3299/35720 Training loss: 1.3295 0.2176 sec/batch\n",
      "Epoch 2/20  Iteration 3300/35720 Training loss: 1.3295 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 3301/35720 Training loss: 1.3295 0.2167 sec/batch\n",
      "Epoch 2/20  Iteration 3302/35720 Training loss: 1.3295 0.2166 sec/batch\n",
      "Epoch 2/20  Iteration 3303/35720 Training loss: 1.3294 0.2226 sec/batch\n",
      "Epoch 2/20  Iteration 3304/35720 Training loss: 1.3294 0.2085 sec/batch\n",
      "Epoch 2/20  Iteration 3305/35720 Training loss: 1.3294 0.2083 sec/batch\n",
      "Epoch 2/20  Iteration 3306/35720 Training loss: 1.3294 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 3307/35720 Training loss: 1.3293 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3308/35720 Training loss: 1.3293 0.2310 sec/batch\n",
      "Epoch 2/20  Iteration 3309/35720 Training loss: 1.3293 0.2074 sec/batch\n",
      "Epoch 2/20  Iteration 3310/35720 Training loss: 1.3293 0.2127 sec/batch\n",
      "Epoch 2/20  Iteration 3311/35720 Training loss: 1.3293 0.2297 sec/batch\n",
      "Epoch 2/20  Iteration 3312/35720 Training loss: 1.3293 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 3313/35720 Training loss: 1.3293 0.2182 sec/batch\n",
      "Epoch 2/20  Iteration 3314/35720 Training loss: 1.3292 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 3315/35720 Training loss: 1.3292 0.2135 sec/batch\n",
      "Epoch 2/20  Iteration 3316/35720 Training loss: 1.3292 0.2089 sec/batch\n",
      "Epoch 2/20  Iteration 3317/35720 Training loss: 1.3292 0.2127 sec/batch\n",
      "Epoch 2/20  Iteration 3318/35720 Training loss: 1.3291 0.2284 sec/batch\n",
      "Epoch 2/20  Iteration 3319/35720 Training loss: 1.3291 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 3320/35720 Training loss: 1.3290 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 3321/35720 Training loss: 1.3290 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 3322/35720 Training loss: 1.3289 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 3323/35720 Training loss: 1.3288 0.2276 sec/batch\n",
      "Epoch 2/20  Iteration 3324/35720 Training loss: 1.3288 0.2075 sec/batch\n",
      "Epoch 2/20  Iteration 3325/35720 Training loss: 1.3287 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 3326/35720 Training loss: 1.3286 0.2304 sec/batch\n",
      "Epoch 2/20  Iteration 3327/35720 Training loss: 1.3285 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3328/35720 Training loss: 1.3285 0.2132 sec/batch\n",
      "Epoch 2/20  Iteration 3329/35720 Training loss: 1.3284 0.2259 sec/batch\n",
      "Epoch 2/20  Iteration 3330/35720 Training loss: 1.3283 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 3331/35720 Training loss: 1.3282 0.2056 sec/batch\n",
      "Epoch 2/20  Iteration 3332/35720 Training loss: 1.3280 0.2170 sec/batch\n",
      "Epoch 2/20  Iteration 3333/35720 Training loss: 1.3281 0.2214 sec/batch\n",
      "Epoch 2/20  Iteration 3334/35720 Training loss: 1.3280 0.2208 sec/batch\n",
      "Epoch 2/20  Iteration 3335/35720 Training loss: 1.3280 0.2075 sec/batch\n",
      "Epoch 2/20  Iteration 3336/35720 Training loss: 1.3279 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 3337/35720 Training loss: 1.3278 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 3338/35720 Training loss: 1.3277 0.2225 sec/batch\n",
      "Epoch 2/20  Iteration 3339/35720 Training loss: 1.3276 0.2181 sec/batch\n",
      "Epoch 2/20  Iteration 3340/35720 Training loss: 1.3275 0.2250 sec/batch\n",
      "Epoch 2/20  Iteration 3341/35720 Training loss: 1.3274 0.2234 sec/batch\n",
      "Epoch 2/20  Iteration 3342/35720 Training loss: 1.3273 0.2102 sec/batch\n",
      "Epoch 2/20  Iteration 3343/35720 Training loss: 1.3272 0.2106 sec/batch\n",
      "Epoch 2/20  Iteration 3344/35720 Training loss: 1.3271 0.2201 sec/batch\n",
      "Epoch 2/20  Iteration 3345/35720 Training loss: 1.3270 0.2242 sec/batch\n",
      "Epoch 2/20  Iteration 3346/35720 Training loss: 1.3269 0.2131 sec/batch\n",
      "Epoch 2/20  Iteration 3347/35720 Training loss: 1.3268 0.2072 sec/batch\n",
      "Epoch 2/20  Iteration 3348/35720 Training loss: 1.3268 0.2254 sec/batch\n",
      "Epoch 2/20  Iteration 3349/35720 Training loss: 1.3267 0.2158 sec/batch\n",
      "Epoch 2/20  Iteration 3350/35720 Training loss: 1.3266 0.2181 sec/batch\n",
      "Epoch 2/20  Iteration 3351/35720 Training loss: 1.3265 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 3352/35720 Training loss: 1.3264 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 3353/35720 Training loss: 1.3264 0.2185 sec/batch\n",
      "Epoch 2/20  Iteration 3354/35720 Training loss: 1.3264 0.2218 sec/batch\n",
      "Epoch 2/20  Iteration 3355/35720 Training loss: 1.3263 0.2248 sec/batch\n",
      "Epoch 2/20  Iteration 3356/35720 Training loss: 1.3263 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 3357/35720 Training loss: 1.3262 0.2080 sec/batch\n",
      "Epoch 2/20  Iteration 3358/35720 Training loss: 1.3261 0.2158 sec/batch\n",
      "Epoch 2/20  Iteration 3359/35720 Training loss: 1.3260 0.2175 sec/batch\n",
      "Epoch 2/20  Iteration 3360/35720 Training loss: 1.3260 0.2177 sec/batch\n",
      "Epoch 2/20  Iteration 3361/35720 Training loss: 1.3259 0.2137 sec/batch\n",
      "Epoch 2/20  Iteration 3362/35720 Training loss: 1.3258 0.2237 sec/batch\n",
      "Epoch 2/20  Iteration 3363/35720 Training loss: 1.3257 0.2091 sec/batch\n",
      "Epoch 2/20  Iteration 3364/35720 Training loss: 1.3257 0.2230 sec/batch\n",
      "Epoch 2/20  Iteration 3365/35720 Training loss: 1.3256 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 3366/35720 Training loss: 1.3255 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 3367/35720 Training loss: 1.3255 0.2189 sec/batch\n",
      "Epoch 2/20  Iteration 3368/35720 Training loss: 1.3254 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 3369/35720 Training loss: 1.3253 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 3370/35720 Training loss: 1.3252 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 3371/35720 Training loss: 1.3251 0.2225 sec/batch\n",
      "Epoch 2/20  Iteration 3372/35720 Training loss: 1.3250 0.2065 sec/batch\n",
      "Epoch 2/20  Iteration 3373/35720 Training loss: 1.3249 0.2179 sec/batch\n",
      "Epoch 2/20  Iteration 3374/35720 Training loss: 1.3248 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 3375/35720 Training loss: 1.3247 0.2092 sec/batch\n",
      "Epoch 2/20  Iteration 3376/35720 Training loss: 1.3247 0.2134 sec/batch\n",
      "Epoch 2/20  Iteration 3377/35720 Training loss: 1.3246 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 3378/35720 Training loss: 1.3245 0.2151 sec/batch\n",
      "Epoch 2/20  Iteration 3379/35720 Training loss: 1.3245 0.2202 sec/batch\n",
      "Epoch 2/20  Iteration 3380/35720 Training loss: 1.3244 0.2221 sec/batch\n",
      "Epoch 2/20  Iteration 3381/35720 Training loss: 1.3243 0.2122 sec/batch\n",
      "Epoch 2/20  Iteration 3382/35720 Training loss: 1.3242 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 3383/35720 Training loss: 1.3242 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 3384/35720 Training loss: 1.3240 0.2147 sec/batch\n",
      "Epoch 2/20  Iteration 3385/35720 Training loss: 1.3240 0.2132 sec/batch\n",
      "Epoch 2/20  Iteration 3386/35720 Training loss: 1.3240 0.2115 sec/batch\n",
      "Epoch 2/20  Iteration 3387/35720 Training loss: 1.3239 0.2136 sec/batch\n",
      "Epoch 2/20  Iteration 3388/35720 Training loss: 1.3239 0.2205 sec/batch\n",
      "Epoch 2/20  Iteration 3389/35720 Training loss: 1.3238 0.2282 sec/batch\n",
      "Epoch 2/20  Iteration 3390/35720 Training loss: 1.3238 0.2247 sec/batch\n",
      "Epoch 2/20  Iteration 3391/35720 Training loss: 1.3237 0.2069 sec/batch\n",
      "Epoch 2/20  Iteration 3392/35720 Training loss: 1.3236 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 3393/35720 Training loss: 1.3236 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 3394/35720 Training loss: 1.3235 0.2221 sec/batch\n",
      "Epoch 2/20  Iteration 3395/35720 Training loss: 1.3234 0.2117 sec/batch\n",
      "Epoch 2/20  Iteration 3396/35720 Training loss: 1.3233 0.2160 sec/batch\n",
      "Epoch 2/20  Iteration 3397/35720 Training loss: 1.3232 0.2131 sec/batch\n",
      "Epoch 2/20  Iteration 3398/35720 Training loss: 1.3232 0.2230 sec/batch\n",
      "Epoch 2/20  Iteration 3399/35720 Training loss: 1.3231 0.2187 sec/batch\n",
      "Epoch 2/20  Iteration 3400/35720 Training loss: 1.3230 0.2194 sec/batch\n",
      "Validation loss: 1.42523 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 3401/35720 Training loss: 1.3230 0.2090 sec/batch\n",
      "Epoch 2/20  Iteration 3402/35720 Training loss: 1.3229 0.2106 sec/batch\n",
      "Epoch 2/20  Iteration 3403/35720 Training loss: 1.3229 0.2117 sec/batch\n",
      "Epoch 2/20  Iteration 3404/35720 Training loss: 1.3228 0.2118 sec/batch\n",
      "Epoch 2/20  Iteration 3405/35720 Training loss: 1.3227 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 3406/35720 Training loss: 1.3226 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 3407/35720 Training loss: 1.3225 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 3408/35720 Training loss: 1.3225 0.2147 sec/batch\n",
      "Epoch 2/20  Iteration 3409/35720 Training loss: 1.3225 0.2254 sec/batch\n",
      "Epoch 2/20  Iteration 3410/35720 Training loss: 1.3224 0.2240 sec/batch\n",
      "Epoch 2/20  Iteration 3411/35720 Training loss: 1.3224 0.2144 sec/batch\n",
      "Epoch 2/20  Iteration 3412/35720 Training loss: 1.3225 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 3413/35720 Training loss: 1.3225 0.2295 sec/batch\n",
      "Epoch 2/20  Iteration 3414/35720 Training loss: 1.3224 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 3415/35720 Training loss: 1.3224 0.2298 sec/batch\n",
      "Epoch 2/20  Iteration 3416/35720 Training loss: 1.3223 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 3417/35720 Training loss: 1.3223 0.2138 sec/batch\n",
      "Epoch 2/20  Iteration 3418/35720 Training loss: 1.3222 0.2140 sec/batch\n",
      "Epoch 2/20  Iteration 3419/35720 Training loss: 1.3221 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 3420/35720 Training loss: 1.3220 0.2276 sec/batch\n",
      "Epoch 2/20  Iteration 3421/35720 Training loss: 1.3220 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 3422/35720 Training loss: 1.3219 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 3423/35720 Training loss: 1.3219 0.2207 sec/batch\n",
      "Epoch 2/20  Iteration 3424/35720 Training loss: 1.3218 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 3425/35720 Training loss: 1.3217 0.2263 sec/batch\n",
      "Epoch 2/20  Iteration 3426/35720 Training loss: 1.3216 0.2289 sec/batch\n",
      "Epoch 2/20  Iteration 3427/35720 Training loss: 1.3216 0.2282 sec/batch\n",
      "Epoch 2/20  Iteration 3428/35720 Training loss: 1.3215 0.2232 sec/batch\n",
      "Epoch 2/20  Iteration 3429/35720 Training loss: 1.3214 0.2251 sec/batch\n",
      "Epoch 2/20  Iteration 3430/35720 Training loss: 1.3213 0.2145 sec/batch\n",
      "Epoch 2/20  Iteration 3431/35720 Training loss: 1.3213 0.2206 sec/batch\n",
      "Epoch 2/20  Iteration 3432/35720 Training loss: 1.3213 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 3433/35720 Training loss: 1.3213 0.2068 sec/batch\n",
      "Epoch 2/20  Iteration 3434/35720 Training loss: 1.3213 0.2093 sec/batch\n",
      "Epoch 2/20  Iteration 3435/35720 Training loss: 1.3213 0.2271 sec/batch\n",
      "Epoch 2/20  Iteration 3436/35720 Training loss: 1.3212 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 3437/35720 Training loss: 1.3212 0.2113 sec/batch\n",
      "Epoch 2/20  Iteration 3438/35720 Training loss: 1.3212 0.2249 sec/batch\n",
      "Epoch 2/20  Iteration 3439/35720 Training loss: 1.3212 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 3440/35720 Training loss: 1.3211 0.2252 sec/batch\n",
      "Epoch 2/20  Iteration 3441/35720 Training loss: 1.3211 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 3442/35720 Training loss: 1.3211 0.2128 sec/batch\n",
      "Epoch 2/20  Iteration 3443/35720 Training loss: 1.3211 0.2076 sec/batch\n",
      "Epoch 2/20  Iteration 3444/35720 Training loss: 1.3211 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 3445/35720 Training loss: 1.3210 0.2231 sec/batch\n",
      "Epoch 2/20  Iteration 3446/35720 Training loss: 1.3210 0.2223 sec/batch\n",
      "Epoch 2/20  Iteration 3447/35720 Training loss: 1.3209 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 3448/35720 Training loss: 1.3208 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 3449/35720 Training loss: 1.3208 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 3450/35720 Training loss: 1.3207 0.2203 sec/batch\n",
      "Epoch 2/20  Iteration 3451/35720 Training loss: 1.3208 0.2164 sec/batch\n",
      "Epoch 2/20  Iteration 3452/35720 Training loss: 1.3207 0.2131 sec/batch\n",
      "Epoch 2/20  Iteration 3453/35720 Training loss: 1.3207 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 3454/35720 Training loss: 1.3207 0.2143 sec/batch\n",
      "Epoch 2/20  Iteration 3455/35720 Training loss: 1.3208 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 3456/35720 Training loss: 1.3207 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 3457/35720 Training loss: 1.3207 0.2234 sec/batch\n",
      "Epoch 2/20  Iteration 3458/35720 Training loss: 1.3206 0.2112 sec/batch\n",
      "Epoch 2/20  Iteration 3459/35720 Training loss: 1.3207 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 3460/35720 Training loss: 1.3207 0.2192 sec/batch\n",
      "Epoch 2/20  Iteration 3461/35720 Training loss: 1.3206 0.2141 sec/batch\n",
      "Epoch 2/20  Iteration 3462/35720 Training loss: 1.3206 0.2171 sec/batch\n",
      "Epoch 2/20  Iteration 3463/35720 Training loss: 1.3205 0.2173 sec/batch\n",
      "Epoch 2/20  Iteration 3464/35720 Training loss: 1.3205 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 3465/35720 Training loss: 1.3204 0.2154 sec/batch\n",
      "Epoch 2/20  Iteration 3466/35720 Training loss: 1.3204 0.2139 sec/batch\n",
      "Epoch 2/20  Iteration 3467/35720 Training loss: 1.3203 0.2221 sec/batch\n",
      "Epoch 2/20  Iteration 3468/35720 Training loss: 1.3202 0.2106 sec/batch\n",
      "Epoch 2/20  Iteration 3469/35720 Training loss: 1.3202 0.2111 sec/batch\n",
      "Epoch 2/20  Iteration 3470/35720 Training loss: 1.3202 0.2277 sec/batch\n",
      "Epoch 2/20  Iteration 3471/35720 Training loss: 1.3201 0.2316 sec/batch\n",
      "Epoch 2/20  Iteration 3472/35720 Training loss: 1.3201 0.2254 sec/batch\n",
      "Epoch 2/20  Iteration 3473/35720 Training loss: 1.3201 0.2272 sec/batch\n",
      "Epoch 2/20  Iteration 3474/35720 Training loss: 1.3200 0.2246 sec/batch\n",
      "Epoch 2/20  Iteration 3475/35720 Training loss: 1.3200 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 3476/35720 Training loss: 1.3199 0.2196 sec/batch\n",
      "Epoch 2/20  Iteration 3477/35720 Training loss: 1.3198 0.2162 sec/batch\n",
      "Epoch 2/20  Iteration 3478/35720 Training loss: 1.3197 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 3479/35720 Training loss: 1.3196 0.2181 sec/batch\n",
      "Epoch 2/20  Iteration 3480/35720 Training loss: 1.3196 0.2182 sec/batch\n",
      "Epoch 2/20  Iteration 3481/35720 Training loss: 1.3195 0.2111 sec/batch\n",
      "Epoch 2/20  Iteration 3482/35720 Training loss: 1.3194 0.2195 sec/batch\n",
      "Epoch 2/20  Iteration 3483/35720 Training loss: 1.3194 0.2073 sec/batch\n",
      "Epoch 2/20  Iteration 3484/35720 Training loss: 1.3193 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 3485/35720 Training loss: 1.3193 0.2214 sec/batch\n",
      "Epoch 2/20  Iteration 3486/35720 Training loss: 1.3192 0.2217 sec/batch\n",
      "Epoch 2/20  Iteration 3487/35720 Training loss: 1.3192 0.2149 sec/batch\n",
      "Epoch 2/20  Iteration 3488/35720 Training loss: 1.3191 0.2199 sec/batch\n",
      "Epoch 2/20  Iteration 3489/35720 Training loss: 1.3190 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 3490/35720 Training loss: 1.3189 0.2210 sec/batch\n",
      "Epoch 2/20  Iteration 3491/35720 Training loss: 1.3188 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 3492/35720 Training loss: 1.3188 0.2184 sec/batch\n",
      "Epoch 2/20  Iteration 3493/35720 Training loss: 1.3186 0.2145 sec/batch\n",
      "Epoch 2/20  Iteration 3494/35720 Training loss: 1.3186 0.2213 sec/batch\n",
      "Epoch 2/20  Iteration 3495/35720 Training loss: 1.3186 0.2276 sec/batch\n",
      "Epoch 2/20  Iteration 3496/35720 Training loss: 1.3186 0.2258 sec/batch\n",
      "Epoch 2/20  Iteration 3497/35720 Training loss: 1.3186 0.2098 sec/batch\n",
      "Epoch 2/20  Iteration 3498/35720 Training loss: 1.3186 0.2165 sec/batch\n",
      "Epoch 2/20  Iteration 3499/35720 Training loss: 1.3185 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 3500/35720 Training loss: 1.3185 0.2129 sec/batch\n",
      "Epoch 2/20  Iteration 3501/35720 Training loss: 1.3185 0.2229 sec/batch\n",
      "Epoch 2/20  Iteration 3502/35720 Training loss: 1.3184 0.2239 sec/batch\n",
      "Epoch 2/20  Iteration 3503/35720 Training loss: 1.3184 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 3504/35720 Training loss: 1.3184 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 3505/35720 Training loss: 1.3183 0.2211 sec/batch\n",
      "Epoch 2/20  Iteration 3506/35720 Training loss: 1.3183 0.2165 sec/batch\n",
      "Epoch 2/20  Iteration 3507/35720 Training loss: 1.3182 0.2178 sec/batch\n",
      "Epoch 2/20  Iteration 3508/35720 Training loss: 1.3182 0.2197 sec/batch\n",
      "Epoch 2/20  Iteration 3509/35720 Training loss: 1.3181 0.2101 sec/batch\n",
      "Epoch 2/20  Iteration 3510/35720 Training loss: 1.3181 0.2269 sec/batch\n",
      "Epoch 2/20  Iteration 3511/35720 Training loss: 1.3179 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 3512/35720 Training loss: 1.3179 0.2083 sec/batch\n",
      "Epoch 2/20  Iteration 3513/35720 Training loss: 1.3179 0.2161 sec/batch\n",
      "Epoch 2/20  Iteration 3514/35720 Training loss: 1.3178 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 3515/35720 Training loss: 1.3177 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 3516/35720 Training loss: 1.3177 0.2168 sec/batch\n",
      "Epoch 2/20  Iteration 3517/35720 Training loss: 1.3176 0.2204 sec/batch\n",
      "Epoch 2/20  Iteration 3518/35720 Training loss: 1.3176 0.2167 sec/batch\n",
      "Epoch 2/20  Iteration 3519/35720 Training loss: 1.3175 0.2218 sec/batch\n",
      "Epoch 2/20  Iteration 3520/35720 Training loss: 1.3174 0.2169 sec/batch\n",
      "Epoch 2/20  Iteration 3521/35720 Training loss: 1.3174 0.2190 sec/batch\n",
      "Epoch 2/20  Iteration 3522/35720 Training loss: 1.3173 0.2262 sec/batch\n",
      "Epoch 2/20  Iteration 3523/35720 Training loss: 1.3174 0.2191 sec/batch\n",
      "Epoch 2/20  Iteration 3524/35720 Training loss: 1.3173 0.2219 sec/batch\n",
      "Epoch 2/20  Iteration 3525/35720 Training loss: 1.3173 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 3526/35720 Training loss: 1.3172 0.2130 sec/batch\n",
      "Epoch 2/20  Iteration 3527/35720 Training loss: 1.3171 0.2172 sec/batch\n",
      "Epoch 2/20  Iteration 3528/35720 Training loss: 1.3171 0.2117 sec/batch\n",
      "Epoch 2/20  Iteration 3529/35720 Training loss: 1.3171 0.2138 sec/batch\n",
      "Epoch 2/20  Iteration 3530/35720 Training loss: 1.3172 0.2159 sec/batch\n",
      "Epoch 2/20  Iteration 3531/35720 Training loss: 1.3172 0.2236 sec/batch\n",
      "Epoch 2/20  Iteration 3532/35720 Training loss: 1.3172 0.2126 sec/batch\n",
      "Epoch 2/20  Iteration 3533/35720 Training loss: 1.3171 0.2113 sec/batch\n",
      "Epoch 2/20  Iteration 3534/35720 Training loss: 1.3171 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 3535/35720 Training loss: 1.3171 0.2149 sec/batch\n",
      "Epoch 2/20  Iteration 3536/35720 Training loss: 1.3170 0.2189 sec/batch\n",
      "Epoch 2/20  Iteration 3537/35720 Training loss: 1.3169 0.2180 sec/batch\n",
      "Epoch 2/20  Iteration 3538/35720 Training loss: 1.3169 0.2155 sec/batch\n",
      "Epoch 2/20  Iteration 3539/35720 Training loss: 1.3168 0.2152 sec/batch\n",
      "Epoch 2/20  Iteration 3540/35720 Training loss: 1.3168 0.2165 sec/batch\n",
      "Epoch 2/20  Iteration 3541/35720 Training loss: 1.3167 0.2252 sec/batch\n",
      "Epoch 2/20  Iteration 3542/35720 Training loss: 1.3167 0.2174 sec/batch\n",
      "Epoch 2/20  Iteration 3543/35720 Training loss: 1.3166 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 3544/35720 Training loss: 1.3165 0.2130 sec/batch\n",
      "Epoch 2/20  Iteration 3545/35720 Training loss: 1.3165 0.2183 sec/batch\n",
      "Epoch 2/20  Iteration 3546/35720 Training loss: 1.3164 0.2224 sec/batch\n",
      "Epoch 2/20  Iteration 3547/35720 Training loss: 1.3164 0.2248 sec/batch\n",
      "Epoch 2/20  Iteration 3548/35720 Training loss: 1.3163 0.2215 sec/batch\n",
      "Epoch 2/20  Iteration 3549/35720 Training loss: 1.3162 0.2245 sec/batch\n",
      "Epoch 2/20  Iteration 3550/35720 Training loss: 1.3162 0.2127 sec/batch\n",
      "Epoch 2/20  Iteration 3551/35720 Training loss: 1.3161 0.2149 sec/batch\n",
      "Epoch 2/20  Iteration 3552/35720 Training loss: 1.3160 0.2120 sec/batch\n",
      "Epoch 2/20  Iteration 3553/35720 Training loss: 1.3160 0.2187 sec/batch\n",
      "Epoch 2/20  Iteration 3554/35720 Training loss: 1.3159 0.2270 sec/batch\n",
      "Epoch 2/20  Iteration 3555/35720 Training loss: 1.3159 0.2194 sec/batch\n",
      "Epoch 2/20  Iteration 3556/35720 Training loss: 1.3159 0.2219 sec/batch\n",
      "Epoch 2/20  Iteration 3557/35720 Training loss: 1.3159 0.2148 sec/batch\n",
      "Epoch 2/20  Iteration 3558/35720 Training loss: 1.3158 0.2133 sec/batch\n",
      "Epoch 2/20  Iteration 3559/35720 Training loss: 1.3156 0.2276 sec/batch\n",
      "Epoch 2/20  Iteration 3560/35720 Training loss: 1.3156 0.2188 sec/batch\n",
      "Epoch 2/20  Iteration 3561/35720 Training loss: 1.3155 0.2186 sec/batch\n",
      "Epoch 2/20  Iteration 3562/35720 Training loss: 1.3154 0.2240 sec/batch\n",
      "Epoch 2/20  Iteration 3563/35720 Training loss: 1.3154 0.2198 sec/batch\n",
      "Epoch 2/20  Iteration 3564/35720 Training loss: 1.3153 0.2200 sec/batch\n",
      "Epoch 2/20  Iteration 3565/35720 Training loss: 1.3152 0.2163 sec/batch\n",
      "Epoch 2/20  Iteration 3566/35720 Training loss: 1.3151 0.2192 sec/batch\n",
      "Epoch 2/20  Iteration 3567/35720 Training loss: 1.3150 0.2107 sec/batch\n",
      "Epoch 2/20  Iteration 3568/35720 Training loss: 1.3150 0.2084 sec/batch\n",
      "Epoch 2/20  Iteration 3569/35720 Training loss: 1.3149 0.2216 sec/batch\n",
      "Epoch 2/20  Iteration 3570/35720 Training loss: 1.3148 0.2239 sec/batch\n",
      "Epoch 2/20  Iteration 3571/35720 Training loss: 1.3147 0.2214 sec/batch\n",
      "Epoch 2/20  Iteration 3572/35720 Training loss: 1.3147 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 3573/35720 Training loss: 1.2059 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 3574/35720 Training loss: 1.2124 0.2305 sec/batch\n",
      "Epoch 3/20  Iteration 3575/35720 Training loss: 1.2229 0.2068 sec/batch\n",
      "Epoch 3/20  Iteration 3576/35720 Training loss: 1.2139 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 3577/35720 Training loss: 1.2306 0.2228 sec/batch\n",
      "Epoch 3/20  Iteration 3578/35720 Training loss: 1.2160 0.2267 sec/batch\n",
      "Epoch 3/20  Iteration 3579/35720 Training loss: 1.2132 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 3580/35720 Training loss: 1.1998 0.2230 sec/batch\n",
      "Epoch 3/20  Iteration 3581/35720 Training loss: 1.1941 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 3582/35720 Training loss: 1.2039 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 3583/35720 Training loss: 1.2073 0.2206 sec/batch\n",
      "Epoch 3/20  Iteration 3584/35720 Training loss: 1.2056 0.2128 sec/batch\n",
      "Epoch 3/20  Iteration 3585/35720 Training loss: 1.2095 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 3586/35720 Training loss: 1.2210 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 3587/35720 Training loss: 1.2280 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 3588/35720 Training loss: 1.2313 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 3589/35720 Training loss: 1.2328 0.2078 sec/batch\n",
      "Epoch 3/20  Iteration 3590/35720 Training loss: 1.2294 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 3591/35720 Training loss: 1.2312 0.2261 sec/batch\n",
      "Epoch 3/20  Iteration 3592/35720 Training loss: 1.2348 0.2181 sec/batch\n",
      "Epoch 3/20  Iteration 3593/35720 Training loss: 1.2387 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 3594/35720 Training loss: 1.2357 0.2092 sec/batch\n",
      "Epoch 3/20  Iteration 3595/35720 Training loss: 1.2381 0.2117 sec/batch\n",
      "Epoch 3/20  Iteration 3596/35720 Training loss: 1.2403 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 3597/35720 Training loss: 1.2457 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 3598/35720 Training loss: 1.2454 0.2181 sec/batch\n",
      "Epoch 3/20  Iteration 3599/35720 Training loss: 1.2527 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 3600/35720 Training loss: 1.2534 0.2186 sec/batch\n",
      "Validation loss: 1.40005 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 3601/35720 Training loss: 1.2574 0.2086 sec/batch\n",
      "Epoch 3/20  Iteration 3602/35720 Training loss: 1.2567 0.2081 sec/batch\n",
      "Epoch 3/20  Iteration 3603/35720 Training loss: 1.2643 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 3604/35720 Training loss: 1.2611 0.2247 sec/batch\n",
      "Epoch 3/20  Iteration 3605/35720 Training loss: 1.2638 0.2099 sec/batch\n",
      "Epoch 3/20  Iteration 3606/35720 Training loss: 1.2673 0.2245 sec/batch\n",
      "Epoch 3/20  Iteration 3607/35720 Training loss: 1.2699 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 3608/35720 Training loss: 1.2693 0.2273 sec/batch\n",
      "Epoch 3/20  Iteration 3609/35720 Training loss: 1.2685 0.2305 sec/batch\n",
      "Epoch 3/20  Iteration 3610/35720 Training loss: 1.2661 0.2102 sec/batch\n",
      "Epoch 3/20  Iteration 3611/35720 Training loss: 1.2633 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 3612/35720 Training loss: 1.2628 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 3613/35720 Training loss: 1.2607 0.2158 sec/batch\n",
      "Epoch 3/20  Iteration 3614/35720 Training loss: 1.2591 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 3615/35720 Training loss: 1.2552 0.2233 sec/batch\n",
      "Epoch 3/20  Iteration 3616/35720 Training loss: 1.2533 0.2226 sec/batch\n",
      "Epoch 3/20  Iteration 3617/35720 Training loss: 1.2525 0.2136 sec/batch\n",
      "Epoch 3/20  Iteration 3618/35720 Training loss: 1.2509 0.2255 sec/batch\n",
      "Epoch 3/20  Iteration 3619/35720 Training loss: 1.2498 0.2290 sec/batch\n",
      "Epoch 3/20  Iteration 3620/35720 Training loss: 1.2487 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 3621/35720 Training loss: 1.2497 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 3622/35720 Training loss: 1.2484 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 3623/35720 Training loss: 1.2489 0.2255 sec/batch\n",
      "Epoch 3/20  Iteration 3624/35720 Training loss: 1.2478 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 3625/35720 Training loss: 1.2474 0.2113 sec/batch\n",
      "Epoch 3/20  Iteration 3626/35720 Training loss: 1.2456 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 3627/35720 Training loss: 1.2449 0.2149 sec/batch\n",
      "Epoch 3/20  Iteration 3628/35720 Training loss: 1.2439 0.2151 sec/batch\n",
      "Epoch 3/20  Iteration 3629/35720 Training loss: 1.2441 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 3630/35720 Training loss: 1.2430 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 3631/35720 Training loss: 1.2406 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 3632/35720 Training loss: 1.2392 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 3633/35720 Training loss: 1.2373 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 3634/35720 Training loss: 1.2348 0.2151 sec/batch\n",
      "Epoch 3/20  Iteration 3635/35720 Training loss: 1.2353 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 3636/35720 Training loss: 1.2349 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 3637/35720 Training loss: 1.2356 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 3638/35720 Training loss: 1.2360 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 3639/35720 Training loss: 1.2359 0.2261 sec/batch\n",
      "Epoch 3/20  Iteration 3640/35720 Training loss: 1.2349 0.2278 sec/batch\n",
      "Epoch 3/20  Iteration 3641/35720 Training loss: 1.2354 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 3642/35720 Training loss: 1.2349 0.2295 sec/batch\n",
      "Epoch 3/20  Iteration 3643/35720 Training loss: 1.2352 0.2170 sec/batch\n",
      "Epoch 3/20  Iteration 3644/35720 Training loss: 1.2357 0.2204 sec/batch\n",
      "Epoch 3/20  Iteration 3645/35720 Training loss: 1.2354 0.2253 sec/batch\n",
      "Epoch 3/20  Iteration 3646/35720 Training loss: 1.2352 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 3647/35720 Training loss: 1.2333 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 3648/35720 Training loss: 1.2332 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 3649/35720 Training loss: 1.2320 0.2231 sec/batch\n",
      "Epoch 3/20  Iteration 3650/35720 Training loss: 1.2329 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 3651/35720 Training loss: 1.2327 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 3652/35720 Training loss: 1.2351 0.2303 sec/batch\n",
      "Epoch 3/20  Iteration 3653/35720 Training loss: 1.2353 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 3654/35720 Training loss: 1.2350 0.2229 sec/batch\n",
      "Epoch 3/20  Iteration 3655/35720 Training loss: 1.2352 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 3656/35720 Training loss: 1.2355 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 3657/35720 Training loss: 1.2355 0.2153 sec/batch\n",
      "Epoch 3/20  Iteration 3658/35720 Training loss: 1.2358 0.2227 sec/batch\n",
      "Epoch 3/20  Iteration 3659/35720 Training loss: 1.2359 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 3660/35720 Training loss: 1.2356 0.2128 sec/batch\n",
      "Epoch 3/20  Iteration 3661/35720 Training loss: 1.2344 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 3662/35720 Training loss: 1.2334 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 3663/35720 Training loss: 1.2333 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 3664/35720 Training loss: 1.2321 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 3665/35720 Training loss: 1.2313 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 3666/35720 Training loss: 1.2306 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 3667/35720 Training loss: 1.2301 0.2154 sec/batch\n",
      "Epoch 3/20  Iteration 3668/35720 Training loss: 1.2292 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 3669/35720 Training loss: 1.2295 0.2165 sec/batch\n",
      "Epoch 3/20  Iteration 3670/35720 Training loss: 1.2290 0.2292 sec/batch\n",
      "Epoch 3/20  Iteration 3671/35720 Training loss: 1.2289 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 3672/35720 Training loss: 1.2284 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 3673/35720 Training loss: 1.2281 0.2253 sec/batch\n",
      "Epoch 3/20  Iteration 3674/35720 Training loss: 1.2286 0.2233 sec/batch\n",
      "Epoch 3/20  Iteration 3675/35720 Training loss: 1.2292 0.2103 sec/batch\n",
      "Epoch 3/20  Iteration 3676/35720 Training loss: 1.2291 0.2154 sec/batch\n",
      "Epoch 3/20  Iteration 3677/35720 Training loss: 1.2291 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 3678/35720 Training loss: 1.2286 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 3679/35720 Training loss: 1.2289 0.2098 sec/batch\n",
      "Epoch 3/20  Iteration 3680/35720 Training loss: 1.2289 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 3681/35720 Training loss: 1.2298 0.2310 sec/batch\n",
      "Epoch 3/20  Iteration 3682/35720 Training loss: 1.2295 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 3683/35720 Training loss: 1.2296 0.2149 sec/batch\n",
      "Epoch 3/20  Iteration 3684/35720 Training loss: 1.2309 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 3685/35720 Training loss: 1.2311 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 3686/35720 Training loss: 1.2324 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 3687/35720 Training loss: 1.2319 0.2116 sec/batch\n",
      "Epoch 3/20  Iteration 3688/35720 Training loss: 1.2321 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 3689/35720 Training loss: 1.2318 0.2205 sec/batch\n",
      "Epoch 3/20  Iteration 3690/35720 Training loss: 1.2329 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 3691/35720 Training loss: 1.2335 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 3692/35720 Training loss: 1.2344 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 3693/35720 Training loss: 1.2351 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 3694/35720 Training loss: 1.2350 0.2311 sec/batch\n",
      "Epoch 3/20  Iteration 3695/35720 Training loss: 1.2357 0.2283 sec/batch\n",
      "Epoch 3/20  Iteration 3696/35720 Training loss: 1.2363 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 3697/35720 Training loss: 1.2355 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 3698/35720 Training loss: 1.2357 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 3699/35720 Training loss: 1.2356 0.2109 sec/batch\n",
      "Epoch 3/20  Iteration 3700/35720 Training loss: 1.2358 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 3701/35720 Training loss: 1.2355 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 3702/35720 Training loss: 1.2351 0.2204 sec/batch\n",
      "Epoch 3/20  Iteration 3703/35720 Training loss: 1.2346 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 3704/35720 Training loss: 1.2344 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 3705/35720 Training loss: 1.2341 0.2190 sec/batch\n",
      "Epoch 3/20  Iteration 3706/35720 Training loss: 1.2341 0.2134 sec/batch\n",
      "Epoch 3/20  Iteration 3707/35720 Training loss: 1.2341 0.2145 sec/batch\n",
      "Epoch 3/20  Iteration 3708/35720 Training loss: 1.2339 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 3709/35720 Training loss: 1.2350 0.2124 sec/batch\n",
      "Epoch 3/20  Iteration 3710/35720 Training loss: 1.2353 0.2214 sec/batch\n",
      "Epoch 3/20  Iteration 3711/35720 Training loss: 1.2355 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 3712/35720 Training loss: 1.2357 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 3713/35720 Training loss: 1.2351 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 3714/35720 Training loss: 1.2342 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 3715/35720 Training loss: 1.2335 0.2149 sec/batch\n",
      "Epoch 3/20  Iteration 3716/35720 Training loss: 1.2328 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 3717/35720 Training loss: 1.2325 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 3718/35720 Training loss: 1.2327 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 3719/35720 Training loss: 1.2320 0.2160 sec/batch\n",
      "Epoch 3/20  Iteration 3720/35720 Training loss: 1.2321 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 3721/35720 Training loss: 1.2318 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 3722/35720 Training loss: 1.2309 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 3723/35720 Training loss: 1.2307 0.2242 sec/batch\n",
      "Epoch 3/20  Iteration 3724/35720 Training loss: 1.2306 0.2290 sec/batch\n",
      "Epoch 3/20  Iteration 3725/35720 Training loss: 1.2306 0.2302 sec/batch\n",
      "Epoch 3/20  Iteration 3726/35720 Training loss: 1.2313 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 3727/35720 Training loss: 1.2319 0.2350 sec/batch\n",
      "Epoch 3/20  Iteration 3728/35720 Training loss: 1.2321 0.2153 sec/batch\n",
      "Epoch 3/20  Iteration 3729/35720 Training loss: 1.2324 0.2225 sec/batch\n",
      "Epoch 3/20  Iteration 3730/35720 Training loss: 1.2328 0.2149 sec/batch\n",
      "Epoch 3/20  Iteration 3731/35720 Training loss: 1.2325 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 3732/35720 Training loss: 1.2331 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 3733/35720 Training loss: 1.2325 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 3734/35720 Training loss: 1.2325 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 3735/35720 Training loss: 1.2327 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 3736/35720 Training loss: 1.2330 0.2252 sec/batch\n",
      "Epoch 3/20  Iteration 3737/35720 Training loss: 1.2334 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 3738/35720 Training loss: 1.2340 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 3739/35720 Training loss: 1.2341 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 3740/35720 Training loss: 1.2346 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 3741/35720 Training loss: 1.2347 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 3742/35720 Training loss: 1.2349 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 3743/35720 Training loss: 1.2367 0.2095 sec/batch\n",
      "Epoch 3/20  Iteration 3744/35720 Training loss: 1.2372 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 3745/35720 Training loss: 1.2374 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 3746/35720 Training loss: 1.2380 0.2105 sec/batch\n",
      "Epoch 3/20  Iteration 3747/35720 Training loss: 1.2381 0.2243 sec/batch\n",
      "Epoch 3/20  Iteration 3748/35720 Training loss: 1.2377 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 3749/35720 Training loss: 1.2378 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 3750/35720 Training loss: 1.2375 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 3751/35720 Training loss: 1.2374 0.2232 sec/batch\n",
      "Epoch 3/20  Iteration 3752/35720 Training loss: 1.2373 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 3753/35720 Training loss: 1.2376 0.2055 sec/batch\n",
      "Epoch 3/20  Iteration 3754/35720 Training loss: 1.2377 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 3755/35720 Training loss: 1.2378 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 3756/35720 Training loss: 1.2381 0.2282 sec/batch\n",
      "Epoch 3/20  Iteration 3757/35720 Training loss: 1.2380 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 3758/35720 Training loss: 1.2379 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 3759/35720 Training loss: 1.2375 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 3760/35720 Training loss: 1.2376 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 3761/35720 Training loss: 1.2381 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 3762/35720 Training loss: 1.2382 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 3763/35720 Training loss: 1.2383 0.2323 sec/batch\n",
      "Epoch 3/20  Iteration 3764/35720 Training loss: 1.2393 0.2221 sec/batch\n",
      "Epoch 3/20  Iteration 3765/35720 Training loss: 1.2396 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 3766/35720 Training loss: 1.2399 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 3767/35720 Training loss: 1.2396 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 3768/35720 Training loss: 1.2398 0.2160 sec/batch\n",
      "Epoch 3/20  Iteration 3769/35720 Training loss: 1.2395 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 3770/35720 Training loss: 1.2396 0.2297 sec/batch\n",
      "Epoch 3/20  Iteration 3771/35720 Training loss: 1.2398 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 3772/35720 Training loss: 1.2400 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 3773/35720 Training loss: 1.2396 0.2233 sec/batch\n",
      "Epoch 3/20  Iteration 3774/35720 Training loss: 1.2393 0.2230 sec/batch\n",
      "Epoch 3/20  Iteration 3775/35720 Training loss: 1.2393 0.2242 sec/batch\n",
      "Epoch 3/20  Iteration 3776/35720 Training loss: 1.2393 0.2094 sec/batch\n",
      "Epoch 3/20  Iteration 3777/35720 Training loss: 1.2392 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 3778/35720 Training loss: 1.2391 0.2214 sec/batch\n",
      "Epoch 3/20  Iteration 3779/35720 Training loss: 1.2397 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 3780/35720 Training loss: 1.2401 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 3781/35720 Training loss: 1.2403 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 3782/35720 Training loss: 1.2404 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 3783/35720 Training loss: 1.2406 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 3784/35720 Training loss: 1.2404 0.2282 sec/batch\n",
      "Epoch 3/20  Iteration 3785/35720 Training loss: 1.2405 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 3786/35720 Training loss: 1.2405 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 3787/35720 Training loss: 1.2404 0.2113 sec/batch\n",
      "Epoch 3/20  Iteration 3788/35720 Training loss: 1.2403 0.2247 sec/batch\n",
      "Epoch 3/20  Iteration 3789/35720 Training loss: 1.2399 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 3790/35720 Training loss: 1.2398 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 3791/35720 Training loss: 1.2399 0.2059 sec/batch\n",
      "Epoch 3/20  Iteration 3792/35720 Training loss: 1.2399 0.2220 sec/batch\n",
      "Epoch 3/20  Iteration 3793/35720 Training loss: 1.2399 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 3794/35720 Training loss: 1.2400 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 3795/35720 Training loss: 1.2404 0.2223 sec/batch\n",
      "Epoch 3/20  Iteration 3796/35720 Training loss: 1.2406 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 3797/35720 Training loss: 1.2407 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 3798/35720 Training loss: 1.2406 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 3799/35720 Training loss: 1.2404 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 3800/35720 Training loss: 1.2399 0.2232 sec/batch\n",
      "Validation loss: 1.39947 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 3801/35720 Training loss: 1.2403 0.2337 sec/batch\n",
      "Epoch 3/20  Iteration 3802/35720 Training loss: 1.2407 0.2094 sec/batch\n",
      "Epoch 3/20  Iteration 3803/35720 Training loss: 1.2408 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 3804/35720 Training loss: 1.2407 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 3805/35720 Training loss: 1.2407 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 3806/35720 Training loss: 1.2408 0.2121 sec/batch\n",
      "Epoch 3/20  Iteration 3807/35720 Training loss: 1.2406 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 3808/35720 Training loss: 1.2404 0.2205 sec/batch\n",
      "Epoch 3/20  Iteration 3809/35720 Training loss: 1.2406 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 3810/35720 Training loss: 1.2405 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 3811/35720 Training loss: 1.2401 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 3812/35720 Training loss: 1.2402 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 3813/35720 Training loss: 1.2399 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 3814/35720 Training loss: 1.2399 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 3815/35720 Training loss: 1.2401 0.2306 sec/batch\n",
      "Epoch 3/20  Iteration 3816/35720 Training loss: 1.2401 0.2302 sec/batch\n",
      "Epoch 3/20  Iteration 3817/35720 Training loss: 1.2400 0.2313 sec/batch\n",
      "Epoch 3/20  Iteration 3818/35720 Training loss: 1.2399 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 3819/35720 Training loss: 1.2401 0.2098 sec/batch\n",
      "Epoch 3/20  Iteration 3820/35720 Training loss: 1.2399 0.2067 sec/batch\n",
      "Epoch 3/20  Iteration 3821/35720 Training loss: 1.2395 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 3822/35720 Training loss: 1.2392 0.2099 sec/batch\n",
      "Epoch 3/20  Iteration 3823/35720 Training loss: 1.2393 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 3824/35720 Training loss: 1.2391 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 3825/35720 Training loss: 1.2388 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 3826/35720 Training loss: 1.2392 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 3827/35720 Training loss: 1.2396 0.2170 sec/batch\n",
      "Epoch 3/20  Iteration 3828/35720 Training loss: 1.2397 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 3829/35720 Training loss: 1.2397 0.2178 sec/batch\n",
      "Epoch 3/20  Iteration 3830/35720 Training loss: 1.2399 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 3831/35720 Training loss: 1.2402 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 3832/35720 Training loss: 1.2400 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 3833/35720 Training loss: 1.2399 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 3834/35720 Training loss: 1.2397 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 3835/35720 Training loss: 1.2396 0.2219 sec/batch\n",
      "Epoch 3/20  Iteration 3836/35720 Training loss: 1.2397 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 3837/35720 Training loss: 1.2398 0.2071 sec/batch\n",
      "Epoch 3/20  Iteration 3838/35720 Training loss: 1.2400 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 3839/35720 Training loss: 1.2397 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 3840/35720 Training loss: 1.2396 0.2218 sec/batch\n",
      "Epoch 3/20  Iteration 3841/35720 Training loss: 1.2391 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 3842/35720 Training loss: 1.2383 0.2178 sec/batch\n",
      "Epoch 3/20  Iteration 3843/35720 Training loss: 1.2377 0.2153 sec/batch\n",
      "Epoch 3/20  Iteration 3844/35720 Training loss: 1.2376 0.2151 sec/batch\n",
      "Epoch 3/20  Iteration 3845/35720 Training loss: 1.2376 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 3846/35720 Training loss: 1.2374 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 3847/35720 Training loss: 1.2370 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 3848/35720 Training loss: 1.2369 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 3849/35720 Training loss: 1.2366 0.2221 sec/batch\n",
      "Epoch 3/20  Iteration 3850/35720 Training loss: 1.2364 0.2230 sec/batch\n",
      "Epoch 3/20  Iteration 3851/35720 Training loss: 1.2360 0.2237 sec/batch\n",
      "Epoch 3/20  Iteration 3852/35720 Training loss: 1.2360 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 3853/35720 Training loss: 1.2360 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 3854/35720 Training loss: 1.2355 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 3855/35720 Training loss: 1.2349 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 3856/35720 Training loss: 1.2346 0.2200 sec/batch\n",
      "Epoch 3/20  Iteration 3857/35720 Training loss: 1.2347 0.2214 sec/batch\n",
      "Epoch 3/20  Iteration 3858/35720 Training loss: 1.2346 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 3859/35720 Training loss: 1.2341 0.2241 sec/batch\n",
      "Epoch 3/20  Iteration 3860/35720 Training loss: 1.2340 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 3861/35720 Training loss: 1.2341 0.2160 sec/batch\n",
      "Epoch 3/20  Iteration 3862/35720 Training loss: 1.2342 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 3863/35720 Training loss: 1.2341 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 3864/35720 Training loss: 1.2342 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 3865/35720 Training loss: 1.2341 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 3866/35720 Training loss: 1.2340 0.2251 sec/batch\n",
      "Epoch 3/20  Iteration 3867/35720 Training loss: 1.2349 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 3868/35720 Training loss: 1.2349 0.2209 sec/batch\n",
      "Epoch 3/20  Iteration 3869/35720 Training loss: 1.2349 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 3870/35720 Training loss: 1.2351 0.2271 sec/batch\n",
      "Epoch 3/20  Iteration 3871/35720 Training loss: 1.2350 0.2122 sec/batch\n",
      "Epoch 3/20  Iteration 3872/35720 Training loss: 1.2350 0.2130 sec/batch\n",
      "Epoch 3/20  Iteration 3873/35720 Training loss: 1.2350 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 3874/35720 Training loss: 1.2347 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 3875/35720 Training loss: 1.2346 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 3876/35720 Training loss: 1.2345 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 3877/35720 Training loss: 1.2345 0.2067 sec/batch\n",
      "Epoch 3/20  Iteration 3878/35720 Training loss: 1.2343 0.2206 sec/batch\n",
      "Epoch 3/20  Iteration 3879/35720 Training loss: 1.2345 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 3880/35720 Training loss: 1.2346 0.2324 sec/batch\n",
      "Epoch 3/20  Iteration 3881/35720 Training loss: 1.2343 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 3882/35720 Training loss: 1.2340 0.2114 sec/batch\n",
      "Epoch 3/20  Iteration 3883/35720 Training loss: 1.2338 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 3884/35720 Training loss: 1.2338 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 3885/35720 Training loss: 1.2336 0.2269 sec/batch\n",
      "Epoch 3/20  Iteration 3886/35720 Training loss: 1.2332 0.2299 sec/batch\n",
      "Epoch 3/20  Iteration 3887/35720 Training loss: 1.2332 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 3888/35720 Training loss: 1.2331 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 3889/35720 Training loss: 1.2328 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 3890/35720 Training loss: 1.2330 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 3891/35720 Training loss: 1.2329 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 3892/35720 Training loss: 1.2326 0.2429 sec/batch\n",
      "Epoch 3/20  Iteration 3893/35720 Training loss: 1.2326 0.2090 sec/batch\n",
      "Epoch 3/20  Iteration 3894/35720 Training loss: 1.2324 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 3895/35720 Training loss: 1.2326 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 3896/35720 Training loss: 1.2326 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 3897/35720 Training loss: 1.2323 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 3898/35720 Training loss: 1.2326 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 3899/35720 Training loss: 1.2326 0.2242 sec/batch\n",
      "Epoch 3/20  Iteration 3900/35720 Training loss: 1.2327 0.2251 sec/batch\n",
      "Epoch 3/20  Iteration 3901/35720 Training loss: 1.2327 0.2290 sec/batch\n",
      "Epoch 3/20  Iteration 3902/35720 Training loss: 1.2326 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 3903/35720 Training loss: 1.2327 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 3904/35720 Training loss: 1.2329 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 3905/35720 Training loss: 1.2327 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 3906/35720 Training loss: 1.2326 0.2372 sec/batch\n",
      "Epoch 3/20  Iteration 3907/35720 Training loss: 1.2324 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 3908/35720 Training loss: 1.2322 0.2063 sec/batch\n",
      "Epoch 3/20  Iteration 3909/35720 Training loss: 1.2321 0.2104 sec/batch\n",
      "Epoch 3/20  Iteration 3910/35720 Training loss: 1.2320 0.2257 sec/batch\n",
      "Epoch 3/20  Iteration 3911/35720 Training loss: 1.2320 0.2200 sec/batch\n",
      "Epoch 3/20  Iteration 3912/35720 Training loss: 1.2322 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 3913/35720 Training loss: 1.2319 0.2223 sec/batch\n",
      "Epoch 3/20  Iteration 3914/35720 Training loss: 1.2318 0.2122 sec/batch\n",
      "Epoch 3/20  Iteration 3915/35720 Training loss: 1.2318 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 3916/35720 Training loss: 1.2318 0.2128 sec/batch\n",
      "Epoch 3/20  Iteration 3917/35720 Training loss: 1.2314 0.2289 sec/batch\n",
      "Epoch 3/20  Iteration 3918/35720 Training loss: 1.2317 0.2289 sec/batch\n",
      "Epoch 3/20  Iteration 3919/35720 Training loss: 1.2317 0.2251 sec/batch\n",
      "Epoch 3/20  Iteration 3920/35720 Training loss: 1.2318 0.2233 sec/batch\n",
      "Epoch 3/20  Iteration 3921/35720 Training loss: 1.2319 0.2108 sec/batch\n",
      "Epoch 3/20  Iteration 3922/35720 Training loss: 1.2318 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 3923/35720 Training loss: 1.2319 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 3924/35720 Training loss: 1.2318 0.2292 sec/batch\n",
      "Epoch 3/20  Iteration 3925/35720 Training loss: 1.2318 0.2262 sec/batch\n",
      "Epoch 3/20  Iteration 3926/35720 Training loss: 1.2317 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 3927/35720 Training loss: 1.2318 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 3928/35720 Training loss: 1.2318 0.2284 sec/batch\n",
      "Epoch 3/20  Iteration 3929/35720 Training loss: 1.2320 0.2249 sec/batch\n",
      "Epoch 3/20  Iteration 3930/35720 Training loss: 1.2318 0.2260 sec/batch\n",
      "Epoch 3/20  Iteration 3931/35720 Training loss: 1.2316 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 3932/35720 Training loss: 1.2314 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 3933/35720 Training loss: 1.2313 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 3934/35720 Training loss: 1.2314 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 3935/35720 Training loss: 1.2312 0.2231 sec/batch\n",
      "Epoch 3/20  Iteration 3936/35720 Training loss: 1.2311 0.2218 sec/batch\n",
      "Epoch 3/20  Iteration 3937/35720 Training loss: 1.2309 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 3938/35720 Training loss: 1.2308 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 3939/35720 Training loss: 1.2307 0.2273 sec/batch\n",
      "Epoch 3/20  Iteration 3940/35720 Training loss: 1.2304 0.2288 sec/batch\n",
      "Epoch 3/20  Iteration 3941/35720 Training loss: 1.2303 0.2274 sec/batch\n",
      "Epoch 3/20  Iteration 3942/35720 Training loss: 1.2301 0.2181 sec/batch\n",
      "Epoch 3/20  Iteration 3943/35720 Training loss: 1.2299 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 3944/35720 Training loss: 1.2297 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 3945/35720 Training loss: 1.2296 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 3946/35720 Training loss: 1.2295 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 3947/35720 Training loss: 1.2294 0.2228 sec/batch\n",
      "Epoch 3/20  Iteration 3948/35720 Training loss: 1.2295 0.2232 sec/batch\n",
      "Epoch 3/20  Iteration 3949/35720 Training loss: 1.2294 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 3950/35720 Training loss: 1.2292 0.2294 sec/batch\n",
      "Epoch 3/20  Iteration 3951/35720 Training loss: 1.2289 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 3952/35720 Training loss: 1.2286 0.2178 sec/batch\n",
      "Epoch 3/20  Iteration 3953/35720 Training loss: 1.2285 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 3954/35720 Training loss: 1.2285 0.2290 sec/batch\n",
      "Epoch 3/20  Iteration 3955/35720 Training loss: 1.2283 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 3956/35720 Training loss: 1.2282 0.2282 sec/batch\n",
      "Epoch 3/20  Iteration 3957/35720 Training loss: 1.2281 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 3958/35720 Training loss: 1.2281 0.2217 sec/batch\n",
      "Epoch 3/20  Iteration 3959/35720 Training loss: 1.2281 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 3960/35720 Training loss: 1.2281 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 3961/35720 Training loss: 1.2283 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 3962/35720 Training loss: 1.2285 0.2231 sec/batch\n",
      "Epoch 3/20  Iteration 3963/35720 Training loss: 1.2285 0.2307 sec/batch\n",
      "Epoch 3/20  Iteration 3964/35720 Training loss: 1.2283 0.2349 sec/batch\n",
      "Epoch 3/20  Iteration 3965/35720 Training loss: 1.2285 0.2255 sec/batch\n",
      "Epoch 3/20  Iteration 3966/35720 Training loss: 1.2282 0.2186 sec/batch\n",
      "Epoch 3/20  Iteration 3967/35720 Training loss: 1.2282 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 3968/35720 Training loss: 1.2280 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 3969/35720 Training loss: 1.2280 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 3970/35720 Training loss: 1.2277 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 3971/35720 Training loss: 1.2275 0.2244 sec/batch\n",
      "Epoch 3/20  Iteration 3972/35720 Training loss: 1.2274 0.2271 sec/batch\n",
      "Epoch 3/20  Iteration 3973/35720 Training loss: 1.2272 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 3974/35720 Training loss: 1.2276 0.2234 sec/batch\n",
      "Epoch 3/20  Iteration 3975/35720 Training loss: 1.2277 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 3976/35720 Training loss: 1.2277 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 3977/35720 Training loss: 1.2276 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 3978/35720 Training loss: 1.2275 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 3979/35720 Training loss: 1.2273 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 3980/35720 Training loss: 1.2272 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 3981/35720 Training loss: 1.2270 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 3982/35720 Training loss: 1.2269 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 3983/35720 Training loss: 1.2270 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 3984/35720 Training loss: 1.2267 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 3985/35720 Training loss: 1.2263 0.2146 sec/batch\n",
      "Epoch 3/20  Iteration 3986/35720 Training loss: 1.2263 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 3987/35720 Training loss: 1.2262 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 3988/35720 Training loss: 1.2262 0.2062 sec/batch\n",
      "Epoch 3/20  Iteration 3989/35720 Training loss: 1.2260 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 3990/35720 Training loss: 1.2258 0.2231 sec/batch\n",
      "Epoch 3/20  Iteration 3991/35720 Training loss: 1.2256 0.2136 sec/batch\n",
      "Epoch 3/20  Iteration 3992/35720 Training loss: 1.2256 0.2274 sec/batch\n",
      "Epoch 3/20  Iteration 3993/35720 Training loss: 1.2256 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 3994/35720 Training loss: 1.2256 0.2263 sec/batch\n",
      "Epoch 3/20  Iteration 3995/35720 Training loss: 1.2257 0.2284 sec/batch\n",
      "Epoch 3/20  Iteration 3996/35720 Training loss: 1.2258 0.2246 sec/batch\n",
      "Epoch 3/20  Iteration 3997/35720 Training loss: 1.2258 0.2230 sec/batch\n",
      "Epoch 3/20  Iteration 3998/35720 Training loss: 1.2256 0.2065 sec/batch\n",
      "Epoch 3/20  Iteration 3999/35720 Training loss: 1.2254 0.2110 sec/batch\n",
      "Epoch 3/20  Iteration 4000/35720 Training loss: 1.2256 0.2299 sec/batch\n",
      "Validation loss: 1.39019 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 4001/35720 Training loss: 1.2258 0.2098 sec/batch\n",
      "Epoch 3/20  Iteration 4002/35720 Training loss: 1.2261 0.2086 sec/batch\n",
      "Epoch 3/20  Iteration 4003/35720 Training loss: 1.2262 0.2145 sec/batch\n",
      "Epoch 3/20  Iteration 4004/35720 Training loss: 1.2263 0.2096 sec/batch\n",
      "Epoch 3/20  Iteration 4005/35720 Training loss: 1.2266 0.2113 sec/batch\n",
      "Epoch 3/20  Iteration 4006/35720 Training loss: 1.2268 0.2142 sec/batch\n",
      "Epoch 3/20  Iteration 4007/35720 Training loss: 1.2270 0.2346 sec/batch\n",
      "Epoch 3/20  Iteration 4008/35720 Training loss: 1.2268 0.2219 sec/batch\n",
      "Epoch 3/20  Iteration 4009/35720 Training loss: 1.2270 0.2299 sec/batch\n",
      "Epoch 3/20  Iteration 4010/35720 Training loss: 1.2272 0.2293 sec/batch\n",
      "Epoch 3/20  Iteration 4011/35720 Training loss: 1.2273 0.2145 sec/batch\n",
      "Epoch 3/20  Iteration 4012/35720 Training loss: 1.2272 0.2248 sec/batch\n",
      "Epoch 3/20  Iteration 4013/35720 Training loss: 1.2277 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 4014/35720 Training loss: 1.2279 0.2221 sec/batch\n",
      "Epoch 3/20  Iteration 4015/35720 Training loss: 1.2278 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 4016/35720 Training loss: 1.2278 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 4017/35720 Training loss: 1.2278 0.2123 sec/batch\n",
      "Epoch 3/20  Iteration 4018/35720 Training loss: 1.2279 0.2108 sec/batch\n",
      "Epoch 3/20  Iteration 4019/35720 Training loss: 1.2281 0.2105 sec/batch\n",
      "Epoch 3/20  Iteration 4020/35720 Training loss: 1.2283 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 4021/35720 Training loss: 1.2284 0.2421 sec/batch\n",
      "Epoch 3/20  Iteration 4022/35720 Training loss: 1.2283 0.2228 sec/batch\n",
      "Epoch 3/20  Iteration 4023/35720 Training loss: 1.2281 0.2279 sec/batch\n",
      "Epoch 3/20  Iteration 4024/35720 Training loss: 1.2280 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 4025/35720 Training loss: 1.2280 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 4026/35720 Training loss: 1.2281 0.2274 sec/batch\n",
      "Epoch 3/20  Iteration 4027/35720 Training loss: 1.2283 0.2277 sec/batch\n",
      "Epoch 3/20  Iteration 4028/35720 Training loss: 1.2285 0.2234 sec/batch\n",
      "Epoch 3/20  Iteration 4029/35720 Training loss: 1.2287 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 4030/35720 Training loss: 1.2288 0.2070 sec/batch\n",
      "Epoch 3/20  Iteration 4031/35720 Training loss: 1.2286 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 4032/35720 Training loss: 1.2284 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 4033/35720 Training loss: 1.2284 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 4034/35720 Training loss: 1.2283 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 4035/35720 Training loss: 1.2285 0.2245 sec/batch\n",
      "Epoch 3/20  Iteration 4036/35720 Training loss: 1.2284 0.2284 sec/batch\n",
      "Epoch 3/20  Iteration 4037/35720 Training loss: 1.2283 0.2260 sec/batch\n",
      "Epoch 3/20  Iteration 4038/35720 Training loss: 1.2282 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 4039/35720 Training loss: 1.2281 0.2250 sec/batch\n",
      "Epoch 3/20  Iteration 4040/35720 Training loss: 1.2279 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 4041/35720 Training loss: 1.2277 0.2242 sec/batch\n",
      "Epoch 3/20  Iteration 4042/35720 Training loss: 1.2275 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 4043/35720 Training loss: 1.2272 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 4044/35720 Training loss: 1.2271 0.2204 sec/batch\n",
      "Epoch 3/20  Iteration 4045/35720 Training loss: 1.2271 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 4046/35720 Training loss: 1.2269 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4047/35720 Training loss: 1.2269 0.2133 sec/batch\n",
      "Epoch 3/20  Iteration 4048/35720 Training loss: 1.2268 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 4049/35720 Training loss: 1.2269 0.2158 sec/batch\n",
      "Epoch 3/20  Iteration 4050/35720 Training loss: 1.2268 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 4051/35720 Training loss: 1.2269 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 4052/35720 Training loss: 1.2267 0.2124 sec/batch\n",
      "Epoch 3/20  Iteration 4053/35720 Training loss: 1.2264 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 4054/35720 Training loss: 1.2262 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 4055/35720 Training loss: 1.2262 0.2273 sec/batch\n",
      "Epoch 3/20  Iteration 4056/35720 Training loss: 1.2262 0.2266 sec/batch\n",
      "Epoch 3/20  Iteration 4057/35720 Training loss: 1.2260 0.2252 sec/batch\n",
      "Epoch 3/20  Iteration 4058/35720 Training loss: 1.2259 0.2266 sec/batch\n",
      "Epoch 3/20  Iteration 4059/35720 Training loss: 1.2257 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 4060/35720 Training loss: 1.2257 0.2249 sec/batch\n",
      "Epoch 3/20  Iteration 4061/35720 Training loss: 1.2257 0.2284 sec/batch\n",
      "Epoch 3/20  Iteration 4062/35720 Training loss: 1.2255 0.2252 sec/batch\n",
      "Epoch 3/20  Iteration 4063/35720 Training loss: 1.2253 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4064/35720 Training loss: 1.2254 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 4065/35720 Training loss: 1.2252 0.2289 sec/batch\n",
      "Epoch 3/20  Iteration 4066/35720 Training loss: 1.2251 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 4067/35720 Training loss: 1.2249 0.2233 sec/batch\n",
      "Epoch 3/20  Iteration 4068/35720 Training loss: 1.2247 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 4069/35720 Training loss: 1.2247 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 4070/35720 Training loss: 1.2248 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 4071/35720 Training loss: 1.2248 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 4072/35720 Training loss: 1.2247 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4073/35720 Training loss: 1.2245 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 4074/35720 Training loss: 1.2242 0.2133 sec/batch\n",
      "Epoch 3/20  Iteration 4075/35720 Training loss: 1.2239 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 4076/35720 Training loss: 1.2239 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 4077/35720 Training loss: 1.2240 0.2234 sec/batch\n",
      "Epoch 3/20  Iteration 4078/35720 Training loss: 1.2239 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 4079/35720 Training loss: 1.2238 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 4080/35720 Training loss: 1.2237 0.2348 sec/batch\n",
      "Epoch 3/20  Iteration 4081/35720 Training loss: 1.2238 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 4082/35720 Training loss: 1.2236 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4083/35720 Training loss: 1.2233 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 4084/35720 Training loss: 1.2231 0.2092 sec/batch\n",
      "Epoch 3/20  Iteration 4085/35720 Training loss: 1.2230 0.2133 sec/batch\n",
      "Epoch 3/20  Iteration 4086/35720 Training loss: 1.2229 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 4087/35720 Training loss: 1.2230 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 4088/35720 Training loss: 1.2229 0.2141 sec/batch\n",
      "Epoch 3/20  Iteration 4089/35720 Training loss: 1.2228 0.2321 sec/batch\n",
      "Epoch 3/20  Iteration 4090/35720 Training loss: 1.2228 0.2136 sec/batch\n",
      "Epoch 3/20  Iteration 4091/35720 Training loss: 1.2227 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 4092/35720 Training loss: 1.2227 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 4093/35720 Training loss: 1.2226 0.2139 sec/batch\n",
      "Epoch 3/20  Iteration 4094/35720 Training loss: 1.2226 0.2116 sec/batch\n",
      "Epoch 3/20  Iteration 4095/35720 Training loss: 1.2226 0.2141 sec/batch\n",
      "Epoch 3/20  Iteration 4096/35720 Training loss: 1.2225 0.2275 sec/batch\n",
      "Epoch 3/20  Iteration 4097/35720 Training loss: 1.2224 0.2244 sec/batch\n",
      "Epoch 3/20  Iteration 4098/35720 Training loss: 1.2223 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 4099/35720 Training loss: 1.2222 0.2283 sec/batch\n",
      "Epoch 3/20  Iteration 4100/35720 Training loss: 1.2221 0.2267 sec/batch\n",
      "Epoch 3/20  Iteration 4101/35720 Training loss: 1.2220 0.2077 sec/batch\n",
      "Epoch 3/20  Iteration 4102/35720 Training loss: 1.2218 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 4103/35720 Training loss: 1.2218 0.2221 sec/batch\n",
      "Epoch 3/20  Iteration 4104/35720 Training loss: 1.2217 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 4105/35720 Training loss: 1.2216 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 4106/35720 Training loss: 1.2216 0.2165 sec/batch\n",
      "Epoch 3/20  Iteration 4107/35720 Training loss: 1.2214 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 4108/35720 Training loss: 1.2213 0.2298 sec/batch\n",
      "Epoch 3/20  Iteration 4109/35720 Training loss: 1.2211 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 4110/35720 Training loss: 1.2209 0.2237 sec/batch\n",
      "Epoch 3/20  Iteration 4111/35720 Training loss: 1.2208 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 4112/35720 Training loss: 1.2206 0.2324 sec/batch\n",
      "Epoch 3/20  Iteration 4113/35720 Training loss: 1.2205 0.2269 sec/batch\n",
      "Epoch 3/20  Iteration 4114/35720 Training loss: 1.2203 0.2262 sec/batch\n",
      "Epoch 3/20  Iteration 4115/35720 Training loss: 1.2201 0.2313 sec/batch\n",
      "Epoch 3/20  Iteration 4116/35720 Training loss: 1.2200 0.2276 sec/batch\n",
      "Epoch 3/20  Iteration 4117/35720 Training loss: 1.2201 0.2225 sec/batch\n",
      "Epoch 3/20  Iteration 4118/35720 Training loss: 1.2202 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 4119/35720 Training loss: 1.2201 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 4120/35720 Training loss: 1.2201 0.2214 sec/batch\n",
      "Epoch 3/20  Iteration 4121/35720 Training loss: 1.2198 0.2144 sec/batch\n",
      "Epoch 3/20  Iteration 4122/35720 Training loss: 1.2199 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 4123/35720 Training loss: 1.2197 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 4124/35720 Training loss: 1.2196 0.2129 sec/batch\n",
      "Epoch 3/20  Iteration 4125/35720 Training loss: 1.2194 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 4126/35720 Training loss: 1.2194 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 4127/35720 Training loss: 1.2192 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 4128/35720 Training loss: 1.2192 0.2286 sec/batch\n",
      "Epoch 3/20  Iteration 4129/35720 Training loss: 1.2191 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 4130/35720 Training loss: 1.2192 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 4131/35720 Training loss: 1.2191 0.2117 sec/batch\n",
      "Epoch 3/20  Iteration 4132/35720 Training loss: 1.2188 0.2205 sec/batch\n",
      "Epoch 3/20  Iteration 4133/35720 Training loss: 1.2189 0.2099 sec/batch\n",
      "Epoch 3/20  Iteration 4134/35720 Training loss: 1.2188 0.2186 sec/batch\n",
      "Epoch 3/20  Iteration 4135/35720 Training loss: 1.2187 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 4136/35720 Training loss: 1.2185 0.2138 sec/batch\n",
      "Epoch 3/20  Iteration 4137/35720 Training loss: 1.2182 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 4138/35720 Training loss: 1.2181 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 4139/35720 Training loss: 1.2182 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 4140/35720 Training loss: 1.2180 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 4141/35720 Training loss: 1.2181 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 4142/35720 Training loss: 1.2181 0.2045 sec/batch\n",
      "Epoch 3/20  Iteration 4143/35720 Training loss: 1.2180 0.2276 sec/batch\n",
      "Epoch 3/20  Iteration 4144/35720 Training loss: 1.2181 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 4145/35720 Training loss: 1.2182 0.2226 sec/batch\n",
      "Epoch 3/20  Iteration 4146/35720 Training loss: 1.2182 0.2114 sec/batch\n",
      "Epoch 3/20  Iteration 4147/35720 Training loss: 1.2182 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 4148/35720 Training loss: 1.2183 0.2108 sec/batch\n",
      "Epoch 3/20  Iteration 4149/35720 Training loss: 1.2183 0.2233 sec/batch\n",
      "Epoch 3/20  Iteration 4150/35720 Training loss: 1.2182 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 4151/35720 Training loss: 1.2181 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 4152/35720 Training loss: 1.2179 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 4153/35720 Training loss: 1.2179 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 4154/35720 Training loss: 1.2179 0.2158 sec/batch\n",
      "Epoch 3/20  Iteration 4155/35720 Training loss: 1.2180 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 4156/35720 Training loss: 1.2180 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 4157/35720 Training loss: 1.2178 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 4158/35720 Training loss: 1.2175 0.2230 sec/batch\n",
      "Epoch 3/20  Iteration 4159/35720 Training loss: 1.2174 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 4160/35720 Training loss: 1.2173 0.2149 sec/batch\n",
      "Epoch 3/20  Iteration 4161/35720 Training loss: 1.2171 0.2121 sec/batch\n",
      "Epoch 3/20  Iteration 4162/35720 Training loss: 1.2169 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 4163/35720 Training loss: 1.2169 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 4164/35720 Training loss: 1.2169 0.2243 sec/batch\n",
      "Epoch 3/20  Iteration 4165/35720 Training loss: 1.2168 0.2306 sec/batch\n",
      "Epoch 3/20  Iteration 4166/35720 Training loss: 1.2169 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 4167/35720 Training loss: 1.2167 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 4168/35720 Training loss: 1.2166 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 4169/35720 Training loss: 1.2165 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 4170/35720 Training loss: 1.2166 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 4171/35720 Training loss: 1.2164 0.2181 sec/batch\n",
      "Epoch 3/20  Iteration 4172/35720 Training loss: 1.2162 0.2058 sec/batch\n",
      "Epoch 3/20  Iteration 4173/35720 Training loss: 1.2160 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 4174/35720 Training loss: 1.2159 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 4175/35720 Training loss: 1.2158 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 4176/35720 Training loss: 1.2157 0.2130 sec/batch\n",
      "Epoch 3/20  Iteration 4177/35720 Training loss: 1.2156 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 4178/35720 Training loss: 1.2156 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 4179/35720 Training loss: 1.2157 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 4180/35720 Training loss: 1.2157 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 4181/35720 Training loss: 1.2158 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 4182/35720 Training loss: 1.2157 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 4183/35720 Training loss: 1.2157 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 4184/35720 Training loss: 1.2155 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 4185/35720 Training loss: 1.2155 0.2070 sec/batch\n",
      "Epoch 3/20  Iteration 4186/35720 Training loss: 1.2155 0.2129 sec/batch\n",
      "Epoch 3/20  Iteration 4187/35720 Training loss: 1.2155 0.2275 sec/batch\n",
      "Epoch 3/20  Iteration 4188/35720 Training loss: 1.2154 0.2246 sec/batch\n",
      "Epoch 3/20  Iteration 4189/35720 Training loss: 1.2153 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 4190/35720 Training loss: 1.2152 0.2262 sec/batch\n",
      "Epoch 3/20  Iteration 4191/35720 Training loss: 1.2150 0.2271 sec/batch\n",
      "Epoch 3/20  Iteration 4192/35720 Training loss: 1.2148 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 4193/35720 Training loss: 1.2147 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 4194/35720 Training loss: 1.2147 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 4195/35720 Training loss: 1.2145 0.2082 sec/batch\n",
      "Epoch 3/20  Iteration 4196/35720 Training loss: 1.2144 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4197/35720 Training loss: 1.2141 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 4198/35720 Training loss: 1.2141 0.2158 sec/batch\n",
      "Epoch 3/20  Iteration 4199/35720 Training loss: 1.2142 0.2121 sec/batch\n",
      "Epoch 3/20  Iteration 4200/35720 Training loss: 1.2139 0.2187 sec/batch\n",
      "Validation loss: 1.37762 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 4201/35720 Training loss: 1.2140 0.2082 sec/batch\n",
      "Epoch 3/20  Iteration 4202/35720 Training loss: 1.2139 0.2123 sec/batch\n",
      "Epoch 3/20  Iteration 4203/35720 Training loss: 1.2140 0.2075 sec/batch\n",
      "Epoch 3/20  Iteration 4204/35720 Training loss: 1.2138 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 4205/35720 Training loss: 1.2137 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 4206/35720 Training loss: 1.2137 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4207/35720 Training loss: 1.2136 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4208/35720 Training loss: 1.2136 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 4209/35720 Training loss: 1.2136 0.2056 sec/batch\n",
      "Epoch 3/20  Iteration 4210/35720 Training loss: 1.2135 0.2088 sec/batch\n",
      "Epoch 3/20  Iteration 4211/35720 Training loss: 1.2135 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4212/35720 Training loss: 1.2135 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 4213/35720 Training loss: 1.2136 0.2209 sec/batch\n",
      "Epoch 3/20  Iteration 4214/35720 Training loss: 1.2137 0.2226 sec/batch\n",
      "Epoch 3/20  Iteration 4215/35720 Training loss: 1.2137 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 4216/35720 Training loss: 1.2136 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 4217/35720 Training loss: 1.2137 0.2205 sec/batch\n",
      "Epoch 3/20  Iteration 4218/35720 Training loss: 1.2138 0.2209 sec/batch\n",
      "Epoch 3/20  Iteration 4219/35720 Training loss: 1.2137 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 4220/35720 Training loss: 1.2138 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 4221/35720 Training loss: 1.2136 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 4222/35720 Training loss: 1.2135 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 4223/35720 Training loss: 1.2135 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 4224/35720 Training loss: 1.2135 0.2252 sec/batch\n",
      "Epoch 3/20  Iteration 4225/35720 Training loss: 1.2135 0.2255 sec/batch\n",
      "Epoch 3/20  Iteration 4226/35720 Training loss: 1.2137 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 4227/35720 Training loss: 1.2137 0.2066 sec/batch\n",
      "Epoch 3/20  Iteration 4228/35720 Training loss: 1.2137 0.2124 sec/batch\n",
      "Epoch 3/20  Iteration 4229/35720 Training loss: 1.2138 0.2165 sec/batch\n",
      "Epoch 3/20  Iteration 4230/35720 Training loss: 1.2139 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 4231/35720 Training loss: 1.2139 0.2181 sec/batch\n",
      "Epoch 3/20  Iteration 4232/35720 Training loss: 1.2139 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 4233/35720 Training loss: 1.2139 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 4234/35720 Training loss: 1.2140 0.2278 sec/batch\n",
      "Epoch 3/20  Iteration 4235/35720 Training loss: 1.2140 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 4236/35720 Training loss: 1.2140 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 4237/35720 Training loss: 1.2141 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 4238/35720 Training loss: 1.2141 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 4239/35720 Training loss: 1.2142 0.2204 sec/batch\n",
      "Epoch 3/20  Iteration 4240/35720 Training loss: 1.2141 0.2278 sec/batch\n",
      "Epoch 3/20  Iteration 4241/35720 Training loss: 1.2140 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 4242/35720 Training loss: 1.2139 0.2872 sec/batch\n",
      "Epoch 3/20  Iteration 4243/35720 Training loss: 1.2137 0.2701 sec/batch\n",
      "Epoch 3/20  Iteration 4244/35720 Training loss: 1.2138 0.2388 sec/batch\n",
      "Epoch 3/20  Iteration 4245/35720 Training loss: 1.2138 0.2230 sec/batch\n",
      "Epoch 3/20  Iteration 4246/35720 Training loss: 1.2136 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 4247/35720 Training loss: 1.2135 0.2105 sec/batch\n",
      "Epoch 3/20  Iteration 4248/35720 Training loss: 1.2134 0.2105 sec/batch\n",
      "Epoch 3/20  Iteration 4249/35720 Training loss: 1.2134 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 4250/35720 Training loss: 1.2133 0.2305 sec/batch\n",
      "Epoch 3/20  Iteration 4251/35720 Training loss: 1.2133 0.2292 sec/batch\n",
      "Epoch 3/20  Iteration 4252/35720 Training loss: 1.2132 0.2104 sec/batch\n",
      "Epoch 3/20  Iteration 4253/35720 Training loss: 1.2132 0.2128 sec/batch\n",
      "Epoch 3/20  Iteration 4254/35720 Training loss: 1.2131 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 4255/35720 Training loss: 1.2130 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 4256/35720 Training loss: 1.2128 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4257/35720 Training loss: 1.2130 0.2136 sec/batch\n",
      "Epoch 3/20  Iteration 4258/35720 Training loss: 1.2129 0.2241 sec/batch\n",
      "Epoch 3/20  Iteration 4259/35720 Training loss: 1.2129 0.2313 sec/batch\n",
      "Epoch 3/20  Iteration 4260/35720 Training loss: 1.2128 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 4261/35720 Training loss: 1.2128 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 4262/35720 Training loss: 1.2127 0.2100 sec/batch\n",
      "Epoch 3/20  Iteration 4263/35720 Training loss: 1.2128 0.2090 sec/batch\n",
      "Epoch 3/20  Iteration 4264/35720 Training loss: 1.2129 0.2165 sec/batch\n",
      "Epoch 3/20  Iteration 4265/35720 Training loss: 1.2131 0.2209 sec/batch\n",
      "Epoch 3/20  Iteration 4266/35720 Training loss: 1.2131 0.2257 sec/batch\n",
      "Epoch 3/20  Iteration 4267/35720 Training loss: 1.2131 0.2276 sec/batch\n",
      "Epoch 3/20  Iteration 4268/35720 Training loss: 1.2130 0.2290 sec/batch\n",
      "Epoch 3/20  Iteration 4269/35720 Training loss: 1.2130 0.2237 sec/batch\n",
      "Epoch 3/20  Iteration 4270/35720 Training loss: 1.2130 0.2262 sec/batch\n",
      "Epoch 3/20  Iteration 4271/35720 Training loss: 1.2129 0.2264 sec/batch\n",
      "Epoch 3/20  Iteration 4272/35720 Training loss: 1.2130 0.2091 sec/batch\n",
      "Epoch 3/20  Iteration 4273/35720 Training loss: 1.2128 0.2221 sec/batch\n",
      "Epoch 3/20  Iteration 4274/35720 Training loss: 1.2129 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 4275/35720 Training loss: 1.2128 0.2136 sec/batch\n",
      "Epoch 3/20  Iteration 4276/35720 Training loss: 1.2128 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 4277/35720 Training loss: 1.2128 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 4278/35720 Training loss: 1.2128 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 4279/35720 Training loss: 1.2130 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 4280/35720 Training loss: 1.2129 0.2236 sec/batch\n",
      "Epoch 3/20  Iteration 4281/35720 Training loss: 1.2130 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 4282/35720 Training loss: 1.2130 0.2250 sec/batch\n",
      "Epoch 3/20  Iteration 4283/35720 Training loss: 1.2131 0.2160 sec/batch\n",
      "Epoch 3/20  Iteration 4284/35720 Training loss: 1.2131 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 4285/35720 Training loss: 1.2131 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 4286/35720 Training loss: 1.2131 0.2066 sec/batch\n",
      "Epoch 3/20  Iteration 4287/35720 Training loss: 1.2132 0.2190 sec/batch\n",
      "Epoch 3/20  Iteration 4288/35720 Training loss: 1.2132 0.2121 sec/batch\n",
      "Epoch 3/20  Iteration 4289/35720 Training loss: 1.2133 0.2288 sec/batch\n",
      "Epoch 3/20  Iteration 4290/35720 Training loss: 1.2133 0.2285 sec/batch\n",
      "Epoch 3/20  Iteration 4291/35720 Training loss: 1.2133 0.2285 sec/batch\n",
      "Epoch 3/20  Iteration 4292/35720 Training loss: 1.2133 0.2255 sec/batch\n",
      "Epoch 3/20  Iteration 4293/35720 Training loss: 1.2133 0.2276 sec/batch\n",
      "Epoch 3/20  Iteration 4294/35720 Training loss: 1.2133 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 4295/35720 Training loss: 1.2134 0.2245 sec/batch\n",
      "Epoch 3/20  Iteration 4296/35720 Training loss: 1.2133 0.2063 sec/batch\n",
      "Epoch 3/20  Iteration 4297/35720 Training loss: 1.2133 0.2239 sec/batch\n",
      "Epoch 3/20  Iteration 4298/35720 Training loss: 1.2133 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 4299/35720 Training loss: 1.2134 0.2240 sec/batch\n",
      "Epoch 3/20  Iteration 4300/35720 Training loss: 1.2134 0.2240 sec/batch\n",
      "Epoch 3/20  Iteration 4301/35720 Training loss: 1.2135 0.2272 sec/batch\n",
      "Epoch 3/20  Iteration 4302/35720 Training loss: 1.2134 0.2287 sec/batch\n",
      "Epoch 3/20  Iteration 4303/35720 Training loss: 1.2134 0.2283 sec/batch\n",
      "Epoch 3/20  Iteration 4304/35720 Training loss: 1.2134 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 4305/35720 Training loss: 1.2134 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 4306/35720 Training loss: 1.2133 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 4307/35720 Training loss: 1.2133 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 4308/35720 Training loss: 1.2133 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 4309/35720 Training loss: 1.2134 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 4310/35720 Training loss: 1.2133 0.2252 sec/batch\n",
      "Epoch 3/20  Iteration 4311/35720 Training loss: 1.2134 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 4312/35720 Training loss: 1.2134 0.2231 sec/batch\n",
      "Epoch 3/20  Iteration 4313/35720 Training loss: 1.2133 0.2229 sec/batch\n",
      "Epoch 3/20  Iteration 4314/35720 Training loss: 1.2133 0.2285 sec/batch\n",
      "Epoch 3/20  Iteration 4315/35720 Training loss: 1.2133 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 4316/35720 Training loss: 1.2133 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 4317/35720 Training loss: 1.2133 0.2223 sec/batch\n",
      "Epoch 3/20  Iteration 4318/35720 Training loss: 1.2133 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 4319/35720 Training loss: 1.2133 0.2186 sec/batch\n",
      "Epoch 3/20  Iteration 4320/35720 Training loss: 1.2131 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 4321/35720 Training loss: 1.2131 0.2227 sec/batch\n",
      "Epoch 3/20  Iteration 4322/35720 Training loss: 1.2131 0.2272 sec/batch\n",
      "Epoch 3/20  Iteration 4323/35720 Training loss: 1.2131 0.2302 sec/batch\n",
      "Epoch 3/20  Iteration 4324/35720 Training loss: 1.2133 0.2339 sec/batch\n",
      "Epoch 3/20  Iteration 4325/35720 Training loss: 1.2131 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4326/35720 Training loss: 1.2131 0.2249 sec/batch\n",
      "Epoch 3/20  Iteration 4327/35720 Training loss: 1.2130 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 4328/35720 Training loss: 1.2129 0.2063 sec/batch\n",
      "Epoch 3/20  Iteration 4329/35720 Training loss: 1.2128 0.2106 sec/batch\n",
      "Epoch 3/20  Iteration 4330/35720 Training loss: 1.2128 0.2102 sec/batch\n",
      "Epoch 3/20  Iteration 4331/35720 Training loss: 1.2129 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 4332/35720 Training loss: 1.2128 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 4333/35720 Training loss: 1.2129 0.2237 sec/batch\n",
      "Epoch 3/20  Iteration 4334/35720 Training loss: 1.2129 0.2295 sec/batch\n",
      "Epoch 3/20  Iteration 4335/35720 Training loss: 1.2129 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 4336/35720 Training loss: 1.2127 0.2282 sec/batch\n",
      "Epoch 3/20  Iteration 4337/35720 Training loss: 1.2127 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 4338/35720 Training loss: 1.2128 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 4339/35720 Training loss: 1.2128 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 4340/35720 Training loss: 1.2127 0.2253 sec/batch\n",
      "Epoch 3/20  Iteration 4341/35720 Training loss: 1.2128 0.2294 sec/batch\n",
      "Epoch 3/20  Iteration 4342/35720 Training loss: 1.2129 0.2306 sec/batch\n",
      "Epoch 3/20  Iteration 4343/35720 Training loss: 1.2130 0.2274 sec/batch\n",
      "Epoch 3/20  Iteration 4344/35720 Training loss: 1.2131 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 4345/35720 Training loss: 1.2130 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 4346/35720 Training loss: 1.2128 0.2240 sec/batch\n",
      "Epoch 3/20  Iteration 4347/35720 Training loss: 1.2126 0.2273 sec/batch\n",
      "Epoch 3/20  Iteration 4348/35720 Training loss: 1.2125 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 4349/35720 Training loss: 1.2125 0.2230 sec/batch\n",
      "Epoch 3/20  Iteration 4350/35720 Training loss: 1.2126 0.2317 sec/batch\n",
      "Epoch 3/20  Iteration 4351/35720 Training loss: 1.2126 0.2243 sec/batch\n",
      "Epoch 3/20  Iteration 4352/35720 Training loss: 1.2126 0.2057 sec/batch\n",
      "Epoch 3/20  Iteration 4353/35720 Training loss: 1.2126 0.2127 sec/batch\n",
      "Epoch 3/20  Iteration 4354/35720 Training loss: 1.2126 0.2090 sec/batch\n",
      "Epoch 3/20  Iteration 4355/35720 Training loss: 1.2127 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 4356/35720 Training loss: 1.2126 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 4357/35720 Training loss: 1.2125 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 4358/35720 Training loss: 1.2124 0.2061 sec/batch\n",
      "Epoch 3/20  Iteration 4359/35720 Training loss: 1.2125 0.2230 sec/batch\n",
      "Epoch 3/20  Iteration 4360/35720 Training loss: 1.2124 0.2122 sec/batch\n",
      "Epoch 3/20  Iteration 4361/35720 Training loss: 1.2125 0.2223 sec/batch\n",
      "Epoch 3/20  Iteration 4362/35720 Training loss: 1.2125 0.2245 sec/batch\n",
      "Epoch 3/20  Iteration 4363/35720 Training loss: 1.2126 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 4364/35720 Training loss: 1.2126 0.2279 sec/batch\n",
      "Epoch 3/20  Iteration 4365/35720 Training loss: 1.2126 0.2248 sec/batch\n",
      "Epoch 3/20  Iteration 4366/35720 Training loss: 1.2125 0.2122 sec/batch\n",
      "Epoch 3/20  Iteration 4367/35720 Training loss: 1.2125 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 4368/35720 Training loss: 1.2125 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 4369/35720 Training loss: 1.2124 0.2106 sec/batch\n",
      "Epoch 3/20  Iteration 4370/35720 Training loss: 1.2124 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 4371/35720 Training loss: 1.2122 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 4372/35720 Training loss: 1.2123 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 4373/35720 Training loss: 1.2122 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 4374/35720 Training loss: 1.2123 0.2226 sec/batch\n",
      "Epoch 3/20  Iteration 4375/35720 Training loss: 1.2124 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 4376/35720 Training loss: 1.2126 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 4377/35720 Training loss: 1.2127 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 4378/35720 Training loss: 1.2128 0.2214 sec/batch\n",
      "Epoch 3/20  Iteration 4379/35720 Training loss: 1.2129 0.2245 sec/batch\n",
      "Epoch 3/20  Iteration 4380/35720 Training loss: 1.2128 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 4381/35720 Training loss: 1.2130 0.2290 sec/batch\n",
      "Epoch 3/20  Iteration 4382/35720 Training loss: 1.2130 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 4383/35720 Training loss: 1.2131 0.2276 sec/batch\n",
      "Epoch 3/20  Iteration 4384/35720 Training loss: 1.2131 0.2260 sec/batch\n",
      "Epoch 3/20  Iteration 4385/35720 Training loss: 1.2131 0.2320 sec/batch\n",
      "Epoch 3/20  Iteration 4386/35720 Training loss: 1.2131 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 4387/35720 Training loss: 1.2132 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 4388/35720 Training loss: 1.2131 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4389/35720 Training loss: 1.2131 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 4390/35720 Training loss: 1.2131 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 4391/35720 Training loss: 1.2131 0.2204 sec/batch\n",
      "Epoch 3/20  Iteration 4392/35720 Training loss: 1.2131 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 4393/35720 Training loss: 1.2131 0.2291 sec/batch\n",
      "Epoch 3/20  Iteration 4394/35720 Training loss: 1.2130 0.2050 sec/batch\n",
      "Epoch 3/20  Iteration 4395/35720 Training loss: 1.2129 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 4396/35720 Training loss: 1.2128 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 4397/35720 Training loss: 1.2126 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 4398/35720 Training loss: 1.2125 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 4399/35720 Training loss: 1.2125 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 4400/35720 Training loss: 1.2123 0.2144 sec/batch\n",
      "Validation loss: 1.36779 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 4401/35720 Training loss: 1.2124 0.2104 sec/batch\n",
      "Epoch 3/20  Iteration 4402/35720 Training loss: 1.2123 0.2095 sec/batch\n",
      "Epoch 3/20  Iteration 4403/35720 Training loss: 1.2122 0.2228 sec/batch\n",
      "Epoch 3/20  Iteration 4404/35720 Training loss: 1.2123 0.2098 sec/batch\n",
      "Epoch 3/20  Iteration 4405/35720 Training loss: 1.2125 0.2160 sec/batch\n",
      "Epoch 3/20  Iteration 4406/35720 Training loss: 1.2126 0.2220 sec/batch\n",
      "Epoch 3/20  Iteration 4407/35720 Training loss: 1.2126 0.2305 sec/batch\n",
      "Epoch 3/20  Iteration 4408/35720 Training loss: 1.2125 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 4409/35720 Training loss: 1.2125 0.2228 sec/batch\n",
      "Epoch 3/20  Iteration 4410/35720 Training loss: 1.2125 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 4411/35720 Training loss: 1.2126 0.2121 sec/batch\n",
      "Epoch 3/20  Iteration 4412/35720 Training loss: 1.2125 0.2100 sec/batch\n",
      "Epoch 3/20  Iteration 4413/35720 Training loss: 1.2127 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 4414/35720 Training loss: 1.2127 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 4415/35720 Training loss: 1.2126 0.2200 sec/batch\n",
      "Epoch 3/20  Iteration 4416/35720 Training loss: 1.2127 0.2178 sec/batch\n",
      "Epoch 3/20  Iteration 4417/35720 Training loss: 1.2126 0.2282 sec/batch\n",
      "Epoch 3/20  Iteration 4418/35720 Training loss: 1.2126 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 4419/35720 Training loss: 1.2124 0.2200 sec/batch\n",
      "Epoch 3/20  Iteration 4420/35720 Training loss: 1.2125 0.2261 sec/batch\n",
      "Epoch 3/20  Iteration 4421/35720 Training loss: 1.2124 0.2072 sec/batch\n",
      "Epoch 3/20  Iteration 4422/35720 Training loss: 1.2124 0.2206 sec/batch\n",
      "Epoch 3/20  Iteration 4423/35720 Training loss: 1.2123 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 4424/35720 Training loss: 1.2123 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 4425/35720 Training loss: 1.2123 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 4426/35720 Training loss: 1.2123 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 4427/35720 Training loss: 1.2123 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 4428/35720 Training loss: 1.2122 0.2119 sec/batch\n",
      "Epoch 3/20  Iteration 4429/35720 Training loss: 1.2122 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 4430/35720 Training loss: 1.2122 0.2070 sec/batch\n",
      "Epoch 3/20  Iteration 4431/35720 Training loss: 1.2121 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 4432/35720 Training loss: 1.2120 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 4433/35720 Training loss: 1.2121 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 4434/35720 Training loss: 1.2120 0.2290 sec/batch\n",
      "Epoch 3/20  Iteration 4435/35720 Training loss: 1.2120 0.2275 sec/batch\n",
      "Epoch 3/20  Iteration 4436/35720 Training loss: 1.2119 0.2287 sec/batch\n",
      "Epoch 3/20  Iteration 4437/35720 Training loss: 1.2118 0.2186 sec/batch\n",
      "Epoch 3/20  Iteration 4438/35720 Training loss: 1.2119 0.2151 sec/batch\n",
      "Epoch 3/20  Iteration 4439/35720 Training loss: 1.2120 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 4440/35720 Training loss: 1.2120 0.2091 sec/batch\n",
      "Epoch 3/20  Iteration 4441/35720 Training loss: 1.2119 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 4442/35720 Training loss: 1.2119 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 4443/35720 Training loss: 1.2117 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 4444/35720 Training loss: 1.2116 0.2154 sec/batch\n",
      "Epoch 3/20  Iteration 4445/35720 Training loss: 1.2115 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 4446/35720 Training loss: 1.2115 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 4447/35720 Training loss: 1.2113 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 4448/35720 Training loss: 1.2113 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 4449/35720 Training loss: 1.2112 0.2121 sec/batch\n",
      "Epoch 3/20  Iteration 4450/35720 Training loss: 1.2111 0.2200 sec/batch\n",
      "Epoch 3/20  Iteration 4451/35720 Training loss: 1.2111 0.2218 sec/batch\n",
      "Epoch 3/20  Iteration 4452/35720 Training loss: 1.2110 0.2237 sec/batch\n",
      "Epoch 3/20  Iteration 4453/35720 Training loss: 1.2111 0.2134 sec/batch\n",
      "Epoch 3/20  Iteration 4454/35720 Training loss: 1.2111 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4455/35720 Training loss: 1.2111 0.2146 sec/batch\n",
      "Epoch 3/20  Iteration 4456/35720 Training loss: 1.2110 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 4457/35720 Training loss: 1.2111 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 4458/35720 Training loss: 1.2110 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 4459/35720 Training loss: 1.2110 0.2222 sec/batch\n",
      "Epoch 3/20  Iteration 4460/35720 Training loss: 1.2109 0.2214 sec/batch\n",
      "Epoch 3/20  Iteration 4461/35720 Training loss: 1.2108 0.2277 sec/batch\n",
      "Epoch 3/20  Iteration 4462/35720 Training loss: 1.2107 0.2154 sec/batch\n",
      "Epoch 3/20  Iteration 4463/35720 Training loss: 1.2106 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 4464/35720 Training loss: 1.2105 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 4465/35720 Training loss: 1.2104 0.2234 sec/batch\n",
      "Epoch 3/20  Iteration 4466/35720 Training loss: 1.2103 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 4467/35720 Training loss: 1.2103 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 4468/35720 Training loss: 1.2103 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 4469/35720 Training loss: 1.2101 0.2130 sec/batch\n",
      "Epoch 3/20  Iteration 4470/35720 Training loss: 1.2100 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 4471/35720 Training loss: 1.2098 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 4472/35720 Training loss: 1.2097 0.2277 sec/batch\n",
      "Epoch 3/20  Iteration 4473/35720 Training loss: 1.2096 0.2271 sec/batch\n",
      "Epoch 3/20  Iteration 4474/35720 Training loss: 1.2095 0.2292 sec/batch\n",
      "Epoch 3/20  Iteration 4475/35720 Training loss: 1.2094 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 4476/35720 Training loss: 1.2093 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 4477/35720 Training loss: 1.2091 0.2246 sec/batch\n",
      "Epoch 3/20  Iteration 4478/35720 Training loss: 1.2091 0.2243 sec/batch\n",
      "Epoch 3/20  Iteration 4479/35720 Training loss: 1.2091 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 4480/35720 Training loss: 1.2091 0.2066 sec/batch\n",
      "Epoch 3/20  Iteration 4481/35720 Training loss: 1.2091 0.2068 sec/batch\n",
      "Epoch 3/20  Iteration 4482/35720 Training loss: 1.2090 0.2099 sec/batch\n",
      "Epoch 3/20  Iteration 4483/35720 Training loss: 1.2090 0.2219 sec/batch\n",
      "Epoch 3/20  Iteration 4484/35720 Training loss: 1.2090 0.2293 sec/batch\n",
      "Epoch 3/20  Iteration 4485/35720 Training loss: 1.2090 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 4486/35720 Training loss: 1.2090 0.2170 sec/batch\n",
      "Epoch 3/20  Iteration 4487/35720 Training loss: 1.2091 0.2116 sec/batch\n",
      "Epoch 3/20  Iteration 4488/35720 Training loss: 1.2090 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 4489/35720 Training loss: 1.2090 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 4490/35720 Training loss: 1.2090 0.2309 sec/batch\n",
      "Epoch 3/20  Iteration 4491/35720 Training loss: 1.2089 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 4492/35720 Training loss: 1.2089 0.2081 sec/batch\n",
      "Epoch 3/20  Iteration 4493/35720 Training loss: 1.2089 0.2122 sec/batch\n",
      "Epoch 3/20  Iteration 4494/35720 Training loss: 1.2087 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 4495/35720 Training loss: 1.2087 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 4496/35720 Training loss: 1.2088 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 4497/35720 Training loss: 1.2087 0.2242 sec/batch\n",
      "Epoch 3/20  Iteration 4498/35720 Training loss: 1.2087 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 4499/35720 Training loss: 1.2088 0.2083 sec/batch\n",
      "Epoch 3/20  Iteration 4500/35720 Training loss: 1.2087 0.2217 sec/batch\n",
      "Epoch 3/20  Iteration 4501/35720 Training loss: 1.2088 0.2110 sec/batch\n",
      "Epoch 3/20  Iteration 4502/35720 Training loss: 1.2088 0.2236 sec/batch\n",
      "Epoch 3/20  Iteration 4503/35720 Training loss: 1.2089 0.2130 sec/batch\n",
      "Epoch 3/20  Iteration 4504/35720 Training loss: 1.2089 0.2065 sec/batch\n",
      "Epoch 3/20  Iteration 4505/35720 Training loss: 1.2088 0.2154 sec/batch\n",
      "Epoch 3/20  Iteration 4506/35720 Training loss: 1.2088 0.2052 sec/batch\n",
      "Epoch 3/20  Iteration 4507/35720 Training loss: 1.2087 0.2141 sec/batch\n",
      "Epoch 3/20  Iteration 4508/35720 Training loss: 1.2085 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 4509/35720 Training loss: 1.2084 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 4510/35720 Training loss: 1.2083 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 4511/35720 Training loss: 1.2083 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 4512/35720 Training loss: 1.2081 0.2055 sec/batch\n",
      "Epoch 3/20  Iteration 4513/35720 Training loss: 1.2081 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 4514/35720 Training loss: 1.2081 0.2226 sec/batch\n",
      "Epoch 3/20  Iteration 4515/35720 Training loss: 1.2080 0.2313 sec/batch\n",
      "Epoch 3/20  Iteration 4516/35720 Training loss: 1.2081 0.2257 sec/batch\n",
      "Epoch 3/20  Iteration 4517/35720 Training loss: 1.2080 0.2170 sec/batch\n",
      "Epoch 3/20  Iteration 4518/35720 Training loss: 1.2080 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 4519/35720 Training loss: 1.2080 0.2099 sec/batch\n",
      "Epoch 3/20  Iteration 4520/35720 Training loss: 1.2079 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 4521/35720 Training loss: 1.2079 0.2271 sec/batch\n",
      "Epoch 3/20  Iteration 4522/35720 Training loss: 1.2077 0.2277 sec/batch\n",
      "Epoch 3/20  Iteration 4523/35720 Training loss: 1.2077 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 4524/35720 Training loss: 1.2075 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 4525/35720 Training loss: 1.2074 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 4526/35720 Training loss: 1.2075 0.2228 sec/batch\n",
      "Epoch 3/20  Iteration 4527/35720 Training loss: 1.2074 0.2144 sec/batch\n",
      "Epoch 3/20  Iteration 4528/35720 Training loss: 1.2073 0.2078 sec/batch\n",
      "Epoch 3/20  Iteration 4529/35720 Training loss: 1.2072 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 4530/35720 Training loss: 1.2072 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 4531/35720 Training loss: 1.2071 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 4532/35720 Training loss: 1.2071 0.2100 sec/batch\n",
      "Epoch 3/20  Iteration 4533/35720 Training loss: 1.2070 0.2261 sec/batch\n",
      "Epoch 3/20  Iteration 4534/35720 Training loss: 1.2070 0.2276 sec/batch\n",
      "Epoch 3/20  Iteration 4535/35720 Training loss: 1.2069 0.2266 sec/batch\n",
      "Epoch 3/20  Iteration 4536/35720 Training loss: 1.2068 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 4537/35720 Training loss: 1.2068 0.2365 sec/batch\n",
      "Epoch 3/20  Iteration 4538/35720 Training loss: 1.2067 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 4539/35720 Training loss: 1.2068 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 4540/35720 Training loss: 1.2067 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 4541/35720 Training loss: 1.2069 0.2072 sec/batch\n",
      "Epoch 3/20  Iteration 4542/35720 Training loss: 1.2071 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4543/35720 Training loss: 1.2070 0.2244 sec/batch\n",
      "Epoch 3/20  Iteration 4544/35720 Training loss: 1.2069 0.2273 sec/batch\n",
      "Epoch 3/20  Iteration 4545/35720 Training loss: 1.2069 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 4546/35720 Training loss: 1.2068 0.2383 sec/batch\n",
      "Epoch 3/20  Iteration 4547/35720 Training loss: 1.2067 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 4548/35720 Training loss: 1.2067 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 4549/35720 Training loss: 1.2067 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 4550/35720 Training loss: 1.2068 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 4551/35720 Training loss: 1.2067 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 4552/35720 Training loss: 1.2067 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 4553/35720 Training loss: 1.2066 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 4554/35720 Training loss: 1.2065 0.2067 sec/batch\n",
      "Epoch 3/20  Iteration 4555/35720 Training loss: 1.2063 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 4556/35720 Training loss: 1.2063 0.2364 sec/batch\n",
      "Epoch 3/20  Iteration 4557/35720 Training loss: 1.2064 0.2291 sec/batch\n",
      "Epoch 3/20  Iteration 4558/35720 Training loss: 1.2064 0.2231 sec/batch\n",
      "Epoch 3/20  Iteration 4559/35720 Training loss: 1.2064 0.2208 sec/batch\n",
      "Epoch 3/20  Iteration 4560/35720 Training loss: 1.2064 0.2278 sec/batch\n",
      "Epoch 3/20  Iteration 4561/35720 Training loss: 1.2063 0.2235 sec/batch\n",
      "Epoch 3/20  Iteration 4562/35720 Training loss: 1.2063 0.2190 sec/batch\n",
      "Epoch 3/20  Iteration 4563/35720 Training loss: 1.2062 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 4564/35720 Training loss: 1.2063 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 4565/35720 Training loss: 1.2063 0.2128 sec/batch\n",
      "Epoch 3/20  Iteration 4566/35720 Training loss: 1.2063 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 4567/35720 Training loss: 1.2063 0.2134 sec/batch\n",
      "Epoch 3/20  Iteration 4568/35720 Training loss: 1.2063 0.2145 sec/batch\n",
      "Epoch 3/20  Iteration 4569/35720 Training loss: 1.2064 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 4570/35720 Training loss: 1.2064 0.2141 sec/batch\n",
      "Epoch 3/20  Iteration 4571/35720 Training loss: 1.2064 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 4572/35720 Training loss: 1.2064 0.2217 sec/batch\n",
      "Epoch 3/20  Iteration 4573/35720 Training loss: 1.2062 0.2294 sec/batch\n",
      "Epoch 3/20  Iteration 4574/35720 Training loss: 1.2061 0.2296 sec/batch\n",
      "Epoch 3/20  Iteration 4575/35720 Training loss: 1.2059 0.2297 sec/batch\n",
      "Epoch 3/20  Iteration 4576/35720 Training loss: 1.2058 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 4577/35720 Training loss: 1.2057 0.2317 sec/batch\n",
      "Epoch 3/20  Iteration 4578/35720 Training loss: 1.2056 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 4579/35720 Training loss: 1.2055 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 4580/35720 Training loss: 1.2054 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4581/35720 Training loss: 1.2053 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4582/35720 Training loss: 1.2051 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 4583/35720 Training loss: 1.2051 0.2264 sec/batch\n",
      "Epoch 3/20  Iteration 4584/35720 Training loss: 1.2051 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 4585/35720 Training loss: 1.2050 0.2091 sec/batch\n",
      "Epoch 3/20  Iteration 4586/35720 Training loss: 1.2049 0.2279 sec/batch\n",
      "Epoch 3/20  Iteration 4587/35720 Training loss: 1.2049 0.2095 sec/batch\n",
      "Epoch 3/20  Iteration 4588/35720 Training loss: 1.2049 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 4589/35720 Training loss: 1.2049 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 4590/35720 Training loss: 1.2049 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4591/35720 Training loss: 1.2048 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 4592/35720 Training loss: 1.2047 0.2209 sec/batch\n",
      "Epoch 3/20  Iteration 4593/35720 Training loss: 1.2047 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 4594/35720 Training loss: 1.2047 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 4595/35720 Training loss: 1.2048 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 4596/35720 Training loss: 1.2048 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 4597/35720 Training loss: 1.2048 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 4598/35720 Training loss: 1.2048 0.2274 sec/batch\n",
      "Epoch 3/20  Iteration 4599/35720 Training loss: 1.2046 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 4600/35720 Training loss: 1.2046 0.2153 sec/batch\n",
      "Validation loss: 1.3661 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 4601/35720 Training loss: 1.2045 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 4602/35720 Training loss: 1.2045 0.2071 sec/batch\n",
      "Epoch 3/20  Iteration 4603/35720 Training loss: 1.2045 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 4604/35720 Training loss: 1.2045 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4605/35720 Training loss: 1.2045 0.2232 sec/batch\n",
      "Epoch 3/20  Iteration 4606/35720 Training loss: 1.2045 0.2278 sec/batch\n",
      "Epoch 3/20  Iteration 4607/35720 Training loss: 1.2045 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 4608/35720 Training loss: 1.2045 0.2223 sec/batch\n",
      "Epoch 3/20  Iteration 4609/35720 Training loss: 1.2046 0.2283 sec/batch\n",
      "Epoch 3/20  Iteration 4610/35720 Training loss: 1.2045 0.2244 sec/batch\n",
      "Epoch 3/20  Iteration 4611/35720 Training loss: 1.2044 0.2250 sec/batch\n",
      "Epoch 3/20  Iteration 4612/35720 Training loss: 1.2044 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 4613/35720 Training loss: 1.2044 0.2264 sec/batch\n",
      "Epoch 3/20  Iteration 4614/35720 Training loss: 1.2043 0.2302 sec/batch\n",
      "Epoch 3/20  Iteration 4615/35720 Training loss: 1.2043 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 4616/35720 Training loss: 1.2043 0.2366 sec/batch\n",
      "Epoch 3/20  Iteration 4617/35720 Training loss: 1.2043 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 4618/35720 Training loss: 1.2043 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 4619/35720 Training loss: 1.2042 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 4620/35720 Training loss: 1.2041 0.2151 sec/batch\n",
      "Epoch 3/20  Iteration 4621/35720 Training loss: 1.2041 0.2321 sec/batch\n",
      "Epoch 3/20  Iteration 4622/35720 Training loss: 1.2040 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 4623/35720 Training loss: 1.2039 0.2109 sec/batch\n",
      "Epoch 3/20  Iteration 4624/35720 Training loss: 1.2040 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 4625/35720 Training loss: 1.2041 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 4626/35720 Training loss: 1.2041 0.2315 sec/batch\n",
      "Epoch 3/20  Iteration 4627/35720 Training loss: 1.2041 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 4628/35720 Training loss: 1.2040 0.2219 sec/batch\n",
      "Epoch 3/20  Iteration 4629/35720 Training loss: 1.2040 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4630/35720 Training loss: 1.2040 0.2295 sec/batch\n",
      "Epoch 3/20  Iteration 4631/35720 Training loss: 1.2040 0.2252 sec/batch\n",
      "Epoch 3/20  Iteration 4632/35720 Training loss: 1.2038 0.2076 sec/batch\n",
      "Epoch 3/20  Iteration 4633/35720 Training loss: 1.2038 0.2113 sec/batch\n",
      "Epoch 3/20  Iteration 4634/35720 Training loss: 1.2038 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 4635/35720 Training loss: 1.2039 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4636/35720 Training loss: 1.2038 0.2061 sec/batch\n",
      "Epoch 3/20  Iteration 4637/35720 Training loss: 1.2037 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 4638/35720 Training loss: 1.2037 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 4639/35720 Training loss: 1.2036 0.2240 sec/batch\n",
      "Epoch 3/20  Iteration 4640/35720 Training loss: 1.2036 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 4641/35720 Training loss: 1.2036 0.2127 sec/batch\n",
      "Epoch 3/20  Iteration 4642/35720 Training loss: 1.2036 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 4643/35720 Training loss: 1.2035 0.2102 sec/batch\n",
      "Epoch 3/20  Iteration 4644/35720 Training loss: 1.2035 0.2275 sec/batch\n",
      "Epoch 3/20  Iteration 4645/35720 Training loss: 1.2034 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 4646/35720 Training loss: 1.2034 0.2261 sec/batch\n",
      "Epoch 3/20  Iteration 4647/35720 Training loss: 1.2034 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 4648/35720 Training loss: 1.2034 0.2266 sec/batch\n",
      "Epoch 3/20  Iteration 4649/35720 Training loss: 1.2034 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 4650/35720 Training loss: 1.2033 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 4651/35720 Training loss: 1.2033 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 4652/35720 Training loss: 1.2032 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 4653/35720 Training loss: 1.2031 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 4654/35720 Training loss: 1.2031 0.2126 sec/batch\n",
      "Epoch 3/20  Iteration 4655/35720 Training loss: 1.2030 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 4656/35720 Training loss: 1.2029 0.2295 sec/batch\n",
      "Epoch 3/20  Iteration 4657/35720 Training loss: 1.2029 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 4658/35720 Training loss: 1.2029 0.2124 sec/batch\n",
      "Epoch 3/20  Iteration 4659/35720 Training loss: 1.2029 0.2086 sec/batch\n",
      "Epoch 3/20  Iteration 4660/35720 Training loss: 1.2029 0.2138 sec/batch\n",
      "Epoch 3/20  Iteration 4661/35720 Training loss: 1.2028 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 4662/35720 Training loss: 1.2028 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 4663/35720 Training loss: 1.2028 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 4664/35720 Training loss: 1.2027 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 4665/35720 Training loss: 1.2027 0.2092 sec/batch\n",
      "Epoch 3/20  Iteration 4666/35720 Training loss: 1.2027 0.2134 sec/batch\n",
      "Epoch 3/20  Iteration 4667/35720 Training loss: 1.2027 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4668/35720 Training loss: 1.2026 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 4669/35720 Training loss: 1.2025 0.2244 sec/batch\n",
      "Epoch 3/20  Iteration 4670/35720 Training loss: 1.2025 0.2264 sec/batch\n",
      "Epoch 3/20  Iteration 4671/35720 Training loss: 1.2026 0.2116 sec/batch\n",
      "Epoch 3/20  Iteration 4672/35720 Training loss: 1.2025 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 4673/35720 Training loss: 1.2026 0.2129 sec/batch\n",
      "Epoch 3/20  Iteration 4674/35720 Training loss: 1.2026 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 4675/35720 Training loss: 1.2026 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 4676/35720 Training loss: 1.2026 0.2252 sec/batch\n",
      "Epoch 3/20  Iteration 4677/35720 Training loss: 1.2026 0.2245 sec/batch\n",
      "Epoch 3/20  Iteration 4678/35720 Training loss: 1.2025 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 4679/35720 Training loss: 1.2025 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 4680/35720 Training loss: 1.2026 0.2105 sec/batch\n",
      "Epoch 3/20  Iteration 4681/35720 Training loss: 1.2026 0.2236 sec/batch\n",
      "Epoch 3/20  Iteration 4682/35720 Training loss: 1.2025 0.2283 sec/batch\n",
      "Epoch 3/20  Iteration 4683/35720 Training loss: 1.2025 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 4684/35720 Training loss: 1.2024 0.2071 sec/batch\n",
      "Epoch 3/20  Iteration 4685/35720 Training loss: 1.2023 0.2325 sec/batch\n",
      "Epoch 3/20  Iteration 4686/35720 Training loss: 1.2023 0.2606 sec/batch\n",
      "Epoch 3/20  Iteration 4687/35720 Training loss: 1.2022 0.2277 sec/batch\n",
      "Epoch 3/20  Iteration 4688/35720 Training loss: 1.2022 0.2299 sec/batch\n",
      "Epoch 3/20  Iteration 4689/35720 Training loss: 1.2021 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 4690/35720 Training loss: 1.2020 0.2219 sec/batch\n",
      "Epoch 3/20  Iteration 4691/35720 Training loss: 1.2020 0.2363 sec/batch\n",
      "Epoch 3/20  Iteration 4692/35720 Training loss: 1.2020 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 4693/35720 Training loss: 1.2021 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 4694/35720 Training loss: 1.2022 0.2144 sec/batch\n",
      "Epoch 3/20  Iteration 4695/35720 Training loss: 1.2023 0.2205 sec/batch\n",
      "Epoch 3/20  Iteration 4696/35720 Training loss: 1.2023 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 4697/35720 Training loss: 1.2023 0.2131 sec/batch\n",
      "Epoch 3/20  Iteration 4698/35720 Training loss: 1.2022 0.2113 sec/batch\n",
      "Epoch 3/20  Iteration 4699/35720 Training loss: 1.2021 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 4700/35720 Training loss: 1.2021 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 4701/35720 Training loss: 1.2020 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 4702/35720 Training loss: 1.2020 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 4703/35720 Training loss: 1.2020 0.2165 sec/batch\n",
      "Epoch 3/20  Iteration 4704/35720 Training loss: 1.2019 0.2236 sec/batch\n",
      "Epoch 3/20  Iteration 4705/35720 Training loss: 1.2018 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 4706/35720 Training loss: 1.2019 0.2206 sec/batch\n",
      "Epoch 3/20  Iteration 4707/35720 Training loss: 1.2018 0.2091 sec/batch\n",
      "Epoch 3/20  Iteration 4708/35720 Training loss: 1.2017 0.2190 sec/batch\n",
      "Epoch 3/20  Iteration 4709/35720 Training loss: 1.2016 0.2134 sec/batch\n",
      "Epoch 3/20  Iteration 4710/35720 Training loss: 1.2015 0.2107 sec/batch\n",
      "Epoch 3/20  Iteration 4711/35720 Training loss: 1.2014 0.2307 sec/batch\n",
      "Epoch 3/20  Iteration 4712/35720 Training loss: 1.2014 0.2323 sec/batch\n",
      "Epoch 3/20  Iteration 4713/35720 Training loss: 1.2013 0.2262 sec/batch\n",
      "Epoch 3/20  Iteration 4714/35720 Training loss: 1.2012 0.2250 sec/batch\n",
      "Epoch 3/20  Iteration 4715/35720 Training loss: 1.2011 0.2101 sec/batch\n",
      "Epoch 3/20  Iteration 4716/35720 Training loss: 1.2010 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 4717/35720 Training loss: 1.2010 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 4718/35720 Training loss: 1.2009 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 4719/35720 Training loss: 1.2008 0.2071 sec/batch\n",
      "Epoch 3/20  Iteration 4720/35720 Training loss: 1.2007 0.2211 sec/batch\n",
      "Epoch 3/20  Iteration 4721/35720 Training loss: 1.2007 0.2205 sec/batch\n",
      "Epoch 3/20  Iteration 4722/35720 Training loss: 1.2006 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 4723/35720 Training loss: 1.2006 0.2262 sec/batch\n",
      "Epoch 3/20  Iteration 4724/35720 Training loss: 1.2006 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 4725/35720 Training loss: 1.2006 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 4726/35720 Training loss: 1.2006 0.2186 sec/batch\n",
      "Epoch 3/20  Iteration 4727/35720 Training loss: 1.2005 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 4728/35720 Training loss: 1.2004 0.2267 sec/batch\n",
      "Epoch 3/20  Iteration 4729/35720 Training loss: 1.2004 0.2209 sec/batch\n",
      "Epoch 3/20  Iteration 4730/35720 Training loss: 1.2003 0.2104 sec/batch\n",
      "Epoch 3/20  Iteration 4731/35720 Training loss: 1.2003 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 4732/35720 Training loss: 1.2002 0.2124 sec/batch\n",
      "Epoch 3/20  Iteration 4733/35720 Training loss: 1.2001 0.2292 sec/batch\n",
      "Epoch 3/20  Iteration 4734/35720 Training loss: 1.2001 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 4735/35720 Training loss: 1.2002 0.2359 sec/batch\n",
      "Epoch 3/20  Iteration 4736/35720 Training loss: 1.2002 0.2220 sec/batch\n",
      "Epoch 3/20  Iteration 4737/35720 Training loss: 1.2002 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 4738/35720 Training loss: 1.2001 0.2153 sec/batch\n",
      "Epoch 3/20  Iteration 4739/35720 Training loss: 1.2000 0.2264 sec/batch\n",
      "Epoch 3/20  Iteration 4740/35720 Training loss: 1.2000 0.2319 sec/batch\n",
      "Epoch 3/20  Iteration 4741/35720 Training loss: 1.2000 0.2278 sec/batch\n",
      "Epoch 3/20  Iteration 4742/35720 Training loss: 1.2001 0.2054 sec/batch\n",
      "Epoch 3/20  Iteration 4743/35720 Training loss: 1.2001 0.2274 sec/batch\n",
      "Epoch 3/20  Iteration 4744/35720 Training loss: 1.2000 0.2093 sec/batch\n",
      "Epoch 3/20  Iteration 4745/35720 Training loss: 1.2000 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 4746/35720 Training loss: 1.1999 0.2090 sec/batch\n",
      "Epoch 3/20  Iteration 4747/35720 Training loss: 1.1999 0.2282 sec/batch\n",
      "Epoch 3/20  Iteration 4748/35720 Training loss: 1.2000 0.2210 sec/batch\n",
      "Epoch 3/20  Iteration 4749/35720 Training loss: 1.2000 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 4750/35720 Training loss: 1.2000 0.2060 sec/batch\n",
      "Epoch 3/20  Iteration 4751/35720 Training loss: 1.2000 0.2089 sec/batch\n",
      "Epoch 3/20  Iteration 4752/35720 Training loss: 1.1999 0.2135 sec/batch\n",
      "Epoch 3/20  Iteration 4753/35720 Training loss: 1.2000 0.2278 sec/batch\n",
      "Epoch 3/20  Iteration 4754/35720 Training loss: 1.2000 0.2277 sec/batch\n",
      "Epoch 3/20  Iteration 4755/35720 Training loss: 1.1999 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 4756/35720 Training loss: 1.1998 0.2133 sec/batch\n",
      "Epoch 3/20  Iteration 4757/35720 Training loss: 1.1997 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 4758/35720 Training loss: 1.1997 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 4759/35720 Training loss: 1.1996 0.2078 sec/batch\n",
      "Epoch 3/20  Iteration 4760/35720 Training loss: 1.1996 0.2095 sec/batch\n",
      "Epoch 3/20  Iteration 4761/35720 Training loss: 1.1996 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 4762/35720 Training loss: 1.1995 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 4763/35720 Training loss: 1.1995 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 4764/35720 Training loss: 1.1996 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 4765/35720 Training loss: 1.1995 0.2092 sec/batch\n",
      "Epoch 3/20  Iteration 4766/35720 Training loss: 1.1995 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 4767/35720 Training loss: 1.1995 0.2225 sec/batch\n",
      "Epoch 3/20  Iteration 4768/35720 Training loss: 1.1995 0.2219 sec/batch\n",
      "Epoch 3/20  Iteration 4769/35720 Training loss: 1.1995 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 4770/35720 Training loss: 1.1995 0.2257 sec/batch\n",
      "Epoch 3/20  Iteration 4771/35720 Training loss: 1.1995 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 4772/35720 Training loss: 1.1994 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 4773/35720 Training loss: 1.1994 0.2100 sec/batch\n",
      "Epoch 3/20  Iteration 4774/35720 Training loss: 1.1993 0.2158 sec/batch\n",
      "Epoch 3/20  Iteration 4775/35720 Training loss: 1.1992 0.2142 sec/batch\n",
      "Epoch 3/20  Iteration 4776/35720 Training loss: 1.1991 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4777/35720 Training loss: 1.1990 0.2071 sec/batch\n",
      "Epoch 3/20  Iteration 4778/35720 Training loss: 1.1990 0.2105 sec/batch\n",
      "Epoch 3/20  Iteration 4779/35720 Training loss: 1.1990 0.2249 sec/batch\n",
      "Epoch 3/20  Iteration 4780/35720 Training loss: 1.1989 0.2073 sec/batch\n",
      "Epoch 3/20  Iteration 4781/35720 Training loss: 1.1989 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 4782/35720 Training loss: 1.1988 0.2133 sec/batch\n",
      "Epoch 3/20  Iteration 4783/35720 Training loss: 1.1988 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 4784/35720 Training loss: 1.1988 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4785/35720 Training loss: 1.1987 0.2081 sec/batch\n",
      "Epoch 3/20  Iteration 4786/35720 Training loss: 1.1987 0.2190 sec/batch\n",
      "Epoch 3/20  Iteration 4787/35720 Training loss: 1.1987 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 4788/35720 Training loss: 1.1986 0.2692 sec/batch\n",
      "Epoch 3/20  Iteration 4789/35720 Training loss: 1.1986 0.2321 sec/batch\n",
      "Epoch 3/20  Iteration 4790/35720 Training loss: 1.1986 0.2141 sec/batch\n",
      "Epoch 3/20  Iteration 4791/35720 Training loss: 1.1985 0.2214 sec/batch\n",
      "Epoch 3/20  Iteration 4792/35720 Training loss: 1.1984 0.2119 sec/batch\n",
      "Epoch 3/20  Iteration 4793/35720 Training loss: 1.1984 0.2328 sec/batch\n",
      "Epoch 3/20  Iteration 4794/35720 Training loss: 1.1984 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 4795/35720 Training loss: 1.1983 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 4796/35720 Training loss: 1.1982 0.2231 sec/batch\n",
      "Epoch 3/20  Iteration 4797/35720 Training loss: 1.1982 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 4798/35720 Training loss: 1.1982 0.2104 sec/batch\n",
      "Epoch 3/20  Iteration 4799/35720 Training loss: 1.1982 0.2127 sec/batch\n",
      "Epoch 3/20  Iteration 4800/35720 Training loss: 1.1981 0.2088 sec/batch\n",
      "Validation loss: 1.35502 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 4801/35720 Training loss: 1.1983 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 4802/35720 Training loss: 1.1983 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 4803/35720 Training loss: 1.1982 0.2182 sec/batch\n",
      "Epoch 3/20  Iteration 4804/35720 Training loss: 1.1981 0.2119 sec/batch\n",
      "Epoch 3/20  Iteration 4805/35720 Training loss: 1.1981 0.2271 sec/batch\n",
      "Epoch 3/20  Iteration 4806/35720 Training loss: 1.1981 0.2330 sec/batch\n",
      "Epoch 3/20  Iteration 4807/35720 Training loss: 1.1980 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 4808/35720 Training loss: 1.1980 0.2204 sec/batch\n",
      "Epoch 3/20  Iteration 4809/35720 Training loss: 1.1978 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 4810/35720 Training loss: 1.1977 0.2218 sec/batch\n",
      "Epoch 3/20  Iteration 4811/35720 Training loss: 1.1976 0.2214 sec/batch\n",
      "Epoch 3/20  Iteration 4812/35720 Training loss: 1.1975 0.2288 sec/batch\n",
      "Epoch 3/20  Iteration 4813/35720 Training loss: 1.1974 0.2237 sec/batch\n",
      "Epoch 3/20  Iteration 4814/35720 Training loss: 1.1974 0.2339 sec/batch\n",
      "Epoch 3/20  Iteration 4815/35720 Training loss: 1.1973 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 4816/35720 Training loss: 1.1972 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 4817/35720 Training loss: 1.1972 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 4818/35720 Training loss: 1.1971 0.2229 sec/batch\n",
      "Epoch 3/20  Iteration 4819/35720 Training loss: 1.1970 0.2128 sec/batch\n",
      "Epoch 3/20  Iteration 4820/35720 Training loss: 1.1970 0.2077 sec/batch\n",
      "Epoch 3/20  Iteration 4821/35720 Training loss: 1.1968 0.2048 sec/batch\n",
      "Epoch 3/20  Iteration 4822/35720 Training loss: 1.1968 0.2076 sec/batch\n",
      "Epoch 3/20  Iteration 4823/35720 Training loss: 1.1967 0.2221 sec/batch\n",
      "Epoch 3/20  Iteration 4824/35720 Training loss: 1.1966 0.2297 sec/batch\n",
      "Epoch 3/20  Iteration 4825/35720 Training loss: 1.1966 0.2165 sec/batch\n",
      "Epoch 3/20  Iteration 4826/35720 Training loss: 1.1965 0.2068 sec/batch\n",
      "Epoch 3/20  Iteration 4827/35720 Training loss: 1.1965 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 4828/35720 Training loss: 1.1964 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4829/35720 Training loss: 1.1963 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 4830/35720 Training loss: 1.1963 0.2072 sec/batch\n",
      "Epoch 3/20  Iteration 4831/35720 Training loss: 1.1962 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4832/35720 Training loss: 1.1963 0.2151 sec/batch\n",
      "Epoch 3/20  Iteration 4833/35720 Training loss: 1.1962 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 4834/35720 Training loss: 1.1961 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 4835/35720 Training loss: 1.1960 0.2109 sec/batch\n",
      "Epoch 3/20  Iteration 4836/35720 Training loss: 1.1960 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 4837/35720 Training loss: 1.1959 0.2129 sec/batch\n",
      "Epoch 3/20  Iteration 4838/35720 Training loss: 1.1958 0.2134 sec/batch\n",
      "Epoch 3/20  Iteration 4839/35720 Training loss: 1.1957 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 4840/35720 Training loss: 1.1957 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 4841/35720 Training loss: 1.1957 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 4842/35720 Training loss: 1.1957 0.2070 sec/batch\n",
      "Epoch 3/20  Iteration 4843/35720 Training loss: 1.1956 0.2225 sec/batch\n",
      "Epoch 3/20  Iteration 4844/35720 Training loss: 1.1955 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 4845/35720 Training loss: 1.1955 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 4846/35720 Training loss: 1.1954 0.2072 sec/batch\n",
      "Epoch 3/20  Iteration 4847/35720 Training loss: 1.1954 0.2250 sec/batch\n",
      "Epoch 3/20  Iteration 4848/35720 Training loss: 1.1953 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 4849/35720 Training loss: 1.1952 0.2073 sec/batch\n",
      "Epoch 3/20  Iteration 4850/35720 Training loss: 1.1951 0.2138 sec/batch\n",
      "Epoch 3/20  Iteration 4851/35720 Training loss: 1.1950 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 4852/35720 Training loss: 1.1949 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 4853/35720 Training loss: 1.1948 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 4854/35720 Training loss: 1.1947 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 4855/35720 Training loss: 1.1946 0.2265 sec/batch\n",
      "Epoch 3/20  Iteration 4856/35720 Training loss: 1.1946 0.2267 sec/batch\n",
      "Epoch 3/20  Iteration 4857/35720 Training loss: 1.1945 0.2306 sec/batch\n",
      "Epoch 3/20  Iteration 4858/35720 Training loss: 1.1944 0.2061 sec/batch\n",
      "Epoch 3/20  Iteration 4859/35720 Training loss: 1.1943 0.2274 sec/batch\n",
      "Epoch 3/20  Iteration 4860/35720 Training loss: 1.1943 0.2247 sec/batch\n",
      "Epoch 3/20  Iteration 4861/35720 Training loss: 1.1943 0.2367 sec/batch\n",
      "Epoch 3/20  Iteration 4862/35720 Training loss: 1.1942 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 4863/35720 Training loss: 1.1942 0.2129 sec/batch\n",
      "Epoch 3/20  Iteration 4864/35720 Training loss: 1.1941 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 4865/35720 Training loss: 1.1941 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 4866/35720 Training loss: 1.1942 0.2129 sec/batch\n",
      "Epoch 3/20  Iteration 4867/35720 Training loss: 1.1942 0.2240 sec/batch\n",
      "Epoch 3/20  Iteration 4868/35720 Training loss: 1.1942 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 4869/35720 Training loss: 1.1942 0.2205 sec/batch\n",
      "Epoch 3/20  Iteration 4870/35720 Training loss: 1.1942 0.2104 sec/batch\n",
      "Epoch 3/20  Iteration 4871/35720 Training loss: 1.1940 0.2249 sec/batch\n",
      "Epoch 3/20  Iteration 4872/35720 Training loss: 1.1939 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 4873/35720 Training loss: 1.1938 0.2241 sec/batch\n",
      "Epoch 3/20  Iteration 4874/35720 Training loss: 1.1939 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 4875/35720 Training loss: 1.1938 0.2133 sec/batch\n",
      "Epoch 3/20  Iteration 4876/35720 Training loss: 1.1938 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 4877/35720 Training loss: 1.1937 0.2113 sec/batch\n",
      "Epoch 3/20  Iteration 4878/35720 Training loss: 1.1936 0.2116 sec/batch\n",
      "Epoch 3/20  Iteration 4879/35720 Training loss: 1.1936 0.2113 sec/batch\n",
      "Epoch 3/20  Iteration 4880/35720 Training loss: 1.1935 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 4881/35720 Training loss: 1.1934 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 4882/35720 Training loss: 1.1933 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 4883/35720 Training loss: 1.1932 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 4884/35720 Training loss: 1.1933 0.2126 sec/batch\n",
      "Epoch 3/20  Iteration 4885/35720 Training loss: 1.1932 0.2105 sec/batch\n",
      "Epoch 3/20  Iteration 4886/35720 Training loss: 1.1932 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 4887/35720 Training loss: 1.1931 0.2231 sec/batch\n",
      "Epoch 3/20  Iteration 4888/35720 Training loss: 1.1931 0.2070 sec/batch\n",
      "Epoch 3/20  Iteration 4889/35720 Training loss: 1.1931 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 4890/35720 Training loss: 1.1930 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 4891/35720 Training loss: 1.1930 0.2264 sec/batch\n",
      "Epoch 3/20  Iteration 4892/35720 Training loss: 1.1929 0.2181 sec/batch\n",
      "Epoch 3/20  Iteration 4893/35720 Training loss: 1.1929 0.2111 sec/batch\n",
      "Epoch 3/20  Iteration 4894/35720 Training loss: 1.1929 0.2154 sec/batch\n",
      "Epoch 3/20  Iteration 4895/35720 Training loss: 1.1928 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 4896/35720 Training loss: 1.1928 0.2334 sec/batch\n",
      "Epoch 3/20  Iteration 4897/35720 Training loss: 1.1928 0.2136 sec/batch\n",
      "Epoch 3/20  Iteration 4898/35720 Training loss: 1.1928 0.2310 sec/batch\n",
      "Epoch 3/20  Iteration 4899/35720 Training loss: 1.1928 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 4900/35720 Training loss: 1.1928 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 4901/35720 Training loss: 1.1927 0.2062 sec/batch\n",
      "Epoch 3/20  Iteration 4902/35720 Training loss: 1.1926 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 4903/35720 Training loss: 1.1925 0.2200 sec/batch\n",
      "Epoch 3/20  Iteration 4904/35720 Training loss: 1.1925 0.2082 sec/batch\n",
      "Epoch 3/20  Iteration 4905/35720 Training loss: 1.1925 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 4906/35720 Training loss: 1.1924 0.2168 sec/batch\n",
      "Epoch 3/20  Iteration 4907/35720 Training loss: 1.1924 0.2264 sec/batch\n",
      "Epoch 3/20  Iteration 4908/35720 Training loss: 1.1925 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 4909/35720 Training loss: 1.1925 0.2773 sec/batch\n",
      "Epoch 3/20  Iteration 4910/35720 Training loss: 1.1925 0.2609 sec/batch\n",
      "Epoch 3/20  Iteration 4911/35720 Training loss: 1.1924 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 4912/35720 Training loss: 1.1924 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 4913/35720 Training loss: 1.1924 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 4914/35720 Training loss: 1.1923 0.2144 sec/batch\n",
      "Epoch 3/20  Iteration 4915/35720 Training loss: 1.1922 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 4916/35720 Training loss: 1.1922 0.2094 sec/batch\n",
      "Epoch 3/20  Iteration 4917/35720 Training loss: 1.1922 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 4918/35720 Training loss: 1.1921 0.2063 sec/batch\n",
      "Epoch 3/20  Iteration 4919/35720 Training loss: 1.1921 0.2121 sec/batch\n",
      "Epoch 3/20  Iteration 4920/35720 Training loss: 1.1921 0.2226 sec/batch\n",
      "Epoch 3/20  Iteration 4921/35720 Training loss: 1.1921 0.2146 sec/batch\n",
      "Epoch 3/20  Iteration 4922/35720 Training loss: 1.1922 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 4923/35720 Training loss: 1.1921 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 4924/35720 Training loss: 1.1922 0.2072 sec/batch\n",
      "Epoch 3/20  Iteration 4925/35720 Training loss: 1.1922 0.2226 sec/batch\n",
      "Epoch 3/20  Iteration 4926/35720 Training loss: 1.1922 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 4927/35720 Training loss: 1.1921 0.2279 sec/batch\n",
      "Epoch 3/20  Iteration 4928/35720 Training loss: 1.1920 0.2237 sec/batch\n",
      "Epoch 3/20  Iteration 4929/35720 Training loss: 1.1919 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 4930/35720 Training loss: 1.1919 0.2099 sec/batch\n",
      "Epoch 3/20  Iteration 4931/35720 Training loss: 1.1918 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 4932/35720 Training loss: 1.1918 0.2078 sec/batch\n",
      "Epoch 3/20  Iteration 4933/35720 Training loss: 1.1917 0.2256 sec/batch\n",
      "Epoch 3/20  Iteration 4934/35720 Training loss: 1.1916 0.2237 sec/batch\n",
      "Epoch 3/20  Iteration 4935/35720 Training loss: 1.1916 0.2149 sec/batch\n",
      "Epoch 3/20  Iteration 4936/35720 Training loss: 1.1917 0.2331 sec/batch\n",
      "Epoch 3/20  Iteration 4937/35720 Training loss: 1.1916 0.2233 sec/batch\n",
      "Epoch 3/20  Iteration 4938/35720 Training loss: 1.1916 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4939/35720 Training loss: 1.1916 0.2065 sec/batch\n",
      "Epoch 3/20  Iteration 4940/35720 Training loss: 1.1916 0.2203 sec/batch\n",
      "Epoch 3/20  Iteration 4941/35720 Training loss: 1.1916 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 4942/35720 Training loss: 1.1916 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 4943/35720 Training loss: 1.1916 0.2235 sec/batch\n",
      "Epoch 3/20  Iteration 4944/35720 Training loss: 1.1916 0.2072 sec/batch\n",
      "Epoch 3/20  Iteration 4945/35720 Training loss: 1.1916 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 4946/35720 Training loss: 1.1916 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 4947/35720 Training loss: 1.1916 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 4948/35720 Training loss: 1.1915 0.2130 sec/batch\n",
      "Epoch 3/20  Iteration 4949/35720 Training loss: 1.1914 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 4950/35720 Training loss: 1.1914 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4951/35720 Training loss: 1.1914 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 4952/35720 Training loss: 1.1914 0.2261 sec/batch\n",
      "Epoch 3/20  Iteration 4953/35720 Training loss: 1.1914 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 4954/35720 Training loss: 1.1913 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 4955/35720 Training loss: 1.1913 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 4956/35720 Training loss: 1.1912 0.2241 sec/batch\n",
      "Epoch 3/20  Iteration 4957/35720 Training loss: 1.1913 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 4958/35720 Training loss: 1.1913 0.2061 sec/batch\n",
      "Epoch 3/20  Iteration 4959/35720 Training loss: 1.1912 0.2133 sec/batch\n",
      "Epoch 3/20  Iteration 4960/35720 Training loss: 1.1912 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 4961/35720 Training loss: 1.1911 0.2257 sec/batch\n",
      "Epoch 3/20  Iteration 4962/35720 Training loss: 1.1911 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 4963/35720 Training loss: 1.1910 0.2067 sec/batch\n",
      "Epoch 3/20  Iteration 4964/35720 Training loss: 1.1909 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 4965/35720 Training loss: 1.1909 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 4966/35720 Training loss: 1.1908 0.2141 sec/batch\n",
      "Epoch 3/20  Iteration 4967/35720 Training loss: 1.1907 0.2064 sec/batch\n",
      "Epoch 3/20  Iteration 4968/35720 Training loss: 1.1906 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 4969/35720 Training loss: 1.1905 0.2066 sec/batch\n",
      "Epoch 3/20  Iteration 4970/35720 Training loss: 1.1905 0.2092 sec/batch\n",
      "Epoch 3/20  Iteration 4971/35720 Training loss: 1.1904 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4972/35720 Training loss: 1.1903 0.2092 sec/batch\n",
      "Epoch 3/20  Iteration 4973/35720 Training loss: 1.1902 0.2144 sec/batch\n",
      "Epoch 3/20  Iteration 4974/35720 Training loss: 1.1902 0.2146 sec/batch\n",
      "Epoch 3/20  Iteration 4975/35720 Training loss: 1.1902 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 4976/35720 Training loss: 1.1902 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 4977/35720 Training loss: 1.1901 0.2092 sec/batch\n",
      "Epoch 3/20  Iteration 4978/35720 Training loss: 1.1901 0.2069 sec/batch\n",
      "Epoch 3/20  Iteration 4979/35720 Training loss: 1.1901 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 4980/35720 Training loss: 1.1901 0.2232 sec/batch\n",
      "Epoch 3/20  Iteration 4981/35720 Training loss: 1.1901 0.2093 sec/batch\n",
      "Epoch 3/20  Iteration 4982/35720 Training loss: 1.1901 0.2119 sec/batch\n",
      "Epoch 3/20  Iteration 4983/35720 Training loss: 1.1901 0.2235 sec/batch\n",
      "Epoch 3/20  Iteration 4984/35720 Training loss: 1.1900 0.2138 sec/batch\n",
      "Epoch 3/20  Iteration 4985/35720 Training loss: 1.1900 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 4986/35720 Training loss: 1.1899 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 4987/35720 Training loss: 1.1899 0.2074 sec/batch\n",
      "Epoch 3/20  Iteration 4988/35720 Training loss: 1.1899 0.2118 sec/batch\n",
      "Epoch 3/20  Iteration 4989/35720 Training loss: 1.1899 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 4990/35720 Training loss: 1.1899 0.2072 sec/batch\n",
      "Epoch 3/20  Iteration 4991/35720 Training loss: 1.1899 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 4992/35720 Training loss: 1.1899 0.2131 sec/batch\n",
      "Epoch 3/20  Iteration 4993/35720 Training loss: 1.1899 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 4994/35720 Training loss: 1.1898 0.2144 sec/batch\n",
      "Epoch 3/20  Iteration 4995/35720 Training loss: 1.1898 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 4996/35720 Training loss: 1.1897 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 4997/35720 Training loss: 1.1897 0.2287 sec/batch\n",
      "Epoch 3/20  Iteration 4998/35720 Training loss: 1.1898 0.2365 sec/batch\n",
      "Epoch 3/20  Iteration 4999/35720 Training loss: 1.1897 0.2064 sec/batch\n",
      "Epoch 3/20  Iteration 5000/35720 Training loss: 1.1898 0.2087 sec/batch\n",
      "Validation loss: 1.35007 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 5001/35720 Training loss: 1.1898 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 5002/35720 Training loss: 1.1897 0.2098 sec/batch\n",
      "Epoch 3/20  Iteration 5003/35720 Training loss: 1.1896 0.2068 sec/batch\n",
      "Epoch 3/20  Iteration 5004/35720 Training loss: 1.1896 0.2091 sec/batch\n",
      "Epoch 3/20  Iteration 5005/35720 Training loss: 1.1895 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 5006/35720 Training loss: 1.1894 0.2057 sec/batch\n",
      "Epoch 3/20  Iteration 5007/35720 Training loss: 1.1894 0.2207 sec/batch\n",
      "Epoch 3/20  Iteration 5008/35720 Training loss: 1.1893 0.2096 sec/batch\n",
      "Epoch 3/20  Iteration 5009/35720 Training loss: 1.1893 0.2295 sec/batch\n",
      "Epoch 3/20  Iteration 5010/35720 Training loss: 1.1893 0.2422 sec/batch\n",
      "Epoch 3/20  Iteration 5011/35720 Training loss: 1.1893 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 5012/35720 Training loss: 1.1893 0.2138 sec/batch\n",
      "Epoch 3/20  Iteration 5013/35720 Training loss: 1.1894 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 5014/35720 Training loss: 1.1893 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 5015/35720 Training loss: 1.1893 0.2129 sec/batch\n",
      "Epoch 3/20  Iteration 5016/35720 Training loss: 1.1892 0.2130 sec/batch\n",
      "Epoch 3/20  Iteration 5017/35720 Training loss: 1.1892 0.2323 sec/batch\n",
      "Epoch 3/20  Iteration 5018/35720 Training loss: 1.1891 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 5019/35720 Training loss: 1.1891 0.2112 sec/batch\n",
      "Epoch 3/20  Iteration 5020/35720 Training loss: 1.1890 0.2107 sec/batch\n",
      "Epoch 3/20  Iteration 5021/35720 Training loss: 1.1890 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 5022/35720 Training loss: 1.1889 0.2221 sec/batch\n",
      "Epoch 3/20  Iteration 5023/35720 Training loss: 1.1888 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 5024/35720 Training loss: 1.1888 0.2075 sec/batch\n",
      "Epoch 3/20  Iteration 5025/35720 Training loss: 1.1887 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 5026/35720 Training loss: 1.1887 0.2165 sec/batch\n",
      "Epoch 3/20  Iteration 5027/35720 Training loss: 1.1887 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 5028/35720 Training loss: 1.1887 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 5029/35720 Training loss: 1.1887 0.2108 sec/batch\n",
      "Epoch 3/20  Iteration 5030/35720 Training loss: 1.1887 0.2093 sec/batch\n",
      "Epoch 3/20  Iteration 5031/35720 Training loss: 1.1888 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 5032/35720 Training loss: 1.1888 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 5033/35720 Training loss: 1.1888 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 5034/35720 Training loss: 1.1888 0.2101 sec/batch\n",
      "Epoch 3/20  Iteration 5035/35720 Training loss: 1.1888 0.2122 sec/batch\n",
      "Epoch 3/20  Iteration 5036/35720 Training loss: 1.1888 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 5037/35720 Training loss: 1.1887 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 5038/35720 Training loss: 1.1886 0.2315 sec/batch\n",
      "Epoch 3/20  Iteration 5039/35720 Training loss: 1.1886 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 5040/35720 Training loss: 1.1886 0.2089 sec/batch\n",
      "Epoch 3/20  Iteration 5041/35720 Training loss: 1.1885 0.2227 sec/batch\n",
      "Epoch 3/20  Iteration 5042/35720 Training loss: 1.1885 0.2228 sec/batch\n",
      "Epoch 3/20  Iteration 5043/35720 Training loss: 1.1885 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 5044/35720 Training loss: 1.1884 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 5045/35720 Training loss: 1.1884 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 5046/35720 Training loss: 1.1882 0.2275 sec/batch\n",
      "Epoch 3/20  Iteration 5047/35720 Training loss: 1.1881 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 5048/35720 Training loss: 1.1880 0.2068 sec/batch\n",
      "Epoch 3/20  Iteration 5049/35720 Training loss: 1.1879 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 5050/35720 Training loss: 1.1878 0.2140 sec/batch\n",
      "Epoch 3/20  Iteration 5051/35720 Training loss: 1.1877 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 5052/35720 Training loss: 1.1876 0.2142 sec/batch\n",
      "Epoch 3/20  Iteration 5053/35720 Training loss: 1.1876 0.2066 sec/batch\n",
      "Epoch 3/20  Iteration 5054/35720 Training loss: 1.1875 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 5055/35720 Training loss: 1.1875 0.2266 sec/batch\n",
      "Epoch 3/20  Iteration 5056/35720 Training loss: 1.1874 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 5057/35720 Training loss: 1.1873 0.2312 sec/batch\n",
      "Epoch 3/20  Iteration 5058/35720 Training loss: 1.1872 0.2267 sec/batch\n",
      "Epoch 3/20  Iteration 5059/35720 Training loss: 1.1872 0.2106 sec/batch\n",
      "Epoch 3/20  Iteration 5060/35720 Training loss: 1.1871 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 5061/35720 Training loss: 1.1871 0.2125 sec/batch\n",
      "Epoch 3/20  Iteration 5062/35720 Training loss: 1.1871 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 5063/35720 Training loss: 1.1871 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 5064/35720 Training loss: 1.1870 0.2263 sec/batch\n",
      "Epoch 3/20  Iteration 5065/35720 Training loss: 1.1870 0.2206 sec/batch\n",
      "Epoch 3/20  Iteration 5066/35720 Training loss: 1.1870 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 5067/35720 Training loss: 1.1869 0.2144 sec/batch\n",
      "Epoch 3/20  Iteration 5068/35720 Training loss: 1.1869 0.2149 sec/batch\n",
      "Epoch 3/20  Iteration 5069/35720 Training loss: 1.1868 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 5070/35720 Training loss: 1.1868 0.2138 sec/batch\n",
      "Epoch 3/20  Iteration 5071/35720 Training loss: 1.1869 0.2054 sec/batch\n",
      "Epoch 3/20  Iteration 5072/35720 Training loss: 1.1868 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 5073/35720 Training loss: 1.1867 0.2124 sec/batch\n",
      "Epoch 3/20  Iteration 5074/35720 Training loss: 1.1866 0.2083 sec/batch\n",
      "Epoch 3/20  Iteration 5075/35720 Training loss: 1.1866 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 5076/35720 Training loss: 1.1866 0.2239 sec/batch\n",
      "Epoch 3/20  Iteration 5077/35720 Training loss: 1.1866 0.2136 sec/batch\n",
      "Epoch 3/20  Iteration 5078/35720 Training loss: 1.1866 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 5079/35720 Training loss: 1.1865 0.2166 sec/batch\n",
      "Epoch 3/20  Iteration 5080/35720 Training loss: 1.1865 0.2264 sec/batch\n",
      "Epoch 3/20  Iteration 5081/35720 Training loss: 1.1865 0.2223 sec/batch\n",
      "Epoch 3/20  Iteration 5082/35720 Training loss: 1.1865 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 5083/35720 Training loss: 1.1865 0.2088 sec/batch\n",
      "Epoch 3/20  Iteration 5084/35720 Training loss: 1.1864 0.2160 sec/batch\n",
      "Epoch 3/20  Iteration 5085/35720 Training loss: 1.1863 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 5086/35720 Training loss: 1.1863 0.2186 sec/batch\n",
      "Epoch 3/20  Iteration 5087/35720 Training loss: 1.1863 0.2141 sec/batch\n",
      "Epoch 3/20  Iteration 5088/35720 Training loss: 1.1863 0.2103 sec/batch\n",
      "Epoch 3/20  Iteration 5089/35720 Training loss: 1.1863 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 5090/35720 Training loss: 1.1863 0.2292 sec/batch\n",
      "Epoch 3/20  Iteration 5091/35720 Training loss: 1.1863 0.2263 sec/batch\n",
      "Epoch 3/20  Iteration 5092/35720 Training loss: 1.1863 0.2285 sec/batch\n",
      "Epoch 3/20  Iteration 5093/35720 Training loss: 1.1863 0.2275 sec/batch\n",
      "Epoch 3/20  Iteration 5094/35720 Training loss: 1.1862 0.2079 sec/batch\n",
      "Epoch 3/20  Iteration 5095/35720 Training loss: 1.1863 0.2130 sec/batch\n",
      "Epoch 3/20  Iteration 5096/35720 Training loss: 1.1863 0.2251 sec/batch\n",
      "Epoch 3/20  Iteration 5097/35720 Training loss: 1.1863 0.2276 sec/batch\n",
      "Epoch 3/20  Iteration 5098/35720 Training loss: 1.1863 0.2280 sec/batch\n",
      "Epoch 3/20  Iteration 5099/35720 Training loss: 1.1863 0.2290 sec/batch\n",
      "Epoch 3/20  Iteration 5100/35720 Training loss: 1.1863 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 5101/35720 Training loss: 1.1863 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 5102/35720 Training loss: 1.1863 0.2078 sec/batch\n",
      "Epoch 3/20  Iteration 5103/35720 Training loss: 1.1863 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 5104/35720 Training loss: 1.1863 0.2192 sec/batch\n",
      "Epoch 3/20  Iteration 5105/35720 Training loss: 1.1862 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 5106/35720 Training loss: 1.1862 0.2198 sec/batch\n",
      "Epoch 3/20  Iteration 5107/35720 Training loss: 1.1862 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 5108/35720 Training loss: 1.1862 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 5109/35720 Training loss: 1.1861 0.2218 sec/batch\n",
      "Epoch 3/20  Iteration 5110/35720 Training loss: 1.1860 0.2234 sec/batch\n",
      "Epoch 3/20  Iteration 5111/35720 Training loss: 1.1860 0.2090 sec/batch\n",
      "Epoch 3/20  Iteration 5112/35720 Training loss: 1.1859 0.2149 sec/batch\n",
      "Epoch 3/20  Iteration 5113/35720 Training loss: 1.1858 0.2177 sec/batch\n",
      "Epoch 3/20  Iteration 5114/35720 Training loss: 1.1858 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 5115/35720 Training loss: 1.1857 0.2174 sec/batch\n",
      "Epoch 3/20  Iteration 5116/35720 Training loss: 1.1857 0.2074 sec/batch\n",
      "Epoch 3/20  Iteration 5117/35720 Training loss: 1.1856 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 5118/35720 Training loss: 1.1855 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 5119/35720 Training loss: 1.1855 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 5120/35720 Training loss: 1.1854 0.2184 sec/batch\n",
      "Epoch 3/20  Iteration 5121/35720 Training loss: 1.1854 0.2091 sec/batch\n",
      "Epoch 3/20  Iteration 5122/35720 Training loss: 1.1854 0.2161 sec/batch\n",
      "Epoch 3/20  Iteration 5123/35720 Training loss: 1.1853 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 5124/35720 Training loss: 1.1852 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 5125/35720 Training loss: 1.1852 0.2162 sec/batch\n",
      "Epoch 3/20  Iteration 5126/35720 Training loss: 1.1851 0.2311 sec/batch\n",
      "Epoch 3/20  Iteration 5127/35720 Training loss: 1.1850 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 5128/35720 Training loss: 1.1850 0.2123 sec/batch\n",
      "Epoch 3/20  Iteration 5129/35720 Training loss: 1.1849 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 5130/35720 Training loss: 1.1848 0.2199 sec/batch\n",
      "Epoch 3/20  Iteration 5131/35720 Training loss: 1.1847 0.2138 sec/batch\n",
      "Epoch 3/20  Iteration 5132/35720 Training loss: 1.1846 0.3143 sec/batch\n",
      "Epoch 3/20  Iteration 5133/35720 Training loss: 1.1846 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 5134/35720 Training loss: 1.1845 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 5135/35720 Training loss: 1.1845 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 5136/35720 Training loss: 1.1844 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 5137/35720 Training loss: 1.1843 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 5138/35720 Training loss: 1.1843 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 5139/35720 Training loss: 1.1842 0.2071 sec/batch\n",
      "Epoch 3/20  Iteration 5140/35720 Training loss: 1.1843 0.2123 sec/batch\n",
      "Epoch 3/20  Iteration 5141/35720 Training loss: 1.1842 0.2334 sec/batch\n",
      "Epoch 3/20  Iteration 5142/35720 Training loss: 1.1842 0.2328 sec/batch\n",
      "Epoch 3/20  Iteration 5143/35720 Training loss: 1.1841 0.2176 sec/batch\n",
      "Epoch 3/20  Iteration 5144/35720 Training loss: 1.1841 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 5145/35720 Training loss: 1.1840 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 5146/35720 Training loss: 1.1840 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 5147/35720 Training loss: 1.1839 0.2226 sec/batch\n",
      "Epoch 3/20  Iteration 5148/35720 Training loss: 1.1838 0.2361 sec/batch\n",
      "Epoch 3/20  Iteration 5149/35720 Training loss: 1.1837 0.2361 sec/batch\n",
      "Epoch 3/20  Iteration 5150/35720 Training loss: 1.1838 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 5151/35720 Training loss: 1.1837 0.2106 sec/batch\n",
      "Epoch 3/20  Iteration 5152/35720 Training loss: 1.1836 0.2139 sec/batch\n",
      "Epoch 3/20  Iteration 5153/35720 Training loss: 1.1836 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 5154/35720 Training loss: 1.1836 0.2113 sec/batch\n",
      "Epoch 3/20  Iteration 5155/35720 Training loss: 1.1835 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 5156/35720 Training loss: 1.1834 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 5157/35720 Training loss: 1.1834 0.2298 sec/batch\n",
      "Epoch 3/20  Iteration 5158/35720 Training loss: 1.1833 0.2294 sec/batch\n",
      "Epoch 3/20  Iteration 5159/35720 Training loss: 1.1832 0.2281 sec/batch\n",
      "Epoch 3/20  Iteration 5160/35720 Training loss: 1.1832 0.2095 sec/batch\n",
      "Epoch 3/20  Iteration 5161/35720 Training loss: 1.1831 0.2257 sec/batch\n",
      "Epoch 3/20  Iteration 5162/35720 Training loss: 1.1830 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 5163/35720 Training loss: 1.1829 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 5164/35720 Training loss: 1.1829 0.2215 sec/batch\n",
      "Epoch 3/20  Iteration 5165/35720 Training loss: 1.1830 0.2101 sec/batch\n",
      "Epoch 3/20  Iteration 5166/35720 Training loss: 1.1829 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 5167/35720 Training loss: 1.1828 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 5168/35720 Training loss: 1.1827 0.2200 sec/batch\n",
      "Epoch 3/20  Iteration 5169/35720 Training loss: 1.1826 0.2061 sec/batch\n",
      "Epoch 3/20  Iteration 5170/35720 Training loss: 1.1826 0.2154 sec/batch\n",
      "Epoch 3/20  Iteration 5171/35720 Training loss: 1.1825 0.2271 sec/batch\n",
      "Epoch 3/20  Iteration 5172/35720 Training loss: 1.1825 0.2342 sec/batch\n",
      "Epoch 3/20  Iteration 5173/35720 Training loss: 1.1825 0.2252 sec/batch\n",
      "Epoch 3/20  Iteration 5174/35720 Training loss: 1.1824 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 5175/35720 Training loss: 1.1824 0.2219 sec/batch\n",
      "Epoch 3/20  Iteration 5176/35720 Training loss: 1.1824 0.2169 sec/batch\n",
      "Epoch 3/20  Iteration 5177/35720 Training loss: 1.1823 0.2160 sec/batch\n",
      "Epoch 3/20  Iteration 5178/35720 Training loss: 1.1823 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 5179/35720 Training loss: 1.1822 0.2060 sec/batch\n",
      "Epoch 3/20  Iteration 5180/35720 Training loss: 1.1822 0.2100 sec/batch\n",
      "Epoch 3/20  Iteration 5181/35720 Training loss: 1.1821 0.2307 sec/batch\n",
      "Epoch 3/20  Iteration 5182/35720 Training loss: 1.1820 0.2227 sec/batch\n",
      "Epoch 3/20  Iteration 5183/35720 Training loss: 1.1820 0.2249 sec/batch\n",
      "Epoch 3/20  Iteration 5184/35720 Training loss: 1.1819 0.2191 sec/batch\n",
      "Epoch 3/20  Iteration 5185/35720 Training loss: 1.1818 0.2154 sec/batch\n",
      "Epoch 3/20  Iteration 5186/35720 Training loss: 1.1818 0.2109 sec/batch\n",
      "Epoch 3/20  Iteration 5187/35720 Training loss: 1.1817 0.2142 sec/batch\n",
      "Epoch 3/20  Iteration 5188/35720 Training loss: 1.1817 0.2186 sec/batch\n",
      "Epoch 3/20  Iteration 5189/35720 Training loss: 1.1817 0.2123 sec/batch\n",
      "Epoch 3/20  Iteration 5190/35720 Training loss: 1.1816 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 5191/35720 Training loss: 1.1815 0.2243 sec/batch\n",
      "Epoch 3/20  Iteration 5192/35720 Training loss: 1.1815 0.2245 sec/batch\n",
      "Epoch 3/20  Iteration 5193/35720 Training loss: 1.1814 0.2128 sec/batch\n",
      "Epoch 3/20  Iteration 5194/35720 Training loss: 1.1814 0.2138 sec/batch\n",
      "Epoch 3/20  Iteration 5195/35720 Training loss: 1.1813 0.2205 sec/batch\n",
      "Epoch 3/20  Iteration 5196/35720 Training loss: 1.1813 0.2087 sec/batch\n",
      "Epoch 3/20  Iteration 5197/35720 Training loss: 1.1813 0.2180 sec/batch\n",
      "Epoch 3/20  Iteration 5198/35720 Training loss: 1.1814 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 5199/35720 Training loss: 1.1814 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 5200/35720 Training loss: 1.1814 0.2159 sec/batch\n",
      "Validation loss: 1.33306 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 5201/35720 Training loss: 1.1814 0.2093 sec/batch\n",
      "Epoch 3/20  Iteration 5202/35720 Training loss: 1.1814 0.2080 sec/batch\n",
      "Epoch 3/20  Iteration 5203/35720 Training loss: 1.1813 0.2093 sec/batch\n",
      "Epoch 3/20  Iteration 5204/35720 Training loss: 1.1813 0.2099 sec/batch\n",
      "Epoch 3/20  Iteration 5205/35720 Training loss: 1.1812 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 5206/35720 Training loss: 1.1812 0.2186 sec/batch\n",
      "Epoch 3/20  Iteration 5207/35720 Training loss: 1.1811 0.2219 sec/batch\n",
      "Epoch 3/20  Iteration 5208/35720 Training loss: 1.1811 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 5209/35720 Training loss: 1.1811 0.2260 sec/batch\n",
      "Epoch 3/20  Iteration 5210/35720 Training loss: 1.1810 0.2285 sec/batch\n",
      "Epoch 3/20  Iteration 5211/35720 Training loss: 1.1809 0.2091 sec/batch\n",
      "Epoch 3/20  Iteration 5212/35720 Training loss: 1.1809 0.2173 sec/batch\n",
      "Epoch 3/20  Iteration 5213/35720 Training loss: 1.1808 0.2167 sec/batch\n",
      "Epoch 3/20  Iteration 5214/35720 Training loss: 1.1808 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 5215/35720 Training loss: 1.1807 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 5216/35720 Training loss: 1.1807 0.2151 sec/batch\n",
      "Epoch 3/20  Iteration 5217/35720 Training loss: 1.1807 0.2083 sec/batch\n",
      "Epoch 3/20  Iteration 5218/35720 Training loss: 1.1807 0.2229 sec/batch\n",
      "Epoch 3/20  Iteration 5219/35720 Training loss: 1.1807 0.2244 sec/batch\n",
      "Epoch 3/20  Iteration 5220/35720 Training loss: 1.1807 0.2311 sec/batch\n",
      "Epoch 3/20  Iteration 5221/35720 Training loss: 1.1806 0.2097 sec/batch\n",
      "Epoch 3/20  Iteration 5222/35720 Training loss: 1.1806 0.2150 sec/batch\n",
      "Epoch 3/20  Iteration 5223/35720 Training loss: 1.1807 0.2096 sec/batch\n",
      "Epoch 3/20  Iteration 5224/35720 Training loss: 1.1806 0.2222 sec/batch\n",
      "Epoch 3/20  Iteration 5225/35720 Training loss: 1.1806 0.2098 sec/batch\n",
      "Epoch 3/20  Iteration 5226/35720 Training loss: 1.1806 0.2282 sec/batch\n",
      "Epoch 3/20  Iteration 5227/35720 Training loss: 1.1806 0.2163 sec/batch\n",
      "Epoch 3/20  Iteration 5228/35720 Training loss: 1.1806 0.2105 sec/batch\n",
      "Epoch 3/20  Iteration 5229/35720 Training loss: 1.1806 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 5230/35720 Training loss: 1.1806 0.2110 sec/batch\n",
      "Epoch 3/20  Iteration 5231/35720 Training loss: 1.1806 0.2064 sec/batch\n",
      "Epoch 3/20  Iteration 5232/35720 Training loss: 1.1805 0.2098 sec/batch\n",
      "Epoch 3/20  Iteration 5233/35720 Training loss: 1.1805 0.2128 sec/batch\n",
      "Epoch 3/20  Iteration 5234/35720 Training loss: 1.1804 0.2280 sec/batch\n",
      "Epoch 3/20  Iteration 5235/35720 Training loss: 1.1804 0.2258 sec/batch\n",
      "Epoch 3/20  Iteration 5236/35720 Training loss: 1.1804 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 5237/35720 Training loss: 1.1804 0.2152 sec/batch\n",
      "Epoch 3/20  Iteration 5238/35720 Training loss: 1.1804 0.2153 sec/batch\n",
      "Epoch 3/20  Iteration 5239/35720 Training loss: 1.1804 0.2309 sec/batch\n",
      "Epoch 3/20  Iteration 5240/35720 Training loss: 1.1804 0.2224 sec/batch\n",
      "Epoch 3/20  Iteration 5241/35720 Training loss: 1.1805 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 5242/35720 Training loss: 1.1804 0.2127 sec/batch\n",
      "Epoch 3/20  Iteration 5243/35720 Training loss: 1.1804 0.2181 sec/batch\n",
      "Epoch 3/20  Iteration 5244/35720 Training loss: 1.1804 0.2068 sec/batch\n",
      "Epoch 3/20  Iteration 5245/35720 Training loss: 1.1804 0.2106 sec/batch\n",
      "Epoch 3/20  Iteration 5246/35720 Training loss: 1.1805 0.2248 sec/batch\n",
      "Epoch 3/20  Iteration 5247/35720 Training loss: 1.1804 0.2324 sec/batch\n",
      "Epoch 3/20  Iteration 5248/35720 Training loss: 1.1804 0.2196 sec/batch\n",
      "Epoch 3/20  Iteration 5249/35720 Training loss: 1.1803 0.2294 sec/batch\n",
      "Epoch 3/20  Iteration 5250/35720 Training loss: 1.1803 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 5251/35720 Training loss: 1.1803 0.2114 sec/batch\n",
      "Epoch 3/20  Iteration 5252/35720 Training loss: 1.1802 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 5253/35720 Training loss: 1.1801 0.2139 sec/batch\n",
      "Epoch 3/20  Iteration 5254/35720 Training loss: 1.1801 0.2158 sec/batch\n",
      "Epoch 3/20  Iteration 5255/35720 Training loss: 1.1800 0.2120 sec/batch\n",
      "Epoch 3/20  Iteration 5256/35720 Training loss: 1.1801 0.2238 sec/batch\n",
      "Epoch 3/20  Iteration 5257/35720 Training loss: 1.1800 0.2061 sec/batch\n",
      "Epoch 3/20  Iteration 5258/35720 Training loss: 1.1801 0.2171 sec/batch\n",
      "Epoch 3/20  Iteration 5259/35720 Training loss: 1.1800 0.2269 sec/batch\n",
      "Epoch 3/20  Iteration 5260/35720 Training loss: 1.1800 0.2259 sec/batch\n",
      "Epoch 3/20  Iteration 5261/35720 Training loss: 1.1800 0.2273 sec/batch\n",
      "Epoch 3/20  Iteration 5262/35720 Training loss: 1.1799 0.2069 sec/batch\n",
      "Epoch 3/20  Iteration 5263/35720 Training loss: 1.1798 0.2155 sec/batch\n",
      "Epoch 3/20  Iteration 5264/35720 Training loss: 1.1797 0.2179 sec/batch\n",
      "Epoch 3/20  Iteration 5265/35720 Training loss: 1.1797 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 5266/35720 Training loss: 1.1797 0.2070 sec/batch\n",
      "Epoch 3/20  Iteration 5267/35720 Training loss: 1.1796 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 5268/35720 Training loss: 1.1795 0.2143 sec/batch\n",
      "Epoch 3/20  Iteration 5269/35720 Training loss: 1.1795 0.2084 sec/batch\n",
      "Epoch 3/20  Iteration 5270/35720 Training loss: 1.1794 0.2145 sec/batch\n",
      "Epoch 3/20  Iteration 5271/35720 Training loss: 1.1794 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 5272/35720 Training loss: 1.1794 0.2055 sec/batch\n",
      "Epoch 3/20  Iteration 5273/35720 Training loss: 1.1794 0.2187 sec/batch\n",
      "Epoch 3/20  Iteration 5274/35720 Training loss: 1.1793 0.2153 sec/batch\n",
      "Epoch 3/20  Iteration 5275/35720 Training loss: 1.1792 0.2156 sec/batch\n",
      "Epoch 3/20  Iteration 5276/35720 Training loss: 1.1792 0.2221 sec/batch\n",
      "Epoch 3/20  Iteration 5277/35720 Training loss: 1.1791 0.2059 sec/batch\n",
      "Epoch 3/20  Iteration 5278/35720 Training loss: 1.1791 0.2158 sec/batch\n",
      "Epoch 3/20  Iteration 5279/35720 Training loss: 1.1790 0.2272 sec/batch\n",
      "Epoch 3/20  Iteration 5280/35720 Training loss: 1.1789 0.2308 sec/batch\n",
      "Epoch 3/20  Iteration 5281/35720 Training loss: 1.1789 0.2217 sec/batch\n",
      "Epoch 3/20  Iteration 5282/35720 Training loss: 1.1789 0.2159 sec/batch\n",
      "Epoch 3/20  Iteration 5283/35720 Training loss: 1.1790 0.2094 sec/batch\n",
      "Epoch 3/20  Iteration 5284/35720 Training loss: 1.1789 0.2266 sec/batch\n",
      "Epoch 3/20  Iteration 5285/35720 Training loss: 1.1789 0.2299 sec/batch\n",
      "Epoch 3/20  Iteration 5286/35720 Training loss: 1.1789 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 5287/35720 Training loss: 1.1789 0.2078 sec/batch\n",
      "Epoch 3/20  Iteration 5288/35720 Training loss: 1.1789 0.2236 sec/batch\n",
      "Epoch 3/20  Iteration 5289/35720 Training loss: 1.1789 0.2289 sec/batch\n",
      "Epoch 3/20  Iteration 5290/35720 Training loss: 1.1789 0.2116 sec/batch\n",
      "Epoch 3/20  Iteration 5291/35720 Training loss: 1.1788 0.2232 sec/batch\n",
      "Epoch 3/20  Iteration 5292/35720 Training loss: 1.1788 0.2134 sec/batch\n",
      "Epoch 3/20  Iteration 5293/35720 Training loss: 1.1788 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 5294/35720 Training loss: 1.1788 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 5295/35720 Training loss: 1.1787 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 5296/35720 Training loss: 1.1787 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 5297/35720 Training loss: 1.1786 0.2288 sec/batch\n",
      "Epoch 3/20  Iteration 5298/35720 Training loss: 1.1786 0.2172 sec/batch\n",
      "Epoch 3/20  Iteration 5299/35720 Training loss: 1.1785 0.2189 sec/batch\n",
      "Epoch 3/20  Iteration 5300/35720 Training loss: 1.1785 0.2095 sec/batch\n",
      "Epoch 3/20  Iteration 5301/35720 Training loss: 1.1785 0.2092 sec/batch\n",
      "Epoch 3/20  Iteration 5302/35720 Training loss: 1.1784 0.2190 sec/batch\n",
      "Epoch 3/20  Iteration 5303/35720 Training loss: 1.1784 0.2183 sec/batch\n",
      "Epoch 3/20  Iteration 5304/35720 Training loss: 1.1784 0.2185 sec/batch\n",
      "Epoch 3/20  Iteration 5305/35720 Training loss: 1.1783 0.2096 sec/batch\n",
      "Epoch 3/20  Iteration 5306/35720 Training loss: 1.1782 0.2233 sec/batch\n",
      "Epoch 3/20  Iteration 5307/35720 Training loss: 1.1782 0.2243 sec/batch\n",
      "Epoch 3/20  Iteration 5308/35720 Training loss: 1.1782 0.2190 sec/batch\n",
      "Epoch 3/20  Iteration 5309/35720 Training loss: 1.1782 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 5310/35720 Training loss: 1.1781 0.2115 sec/batch\n",
      "Epoch 3/20  Iteration 5311/35720 Training loss: 1.1782 0.2170 sec/batch\n",
      "Epoch 3/20  Iteration 5312/35720 Training loss: 1.1781 0.2262 sec/batch\n",
      "Epoch 3/20  Iteration 5313/35720 Training loss: 1.1781 0.2249 sec/batch\n",
      "Epoch 3/20  Iteration 5314/35720 Training loss: 1.1781 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 5315/35720 Training loss: 1.1781 0.2093 sec/batch\n",
      "Epoch 3/20  Iteration 5316/35720 Training loss: 1.1781 0.2194 sec/batch\n",
      "Epoch 3/20  Iteration 5317/35720 Training loss: 1.1781 0.2269 sec/batch\n",
      "Epoch 3/20  Iteration 5318/35720 Training loss: 1.1782 0.2216 sec/batch\n",
      "Epoch 3/20  Iteration 5319/35720 Training loss: 1.1782 0.2201 sec/batch\n",
      "Epoch 3/20  Iteration 5320/35720 Training loss: 1.1781 0.2090 sec/batch\n",
      "Epoch 3/20  Iteration 5321/35720 Training loss: 1.1781 0.2137 sec/batch\n",
      "Epoch 3/20  Iteration 5322/35720 Training loss: 1.1780 0.2254 sec/batch\n",
      "Epoch 3/20  Iteration 5323/35720 Training loss: 1.1780 0.2148 sec/batch\n",
      "Epoch 3/20  Iteration 5324/35720 Training loss: 1.1779 0.2085 sec/batch\n",
      "Epoch 3/20  Iteration 5325/35720 Training loss: 1.1779 0.2164 sec/batch\n",
      "Epoch 3/20  Iteration 5326/35720 Training loss: 1.1779 0.2147 sec/batch\n",
      "Epoch 3/20  Iteration 5327/35720 Training loss: 1.1778 0.2136 sec/batch\n",
      "Epoch 3/20  Iteration 5328/35720 Training loss: 1.1778 0.2132 sec/batch\n",
      "Epoch 3/20  Iteration 5329/35720 Training loss: 1.1778 0.2298 sec/batch\n",
      "Epoch 3/20  Iteration 5330/35720 Training loss: 1.1777 0.2165 sec/batch\n",
      "Epoch 3/20  Iteration 5331/35720 Training loss: 1.1777 0.2212 sec/batch\n",
      "Epoch 3/20  Iteration 5332/35720 Training loss: 1.1777 0.2139 sec/batch\n",
      "Epoch 3/20  Iteration 5333/35720 Training loss: 1.1776 0.2232 sec/batch\n",
      "Epoch 3/20  Iteration 5334/35720 Training loss: 1.1776 0.2245 sec/batch\n",
      "Epoch 3/20  Iteration 5335/35720 Training loss: 1.1775 0.2283 sec/batch\n",
      "Epoch 3/20  Iteration 5336/35720 Training loss: 1.1775 0.2330 sec/batch\n",
      "Epoch 3/20  Iteration 5337/35720 Training loss: 1.1774 0.2240 sec/batch\n",
      "Epoch 3/20  Iteration 5338/35720 Training loss: 1.1774 0.2195 sec/batch\n",
      "Epoch 3/20  Iteration 5339/35720 Training loss: 1.1774 0.2193 sec/batch\n",
      "Epoch 3/20  Iteration 5340/35720 Training loss: 1.1773 0.2307 sec/batch\n",
      "Epoch 3/20  Iteration 5341/35720 Training loss: 1.1773 0.2268 sec/batch\n",
      "Epoch 3/20  Iteration 5342/35720 Training loss: 1.1773 0.2090 sec/batch\n",
      "Epoch 3/20  Iteration 5343/35720 Training loss: 1.1773 0.2270 sec/batch\n",
      "Epoch 3/20  Iteration 5344/35720 Training loss: 1.1772 0.2271 sec/batch\n",
      "Epoch 3/20  Iteration 5345/35720 Training loss: 1.1771 0.2276 sec/batch\n",
      "Epoch 3/20  Iteration 5346/35720 Training loss: 1.1770 0.2275 sec/batch\n",
      "Epoch 3/20  Iteration 5347/35720 Training loss: 1.1770 0.2188 sec/batch\n",
      "Epoch 3/20  Iteration 5348/35720 Training loss: 1.1770 0.2279 sec/batch\n",
      "Epoch 3/20  Iteration 5349/35720 Training loss: 1.1770 0.2213 sec/batch\n",
      "Epoch 3/20  Iteration 5350/35720 Training loss: 1.1769 0.2197 sec/batch\n",
      "Epoch 3/20  Iteration 5351/35720 Training loss: 1.1768 0.2244 sec/batch\n",
      "Epoch 3/20  Iteration 5352/35720 Training loss: 1.1767 0.2218 sec/batch\n",
      "Epoch 3/20  Iteration 5353/35720 Training loss: 1.1767 0.2071 sec/batch\n",
      "Epoch 3/20  Iteration 5354/35720 Training loss: 1.1766 0.2232 sec/batch\n",
      "Epoch 3/20  Iteration 5355/35720 Training loss: 1.1766 0.2175 sec/batch\n",
      "Epoch 3/20  Iteration 5356/35720 Training loss: 1.1765 0.2157 sec/batch\n",
      "Epoch 3/20  Iteration 5357/35720 Training loss: 1.1764 0.2202 sec/batch\n",
      "Epoch 3/20  Iteration 5358/35720 Training loss: 1.1764 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 5359/35720 Training loss: 1.1400 0.2264 sec/batch\n",
      "Epoch 4/20  Iteration 5360/35720 Training loss: 1.1461 0.2272 sec/batch\n",
      "Epoch 4/20  Iteration 5361/35720 Training loss: 1.1451 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 5362/35720 Training loss: 1.1337 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 5363/35720 Training loss: 1.1439 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 5364/35720 Training loss: 1.1277 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 5365/35720 Training loss: 1.1262 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 5366/35720 Training loss: 1.1109 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 5367/35720 Training loss: 1.1056 0.2180 sec/batch\n",
      "Epoch 4/20  Iteration 5368/35720 Training loss: 1.1144 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 5369/35720 Training loss: 1.1170 0.2295 sec/batch\n",
      "Epoch 4/20  Iteration 5370/35720 Training loss: 1.1148 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 5371/35720 Training loss: 1.1170 0.2255 sec/batch\n",
      "Epoch 4/20  Iteration 5372/35720 Training loss: 1.1261 0.2264 sec/batch\n",
      "Epoch 4/20  Iteration 5373/35720 Training loss: 1.1312 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 5374/35720 Training loss: 1.1329 0.2334 sec/batch\n",
      "Epoch 4/20  Iteration 5375/35720 Training loss: 1.1353 0.2336 sec/batch\n",
      "Epoch 4/20  Iteration 5376/35720 Training loss: 1.1316 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 5377/35720 Training loss: 1.1311 0.2181 sec/batch\n",
      "Epoch 4/20  Iteration 5378/35720 Training loss: 1.1330 0.2172 sec/batch\n",
      "Epoch 4/20  Iteration 5379/35720 Training loss: 1.1363 0.2215 sec/batch\n",
      "Epoch 4/20  Iteration 5380/35720 Training loss: 1.1329 0.2291 sec/batch\n",
      "Epoch 4/20  Iteration 5381/35720 Training loss: 1.1337 0.2135 sec/batch\n",
      "Epoch 4/20  Iteration 5382/35720 Training loss: 1.1352 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 5383/35720 Training loss: 1.1390 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 5384/35720 Training loss: 1.1387 0.2198 sec/batch\n",
      "Epoch 4/20  Iteration 5385/35720 Training loss: 1.1446 0.2191 sec/batch\n",
      "Epoch 4/20  Iteration 5386/35720 Training loss: 1.1454 0.2189 sec/batch\n",
      "Epoch 4/20  Iteration 5387/35720 Training loss: 1.1446 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 5388/35720 Training loss: 1.1437 0.2239 sec/batch\n",
      "Epoch 4/20  Iteration 5389/35720 Training loss: 1.1508 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 5390/35720 Training loss: 1.1479 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 5391/35720 Training loss: 1.1503 0.2212 sec/batch\n",
      "Epoch 4/20  Iteration 5392/35720 Training loss: 1.1531 0.2157 sec/batch\n",
      "Epoch 4/20  Iteration 5393/35720 Training loss: 1.1560 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 5394/35720 Training loss: 1.1557 0.2125 sec/batch\n",
      "Epoch 4/20  Iteration 5395/35720 Training loss: 1.1548 0.2196 sec/batch\n",
      "Epoch 4/20  Iteration 5396/35720 Training loss: 1.1532 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 5397/35720 Training loss: 1.1502 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 5398/35720 Training loss: 1.1503 0.2204 sec/batch\n",
      "Epoch 4/20  Iteration 5399/35720 Training loss: 1.1486 0.2218 sec/batch\n",
      "Epoch 4/20  Iteration 5400/35720 Training loss: 1.1468 0.2111 sec/batch\n",
      "Validation loss: 1.32776 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 5401/35720 Training loss: 1.1479 0.2079 sec/batch\n",
      "Epoch 4/20  Iteration 5402/35720 Training loss: 1.1465 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 5403/35720 Training loss: 1.1457 0.2222 sec/batch\n",
      "Epoch 4/20  Iteration 5404/35720 Training loss: 1.1441 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 5405/35720 Training loss: 1.1427 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 5406/35720 Training loss: 1.1417 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 5407/35720 Training loss: 1.1424 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 5408/35720 Training loss: 1.1410 0.2136 sec/batch\n",
      "Epoch 4/20  Iteration 5409/35720 Training loss: 1.1413 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 5410/35720 Training loss: 1.1406 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 5411/35720 Training loss: 1.1402 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 5412/35720 Training loss: 1.1382 0.2186 sec/batch\n",
      "Epoch 4/20  Iteration 5413/35720 Training loss: 1.1374 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 5414/35720 Training loss: 1.1367 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 5415/35720 Training loss: 1.1371 0.2075 sec/batch\n",
      "Epoch 4/20  Iteration 5416/35720 Training loss: 1.1363 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 5417/35720 Training loss: 1.1342 0.2228 sec/batch\n",
      "Epoch 4/20  Iteration 5418/35720 Training loss: 1.1328 0.2276 sec/batch\n",
      "Epoch 4/20  Iteration 5419/35720 Training loss: 1.1310 0.2333 sec/batch\n",
      "Epoch 4/20  Iteration 5420/35720 Training loss: 1.1286 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 5421/35720 Training loss: 1.1289 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 5422/35720 Training loss: 1.1285 0.2225 sec/batch\n",
      "Epoch 4/20  Iteration 5423/35720 Training loss: 1.1290 0.2210 sec/batch\n",
      "Epoch 4/20  Iteration 5424/35720 Training loss: 1.1292 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 5425/35720 Training loss: 1.1288 0.2254 sec/batch\n",
      "Epoch 4/20  Iteration 5426/35720 Training loss: 1.1280 0.2288 sec/batch\n",
      "Epoch 4/20  Iteration 5427/35720 Training loss: 1.1284 0.2067 sec/batch\n",
      "Epoch 4/20  Iteration 5428/35720 Training loss: 1.1279 0.2251 sec/batch\n",
      "Epoch 4/20  Iteration 5429/35720 Training loss: 1.1282 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 5430/35720 Training loss: 1.1286 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 5431/35720 Training loss: 1.1286 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 5432/35720 Training loss: 1.1284 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 5433/35720 Training loss: 1.1266 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 5434/35720 Training loss: 1.1263 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 5435/35720 Training loss: 1.1251 0.2118 sec/batch\n",
      "Epoch 4/20  Iteration 5436/35720 Training loss: 1.1262 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 5437/35720 Training loss: 1.1259 0.2196 sec/batch\n",
      "Epoch 4/20  Iteration 5438/35720 Training loss: 1.1286 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 5439/35720 Training loss: 1.1289 0.2218 sec/batch\n",
      "Epoch 4/20  Iteration 5440/35720 Training loss: 1.1286 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 5441/35720 Training loss: 1.1284 0.2249 sec/batch\n",
      "Epoch 4/20  Iteration 5442/35720 Training loss: 1.1288 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 5443/35720 Training loss: 1.1286 0.2182 sec/batch\n",
      "Epoch 4/20  Iteration 5444/35720 Training loss: 1.1288 0.2290 sec/batch\n",
      "Epoch 4/20  Iteration 5445/35720 Training loss: 1.1289 0.2064 sec/batch\n",
      "Epoch 4/20  Iteration 5446/35720 Training loss: 1.1285 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 5447/35720 Training loss: 1.1275 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 5448/35720 Training loss: 1.1263 0.2157 sec/batch\n",
      "Epoch 4/20  Iteration 5449/35720 Training loss: 1.1261 0.2068 sec/batch\n",
      "Epoch 4/20  Iteration 5450/35720 Training loss: 1.1251 0.2133 sec/batch\n",
      "Epoch 4/20  Iteration 5451/35720 Training loss: 1.1244 0.2290 sec/batch\n",
      "Epoch 4/20  Iteration 5452/35720 Training loss: 1.1239 0.2334 sec/batch\n",
      "Epoch 4/20  Iteration 5453/35720 Training loss: 1.1233 0.2229 sec/batch\n",
      "Epoch 4/20  Iteration 5454/35720 Training loss: 1.1224 0.2376 sec/batch\n",
      "Epoch 4/20  Iteration 5455/35720 Training loss: 1.1227 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 5456/35720 Training loss: 1.1223 0.2308 sec/batch\n",
      "Epoch 4/20  Iteration 5457/35720 Training loss: 1.1220 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 5458/35720 Training loss: 1.1216 0.2222 sec/batch\n",
      "Epoch 4/20  Iteration 5459/35720 Training loss: 1.1214 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 5460/35720 Training loss: 1.1219 0.2266 sec/batch\n",
      "Epoch 4/20  Iteration 5461/35720 Training loss: 1.1220 0.2330 sec/batch\n",
      "Epoch 4/20  Iteration 5462/35720 Training loss: 1.1219 0.2206 sec/batch\n",
      "Epoch 4/20  Iteration 5463/35720 Training loss: 1.1220 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 5464/35720 Training loss: 1.1216 0.2119 sec/batch\n",
      "Epoch 4/20  Iteration 5465/35720 Training loss: 1.1220 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 5466/35720 Training loss: 1.1221 0.2286 sec/batch\n",
      "Epoch 4/20  Iteration 5467/35720 Training loss: 1.1228 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 5468/35720 Training loss: 1.1226 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 5469/35720 Training loss: 1.1226 0.2209 sec/batch\n",
      "Epoch 4/20  Iteration 5470/35720 Training loss: 1.1238 0.2218 sec/batch\n",
      "Epoch 4/20  Iteration 5471/35720 Training loss: 1.1239 0.2228 sec/batch\n",
      "Epoch 4/20  Iteration 5472/35720 Training loss: 1.1249 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 5473/35720 Training loss: 1.1245 0.2202 sec/batch\n",
      "Epoch 4/20  Iteration 5474/35720 Training loss: 1.1247 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 5475/35720 Training loss: 1.1245 0.2193 sec/batch\n",
      "Epoch 4/20  Iteration 5476/35720 Training loss: 1.1254 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 5477/35720 Training loss: 1.1258 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 5478/35720 Training loss: 1.1267 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 5479/35720 Training loss: 1.1276 0.2183 sec/batch\n",
      "Epoch 4/20  Iteration 5480/35720 Training loss: 1.1273 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 5481/35720 Training loss: 1.1280 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 5482/35720 Training loss: 1.1286 0.2173 sec/batch\n",
      "Epoch 4/20  Iteration 5483/35720 Training loss: 1.1279 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 5484/35720 Training loss: 1.1280 0.2078 sec/batch\n",
      "Epoch 4/20  Iteration 5485/35720 Training loss: 1.1279 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 5486/35720 Training loss: 1.1280 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 5487/35720 Training loss: 1.1276 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 5488/35720 Training loss: 1.1272 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 5489/35720 Training loss: 1.1267 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 5490/35720 Training loss: 1.1265 0.2248 sec/batch\n",
      "Epoch 4/20  Iteration 5491/35720 Training loss: 1.1263 0.2213 sec/batch\n",
      "Epoch 4/20  Iteration 5492/35720 Training loss: 1.1263 0.2208 sec/batch\n",
      "Epoch 4/20  Iteration 5493/35720 Training loss: 1.1262 0.2223 sec/batch\n",
      "Epoch 4/20  Iteration 5494/35720 Training loss: 1.1260 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 5495/35720 Training loss: 1.1272 0.2301 sec/batch\n",
      "Epoch 4/20  Iteration 5496/35720 Training loss: 1.1275 0.2248 sec/batch\n",
      "Epoch 4/20  Iteration 5497/35720 Training loss: 1.1277 0.2069 sec/batch\n",
      "Epoch 4/20  Iteration 5498/35720 Training loss: 1.1278 0.2213 sec/batch\n",
      "Epoch 4/20  Iteration 5499/35720 Training loss: 1.1272 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 5500/35720 Training loss: 1.1264 0.2251 sec/batch\n",
      "Epoch 4/20  Iteration 5501/35720 Training loss: 1.1257 0.2233 sec/batch\n",
      "Epoch 4/20  Iteration 5502/35720 Training loss: 1.1250 0.2238 sec/batch\n",
      "Epoch 4/20  Iteration 5503/35720 Training loss: 1.1247 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 5504/35720 Training loss: 1.1249 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 5505/35720 Training loss: 1.1244 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 5506/35720 Training loss: 1.1244 0.2245 sec/batch\n",
      "Epoch 4/20  Iteration 5507/35720 Training loss: 1.1241 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 5508/35720 Training loss: 1.1232 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 5509/35720 Training loss: 1.1230 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 5510/35720 Training loss: 1.1229 0.2226 sec/batch\n",
      "Epoch 4/20  Iteration 5511/35720 Training loss: 1.1230 0.2228 sec/batch\n",
      "Epoch 4/20  Iteration 5512/35720 Training loss: 1.1236 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 5513/35720 Training loss: 1.1240 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 5514/35720 Training loss: 1.1242 0.2200 sec/batch\n",
      "Epoch 4/20  Iteration 5515/35720 Training loss: 1.1245 0.2251 sec/batch\n",
      "Epoch 4/20  Iteration 5516/35720 Training loss: 1.1248 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 5517/35720 Training loss: 1.1246 0.2304 sec/batch\n",
      "Epoch 4/20  Iteration 5518/35720 Training loss: 1.1251 0.2068 sec/batch\n",
      "Epoch 4/20  Iteration 5519/35720 Training loss: 1.1246 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 5520/35720 Training loss: 1.1246 0.2193 sec/batch\n",
      "Epoch 4/20  Iteration 5521/35720 Training loss: 1.1247 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 5522/35720 Training loss: 1.1250 0.2223 sec/batch\n",
      "Epoch 4/20  Iteration 5523/35720 Training loss: 1.1253 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 5524/35720 Training loss: 1.1258 0.2221 sec/batch\n",
      "Epoch 4/20  Iteration 5525/35720 Training loss: 1.1259 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 5526/35720 Training loss: 1.1263 0.2189 sec/batch\n",
      "Epoch 4/20  Iteration 5527/35720 Training loss: 1.1265 0.2170 sec/batch\n",
      "Epoch 4/20  Iteration 5528/35720 Training loss: 1.1268 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 5529/35720 Training loss: 1.1281 0.2212 sec/batch\n",
      "Epoch 4/20  Iteration 5530/35720 Training loss: 1.1286 0.2289 sec/batch\n",
      "Epoch 4/20  Iteration 5531/35720 Training loss: 1.1288 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 5532/35720 Training loss: 1.1294 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 5533/35720 Training loss: 1.1296 0.2136 sec/batch\n",
      "Epoch 4/20  Iteration 5534/35720 Training loss: 1.1293 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 5535/35720 Training loss: 1.1295 0.2244 sec/batch\n",
      "Epoch 4/20  Iteration 5536/35720 Training loss: 1.1293 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 5537/35720 Training loss: 1.1291 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 5538/35720 Training loss: 1.1289 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 5539/35720 Training loss: 1.1292 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 5540/35720 Training loss: 1.1293 0.2235 sec/batch\n",
      "Epoch 4/20  Iteration 5541/35720 Training loss: 1.1294 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 5542/35720 Training loss: 1.1296 0.2140 sec/batch\n",
      "Epoch 4/20  Iteration 5543/35720 Training loss: 1.1295 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 5544/35720 Training loss: 1.1293 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 5545/35720 Training loss: 1.1290 0.2279 sec/batch\n",
      "Epoch 4/20  Iteration 5546/35720 Training loss: 1.1291 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 5547/35720 Training loss: 1.1296 0.2231 sec/batch\n",
      "Epoch 4/20  Iteration 5548/35720 Training loss: 1.1297 0.2178 sec/batch\n",
      "Epoch 4/20  Iteration 5549/35720 Training loss: 1.1299 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 5550/35720 Training loss: 1.1308 0.2077 sec/batch\n",
      "Epoch 4/20  Iteration 5551/35720 Training loss: 1.1311 0.2150 sec/batch\n",
      "Epoch 4/20  Iteration 5552/35720 Training loss: 1.1313 0.2198 sec/batch\n",
      "Epoch 4/20  Iteration 5553/35720 Training loss: 1.1312 0.2180 sec/batch\n",
      "Epoch 4/20  Iteration 5554/35720 Training loss: 1.1315 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 5555/35720 Training loss: 1.1313 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 5556/35720 Training loss: 1.1313 0.2244 sec/batch\n",
      "Epoch 4/20  Iteration 5557/35720 Training loss: 1.1315 0.2210 sec/batch\n",
      "Epoch 4/20  Iteration 5558/35720 Training loss: 1.1318 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 5559/35720 Training loss: 1.1314 0.2266 sec/batch\n",
      "Epoch 4/20  Iteration 5560/35720 Training loss: 1.1311 0.2188 sec/batch\n",
      "Epoch 4/20  Iteration 5561/35720 Training loss: 1.1311 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 5562/35720 Training loss: 1.1310 0.2216 sec/batch\n",
      "Epoch 4/20  Iteration 5563/35720 Training loss: 1.1309 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 5564/35720 Training loss: 1.1308 0.2248 sec/batch\n",
      "Epoch 4/20  Iteration 5565/35720 Training loss: 1.1314 0.2289 sec/batch\n",
      "Epoch 4/20  Iteration 5566/35720 Training loss: 1.1317 0.2191 sec/batch\n",
      "Epoch 4/20  Iteration 5567/35720 Training loss: 1.1320 0.2125 sec/batch\n",
      "Epoch 4/20  Iteration 5568/35720 Training loss: 1.1320 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 5569/35720 Training loss: 1.1322 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 5570/35720 Training loss: 1.1320 0.2125 sec/batch\n",
      "Epoch 4/20  Iteration 5571/35720 Training loss: 1.1321 0.2150 sec/batch\n",
      "Epoch 4/20  Iteration 5572/35720 Training loss: 1.1321 0.2255 sec/batch\n",
      "Epoch 4/20  Iteration 5573/35720 Training loss: 1.1320 0.2320 sec/batch\n",
      "Epoch 4/20  Iteration 5574/35720 Training loss: 1.1319 0.2308 sec/batch\n",
      "Epoch 4/20  Iteration 5575/35720 Training loss: 1.1315 0.2139 sec/batch\n",
      "Epoch 4/20  Iteration 5576/35720 Training loss: 1.1314 0.2232 sec/batch\n",
      "Epoch 4/20  Iteration 5577/35720 Training loss: 1.1314 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 5578/35720 Training loss: 1.1314 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 5579/35720 Training loss: 1.1315 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 5580/35720 Training loss: 1.1316 0.2202 sec/batch\n",
      "Epoch 4/20  Iteration 5581/35720 Training loss: 1.1321 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 5582/35720 Training loss: 1.1323 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 5583/35720 Training loss: 1.1324 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 5584/35720 Training loss: 1.1323 0.2249 sec/batch\n",
      "Epoch 4/20  Iteration 5585/35720 Training loss: 1.1322 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 5586/35720 Training loss: 1.1317 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 5587/35720 Training loss: 1.1315 0.2167 sec/batch\n",
      "Epoch 4/20  Iteration 5588/35720 Training loss: 1.1318 0.2069 sec/batch\n",
      "Epoch 4/20  Iteration 5589/35720 Training loss: 1.1320 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 5590/35720 Training loss: 1.1319 0.2246 sec/batch\n",
      "Epoch 4/20  Iteration 5591/35720 Training loss: 1.1320 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 5592/35720 Training loss: 1.1321 0.2174 sec/batch\n",
      "Epoch 4/20  Iteration 5593/35720 Training loss: 1.1320 0.2249 sec/batch\n",
      "Epoch 4/20  Iteration 5594/35720 Training loss: 1.1318 0.2266 sec/batch\n",
      "Epoch 4/20  Iteration 5595/35720 Training loss: 1.1321 0.2189 sec/batch\n",
      "Epoch 4/20  Iteration 5596/35720 Training loss: 1.1319 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 5597/35720 Training loss: 1.1316 0.2061 sec/batch\n",
      "Epoch 4/20  Iteration 5598/35720 Training loss: 1.1317 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 5599/35720 Training loss: 1.1314 0.2208 sec/batch\n",
      "Epoch 4/20  Iteration 5600/35720 Training loss: 1.1313 0.2267 sec/batch\n",
      "Validation loss: 1.32809 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 5601/35720 Training loss: 1.1320 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 5602/35720 Training loss: 1.1321 0.2117 sec/batch\n",
      "Epoch 4/20  Iteration 5603/35720 Training loss: 1.1319 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 5604/35720 Training loss: 1.1319 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 5605/35720 Training loss: 1.1320 0.2275 sec/batch\n",
      "Epoch 4/20  Iteration 5606/35720 Training loss: 1.1318 0.2302 sec/batch\n",
      "Epoch 4/20  Iteration 5607/35720 Training loss: 1.1314 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 5608/35720 Training loss: 1.1312 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 5609/35720 Training loss: 1.1313 0.2216 sec/batch\n",
      "Epoch 4/20  Iteration 5610/35720 Training loss: 1.1310 0.2111 sec/batch\n",
      "Epoch 4/20  Iteration 5611/35720 Training loss: 1.1308 0.2149 sec/batch\n",
      "Epoch 4/20  Iteration 5612/35720 Training loss: 1.1311 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 5613/35720 Training loss: 1.1315 0.2191 sec/batch\n",
      "Epoch 4/20  Iteration 5614/35720 Training loss: 1.1316 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 5615/35720 Training loss: 1.1316 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 5616/35720 Training loss: 1.1317 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 5617/35720 Training loss: 1.1321 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 5618/35720 Training loss: 1.1319 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 5619/35720 Training loss: 1.1318 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 5620/35720 Training loss: 1.1316 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 5621/35720 Training loss: 1.1315 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 5622/35720 Training loss: 1.1316 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 5623/35720 Training loss: 1.1317 0.2213 sec/batch\n",
      "Epoch 4/20  Iteration 5624/35720 Training loss: 1.1318 0.2072 sec/batch\n",
      "Epoch 4/20  Iteration 5625/35720 Training loss: 1.1316 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 5626/35720 Training loss: 1.1315 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 5627/35720 Training loss: 1.1310 0.2233 sec/batch\n",
      "Epoch 4/20  Iteration 5628/35720 Training loss: 1.1303 0.2062 sec/batch\n",
      "Epoch 4/20  Iteration 5629/35720 Training loss: 1.1298 0.2120 sec/batch\n",
      "Epoch 4/20  Iteration 5630/35720 Training loss: 1.1297 0.2259 sec/batch\n",
      "Epoch 4/20  Iteration 5631/35720 Training loss: 1.1297 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 5632/35720 Training loss: 1.1296 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 5633/35720 Training loss: 1.1292 0.2208 sec/batch\n",
      "Epoch 4/20  Iteration 5634/35720 Training loss: 1.1291 0.2289 sec/batch\n",
      "Epoch 4/20  Iteration 5635/35720 Training loss: 1.1288 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 5636/35720 Training loss: 1.1286 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 5637/35720 Training loss: 1.1283 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 5638/35720 Training loss: 1.1283 0.2246 sec/batch\n",
      "Epoch 4/20  Iteration 5639/35720 Training loss: 1.1283 0.2306 sec/batch\n",
      "Epoch 4/20  Iteration 5640/35720 Training loss: 1.1279 0.2060 sec/batch\n",
      "Epoch 4/20  Iteration 5641/35720 Training loss: 1.1274 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 5642/35720 Training loss: 1.1271 0.2107 sec/batch\n",
      "Epoch 4/20  Iteration 5643/35720 Training loss: 1.1273 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 5644/35720 Training loss: 1.1272 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 5645/35720 Training loss: 1.1268 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 5646/35720 Training loss: 1.1267 0.2287 sec/batch\n",
      "Epoch 4/20  Iteration 5647/35720 Training loss: 1.1267 0.2232 sec/batch\n",
      "Epoch 4/20  Iteration 5648/35720 Training loss: 1.1268 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 5649/35720 Training loss: 1.1268 0.2207 sec/batch\n",
      "Epoch 4/20  Iteration 5650/35720 Training loss: 1.1268 0.2204 sec/batch\n",
      "Epoch 4/20  Iteration 5651/35720 Training loss: 1.1267 0.2181 sec/batch\n",
      "Epoch 4/20  Iteration 5652/35720 Training loss: 1.1267 0.2111 sec/batch\n",
      "Epoch 4/20  Iteration 5653/35720 Training loss: 1.1275 0.2140 sec/batch\n",
      "Epoch 4/20  Iteration 5654/35720 Training loss: 1.1275 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 5655/35720 Training loss: 1.1275 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 5656/35720 Training loss: 1.1277 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 5657/35720 Training loss: 1.1276 0.2136 sec/batch\n",
      "Epoch 4/20  Iteration 5658/35720 Training loss: 1.1276 0.2076 sec/batch\n",
      "Epoch 4/20  Iteration 5659/35720 Training loss: 1.1276 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 5660/35720 Training loss: 1.1273 0.2192 sec/batch\n",
      "Epoch 4/20  Iteration 5661/35720 Training loss: 1.1272 0.2153 sec/batch\n",
      "Epoch 4/20  Iteration 5662/35720 Training loss: 1.1271 0.2157 sec/batch\n",
      "Epoch 4/20  Iteration 5663/35720 Training loss: 1.1270 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 5664/35720 Training loss: 1.1269 0.2217 sec/batch\n",
      "Epoch 4/20  Iteration 5665/35720 Training loss: 1.1271 0.2240 sec/batch\n",
      "Epoch 4/20  Iteration 5666/35720 Training loss: 1.1272 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 5667/35720 Training loss: 1.1269 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 5668/35720 Training loss: 1.1266 0.2185 sec/batch\n",
      "Epoch 4/20  Iteration 5669/35720 Training loss: 1.1265 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 5670/35720 Training loss: 1.1264 0.2209 sec/batch\n",
      "Epoch 4/20  Iteration 5671/35720 Training loss: 1.1262 0.2206 sec/batch\n",
      "Epoch 4/20  Iteration 5672/35720 Training loss: 1.1259 0.2205 sec/batch\n",
      "Epoch 4/20  Iteration 5673/35720 Training loss: 1.1259 0.2199 sec/batch\n",
      "Epoch 4/20  Iteration 5674/35720 Training loss: 1.1259 0.2231 sec/batch\n",
      "Epoch 4/20  Iteration 5675/35720 Training loss: 1.1256 0.2113 sec/batch\n",
      "Epoch 4/20  Iteration 5676/35720 Training loss: 1.1257 0.2268 sec/batch\n",
      "Epoch 4/20  Iteration 5677/35720 Training loss: 1.1257 0.2231 sec/batch\n",
      "Epoch 4/20  Iteration 5678/35720 Training loss: 1.1255 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 5679/35720 Training loss: 1.1256 0.2216 sec/batch\n",
      "Epoch 4/20  Iteration 5680/35720 Training loss: 1.1254 0.2120 sec/batch\n",
      "Epoch 4/20  Iteration 5681/35720 Training loss: 1.1256 0.2194 sec/batch\n",
      "Epoch 4/20  Iteration 5682/35720 Training loss: 1.1256 0.2077 sec/batch\n",
      "Epoch 4/20  Iteration 5683/35720 Training loss: 1.1254 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 5684/35720 Training loss: 1.1257 0.2119 sec/batch\n",
      "Epoch 4/20  Iteration 5685/35720 Training loss: 1.1257 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 5686/35720 Training loss: 1.1258 0.2231 sec/batch\n",
      "Epoch 4/20  Iteration 5687/35720 Training loss: 1.1258 0.2211 sec/batch\n",
      "Epoch 4/20  Iteration 5688/35720 Training loss: 1.1258 0.2218 sec/batch\n",
      "Epoch 4/20  Iteration 5689/35720 Training loss: 1.1259 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 5690/35720 Training loss: 1.1260 0.2180 sec/batch\n",
      "Epoch 4/20  Iteration 5691/35720 Training loss: 1.1259 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 5692/35720 Training loss: 1.1259 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 5693/35720 Training loss: 1.1257 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 5694/35720 Training loss: 1.1256 0.2136 sec/batch\n",
      "Epoch 4/20  Iteration 5695/35720 Training loss: 1.1255 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 5696/35720 Training loss: 1.1253 0.2174 sec/batch\n",
      "Epoch 4/20  Iteration 5697/35720 Training loss: 1.1254 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 5698/35720 Training loss: 1.1255 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 5699/35720 Training loss: 1.1253 0.2351 sec/batch\n",
      "Epoch 4/20  Iteration 5700/35720 Training loss: 1.1253 0.2308 sec/batch\n",
      "Epoch 4/20  Iteration 5701/35720 Training loss: 1.1253 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 5702/35720 Training loss: 1.1253 0.2173 sec/batch\n",
      "Epoch 4/20  Iteration 5703/35720 Training loss: 1.1250 0.2268 sec/batch\n",
      "Epoch 4/20  Iteration 5704/35720 Training loss: 1.1253 0.2218 sec/batch\n",
      "Epoch 4/20  Iteration 5705/35720 Training loss: 1.1253 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 5706/35720 Training loss: 1.1254 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 5707/35720 Training loss: 1.1254 0.2189 sec/batch\n",
      "Epoch 4/20  Iteration 5708/35720 Training loss: 1.1253 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 5709/35720 Training loss: 1.1254 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 5710/35720 Training loss: 1.1253 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 5711/35720 Training loss: 1.1254 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 5712/35720 Training loss: 1.1252 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 5713/35720 Training loss: 1.1254 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 5714/35720 Training loss: 1.1254 0.2221 sec/batch\n",
      "Epoch 4/20  Iteration 5715/35720 Training loss: 1.1255 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 5716/35720 Training loss: 1.1254 0.2267 sec/batch\n",
      "Epoch 4/20  Iteration 5717/35720 Training loss: 1.1252 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 5718/35720 Training loss: 1.1251 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 5719/35720 Training loss: 1.1250 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 5720/35720 Training loss: 1.1252 0.2213 sec/batch\n",
      "Epoch 4/20  Iteration 5721/35720 Training loss: 1.1250 0.2263 sec/batch\n",
      "Epoch 4/20  Iteration 5722/35720 Training loss: 1.1250 0.2075 sec/batch\n",
      "Epoch 4/20  Iteration 5723/35720 Training loss: 1.1248 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 5724/35720 Training loss: 1.1247 0.2265 sec/batch\n",
      "Epoch 4/20  Iteration 5725/35720 Training loss: 1.1246 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 5726/35720 Training loss: 1.1243 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 5727/35720 Training loss: 1.1241 0.2139 sec/batch\n",
      "Epoch 4/20  Iteration 5728/35720 Training loss: 1.1241 0.2212 sec/batch\n",
      "Epoch 4/20  Iteration 5729/35720 Training loss: 1.1238 0.2109 sec/batch\n",
      "Epoch 4/20  Iteration 5730/35720 Training loss: 1.1237 0.2173 sec/batch\n",
      "Epoch 4/20  Iteration 5731/35720 Training loss: 1.1236 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 5732/35720 Training loss: 1.1235 0.2209 sec/batch\n",
      "Epoch 4/20  Iteration 5733/35720 Training loss: 1.1234 0.2112 sec/batch\n",
      "Epoch 4/20  Iteration 5734/35720 Training loss: 1.1235 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 5735/35720 Training loss: 1.1235 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 5736/35720 Training loss: 1.1233 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 5737/35720 Training loss: 1.1231 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 5738/35720 Training loss: 1.1228 0.2188 sec/batch\n",
      "Epoch 4/20  Iteration 5739/35720 Training loss: 1.1226 0.2062 sec/batch\n",
      "Epoch 4/20  Iteration 5740/35720 Training loss: 1.1226 0.2341 sec/batch\n",
      "Epoch 4/20  Iteration 5741/35720 Training loss: 1.1225 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 5742/35720 Training loss: 1.1224 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 5743/35720 Training loss: 1.1223 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 5744/35720 Training loss: 1.1222 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 5745/35720 Training loss: 1.1223 0.2197 sec/batch\n",
      "Epoch 4/20  Iteration 5746/35720 Training loss: 1.1223 0.2129 sec/batch\n",
      "Epoch 4/20  Iteration 5747/35720 Training loss: 1.1225 0.2153 sec/batch\n",
      "Epoch 4/20  Iteration 5748/35720 Training loss: 1.1226 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 5749/35720 Training loss: 1.1226 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 5750/35720 Training loss: 1.1224 0.2217 sec/batch\n",
      "Epoch 4/20  Iteration 5751/35720 Training loss: 1.1225 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 5752/35720 Training loss: 1.1222 0.2291 sec/batch\n",
      "Epoch 4/20  Iteration 5753/35720 Training loss: 1.1222 0.2276 sec/batch\n",
      "Epoch 4/20  Iteration 5754/35720 Training loss: 1.1221 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 5755/35720 Training loss: 1.1221 0.2058 sec/batch\n",
      "Epoch 4/20  Iteration 5756/35720 Training loss: 1.1218 0.2164 sec/batch\n",
      "Epoch 4/20  Iteration 5757/35720 Training loss: 1.1216 0.2273 sec/batch\n",
      "Epoch 4/20  Iteration 5758/35720 Training loss: 1.1215 0.2280 sec/batch\n",
      "Epoch 4/20  Iteration 5759/35720 Training loss: 1.1214 0.2264 sec/batch\n",
      "Epoch 4/20  Iteration 5760/35720 Training loss: 1.1216 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 5761/35720 Training loss: 1.1218 0.2113 sec/batch\n",
      "Epoch 4/20  Iteration 5762/35720 Training loss: 1.1217 0.2135 sec/batch\n",
      "Epoch 4/20  Iteration 5763/35720 Training loss: 1.1216 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 5764/35720 Training loss: 1.1216 0.2144 sec/batch\n",
      "Epoch 4/20  Iteration 5765/35720 Training loss: 1.1214 0.2265 sec/batch\n",
      "Epoch 4/20  Iteration 5766/35720 Training loss: 1.1214 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 5767/35720 Training loss: 1.1212 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 5768/35720 Training loss: 1.1211 0.2218 sec/batch\n",
      "Epoch 4/20  Iteration 5769/35720 Training loss: 1.1212 0.2267 sec/batch\n",
      "Epoch 4/20  Iteration 5770/35720 Training loss: 1.1209 0.2071 sec/batch\n",
      "Epoch 4/20  Iteration 5771/35720 Training loss: 1.1206 0.2198 sec/batch\n",
      "Epoch 4/20  Iteration 5772/35720 Training loss: 1.1206 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 5773/35720 Training loss: 1.1205 0.2146 sec/batch\n",
      "Epoch 4/20  Iteration 5774/35720 Training loss: 1.1204 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 5775/35720 Training loss: 1.1203 0.2222 sec/batch\n",
      "Epoch 4/20  Iteration 5776/35720 Training loss: 1.1201 0.2217 sec/batch\n",
      "Epoch 4/20  Iteration 5777/35720 Training loss: 1.1199 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 5778/35720 Training loss: 1.1199 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 5779/35720 Training loss: 1.1200 0.2239 sec/batch\n",
      "Epoch 4/20  Iteration 5780/35720 Training loss: 1.1200 0.2064 sec/batch\n",
      "Epoch 4/20  Iteration 5781/35720 Training loss: 1.1201 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 5782/35720 Training loss: 1.1202 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 5783/35720 Training loss: 1.1202 0.2210 sec/batch\n",
      "Epoch 4/20  Iteration 5784/35720 Training loss: 1.1201 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 5785/35720 Training loss: 1.1199 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 5786/35720 Training loss: 1.1200 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 5787/35720 Training loss: 1.1201 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 5788/35720 Training loss: 1.1203 0.2149 sec/batch\n",
      "Epoch 4/20  Iteration 5789/35720 Training loss: 1.1205 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 5790/35720 Training loss: 1.1205 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 5791/35720 Training loss: 1.1208 0.2219 sec/batch\n",
      "Epoch 4/20  Iteration 5792/35720 Training loss: 1.1210 0.2205 sec/batch\n",
      "Epoch 4/20  Iteration 5793/35720 Training loss: 1.1211 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 5794/35720 Training loss: 1.1210 0.2256 sec/batch\n",
      "Epoch 4/20  Iteration 5795/35720 Training loss: 1.1211 0.2174 sec/batch\n",
      "Epoch 4/20  Iteration 5796/35720 Training loss: 1.1213 0.2047 sec/batch\n",
      "Epoch 4/20  Iteration 5797/35720 Training loss: 1.1214 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 5798/35720 Training loss: 1.1214 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 5799/35720 Training loss: 1.1218 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 5800/35720 Training loss: 1.1220 0.2171 sec/batch\n",
      "Validation loss: 1.31436 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 5801/35720 Training loss: 1.1223 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 5802/35720 Training loss: 1.1223 0.2150 sec/batch\n",
      "Epoch 4/20  Iteration 5803/35720 Training loss: 1.1223 0.2200 sec/batch\n",
      "Epoch 4/20  Iteration 5804/35720 Training loss: 1.1224 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 5805/35720 Training loss: 1.1226 0.2288 sec/batch\n",
      "Epoch 4/20  Iteration 5806/35720 Training loss: 1.1228 0.2149 sec/batch\n",
      "Epoch 4/20  Iteration 5807/35720 Training loss: 1.1229 0.2410 sec/batch\n",
      "Epoch 4/20  Iteration 5808/35720 Training loss: 1.1228 0.2279 sec/batch\n",
      "Epoch 4/20  Iteration 5809/35720 Training loss: 1.1226 0.2153 sec/batch\n",
      "Epoch 4/20  Iteration 5810/35720 Training loss: 1.1226 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 5811/35720 Training loss: 1.1225 0.2153 sec/batch\n",
      "Epoch 4/20  Iteration 5812/35720 Training loss: 1.1227 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 5813/35720 Training loss: 1.1229 0.2078 sec/batch\n",
      "Epoch 4/20  Iteration 5814/35720 Training loss: 1.1230 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 5815/35720 Training loss: 1.1232 0.2146 sec/batch\n",
      "Epoch 4/20  Iteration 5816/35720 Training loss: 1.1234 0.2062 sec/batch\n",
      "Epoch 4/20  Iteration 5817/35720 Training loss: 1.1232 0.2208 sec/batch\n",
      "Epoch 4/20  Iteration 5818/35720 Training loss: 1.1230 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 5819/35720 Training loss: 1.1230 0.2340 sec/batch\n",
      "Epoch 4/20  Iteration 5820/35720 Training loss: 1.1230 0.2278 sec/batch\n",
      "Epoch 4/20  Iteration 5821/35720 Training loss: 1.1232 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 5822/35720 Training loss: 1.1231 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 5823/35720 Training loss: 1.1230 0.2236 sec/batch\n",
      "Epoch 4/20  Iteration 5824/35720 Training loss: 1.1229 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 5825/35720 Training loss: 1.1228 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 5826/35720 Training loss: 1.1227 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 5827/35720 Training loss: 1.1225 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 5828/35720 Training loss: 1.1223 0.2196 sec/batch\n",
      "Epoch 4/20  Iteration 5829/35720 Training loss: 1.1221 0.2241 sec/batch\n",
      "Epoch 4/20  Iteration 5830/35720 Training loss: 1.1220 0.2063 sec/batch\n",
      "Epoch 4/20  Iteration 5831/35720 Training loss: 1.1220 0.2073 sec/batch\n",
      "Epoch 4/20  Iteration 5832/35720 Training loss: 1.1218 0.2120 sec/batch\n",
      "Epoch 4/20  Iteration 5833/35720 Training loss: 1.1218 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 5834/35720 Training loss: 1.1217 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 5835/35720 Training loss: 1.1218 0.2064 sec/batch\n",
      "Epoch 4/20  Iteration 5836/35720 Training loss: 1.1217 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 5837/35720 Training loss: 1.1217 0.2210 sec/batch\n",
      "Epoch 4/20  Iteration 5838/35720 Training loss: 1.1216 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 5839/35720 Training loss: 1.1214 0.2205 sec/batch\n",
      "Epoch 4/20  Iteration 5840/35720 Training loss: 1.1212 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 5841/35720 Training loss: 1.1213 0.2216 sec/batch\n",
      "Epoch 4/20  Iteration 5842/35720 Training loss: 1.1212 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 5843/35720 Training loss: 1.1211 0.2114 sec/batch\n",
      "Epoch 4/20  Iteration 5844/35720 Training loss: 1.1210 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 5845/35720 Training loss: 1.1208 0.2205 sec/batch\n",
      "Epoch 4/20  Iteration 5846/35720 Training loss: 1.1208 0.2282 sec/batch\n",
      "Epoch 4/20  Iteration 5847/35720 Training loss: 1.1207 0.2255 sec/batch\n",
      "Epoch 4/20  Iteration 5848/35720 Training loss: 1.1205 0.2065 sec/batch\n",
      "Epoch 4/20  Iteration 5849/35720 Training loss: 1.1204 0.2174 sec/batch\n",
      "Epoch 4/20  Iteration 5850/35720 Training loss: 1.1204 0.2331 sec/batch\n",
      "Epoch 4/20  Iteration 5851/35720 Training loss: 1.1203 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 5852/35720 Training loss: 1.1202 0.2270 sec/batch\n",
      "Epoch 4/20  Iteration 5853/35720 Training loss: 1.1201 0.2289 sec/batch\n",
      "Epoch 4/20  Iteration 5854/35720 Training loss: 1.1198 0.2495 sec/batch\n",
      "Epoch 4/20  Iteration 5855/35720 Training loss: 1.1199 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 5856/35720 Training loss: 1.1200 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 5857/35720 Training loss: 1.1199 0.2182 sec/batch\n",
      "Epoch 4/20  Iteration 5858/35720 Training loss: 1.1199 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 5859/35720 Training loss: 1.1197 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 5860/35720 Training loss: 1.1194 0.2223 sec/batch\n",
      "Epoch 4/20  Iteration 5861/35720 Training loss: 1.1191 0.2361 sec/batch\n",
      "Epoch 4/20  Iteration 5862/35720 Training loss: 1.1191 0.2231 sec/batch\n",
      "Epoch 4/20  Iteration 5863/35720 Training loss: 1.1191 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 5864/35720 Training loss: 1.1190 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 5865/35720 Training loss: 1.1190 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 5866/35720 Training loss: 1.1189 0.2071 sec/batch\n",
      "Epoch 4/20  Iteration 5867/35720 Training loss: 1.1189 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 5868/35720 Training loss: 1.1187 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 5869/35720 Training loss: 1.1185 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 5870/35720 Training loss: 1.1183 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 5871/35720 Training loss: 1.1183 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 5872/35720 Training loss: 1.1182 0.2170 sec/batch\n",
      "Epoch 4/20  Iteration 5873/35720 Training loss: 1.1183 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 5874/35720 Training loss: 1.1182 0.2189 sec/batch\n",
      "Epoch 4/20  Iteration 5875/35720 Training loss: 1.1181 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 5876/35720 Training loss: 1.1182 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 5877/35720 Training loss: 1.1181 0.2146 sec/batch\n",
      "Epoch 4/20  Iteration 5878/35720 Training loss: 1.1181 0.2164 sec/batch\n",
      "Epoch 4/20  Iteration 5879/35720 Training loss: 1.1180 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 5880/35720 Training loss: 1.1180 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 5881/35720 Training loss: 1.1181 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 5882/35720 Training loss: 1.1179 0.2069 sec/batch\n",
      "Epoch 4/20  Iteration 5883/35720 Training loss: 1.1179 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 5884/35720 Training loss: 1.1179 0.2229 sec/batch\n",
      "Epoch 4/20  Iteration 5885/35720 Training loss: 1.1178 0.2146 sec/batch\n",
      "Epoch 4/20  Iteration 5886/35720 Training loss: 1.1176 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 5887/35720 Training loss: 1.1176 0.2211 sec/batch\n",
      "Epoch 4/20  Iteration 5888/35720 Training loss: 1.1174 0.2069 sec/batch\n",
      "Epoch 4/20  Iteration 5889/35720 Training loss: 1.1174 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 5890/35720 Training loss: 1.1173 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 5891/35720 Training loss: 1.1172 0.2227 sec/batch\n",
      "Epoch 4/20  Iteration 5892/35720 Training loss: 1.1173 0.2204 sec/batch\n",
      "Epoch 4/20  Iteration 5893/35720 Training loss: 1.1171 0.2277 sec/batch\n",
      "Epoch 4/20  Iteration 5894/35720 Training loss: 1.1169 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 5895/35720 Training loss: 1.1167 0.2294 sec/batch\n",
      "Epoch 4/20  Iteration 5896/35720 Training loss: 1.1166 0.2192 sec/batch\n",
      "Epoch 4/20  Iteration 5897/35720 Training loss: 1.1166 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 5898/35720 Training loss: 1.1164 0.2194 sec/batch\n",
      "Epoch 4/20  Iteration 5899/35720 Training loss: 1.1163 0.2296 sec/batch\n",
      "Epoch 4/20  Iteration 5900/35720 Training loss: 1.1162 0.2278 sec/batch\n",
      "Epoch 4/20  Iteration 5901/35720 Training loss: 1.1160 0.2266 sec/batch\n",
      "Epoch 4/20  Iteration 5902/35720 Training loss: 1.1159 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 5903/35720 Training loss: 1.1160 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 5904/35720 Training loss: 1.1161 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 5905/35720 Training loss: 1.1160 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 5906/35720 Training loss: 1.1160 0.2113 sec/batch\n",
      "Epoch 4/20  Iteration 5907/35720 Training loss: 1.1158 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 5908/35720 Training loss: 1.1159 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 5909/35720 Training loss: 1.1158 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 5910/35720 Training loss: 1.1156 0.2136 sec/batch\n",
      "Epoch 4/20  Iteration 5911/35720 Training loss: 1.1155 0.2167 sec/batch\n",
      "Epoch 4/20  Iteration 5912/35720 Training loss: 1.1155 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 5913/35720 Training loss: 1.1154 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 5914/35720 Training loss: 1.1154 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 5915/35720 Training loss: 1.1153 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 5916/35720 Training loss: 1.1155 0.2193 sec/batch\n",
      "Epoch 4/20  Iteration 5917/35720 Training loss: 1.1153 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 5918/35720 Training loss: 1.1151 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 5919/35720 Training loss: 1.1152 0.2200 sec/batch\n",
      "Epoch 4/20  Iteration 5920/35720 Training loss: 1.1151 0.2210 sec/batch\n",
      "Epoch 4/20  Iteration 5921/35720 Training loss: 1.1150 0.2275 sec/batch\n",
      "Epoch 4/20  Iteration 5922/35720 Training loss: 1.1148 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 5923/35720 Training loss: 1.1146 0.2269 sec/batch\n",
      "Epoch 4/20  Iteration 5924/35720 Training loss: 1.1145 0.2226 sec/batch\n",
      "Epoch 4/20  Iteration 5925/35720 Training loss: 1.1145 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 5926/35720 Training loss: 1.1144 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 5927/35720 Training loss: 1.1143 0.2220 sec/batch\n",
      "Epoch 4/20  Iteration 5928/35720 Training loss: 1.1143 0.2197 sec/batch\n",
      "Epoch 4/20  Iteration 5929/35720 Training loss: 1.1143 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 5930/35720 Training loss: 1.1144 0.2243 sec/batch\n",
      "Epoch 4/20  Iteration 5931/35720 Training loss: 1.1145 0.2253 sec/batch\n",
      "Epoch 4/20  Iteration 5932/35720 Training loss: 1.1145 0.2186 sec/batch\n",
      "Epoch 4/20  Iteration 5933/35720 Training loss: 1.1145 0.2313 sec/batch\n",
      "Epoch 4/20  Iteration 5934/35720 Training loss: 1.1146 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 5935/35720 Training loss: 1.1146 0.2194 sec/batch\n",
      "Epoch 4/20  Iteration 5936/35720 Training loss: 1.1146 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 5937/35720 Training loss: 1.1145 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 5938/35720 Training loss: 1.1144 0.2178 sec/batch\n",
      "Epoch 4/20  Iteration 5939/35720 Training loss: 1.1144 0.2293 sec/batch\n",
      "Epoch 4/20  Iteration 5940/35720 Training loss: 1.1144 0.2230 sec/batch\n",
      "Epoch 4/20  Iteration 5941/35720 Training loss: 1.1144 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 5942/35720 Training loss: 1.1145 0.2230 sec/batch\n",
      "Epoch 4/20  Iteration 5943/35720 Training loss: 1.1143 0.2232 sec/batch\n",
      "Epoch 4/20  Iteration 5944/35720 Training loss: 1.1141 0.2139 sec/batch\n",
      "Epoch 4/20  Iteration 5945/35720 Training loss: 1.1139 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 5946/35720 Training loss: 1.1138 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 5947/35720 Training loss: 1.1137 0.2249 sec/batch\n",
      "Epoch 4/20  Iteration 5948/35720 Training loss: 1.1135 0.2238 sec/batch\n",
      "Epoch 4/20  Iteration 5949/35720 Training loss: 1.1136 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 5950/35720 Training loss: 1.1135 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 5951/35720 Training loss: 1.1135 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 5952/35720 Training loss: 1.1135 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 5953/35720 Training loss: 1.1134 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 5954/35720 Training loss: 1.1133 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 5955/35720 Training loss: 1.1132 0.2073 sec/batch\n",
      "Epoch 4/20  Iteration 5956/35720 Training loss: 1.1133 0.2140 sec/batch\n",
      "Epoch 4/20  Iteration 5957/35720 Training loss: 1.1132 0.2185 sec/batch\n",
      "Epoch 4/20  Iteration 5958/35720 Training loss: 1.1130 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 5959/35720 Training loss: 1.1128 0.2118 sec/batch\n",
      "Epoch 4/20  Iteration 5960/35720 Training loss: 1.1127 0.2221 sec/batch\n",
      "Epoch 4/20  Iteration 5961/35720 Training loss: 1.1126 0.2272 sec/batch\n",
      "Epoch 4/20  Iteration 5962/35720 Training loss: 1.1125 0.2234 sec/batch\n",
      "Epoch 4/20  Iteration 5963/35720 Training loss: 1.1125 0.2221 sec/batch\n",
      "Epoch 4/20  Iteration 5964/35720 Training loss: 1.1125 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 5965/35720 Training loss: 1.1126 0.2361 sec/batch\n",
      "Epoch 4/20  Iteration 5966/35720 Training loss: 1.1125 0.2220 sec/batch\n",
      "Epoch 4/20  Iteration 5967/35720 Training loss: 1.1127 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 5968/35720 Training loss: 1.1126 0.2074 sec/batch\n",
      "Epoch 4/20  Iteration 5969/35720 Training loss: 1.1126 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 5970/35720 Training loss: 1.1124 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 5971/35720 Training loss: 1.1124 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 5972/35720 Training loss: 1.1124 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 5973/35720 Training loss: 1.1124 0.2079 sec/batch\n",
      "Epoch 4/20  Iteration 5974/35720 Training loss: 1.1123 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 5975/35720 Training loss: 1.1122 0.2208 sec/batch\n",
      "Epoch 4/20  Iteration 5976/35720 Training loss: 1.1121 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 5977/35720 Training loss: 1.1120 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 5978/35720 Training loss: 1.1118 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 5979/35720 Training loss: 1.1117 0.2188 sec/batch\n",
      "Epoch 4/20  Iteration 5980/35720 Training loss: 1.1117 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 5981/35720 Training loss: 1.1116 0.2199 sec/batch\n",
      "Epoch 4/20  Iteration 5982/35720 Training loss: 1.1115 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 5983/35720 Training loss: 1.1113 0.2255 sec/batch\n",
      "Epoch 4/20  Iteration 5984/35720 Training loss: 1.1113 0.2061 sec/batch\n",
      "Epoch 4/20  Iteration 5985/35720 Training loss: 1.1113 0.2207 sec/batch\n",
      "Epoch 4/20  Iteration 5986/35720 Training loss: 1.1111 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 5987/35720 Training loss: 1.1111 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 5988/35720 Training loss: 1.1109 0.2246 sec/batch\n",
      "Epoch 4/20  Iteration 5989/35720 Training loss: 1.1111 0.2157 sec/batch\n",
      "Epoch 4/20  Iteration 5990/35720 Training loss: 1.1109 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 5991/35720 Training loss: 1.1108 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 5992/35720 Training loss: 1.1108 0.2306 sec/batch\n",
      "Epoch 4/20  Iteration 5993/35720 Training loss: 1.1108 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 5994/35720 Training loss: 1.1107 0.2074 sec/batch\n",
      "Epoch 4/20  Iteration 5995/35720 Training loss: 1.1107 0.2240 sec/batch\n",
      "Epoch 4/20  Iteration 5996/35720 Training loss: 1.1106 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 5997/35720 Training loss: 1.1106 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 5998/35720 Training loss: 1.1107 0.2200 sec/batch\n",
      "Epoch 4/20  Iteration 5999/35720 Training loss: 1.1108 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 6000/35720 Training loss: 1.1108 0.2157 sec/batch\n",
      "Validation loss: 1.31257 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 6001/35720 Training loss: 1.1111 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6002/35720 Training loss: 1.1110 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 6003/35720 Training loss: 1.1111 0.2079 sec/batch\n",
      "Epoch 4/20  Iteration 6004/35720 Training loss: 1.1111 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 6005/35720 Training loss: 1.1111 0.2054 sec/batch\n",
      "Epoch 4/20  Iteration 6006/35720 Training loss: 1.1111 0.2150 sec/batch\n",
      "Epoch 4/20  Iteration 6007/35720 Training loss: 1.1110 0.2338 sec/batch\n",
      "Epoch 4/20  Iteration 6008/35720 Training loss: 1.1108 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 6009/35720 Training loss: 1.1108 0.2223 sec/batch\n",
      "Epoch 4/20  Iteration 6010/35720 Training loss: 1.1108 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6011/35720 Training loss: 1.1108 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6012/35720 Training loss: 1.1110 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6013/35720 Training loss: 1.1110 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6014/35720 Training loss: 1.1111 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6015/35720 Training loss: 1.1112 0.2197 sec/batch\n",
      "Epoch 4/20  Iteration 6016/35720 Training loss: 1.1113 0.2068 sec/batch\n",
      "Epoch 4/20  Iteration 6017/35720 Training loss: 1.1113 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 6018/35720 Training loss: 1.1113 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6019/35720 Training loss: 1.1114 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6020/35720 Training loss: 1.1114 0.2222 sec/batch\n",
      "Epoch 4/20  Iteration 6021/35720 Training loss: 1.1114 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6022/35720 Training loss: 1.1114 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 6023/35720 Training loss: 1.1115 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 6024/35720 Training loss: 1.1115 0.2296 sec/batch\n",
      "Epoch 4/20  Iteration 6025/35720 Training loss: 1.1117 0.2282 sec/batch\n",
      "Epoch 4/20  Iteration 6026/35720 Training loss: 1.1116 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6027/35720 Training loss: 1.1114 0.2120 sec/batch\n",
      "Epoch 4/20  Iteration 6028/35720 Training loss: 1.1114 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6029/35720 Training loss: 1.1112 0.2253 sec/batch\n",
      "Epoch 4/20  Iteration 6030/35720 Training loss: 1.1112 0.2196 sec/batch\n",
      "Epoch 4/20  Iteration 6031/35720 Training loss: 1.1112 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6032/35720 Training loss: 1.1111 0.2114 sec/batch\n",
      "Epoch 4/20  Iteration 6033/35720 Training loss: 1.1110 0.2233 sec/batch\n",
      "Epoch 4/20  Iteration 6034/35720 Training loss: 1.1109 0.2211 sec/batch\n",
      "Epoch 4/20  Iteration 6035/35720 Training loss: 1.1109 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6036/35720 Training loss: 1.1109 0.2211 sec/batch\n",
      "Epoch 4/20  Iteration 6037/35720 Training loss: 1.1109 0.2186 sec/batch\n",
      "Epoch 4/20  Iteration 6038/35720 Training loss: 1.1108 0.2246 sec/batch\n",
      "Epoch 4/20  Iteration 6039/35720 Training loss: 1.1108 0.2149 sec/batch\n",
      "Epoch 4/20  Iteration 6040/35720 Training loss: 1.1107 0.2237 sec/batch\n",
      "Epoch 4/20  Iteration 6041/35720 Training loss: 1.1106 0.2185 sec/batch\n",
      "Epoch 4/20  Iteration 6042/35720 Training loss: 1.1105 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 6043/35720 Training loss: 1.1106 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6044/35720 Training loss: 1.1106 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6045/35720 Training loss: 1.1106 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6046/35720 Training loss: 1.1105 0.2181 sec/batch\n",
      "Epoch 4/20  Iteration 6047/35720 Training loss: 1.1105 0.2229 sec/batch\n",
      "Epoch 4/20  Iteration 6048/35720 Training loss: 1.1104 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 6049/35720 Training loss: 1.1105 0.2241 sec/batch\n",
      "Epoch 4/20  Iteration 6050/35720 Training loss: 1.1106 0.2198 sec/batch\n",
      "Epoch 4/20  Iteration 6051/35720 Training loss: 1.1108 0.2267 sec/batch\n",
      "Epoch 4/20  Iteration 6052/35720 Training loss: 1.1108 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6053/35720 Training loss: 1.1108 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 6054/35720 Training loss: 1.1107 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6055/35720 Training loss: 1.1108 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 6056/35720 Training loss: 1.1107 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 6057/35720 Training loss: 1.1106 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6058/35720 Training loss: 1.1106 0.2255 sec/batch\n",
      "Epoch 4/20  Iteration 6059/35720 Training loss: 1.1105 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 6060/35720 Training loss: 1.1105 0.2267 sec/batch\n",
      "Epoch 4/20  Iteration 6061/35720 Training loss: 1.1105 0.2192 sec/batch\n",
      "Epoch 4/20  Iteration 6062/35720 Training loss: 1.1104 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 6063/35720 Training loss: 1.1104 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 6064/35720 Training loss: 1.1105 0.2114 sec/batch\n",
      "Epoch 4/20  Iteration 6065/35720 Training loss: 1.1106 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 6066/35720 Training loss: 1.1106 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 6067/35720 Training loss: 1.1107 0.2186 sec/batch\n",
      "Epoch 4/20  Iteration 6068/35720 Training loss: 1.1107 0.2125 sec/batch\n",
      "Epoch 4/20  Iteration 6069/35720 Training loss: 1.1108 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6070/35720 Training loss: 1.1107 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6071/35720 Training loss: 1.1107 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6072/35720 Training loss: 1.1107 0.2182 sec/batch\n",
      "Epoch 4/20  Iteration 6073/35720 Training loss: 1.1108 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6074/35720 Training loss: 1.1108 0.2172 sec/batch\n",
      "Epoch 4/20  Iteration 6075/35720 Training loss: 1.1109 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6076/35720 Training loss: 1.1109 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6077/35720 Training loss: 1.1109 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6078/35720 Training loss: 1.1110 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 6079/35720 Training loss: 1.1110 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6080/35720 Training loss: 1.1110 0.2107 sec/batch\n",
      "Epoch 4/20  Iteration 6081/35720 Training loss: 1.1111 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6082/35720 Training loss: 1.1110 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6083/35720 Training loss: 1.1110 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6084/35720 Training loss: 1.1110 0.2269 sec/batch\n",
      "Epoch 4/20  Iteration 6085/35720 Training loss: 1.1111 0.2139 sec/batch\n",
      "Epoch 4/20  Iteration 6086/35720 Training loss: 1.1111 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 6087/35720 Training loss: 1.1112 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6088/35720 Training loss: 1.1112 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6089/35720 Training loss: 1.1111 0.2153 sec/batch\n",
      "Epoch 4/20  Iteration 6090/35720 Training loss: 1.1111 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 6091/35720 Training loss: 1.1111 0.2181 sec/batch\n",
      "Epoch 4/20  Iteration 6092/35720 Training loss: 1.1111 0.2200 sec/batch\n",
      "Epoch 4/20  Iteration 6093/35720 Training loss: 1.1110 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 6094/35720 Training loss: 1.1111 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 6095/35720 Training loss: 1.1112 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 6096/35720 Training loss: 1.1111 0.2245 sec/batch\n",
      "Epoch 4/20  Iteration 6097/35720 Training loss: 1.1112 0.2197 sec/batch\n",
      "Epoch 4/20  Iteration 6098/35720 Training loss: 1.1112 0.2217 sec/batch\n",
      "Epoch 4/20  Iteration 6099/35720 Training loss: 1.1111 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 6100/35720 Training loss: 1.1111 0.2253 sec/batch\n",
      "Epoch 4/20  Iteration 6101/35720 Training loss: 1.1111 0.2284 sec/batch\n",
      "Epoch 4/20  Iteration 6102/35720 Training loss: 1.1111 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6103/35720 Training loss: 1.1111 0.2263 sec/batch\n",
      "Epoch 4/20  Iteration 6104/35720 Training loss: 1.1111 0.2087 sec/batch\n",
      "Epoch 4/20  Iteration 6105/35720 Training loss: 1.1111 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6106/35720 Training loss: 1.1110 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6107/35720 Training loss: 1.1109 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6108/35720 Training loss: 1.1109 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6109/35720 Training loss: 1.1109 0.2182 sec/batch\n",
      "Epoch 4/20  Iteration 6110/35720 Training loss: 1.1111 0.2224 sec/batch\n",
      "Epoch 4/20  Iteration 6111/35720 Training loss: 1.1109 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6112/35720 Training loss: 1.1109 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6113/35720 Training loss: 1.1109 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 6114/35720 Training loss: 1.1108 0.2197 sec/batch\n",
      "Epoch 4/20  Iteration 6115/35720 Training loss: 1.1107 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 6116/35720 Training loss: 1.1107 0.2053 sec/batch\n",
      "Epoch 4/20  Iteration 6117/35720 Training loss: 1.1108 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6118/35720 Training loss: 1.1108 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6119/35720 Training loss: 1.1108 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6120/35720 Training loss: 1.1108 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6121/35720 Training loss: 1.1108 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 6122/35720 Training loss: 1.1107 0.2065 sec/batch\n",
      "Epoch 4/20  Iteration 6123/35720 Training loss: 1.1106 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6124/35720 Training loss: 1.1107 0.2122 sec/batch\n",
      "Epoch 4/20  Iteration 6125/35720 Training loss: 1.1107 0.2074 sec/batch\n",
      "Epoch 4/20  Iteration 6126/35720 Training loss: 1.1107 0.2057 sec/batch\n",
      "Epoch 4/20  Iteration 6127/35720 Training loss: 1.1107 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6128/35720 Training loss: 1.1109 0.2129 sec/batch\n",
      "Epoch 4/20  Iteration 6129/35720 Training loss: 1.1109 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6130/35720 Training loss: 1.1111 0.2120 sec/batch\n",
      "Epoch 4/20  Iteration 6131/35720 Training loss: 1.1110 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6132/35720 Training loss: 1.1108 0.2196 sec/batch\n",
      "Epoch 4/20  Iteration 6133/35720 Training loss: 1.1107 0.2119 sec/batch\n",
      "Epoch 4/20  Iteration 6134/35720 Training loss: 1.1106 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 6135/35720 Training loss: 1.1106 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6136/35720 Training loss: 1.1106 0.2197 sec/batch\n",
      "Epoch 4/20  Iteration 6137/35720 Training loss: 1.1106 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 6138/35720 Training loss: 1.1107 0.2236 sec/batch\n",
      "Epoch 4/20  Iteration 6139/35720 Training loss: 1.1106 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 6140/35720 Training loss: 1.1107 0.2305 sec/batch\n",
      "Epoch 4/20  Iteration 6141/35720 Training loss: 1.1107 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 6142/35720 Training loss: 1.1107 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 6143/35720 Training loss: 1.1106 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6144/35720 Training loss: 1.1105 0.2200 sec/batch\n",
      "Epoch 4/20  Iteration 6145/35720 Training loss: 1.1105 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 6146/35720 Training loss: 1.1105 0.2315 sec/batch\n",
      "Epoch 4/20  Iteration 6147/35720 Training loss: 1.1105 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6148/35720 Training loss: 1.1105 0.2230 sec/batch\n",
      "Epoch 4/20  Iteration 6149/35720 Training loss: 1.1107 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6150/35720 Training loss: 1.1107 0.2204 sec/batch\n",
      "Epoch 4/20  Iteration 6151/35720 Training loss: 1.1107 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 6152/35720 Training loss: 1.1106 0.2118 sec/batch\n",
      "Epoch 4/20  Iteration 6153/35720 Training loss: 1.1106 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 6154/35720 Training loss: 1.1106 0.2133 sec/batch\n",
      "Epoch 4/20  Iteration 6155/35720 Training loss: 1.1105 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6156/35720 Training loss: 1.1104 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 6157/35720 Training loss: 1.1103 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 6158/35720 Training loss: 1.1104 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 6159/35720 Training loss: 1.1104 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6160/35720 Training loss: 1.1105 0.2283 sec/batch\n",
      "Epoch 4/20  Iteration 6161/35720 Training loss: 1.1105 0.2069 sec/batch\n",
      "Epoch 4/20  Iteration 6162/35720 Training loss: 1.1107 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6163/35720 Training loss: 1.1108 0.2059 sec/batch\n",
      "Epoch 4/20  Iteration 6164/35720 Training loss: 1.1109 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 6165/35720 Training loss: 1.1109 0.2203 sec/batch\n",
      "Epoch 4/20  Iteration 6166/35720 Training loss: 1.1109 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6167/35720 Training loss: 1.1110 0.2247 sec/batch\n",
      "Epoch 4/20  Iteration 6168/35720 Training loss: 1.1111 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6169/35720 Training loss: 1.1111 0.2256 sec/batch\n",
      "Epoch 4/20  Iteration 6170/35720 Training loss: 1.1111 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 6171/35720 Training loss: 1.1112 0.2119 sec/batch\n",
      "Epoch 4/20  Iteration 6172/35720 Training loss: 1.1112 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6173/35720 Training loss: 1.1112 0.2236 sec/batch\n",
      "Epoch 4/20  Iteration 6174/35720 Training loss: 1.1112 0.2114 sec/batch\n",
      "Epoch 4/20  Iteration 6175/35720 Training loss: 1.1111 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6176/35720 Training loss: 1.1112 0.2204 sec/batch\n",
      "Epoch 4/20  Iteration 6177/35720 Training loss: 1.1112 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 6178/35720 Training loss: 1.1112 0.2236 sec/batch\n",
      "Epoch 4/20  Iteration 6179/35720 Training loss: 1.1112 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 6180/35720 Training loss: 1.1111 0.2220 sec/batch\n",
      "Epoch 4/20  Iteration 6181/35720 Training loss: 1.1110 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 6182/35720 Training loss: 1.1109 0.2232 sec/batch\n",
      "Epoch 4/20  Iteration 6183/35720 Training loss: 1.1107 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6184/35720 Training loss: 1.1107 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6185/35720 Training loss: 1.1107 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6186/35720 Training loss: 1.1105 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6187/35720 Training loss: 1.1105 0.2186 sec/batch\n",
      "Epoch 4/20  Iteration 6188/35720 Training loss: 1.1104 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6189/35720 Training loss: 1.1103 0.2070 sec/batch\n",
      "Epoch 4/20  Iteration 6190/35720 Training loss: 1.1104 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 6191/35720 Training loss: 1.1106 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6192/35720 Training loss: 1.1107 0.2120 sec/batch\n",
      "Epoch 4/20  Iteration 6193/35720 Training loss: 1.1106 0.2270 sec/batch\n",
      "Epoch 4/20  Iteration 6194/35720 Training loss: 1.1105 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6195/35720 Training loss: 1.1106 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 6196/35720 Training loss: 1.1106 0.2167 sec/batch\n",
      "Epoch 4/20  Iteration 6197/35720 Training loss: 1.1107 0.2170 sec/batch\n",
      "Epoch 4/20  Iteration 6198/35720 Training loss: 1.1106 0.2113 sec/batch\n",
      "Epoch 4/20  Iteration 6199/35720 Training loss: 1.1107 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 6200/35720 Training loss: 1.1108 0.2102 sec/batch\n",
      "Validation loss: 1.31071 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 6201/35720 Training loss: 1.1110 0.2070 sec/batch\n",
      "Epoch 4/20  Iteration 6202/35720 Training loss: 1.1111 0.2065 sec/batch\n",
      "Epoch 4/20  Iteration 6203/35720 Training loss: 1.1110 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6204/35720 Training loss: 1.1110 0.2216 sec/batch\n",
      "Epoch 4/20  Iteration 6205/35720 Training loss: 1.1109 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 6206/35720 Training loss: 1.1109 0.2072 sec/batch\n",
      "Epoch 4/20  Iteration 6207/35720 Training loss: 1.1108 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6208/35720 Training loss: 1.1109 0.2178 sec/batch\n",
      "Epoch 4/20  Iteration 6209/35720 Training loss: 1.1109 0.2068 sec/batch\n",
      "Epoch 4/20  Iteration 6210/35720 Training loss: 1.1109 0.2118 sec/batch\n",
      "Epoch 4/20  Iteration 6211/35720 Training loss: 1.1108 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 6212/35720 Training loss: 1.1108 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6213/35720 Training loss: 1.1108 0.2072 sec/batch\n",
      "Epoch 4/20  Iteration 6214/35720 Training loss: 1.1108 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 6215/35720 Training loss: 1.1107 0.2071 sec/batch\n",
      "Epoch 4/20  Iteration 6216/35720 Training loss: 1.1108 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 6217/35720 Training loss: 1.1107 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 6218/35720 Training loss: 1.1106 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 6219/35720 Training loss: 1.1106 0.2306 sec/batch\n",
      "Epoch 4/20  Iteration 6220/35720 Training loss: 1.1106 0.2232 sec/batch\n",
      "Epoch 4/20  Iteration 6221/35720 Training loss: 1.1106 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6222/35720 Training loss: 1.1105 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6223/35720 Training loss: 1.1104 0.2261 sec/batch\n",
      "Epoch 4/20  Iteration 6224/35720 Training loss: 1.1105 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 6225/35720 Training loss: 1.1106 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 6226/35720 Training loss: 1.1106 0.2253 sec/batch\n",
      "Epoch 4/20  Iteration 6227/35720 Training loss: 1.1105 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6228/35720 Training loss: 1.1105 0.2167 sec/batch\n",
      "Epoch 4/20  Iteration 6229/35720 Training loss: 1.1104 0.2223 sec/batch\n",
      "Epoch 4/20  Iteration 6230/35720 Training loss: 1.1103 0.2268 sec/batch\n",
      "Epoch 4/20  Iteration 6231/35720 Training loss: 1.1102 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 6232/35720 Training loss: 1.1102 0.2181 sec/batch\n",
      "Epoch 4/20  Iteration 6233/35720 Training loss: 1.1101 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 6234/35720 Training loss: 1.1100 0.2240 sec/batch\n",
      "Epoch 4/20  Iteration 6235/35720 Training loss: 1.1099 0.2228 sec/batch\n",
      "Epoch 4/20  Iteration 6236/35720 Training loss: 1.1099 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 6237/35720 Training loss: 1.1099 0.2056 sec/batch\n",
      "Epoch 4/20  Iteration 6238/35720 Training loss: 1.1098 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6239/35720 Training loss: 1.1099 0.2078 sec/batch\n",
      "Epoch 4/20  Iteration 6240/35720 Training loss: 1.1099 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6241/35720 Training loss: 1.1099 0.2236 sec/batch\n",
      "Epoch 4/20  Iteration 6242/35720 Training loss: 1.1099 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6243/35720 Training loss: 1.1099 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6244/35720 Training loss: 1.1099 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 6245/35720 Training loss: 1.1099 0.2234 sec/batch\n",
      "Epoch 4/20  Iteration 6246/35720 Training loss: 1.1098 0.2143 sec/batch\n",
      "Epoch 4/20  Iteration 6247/35720 Training loss: 1.1098 0.2266 sec/batch\n",
      "Epoch 4/20  Iteration 6248/35720 Training loss: 1.1096 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6249/35720 Training loss: 1.1096 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 6250/35720 Training loss: 1.1095 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6251/35720 Training loss: 1.1094 0.2064 sec/batch\n",
      "Epoch 4/20  Iteration 6252/35720 Training loss: 1.1093 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 6253/35720 Training loss: 1.1093 0.2245 sec/batch\n",
      "Epoch 4/20  Iteration 6254/35720 Training loss: 1.1093 0.2470 sec/batch\n",
      "Epoch 4/20  Iteration 6255/35720 Training loss: 1.1091 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 6256/35720 Training loss: 1.1090 0.2254 sec/batch\n",
      "Epoch 4/20  Iteration 6257/35720 Training loss: 1.1088 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 6258/35720 Training loss: 1.1088 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6259/35720 Training loss: 1.1087 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 6260/35720 Training loss: 1.1086 0.2129 sec/batch\n",
      "Epoch 4/20  Iteration 6261/35720 Training loss: 1.1085 0.2063 sec/batch\n",
      "Epoch 4/20  Iteration 6262/35720 Training loss: 1.1084 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6263/35720 Training loss: 1.1083 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6264/35720 Training loss: 1.1083 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 6265/35720 Training loss: 1.1083 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6266/35720 Training loss: 1.1084 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 6267/35720 Training loss: 1.1083 0.2244 sec/batch\n",
      "Epoch 4/20  Iteration 6268/35720 Training loss: 1.1083 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6269/35720 Training loss: 1.1083 0.2172 sec/batch\n",
      "Epoch 4/20  Iteration 6270/35720 Training loss: 1.1083 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6271/35720 Training loss: 1.1083 0.2217 sec/batch\n",
      "Epoch 4/20  Iteration 6272/35720 Training loss: 1.1083 0.2194 sec/batch\n",
      "Epoch 4/20  Iteration 6273/35720 Training loss: 1.1084 0.2219 sec/batch\n",
      "Epoch 4/20  Iteration 6274/35720 Training loss: 1.1084 0.2192 sec/batch\n",
      "Epoch 4/20  Iteration 6275/35720 Training loss: 1.1084 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 6276/35720 Training loss: 1.1084 0.2207 sec/batch\n",
      "Epoch 4/20  Iteration 6277/35720 Training loss: 1.1083 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 6278/35720 Training loss: 1.1083 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 6279/35720 Training loss: 1.1083 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 6280/35720 Training loss: 1.1082 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 6281/35720 Training loss: 1.1082 0.2119 sec/batch\n",
      "Epoch 4/20  Iteration 6282/35720 Training loss: 1.1082 0.2235 sec/batch\n",
      "Epoch 4/20  Iteration 6283/35720 Training loss: 1.1082 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6284/35720 Training loss: 1.1082 0.2216 sec/batch\n",
      "Epoch 4/20  Iteration 6285/35720 Training loss: 1.1083 0.2855 sec/batch\n",
      "Epoch 4/20  Iteration 6286/35720 Training loss: 1.1082 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6287/35720 Training loss: 1.1084 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6288/35720 Training loss: 1.1084 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6289/35720 Training loss: 1.1084 0.2232 sec/batch\n",
      "Epoch 4/20  Iteration 6290/35720 Training loss: 1.1084 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6291/35720 Training loss: 1.1084 0.2303 sec/batch\n",
      "Epoch 4/20  Iteration 6292/35720 Training loss: 1.1084 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6293/35720 Training loss: 1.1082 0.2225 sec/batch\n",
      "Epoch 4/20  Iteration 6294/35720 Training loss: 1.1081 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 6295/35720 Training loss: 1.1080 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 6296/35720 Training loss: 1.1079 0.2079 sec/batch\n",
      "Epoch 4/20  Iteration 6297/35720 Training loss: 1.1079 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 6298/35720 Training loss: 1.1077 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6299/35720 Training loss: 1.1077 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6300/35720 Training loss: 1.1077 0.2207 sec/batch\n",
      "Epoch 4/20  Iteration 6301/35720 Training loss: 1.1076 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 6302/35720 Training loss: 1.1078 0.2056 sec/batch\n",
      "Epoch 4/20  Iteration 6303/35720 Training loss: 1.1077 0.2071 sec/batch\n",
      "Epoch 4/20  Iteration 6304/35720 Training loss: 1.1077 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6305/35720 Training loss: 1.1077 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 6306/35720 Training loss: 1.1075 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6307/35720 Training loss: 1.1076 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 6308/35720 Training loss: 1.1074 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6309/35720 Training loss: 1.1074 0.2079 sec/batch\n",
      "Epoch 4/20  Iteration 6310/35720 Training loss: 1.1073 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6311/35720 Training loss: 1.1072 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6312/35720 Training loss: 1.1072 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6313/35720 Training loss: 1.1071 0.2282 sec/batch\n",
      "Epoch 4/20  Iteration 6314/35720 Training loss: 1.1071 0.2213 sec/batch\n",
      "Epoch 4/20  Iteration 6315/35720 Training loss: 1.1070 0.2266 sec/batch\n",
      "Epoch 4/20  Iteration 6316/35720 Training loss: 1.1070 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6317/35720 Training loss: 1.1069 0.2261 sec/batch\n",
      "Epoch 4/20  Iteration 6318/35720 Training loss: 1.1069 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6319/35720 Training loss: 1.1069 0.2268 sec/batch\n",
      "Epoch 4/20  Iteration 6320/35720 Training loss: 1.1068 0.2218 sec/batch\n",
      "Epoch 4/20  Iteration 6321/35720 Training loss: 1.1067 0.2062 sec/batch\n",
      "Epoch 4/20  Iteration 6322/35720 Training loss: 1.1067 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 6323/35720 Training loss: 1.1067 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6324/35720 Training loss: 1.1066 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 6325/35720 Training loss: 1.1066 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6326/35720 Training loss: 1.1065 0.2062 sec/batch\n",
      "Epoch 4/20  Iteration 6327/35720 Training loss: 1.1068 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 6328/35720 Training loss: 1.1070 0.2192 sec/batch\n",
      "Epoch 4/20  Iteration 6329/35720 Training loss: 1.1069 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 6330/35720 Training loss: 1.1068 0.2183 sec/batch\n",
      "Epoch 4/20  Iteration 6331/35720 Training loss: 1.1068 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 6332/35720 Training loss: 1.1067 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 6333/35720 Training loss: 1.1066 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6334/35720 Training loss: 1.1066 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 6335/35720 Training loss: 1.1066 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 6336/35720 Training loss: 1.1067 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 6337/35720 Training loss: 1.1066 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 6338/35720 Training loss: 1.1066 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 6339/35720 Training loss: 1.1066 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 6340/35720 Training loss: 1.1064 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6341/35720 Training loss: 1.1063 0.2077 sec/batch\n",
      "Epoch 4/20  Iteration 6342/35720 Training loss: 1.1063 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 6343/35720 Training loss: 1.1064 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 6344/35720 Training loss: 1.1063 0.2117 sec/batch\n",
      "Epoch 4/20  Iteration 6345/35720 Training loss: 1.1063 0.2065 sec/batch\n",
      "Epoch 4/20  Iteration 6346/35720 Training loss: 1.1063 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6347/35720 Training loss: 1.1062 0.2221 sec/batch\n",
      "Epoch 4/20  Iteration 6348/35720 Training loss: 1.1063 0.2238 sec/batch\n",
      "Epoch 4/20  Iteration 6349/35720 Training loss: 1.1062 0.2058 sec/batch\n",
      "Epoch 4/20  Iteration 6350/35720 Training loss: 1.1063 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 6351/35720 Training loss: 1.1063 0.2070 sec/batch\n",
      "Epoch 4/20  Iteration 6352/35720 Training loss: 1.1063 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6353/35720 Training loss: 1.1063 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 6354/35720 Training loss: 1.1063 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6355/35720 Training loss: 1.1064 0.2249 sec/batch\n",
      "Epoch 4/20  Iteration 6356/35720 Training loss: 1.1065 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 6357/35720 Training loss: 1.1065 0.2214 sec/batch\n",
      "Epoch 4/20  Iteration 6358/35720 Training loss: 1.1064 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6359/35720 Training loss: 1.1063 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6360/35720 Training loss: 1.1062 0.2129 sec/batch\n",
      "Epoch 4/20  Iteration 6361/35720 Training loss: 1.1061 0.2271 sec/batch\n",
      "Epoch 4/20  Iteration 6362/35720 Training loss: 1.1060 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6363/35720 Training loss: 1.1059 0.2208 sec/batch\n",
      "Epoch 4/20  Iteration 6364/35720 Training loss: 1.1058 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6365/35720 Training loss: 1.1057 0.2122 sec/batch\n",
      "Epoch 4/20  Iteration 6366/35720 Training loss: 1.1056 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6367/35720 Training loss: 1.1055 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6368/35720 Training loss: 1.1054 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6369/35720 Training loss: 1.1053 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6370/35720 Training loss: 1.1054 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6371/35720 Training loss: 1.1053 0.2060 sec/batch\n",
      "Epoch 4/20  Iteration 6372/35720 Training loss: 1.1052 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 6373/35720 Training loss: 1.1052 0.2117 sec/batch\n",
      "Epoch 4/20  Iteration 6374/35720 Training loss: 1.1052 0.2205 sec/batch\n",
      "Epoch 4/20  Iteration 6375/35720 Training loss: 1.1052 0.2212 sec/batch\n",
      "Epoch 4/20  Iteration 6376/35720 Training loss: 1.1052 0.2261 sec/batch\n",
      "Epoch 4/20  Iteration 6377/35720 Training loss: 1.1051 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 6378/35720 Training loss: 1.1051 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6379/35720 Training loss: 1.1050 0.2267 sec/batch\n",
      "Epoch 4/20  Iteration 6380/35720 Training loss: 1.1051 0.2071 sec/batch\n",
      "Epoch 4/20  Iteration 6381/35720 Training loss: 1.1052 0.2321 sec/batch\n",
      "Epoch 4/20  Iteration 6382/35720 Training loss: 1.1052 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6383/35720 Training loss: 1.1052 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6384/35720 Training loss: 1.1052 0.2302 sec/batch\n",
      "Epoch 4/20  Iteration 6385/35720 Training loss: 1.1051 0.2113 sec/batch\n",
      "Epoch 4/20  Iteration 6386/35720 Training loss: 1.1050 0.2206 sec/batch\n",
      "Epoch 4/20  Iteration 6387/35720 Training loss: 1.1049 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6388/35720 Training loss: 1.1049 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6389/35720 Training loss: 1.1049 0.2252 sec/batch\n",
      "Epoch 4/20  Iteration 6390/35720 Training loss: 1.1049 0.2233 sec/batch\n",
      "Epoch 4/20  Iteration 6391/35720 Training loss: 1.1049 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 6392/35720 Training loss: 1.1049 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6393/35720 Training loss: 1.1049 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6394/35720 Training loss: 1.1049 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 6395/35720 Training loss: 1.1050 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 6396/35720 Training loss: 1.1049 0.2191 sec/batch\n",
      "Epoch 4/20  Iteration 6397/35720 Training loss: 1.1049 0.2059 sec/batch\n",
      "Epoch 4/20  Iteration 6398/35720 Training loss: 1.1048 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6399/35720 Training loss: 1.1048 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6400/35720 Training loss: 1.1048 0.2179 sec/batch\n",
      "Validation loss: 1.31675 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 6401/35720 Training loss: 1.1050 0.2076 sec/batch\n",
      "Epoch 4/20  Iteration 6402/35720 Training loss: 1.1050 0.2067 sec/batch\n",
      "Epoch 4/20  Iteration 6403/35720 Training loss: 1.1050 0.2157 sec/batch\n",
      "Epoch 4/20  Iteration 6404/35720 Training loss: 1.1050 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 6405/35720 Training loss: 1.1049 0.2076 sec/batch\n",
      "Epoch 4/20  Iteration 6406/35720 Training loss: 1.1049 0.2215 sec/batch\n",
      "Epoch 4/20  Iteration 6407/35720 Training loss: 1.1049 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 6408/35720 Training loss: 1.1048 0.2133 sec/batch\n",
      "Epoch 4/20  Iteration 6409/35720 Training loss: 1.1048 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 6410/35720 Training loss: 1.1049 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6411/35720 Training loss: 1.1049 0.2206 sec/batch\n",
      "Epoch 4/20  Iteration 6412/35720 Training loss: 1.1049 0.2457 sec/batch\n",
      "Epoch 4/20  Iteration 6413/35720 Training loss: 1.1050 0.2164 sec/batch\n",
      "Epoch 4/20  Iteration 6414/35720 Training loss: 1.1049 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6415/35720 Training loss: 1.1049 0.2109 sec/batch\n",
      "Epoch 4/20  Iteration 6416/35720 Training loss: 1.1049 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6417/35720 Training loss: 1.1049 0.2227 sec/batch\n",
      "Epoch 4/20  Iteration 6418/35720 Training loss: 1.1048 0.2240 sec/batch\n",
      "Epoch 4/20  Iteration 6419/35720 Training loss: 1.1048 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 6420/35720 Training loss: 1.1048 0.2114 sec/batch\n",
      "Epoch 4/20  Iteration 6421/35720 Training loss: 1.1049 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6422/35720 Training loss: 1.1048 0.2139 sec/batch\n",
      "Epoch 4/20  Iteration 6423/35720 Training loss: 1.1048 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6424/35720 Training loss: 1.1047 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6425/35720 Training loss: 1.1047 0.2061 sec/batch\n",
      "Epoch 4/20  Iteration 6426/35720 Training loss: 1.1047 0.2077 sec/batch\n",
      "Epoch 4/20  Iteration 6427/35720 Training loss: 1.1047 0.2075 sec/batch\n",
      "Epoch 4/20  Iteration 6428/35720 Training loss: 1.1047 0.2075 sec/batch\n",
      "Epoch 4/20  Iteration 6429/35720 Training loss: 1.1046 0.2068 sec/batch\n",
      "Epoch 4/20  Iteration 6430/35720 Training loss: 1.1046 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 6431/35720 Training loss: 1.1046 0.2233 sec/batch\n",
      "Epoch 4/20  Iteration 6432/35720 Training loss: 1.1046 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 6433/35720 Training loss: 1.1046 0.2229 sec/batch\n",
      "Epoch 4/20  Iteration 6434/35720 Training loss: 1.1046 0.2114 sec/batch\n",
      "Epoch 4/20  Iteration 6435/35720 Training loss: 1.1046 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6436/35720 Training loss: 1.1045 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6437/35720 Training loss: 1.1045 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6438/35720 Training loss: 1.1045 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6439/35720 Training loss: 1.1044 0.2188 sec/batch\n",
      "Epoch 4/20  Iteration 6440/35720 Training loss: 1.1043 0.2078 sec/batch\n",
      "Epoch 4/20  Iteration 6441/35720 Training loss: 1.1043 0.2234 sec/batch\n",
      "Epoch 4/20  Iteration 6442/35720 Training loss: 1.1043 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6443/35720 Training loss: 1.1042 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6444/35720 Training loss: 1.1042 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 6445/35720 Training loss: 1.1042 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6446/35720 Training loss: 1.1043 0.2232 sec/batch\n",
      "Epoch 4/20  Iteration 6447/35720 Training loss: 1.1042 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6448/35720 Training loss: 1.1042 0.2122 sec/batch\n",
      "Epoch 4/20  Iteration 6449/35720 Training loss: 1.1042 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6450/35720 Training loss: 1.1042 0.2125 sec/batch\n",
      "Epoch 4/20  Iteration 6451/35720 Training loss: 1.1042 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 6452/35720 Training loss: 1.1042 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 6453/35720 Training loss: 1.1041 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 6454/35720 Training loss: 1.1041 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6455/35720 Training loss: 1.1040 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 6456/35720 Training loss: 1.1040 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 6457/35720 Training loss: 1.1040 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 6458/35720 Training loss: 1.1040 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 6459/35720 Training loss: 1.1041 0.2246 sec/batch\n",
      "Epoch 4/20  Iteration 6460/35720 Training loss: 1.1040 0.2120 sec/batch\n",
      "Epoch 4/20  Iteration 6461/35720 Training loss: 1.1041 0.2263 sec/batch\n",
      "Epoch 4/20  Iteration 6462/35720 Training loss: 1.1040 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6463/35720 Training loss: 1.1040 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 6464/35720 Training loss: 1.1040 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 6465/35720 Training loss: 1.1040 0.2144 sec/batch\n",
      "Epoch 4/20  Iteration 6466/35720 Training loss: 1.1041 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 6467/35720 Training loss: 1.1040 0.2149 sec/batch\n",
      "Epoch 4/20  Iteration 6468/35720 Training loss: 1.1040 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6469/35720 Training loss: 1.1040 0.2136 sec/batch\n",
      "Epoch 4/20  Iteration 6470/35720 Training loss: 1.1039 0.2287 sec/batch\n",
      "Epoch 4/20  Iteration 6471/35720 Training loss: 1.1038 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 6472/35720 Training loss: 1.1038 0.2236 sec/batch\n",
      "Epoch 4/20  Iteration 6473/35720 Training loss: 1.1038 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6474/35720 Training loss: 1.1038 0.2237 sec/batch\n",
      "Epoch 4/20  Iteration 6475/35720 Training loss: 1.1037 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6476/35720 Training loss: 1.1036 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 6477/35720 Training loss: 1.1036 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6478/35720 Training loss: 1.1037 0.2146 sec/batch\n",
      "Epoch 4/20  Iteration 6479/35720 Training loss: 1.1037 0.2118 sec/batch\n",
      "Epoch 4/20  Iteration 6480/35720 Training loss: 1.1038 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 6481/35720 Training loss: 1.1040 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6482/35720 Training loss: 1.1040 0.2129 sec/batch\n",
      "Epoch 4/20  Iteration 6483/35720 Training loss: 1.1039 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6484/35720 Training loss: 1.1039 0.2236 sec/batch\n",
      "Epoch 4/20  Iteration 6485/35720 Training loss: 1.1039 0.2285 sec/batch\n",
      "Epoch 4/20  Iteration 6486/35720 Training loss: 1.1038 0.2228 sec/batch\n",
      "Epoch 4/20  Iteration 6487/35720 Training loss: 1.1037 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6488/35720 Training loss: 1.1038 0.2235 sec/batch\n",
      "Epoch 4/20  Iteration 6489/35720 Training loss: 1.1038 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6490/35720 Training loss: 1.1037 0.2264 sec/batch\n",
      "Epoch 4/20  Iteration 6491/35720 Training loss: 1.1036 0.2203 sec/batch\n",
      "Epoch 4/20  Iteration 6492/35720 Training loss: 1.1037 0.2087 sec/batch\n",
      "Epoch 4/20  Iteration 6493/35720 Training loss: 1.1036 0.2285 sec/batch\n",
      "Epoch 4/20  Iteration 6494/35720 Training loss: 1.1036 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 6495/35720 Training loss: 1.1035 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6496/35720 Training loss: 1.1034 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6497/35720 Training loss: 1.1033 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 6498/35720 Training loss: 1.1033 0.2112 sec/batch\n",
      "Epoch 4/20  Iteration 6499/35720 Training loss: 1.1033 0.2214 sec/batch\n",
      "Epoch 4/20  Iteration 6500/35720 Training loss: 1.1032 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 6501/35720 Training loss: 1.1031 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6502/35720 Training loss: 1.1030 0.2186 sec/batch\n",
      "Epoch 4/20  Iteration 6503/35720 Training loss: 1.1030 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 6504/35720 Training loss: 1.1029 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6505/35720 Training loss: 1.1029 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6506/35720 Training loss: 1.1028 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6507/35720 Training loss: 1.1027 0.2129 sec/batch\n",
      "Epoch 4/20  Iteration 6508/35720 Training loss: 1.1027 0.2213 sec/batch\n",
      "Epoch 4/20  Iteration 6509/35720 Training loss: 1.1027 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 6510/35720 Training loss: 1.1027 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6511/35720 Training loss: 1.1027 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 6512/35720 Training loss: 1.1027 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6513/35720 Training loss: 1.1027 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 6514/35720 Training loss: 1.1026 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 6515/35720 Training loss: 1.1026 0.2289 sec/batch\n",
      "Epoch 4/20  Iteration 6516/35720 Training loss: 1.1025 0.2070 sec/batch\n",
      "Epoch 4/20  Iteration 6517/35720 Training loss: 1.1024 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6518/35720 Training loss: 1.1024 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6519/35720 Training loss: 1.1023 0.2053 sec/batch\n",
      "Epoch 4/20  Iteration 6520/35720 Training loss: 1.1023 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6521/35720 Training loss: 1.1024 0.2059 sec/batch\n",
      "Epoch 4/20  Iteration 6522/35720 Training loss: 1.1025 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 6523/35720 Training loss: 1.1024 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 6524/35720 Training loss: 1.1024 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 6525/35720 Training loss: 1.1022 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6526/35720 Training loss: 1.1023 0.2070 sec/batch\n",
      "Epoch 4/20  Iteration 6527/35720 Training loss: 1.1023 0.2192 sec/batch\n",
      "Epoch 4/20  Iteration 6528/35720 Training loss: 1.1024 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 6529/35720 Training loss: 1.1024 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6530/35720 Training loss: 1.1024 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 6531/35720 Training loss: 1.1023 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6532/35720 Training loss: 1.1022 0.2068 sec/batch\n",
      "Epoch 4/20  Iteration 6533/35720 Training loss: 1.1023 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6534/35720 Training loss: 1.1023 0.2233 sec/batch\n",
      "Epoch 4/20  Iteration 6535/35720 Training loss: 1.1024 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6536/35720 Training loss: 1.1024 0.2167 sec/batch\n",
      "Epoch 4/20  Iteration 6537/35720 Training loss: 1.1023 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 6538/35720 Training loss: 1.1023 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6539/35720 Training loss: 1.1024 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6540/35720 Training loss: 1.1024 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 6541/35720 Training loss: 1.1023 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 6542/35720 Training loss: 1.1022 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6543/35720 Training loss: 1.1022 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6544/35720 Training loss: 1.1021 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 6545/35720 Training loss: 1.1021 0.2064 sec/batch\n",
      "Epoch 4/20  Iteration 6546/35720 Training loss: 1.1021 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6547/35720 Training loss: 1.1020 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 6548/35720 Training loss: 1.1020 0.2174 sec/batch\n",
      "Epoch 4/20  Iteration 6549/35720 Training loss: 1.1020 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 6550/35720 Training loss: 1.1020 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6551/35720 Training loss: 1.1020 0.2269 sec/batch\n",
      "Epoch 4/20  Iteration 6552/35720 Training loss: 1.1020 0.2112 sec/batch\n",
      "Epoch 4/20  Iteration 6553/35720 Training loss: 1.1020 0.2278 sec/batch\n",
      "Epoch 4/20  Iteration 6554/35720 Training loss: 1.1020 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6555/35720 Training loss: 1.1021 0.2074 sec/batch\n",
      "Epoch 4/20  Iteration 6556/35720 Training loss: 1.1020 0.2201 sec/batch\n",
      "Epoch 4/20  Iteration 6557/35720 Training loss: 1.1020 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 6558/35720 Training loss: 1.1020 0.2129 sec/batch\n",
      "Epoch 4/20  Iteration 6559/35720 Training loss: 1.1019 0.2229 sec/batch\n",
      "Epoch 4/20  Iteration 6560/35720 Training loss: 1.1019 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6561/35720 Training loss: 1.1018 0.2212 sec/batch\n",
      "Epoch 4/20  Iteration 6562/35720 Training loss: 1.1017 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 6563/35720 Training loss: 1.1016 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 6564/35720 Training loss: 1.1016 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6565/35720 Training loss: 1.1016 0.2251 sec/batch\n",
      "Epoch 4/20  Iteration 6566/35720 Training loss: 1.1016 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 6567/35720 Training loss: 1.1016 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 6568/35720 Training loss: 1.1016 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6569/35720 Training loss: 1.1016 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6570/35720 Training loss: 1.1016 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 6571/35720 Training loss: 1.1015 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 6572/35720 Training loss: 1.1015 0.2293 sec/batch\n",
      "Epoch 4/20  Iteration 6573/35720 Training loss: 1.1015 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6574/35720 Training loss: 1.1015 0.2269 sec/batch\n",
      "Epoch 4/20  Iteration 6575/35720 Training loss: 1.1015 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6576/35720 Training loss: 1.1014 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 6577/35720 Training loss: 1.1014 0.2071 sec/batch\n",
      "Epoch 4/20  Iteration 6578/35720 Training loss: 1.1013 0.2061 sec/batch\n",
      "Epoch 4/20  Iteration 6579/35720 Training loss: 1.1013 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6580/35720 Training loss: 1.1013 0.2087 sec/batch\n",
      "Epoch 4/20  Iteration 6581/35720 Training loss: 1.1013 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 6582/35720 Training loss: 1.1012 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6583/35720 Training loss: 1.1012 0.2144 sec/batch\n",
      "Epoch 4/20  Iteration 6584/35720 Training loss: 1.1012 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6585/35720 Training loss: 1.1012 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6586/35720 Training loss: 1.1012 0.2087 sec/batch\n",
      "Epoch 4/20  Iteration 6587/35720 Training loss: 1.1012 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6588/35720 Training loss: 1.1013 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 6589/35720 Training loss: 1.1012 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6590/35720 Training loss: 1.1011 0.2087 sec/batch\n",
      "Epoch 4/20  Iteration 6591/35720 Training loss: 1.1011 0.2185 sec/batch\n",
      "Epoch 4/20  Iteration 6592/35720 Training loss: 1.1011 0.2148 sec/batch\n",
      "Epoch 4/20  Iteration 6593/35720 Training loss: 1.1011 0.2273 sec/batch\n",
      "Epoch 4/20  Iteration 6594/35720 Training loss: 1.1011 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6595/35720 Training loss: 1.1009 0.2271 sec/batch\n",
      "Epoch 4/20  Iteration 6596/35720 Training loss: 1.1009 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6597/35720 Training loss: 1.1007 0.2262 sec/batch\n",
      "Epoch 4/20  Iteration 6598/35720 Training loss: 1.1007 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 6599/35720 Training loss: 1.1006 0.2146 sec/batch\n",
      "Epoch 4/20  Iteration 6600/35720 Training loss: 1.1006 0.2115 sec/batch\n",
      "Validation loss: 1.3128 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 6601/35720 Training loss: 1.1007 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6602/35720 Training loss: 1.1006 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 6603/35720 Training loss: 1.1006 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6604/35720 Training loss: 1.1005 0.2174 sec/batch\n",
      "Epoch 4/20  Iteration 6605/35720 Training loss: 1.1004 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6606/35720 Training loss: 1.1004 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6607/35720 Training loss: 1.1002 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6608/35720 Training loss: 1.1002 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6609/35720 Training loss: 1.1002 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6610/35720 Training loss: 1.1001 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6611/35720 Training loss: 1.1001 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6612/35720 Training loss: 1.1001 0.2139 sec/batch\n",
      "Epoch 4/20  Iteration 6613/35720 Training loss: 1.1001 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 6614/35720 Training loss: 1.0999 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6615/35720 Training loss: 1.0999 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6616/35720 Training loss: 1.0998 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 6617/35720 Training loss: 1.0998 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 6618/35720 Training loss: 1.0998 0.2191 sec/batch\n",
      "Epoch 4/20  Iteration 6619/35720 Training loss: 1.0998 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6620/35720 Training loss: 1.0997 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6621/35720 Training loss: 1.0997 0.2074 sec/batch\n",
      "Epoch 4/20  Iteration 6622/35720 Training loss: 1.0996 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6623/35720 Training loss: 1.0995 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 6624/35720 Training loss: 1.0995 0.2119 sec/batch\n",
      "Epoch 4/20  Iteration 6625/35720 Training loss: 1.0994 0.2241 sec/batch\n",
      "Epoch 4/20  Iteration 6626/35720 Training loss: 1.0994 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 6627/35720 Training loss: 1.0993 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 6628/35720 Training loss: 1.0993 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6629/35720 Training loss: 1.0993 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6630/35720 Training loss: 1.0992 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6631/35720 Training loss: 1.0992 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6632/35720 Training loss: 1.0992 0.2244 sec/batch\n",
      "Epoch 4/20  Iteration 6633/35720 Training loss: 1.0991 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 6634/35720 Training loss: 1.0991 0.2181 sec/batch\n",
      "Epoch 4/20  Iteration 6635/35720 Training loss: 1.0990 0.2255 sec/batch\n",
      "Epoch 4/20  Iteration 6636/35720 Training loss: 1.0989 0.2204 sec/batch\n",
      "Epoch 4/20  Iteration 6637/35720 Training loss: 1.0989 0.2334 sec/batch\n",
      "Epoch 4/20  Iteration 6638/35720 Training loss: 1.0988 0.2109 sec/batch\n",
      "Epoch 4/20  Iteration 6639/35720 Training loss: 1.0987 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 6640/35720 Training loss: 1.0986 0.2143 sec/batch\n",
      "Epoch 4/20  Iteration 6641/35720 Training loss: 1.0985 0.2164 sec/batch\n",
      "Epoch 4/20  Iteration 6642/35720 Training loss: 1.0985 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 6643/35720 Training loss: 1.0984 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6644/35720 Training loss: 1.0983 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 6645/35720 Training loss: 1.0983 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 6646/35720 Training loss: 1.0983 0.2255 sec/batch\n",
      "Epoch 4/20  Iteration 6647/35720 Training loss: 1.0983 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 6648/35720 Training loss: 1.0983 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 6649/35720 Training loss: 1.0982 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6650/35720 Training loss: 1.0981 0.2253 sec/batch\n",
      "Epoch 4/20  Iteration 6651/35720 Training loss: 1.0982 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6652/35720 Training loss: 1.0983 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6653/35720 Training loss: 1.0983 0.2178 sec/batch\n",
      "Epoch 4/20  Iteration 6654/35720 Training loss: 1.0982 0.2221 sec/batch\n",
      "Epoch 4/20  Iteration 6655/35720 Training loss: 1.0983 0.2292 sec/batch\n",
      "Epoch 4/20  Iteration 6656/35720 Training loss: 1.0982 0.2120 sec/batch\n",
      "Epoch 4/20  Iteration 6657/35720 Training loss: 1.0981 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 6658/35720 Training loss: 1.0981 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 6659/35720 Training loss: 1.0980 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6660/35720 Training loss: 1.0980 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6661/35720 Training loss: 1.0980 0.2117 sec/batch\n",
      "Epoch 4/20  Iteration 6662/35720 Training loss: 1.0980 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 6663/35720 Training loss: 1.0979 0.2250 sec/batch\n",
      "Epoch 4/20  Iteration 6664/35720 Training loss: 1.0979 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 6665/35720 Training loss: 1.0978 0.2262 sec/batch\n",
      "Epoch 4/20  Iteration 6666/35720 Training loss: 1.0977 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 6667/35720 Training loss: 1.0977 0.2250 sec/batch\n",
      "Epoch 4/20  Iteration 6668/35720 Training loss: 1.0976 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 6669/35720 Training loss: 1.0975 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 6670/35720 Training loss: 1.0975 0.2185 sec/batch\n",
      "Epoch 4/20  Iteration 6671/35720 Training loss: 1.0975 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 6672/35720 Training loss: 1.0975 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6673/35720 Training loss: 1.0975 0.2247 sec/batch\n",
      "Epoch 4/20  Iteration 6674/35720 Training loss: 1.0974 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6675/35720 Training loss: 1.0974 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6676/35720 Training loss: 1.0974 0.2154 sec/batch\n",
      "Epoch 4/20  Iteration 6677/35720 Training loss: 1.0973 0.2109 sec/batch\n",
      "Epoch 4/20  Iteration 6678/35720 Training loss: 1.0973 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6679/35720 Training loss: 1.0973 0.2183 sec/batch\n",
      "Epoch 4/20  Iteration 6680/35720 Training loss: 1.0973 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 6681/35720 Training loss: 1.0973 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 6682/35720 Training loss: 1.0973 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 6683/35720 Training loss: 1.0973 0.2187 sec/batch\n",
      "Epoch 4/20  Iteration 6684/35720 Training loss: 1.0973 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 6685/35720 Training loss: 1.0973 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 6686/35720 Training loss: 1.0973 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 6687/35720 Training loss: 1.0972 0.2167 sec/batch\n",
      "Epoch 4/20  Iteration 6688/35720 Training loss: 1.0972 0.2143 sec/batch\n",
      "Epoch 4/20  Iteration 6689/35720 Training loss: 1.0971 0.2265 sec/batch\n",
      "Epoch 4/20  Iteration 6690/35720 Training loss: 1.0971 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 6691/35720 Training loss: 1.0971 0.2189 sec/batch\n",
      "Epoch 4/20  Iteration 6692/35720 Training loss: 1.0970 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6693/35720 Training loss: 1.0971 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6694/35720 Training loss: 1.0971 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6695/35720 Training loss: 1.0972 0.2139 sec/batch\n",
      "Epoch 4/20  Iteration 6696/35720 Training loss: 1.0972 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6697/35720 Training loss: 1.0971 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 6698/35720 Training loss: 1.0971 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6699/35720 Training loss: 1.0971 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 6700/35720 Training loss: 1.0971 0.2167 sec/batch\n",
      "Epoch 4/20  Iteration 6701/35720 Training loss: 1.0970 0.2176 sec/batch\n",
      "Epoch 4/20  Iteration 6702/35720 Training loss: 1.0970 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 6703/35720 Training loss: 1.0969 0.2112 sec/batch\n",
      "Epoch 4/20  Iteration 6704/35720 Training loss: 1.0969 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6705/35720 Training loss: 1.0969 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6706/35720 Training loss: 1.0969 0.2179 sec/batch\n",
      "Epoch 4/20  Iteration 6707/35720 Training loss: 1.0969 0.2067 sec/batch\n",
      "Epoch 4/20  Iteration 6708/35720 Training loss: 1.0970 0.2174 sec/batch\n",
      "Epoch 4/20  Iteration 6709/35720 Training loss: 1.0970 0.2234 sec/batch\n",
      "Epoch 4/20  Iteration 6710/35720 Training loss: 1.0971 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6711/35720 Training loss: 1.0971 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 6712/35720 Training loss: 1.0971 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 6713/35720 Training loss: 1.0970 0.2241 sec/batch\n",
      "Epoch 4/20  Iteration 6714/35720 Training loss: 1.0970 0.2172 sec/batch\n",
      "Epoch 4/20  Iteration 6715/35720 Training loss: 1.0969 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 6716/35720 Training loss: 1.0969 0.2107 sec/batch\n",
      "Epoch 4/20  Iteration 6717/35720 Training loss: 1.0968 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 6718/35720 Training loss: 1.0968 0.2251 sec/batch\n",
      "Epoch 4/20  Iteration 6719/35720 Training loss: 1.0967 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 6720/35720 Training loss: 1.0967 0.2061 sec/batch\n",
      "Epoch 4/20  Iteration 6721/35720 Training loss: 1.0967 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6722/35720 Training loss: 1.0967 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 6723/35720 Training loss: 1.0967 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 6724/35720 Training loss: 1.0966 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 6725/35720 Training loss: 1.0967 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6726/35720 Training loss: 1.0967 0.2080 sec/batch\n",
      "Epoch 4/20  Iteration 6727/35720 Training loss: 1.0967 0.2076 sec/batch\n",
      "Epoch 4/20  Iteration 6728/35720 Training loss: 1.0967 0.2138 sec/batch\n",
      "Epoch 4/20  Iteration 6729/35720 Training loss: 1.0967 0.2246 sec/batch\n",
      "Epoch 4/20  Iteration 6730/35720 Training loss: 1.0968 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6731/35720 Training loss: 1.0967 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6732/35720 Training loss: 1.0967 0.2114 sec/batch\n",
      "Epoch 4/20  Iteration 6733/35720 Training loss: 1.0967 0.2215 sec/batch\n",
      "Epoch 4/20  Iteration 6734/35720 Training loss: 1.0967 0.2075 sec/batch\n",
      "Epoch 4/20  Iteration 6735/35720 Training loss: 1.0966 0.2172 sec/batch\n",
      "Epoch 4/20  Iteration 6736/35720 Training loss: 1.0966 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6737/35720 Training loss: 1.0966 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 6738/35720 Training loss: 1.0966 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6739/35720 Training loss: 1.0966 0.2177 sec/batch\n",
      "Epoch 4/20  Iteration 6740/35720 Training loss: 1.0966 0.2164 sec/batch\n",
      "Epoch 4/20  Iteration 6741/35720 Training loss: 1.0966 0.2268 sec/batch\n",
      "Epoch 4/20  Iteration 6742/35720 Training loss: 1.0965 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6743/35720 Training loss: 1.0966 0.2182 sec/batch\n",
      "Epoch 4/20  Iteration 6744/35720 Training loss: 1.0966 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6745/35720 Training loss: 1.0966 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 6746/35720 Training loss: 1.0965 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 6747/35720 Training loss: 1.0965 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6748/35720 Training loss: 1.0964 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 6749/35720 Training loss: 1.0964 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 6750/35720 Training loss: 1.0963 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 6751/35720 Training loss: 1.0963 0.2205 sec/batch\n",
      "Epoch 4/20  Iteration 6752/35720 Training loss: 1.0962 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 6753/35720 Training loss: 1.0961 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 6754/35720 Training loss: 1.0961 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6755/35720 Training loss: 1.0960 0.2065 sec/batch\n",
      "Epoch 4/20  Iteration 6756/35720 Training loss: 1.0959 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6757/35720 Training loss: 1.0959 0.2150 sec/batch\n",
      "Epoch 4/20  Iteration 6758/35720 Training loss: 1.0958 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 6759/35720 Training loss: 1.0958 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 6760/35720 Training loss: 1.0957 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6761/35720 Training loss: 1.0957 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6762/35720 Training loss: 1.0957 0.2178 sec/batch\n",
      "Epoch 4/20  Iteration 6763/35720 Training loss: 1.0957 0.2248 sec/batch\n",
      "Epoch 4/20  Iteration 6764/35720 Training loss: 1.0957 0.2143 sec/batch\n",
      "Epoch 4/20  Iteration 6765/35720 Training loss: 1.0957 0.2213 sec/batch\n",
      "Epoch 4/20  Iteration 6766/35720 Training loss: 1.0957 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 6767/35720 Training loss: 1.0957 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 6768/35720 Training loss: 1.0957 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6769/35720 Training loss: 1.0957 0.2136 sec/batch\n",
      "Epoch 4/20  Iteration 6770/35720 Training loss: 1.0956 0.2185 sec/batch\n",
      "Epoch 4/20  Iteration 6771/35720 Training loss: 1.0956 0.2084 sec/batch\n",
      "Epoch 4/20  Iteration 6772/35720 Training loss: 1.0956 0.2312 sec/batch\n",
      "Epoch 4/20  Iteration 6773/35720 Training loss: 1.0956 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6774/35720 Training loss: 1.0956 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 6775/35720 Training loss: 1.0956 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6776/35720 Training loss: 1.0956 0.2225 sec/batch\n",
      "Epoch 4/20  Iteration 6777/35720 Training loss: 1.0956 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6778/35720 Training loss: 1.0957 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 6779/35720 Training loss: 1.0957 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 6780/35720 Training loss: 1.0956 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6781/35720 Training loss: 1.0956 0.2064 sec/batch\n",
      "Epoch 4/20  Iteration 6782/35720 Training loss: 1.0955 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 6783/35720 Training loss: 1.0956 0.2210 sec/batch\n",
      "Epoch 4/20  Iteration 6784/35720 Training loss: 1.0956 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 6785/35720 Training loss: 1.0956 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6786/35720 Training loss: 1.0956 0.2174 sec/batch\n",
      "Epoch 4/20  Iteration 6787/35720 Training loss: 1.0956 0.2078 sec/batch\n",
      "Epoch 4/20  Iteration 6788/35720 Training loss: 1.0956 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6789/35720 Training loss: 1.0955 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6790/35720 Training loss: 1.0955 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6791/35720 Training loss: 1.0954 0.2237 sec/batch\n",
      "Epoch 4/20  Iteration 6792/35720 Training loss: 1.0954 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 6793/35720 Training loss: 1.0954 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 6794/35720 Training loss: 1.0953 0.2209 sec/batch\n",
      "Epoch 4/20  Iteration 6795/35720 Training loss: 1.0952 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6796/35720 Training loss: 1.0953 0.2397 sec/batch\n",
      "Epoch 4/20  Iteration 6797/35720 Training loss: 1.0953 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6798/35720 Training loss: 1.0953 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 6799/35720 Training loss: 1.0954 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6800/35720 Training loss: 1.0953 0.2323 sec/batch\n",
      "Validation loss: 1.3113 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 6801/35720 Training loss: 1.0954 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 6802/35720 Training loss: 1.0954 0.2052 sec/batch\n",
      "Epoch 4/20  Iteration 6803/35720 Training loss: 1.0954 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 6804/35720 Training loss: 1.0953 0.2181 sec/batch\n",
      "Epoch 4/20  Iteration 6805/35720 Training loss: 1.0953 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 6806/35720 Training loss: 1.0953 0.2133 sec/batch\n",
      "Epoch 4/20  Iteration 6807/35720 Training loss: 1.0952 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6808/35720 Training loss: 1.0951 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6809/35720 Training loss: 1.0951 0.2149 sec/batch\n",
      "Epoch 4/20  Iteration 6810/35720 Training loss: 1.0950 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6811/35720 Training loss: 1.0950 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6812/35720 Training loss: 1.0950 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6813/35720 Training loss: 1.0950 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6814/35720 Training loss: 1.0950 0.2117 sec/batch\n",
      "Epoch 4/20  Iteration 6815/35720 Training loss: 1.0950 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6816/35720 Training loss: 1.0951 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6817/35720 Training loss: 1.0951 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 6818/35720 Training loss: 1.0952 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 6819/35720 Training loss: 1.0952 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 6820/35720 Training loss: 1.0952 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6821/35720 Training loss: 1.0952 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6822/35720 Training loss: 1.0952 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6823/35720 Training loss: 1.0951 0.2235 sec/batch\n",
      "Epoch 4/20  Iteration 6824/35720 Training loss: 1.0951 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6825/35720 Training loss: 1.0951 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6826/35720 Training loss: 1.0951 0.2142 sec/batch\n",
      "Epoch 4/20  Iteration 6827/35720 Training loss: 1.0950 0.2135 sec/batch\n",
      "Epoch 4/20  Iteration 6828/35720 Training loss: 1.0950 0.2257 sec/batch\n",
      "Epoch 4/20  Iteration 6829/35720 Training loss: 1.0950 0.2096 sec/batch\n",
      "Epoch 4/20  Iteration 6830/35720 Training loss: 1.0950 0.2210 sec/batch\n",
      "Epoch 4/20  Iteration 6831/35720 Training loss: 1.0949 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 6832/35720 Training loss: 1.0948 0.2268 sec/batch\n",
      "Epoch 4/20  Iteration 6833/35720 Training loss: 1.0947 0.2228 sec/batch\n",
      "Epoch 4/20  Iteration 6834/35720 Training loss: 1.0946 0.2268 sec/batch\n",
      "Epoch 4/20  Iteration 6835/35720 Training loss: 1.0945 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6836/35720 Training loss: 1.0945 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 6837/35720 Training loss: 1.0944 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 6838/35720 Training loss: 1.0944 0.2157 sec/batch\n",
      "Epoch 4/20  Iteration 6839/35720 Training loss: 1.0944 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6840/35720 Training loss: 1.0943 0.2140 sec/batch\n",
      "Epoch 4/20  Iteration 6841/35720 Training loss: 1.0942 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6842/35720 Training loss: 1.0942 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6843/35720 Training loss: 1.0941 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 6844/35720 Training loss: 1.0940 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6845/35720 Training loss: 1.0940 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 6846/35720 Training loss: 1.0940 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 6847/35720 Training loss: 1.0940 0.2168 sec/batch\n",
      "Epoch 4/20  Iteration 6848/35720 Training loss: 1.0940 0.2248 sec/batch\n",
      "Epoch 4/20  Iteration 6849/35720 Training loss: 1.0940 0.2117 sec/batch\n",
      "Epoch 4/20  Iteration 6850/35720 Training loss: 1.0939 0.2226 sec/batch\n",
      "Epoch 4/20  Iteration 6851/35720 Training loss: 1.0939 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 6852/35720 Training loss: 1.0939 0.2246 sec/batch\n",
      "Epoch 4/20  Iteration 6853/35720 Training loss: 1.0938 0.2109 sec/batch\n",
      "Epoch 4/20  Iteration 6854/35720 Training loss: 1.0938 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6855/35720 Training loss: 1.0937 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 6856/35720 Training loss: 1.0938 0.2181 sec/batch\n",
      "Epoch 4/20  Iteration 6857/35720 Training loss: 1.0938 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6858/35720 Training loss: 1.0938 0.2739 sec/batch\n",
      "Epoch 4/20  Iteration 6859/35720 Training loss: 1.0937 0.2202 sec/batch\n",
      "Epoch 4/20  Iteration 6860/35720 Training loss: 1.0936 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 6861/35720 Training loss: 1.0936 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 6862/35720 Training loss: 1.0936 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 6863/35720 Training loss: 1.0936 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 6864/35720 Training loss: 1.0937 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 6865/35720 Training loss: 1.0936 0.2053 sec/batch\n",
      "Epoch 4/20  Iteration 6866/35720 Training loss: 1.0936 0.2227 sec/batch\n",
      "Epoch 4/20  Iteration 6867/35720 Training loss: 1.0936 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 6868/35720 Training loss: 1.0936 0.2060 sec/batch\n",
      "Epoch 4/20  Iteration 6869/35720 Training loss: 1.0936 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6870/35720 Training loss: 1.0935 0.2253 sec/batch\n",
      "Epoch 4/20  Iteration 6871/35720 Training loss: 1.0935 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 6872/35720 Training loss: 1.0935 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6873/35720 Training loss: 1.0935 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6874/35720 Training loss: 1.0935 0.2074 sec/batch\n",
      "Epoch 4/20  Iteration 6875/35720 Training loss: 1.0935 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6876/35720 Training loss: 1.0935 0.2259 sec/batch\n",
      "Epoch 4/20  Iteration 6877/35720 Training loss: 1.0935 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 6878/35720 Training loss: 1.0935 0.2260 sec/batch\n",
      "Epoch 4/20  Iteration 6879/35720 Training loss: 1.0935 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6880/35720 Training loss: 1.0935 0.2218 sec/batch\n",
      "Epoch 4/20  Iteration 6881/35720 Training loss: 1.0936 0.2068 sec/batch\n",
      "Epoch 4/20  Iteration 6882/35720 Training loss: 1.0936 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6883/35720 Training loss: 1.0936 0.2065 sec/batch\n",
      "Epoch 4/20  Iteration 6884/35720 Training loss: 1.0936 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6885/35720 Training loss: 1.0936 0.2067 sec/batch\n",
      "Epoch 4/20  Iteration 6886/35720 Training loss: 1.0936 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 6887/35720 Training loss: 1.0936 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 6888/35720 Training loss: 1.0937 0.2125 sec/batch\n",
      "Epoch 4/20  Iteration 6889/35720 Training loss: 1.0937 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 6890/35720 Training loss: 1.0937 0.2149 sec/batch\n",
      "Epoch 4/20  Iteration 6891/35720 Training loss: 1.0937 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 6892/35720 Training loss: 1.0936 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 6893/35720 Training loss: 1.0936 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 6894/35720 Training loss: 1.0936 0.2244 sec/batch\n",
      "Epoch 4/20  Iteration 6895/35720 Training loss: 1.0935 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6896/35720 Training loss: 1.0935 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 6897/35720 Training loss: 1.0935 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6898/35720 Training loss: 1.0934 0.2102 sec/batch\n",
      "Epoch 4/20  Iteration 6899/35720 Training loss: 1.0933 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 6900/35720 Training loss: 1.0933 0.2205 sec/batch\n",
      "Epoch 4/20  Iteration 6901/35720 Training loss: 1.0932 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6902/35720 Training loss: 1.0932 0.2193 sec/batch\n",
      "Epoch 4/20  Iteration 6903/35720 Training loss: 1.0932 0.2118 sec/batch\n",
      "Epoch 4/20  Iteration 6904/35720 Training loss: 1.0931 0.2164 sec/batch\n",
      "Epoch 4/20  Iteration 6905/35720 Training loss: 1.0931 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6906/35720 Training loss: 1.0931 0.2079 sec/batch\n",
      "Epoch 4/20  Iteration 6907/35720 Training loss: 1.0931 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 6908/35720 Training loss: 1.0931 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6909/35720 Training loss: 1.0930 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 6910/35720 Training loss: 1.0929 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6911/35720 Training loss: 1.0929 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6912/35720 Training loss: 1.0929 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6913/35720 Training loss: 1.0928 0.2312 sec/batch\n",
      "Epoch 4/20  Iteration 6914/35720 Training loss: 1.0928 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6915/35720 Training loss: 1.0927 0.2243 sec/batch\n",
      "Epoch 4/20  Iteration 6916/35720 Training loss: 1.0926 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 6917/35720 Training loss: 1.0926 0.2293 sec/batch\n",
      "Epoch 4/20  Iteration 6918/35720 Training loss: 1.0925 0.2254 sec/batch\n",
      "Epoch 4/20  Iteration 6919/35720 Training loss: 1.0924 0.2186 sec/batch\n",
      "Epoch 4/20  Iteration 6920/35720 Training loss: 1.0924 0.2099 sec/batch\n",
      "Epoch 4/20  Iteration 6921/35720 Training loss: 1.0924 0.2226 sec/batch\n",
      "Epoch 4/20  Iteration 6922/35720 Training loss: 1.0923 0.2112 sec/batch\n",
      "Epoch 4/20  Iteration 6923/35720 Training loss: 1.0923 0.2076 sec/batch\n",
      "Epoch 4/20  Iteration 6924/35720 Training loss: 1.0923 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 6925/35720 Training loss: 1.0922 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6926/35720 Training loss: 1.0923 0.2275 sec/batch\n",
      "Epoch 4/20  Iteration 6927/35720 Training loss: 1.0922 0.2235 sec/batch\n",
      "Epoch 4/20  Iteration 6928/35720 Training loss: 1.0922 0.2209 sec/batch\n",
      "Epoch 4/20  Iteration 6929/35720 Training loss: 1.0922 0.2122 sec/batch\n",
      "Epoch 4/20  Iteration 6930/35720 Training loss: 1.0921 0.2406 sec/batch\n",
      "Epoch 4/20  Iteration 6931/35720 Training loss: 1.0921 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 6932/35720 Training loss: 1.0921 0.2089 sec/batch\n",
      "Epoch 4/20  Iteration 6933/35720 Training loss: 1.0920 0.2222 sec/batch\n",
      "Epoch 4/20  Iteration 6934/35720 Training loss: 1.0920 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6935/35720 Training loss: 1.0919 0.2169 sec/batch\n",
      "Epoch 4/20  Iteration 6936/35720 Training loss: 1.0919 0.2144 sec/batch\n",
      "Epoch 4/20  Iteration 6937/35720 Training loss: 1.0919 0.2268 sec/batch\n",
      "Epoch 4/20  Iteration 6938/35720 Training loss: 1.0918 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 6939/35720 Training loss: 1.0918 0.2112 sec/batch\n",
      "Epoch 4/20  Iteration 6940/35720 Training loss: 1.0918 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 6941/35720 Training loss: 1.0917 0.2332 sec/batch\n",
      "Epoch 4/20  Iteration 6942/35720 Training loss: 1.0916 0.2081 sec/batch\n",
      "Epoch 4/20  Iteration 6943/35720 Training loss: 1.0916 0.2330 sec/batch\n",
      "Epoch 4/20  Iteration 6944/35720 Training loss: 1.0916 0.2210 sec/batch\n",
      "Epoch 4/20  Iteration 6945/35720 Training loss: 1.0915 0.2251 sec/batch\n",
      "Epoch 4/20  Iteration 6946/35720 Training loss: 1.0915 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 6947/35720 Training loss: 1.0914 0.2196 sec/batch\n",
      "Epoch 4/20  Iteration 6948/35720 Training loss: 1.0914 0.2127 sec/batch\n",
      "Epoch 4/20  Iteration 6949/35720 Training loss: 1.0913 0.2243 sec/batch\n",
      "Epoch 4/20  Iteration 6950/35720 Training loss: 1.0913 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 6951/35720 Training loss: 1.0914 0.2193 sec/batch\n",
      "Epoch 4/20  Iteration 6952/35720 Training loss: 1.0913 0.2113 sec/batch\n",
      "Epoch 4/20  Iteration 6953/35720 Training loss: 1.0912 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 6954/35720 Training loss: 1.0911 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 6955/35720 Training loss: 1.0911 0.2072 sec/batch\n",
      "Epoch 4/20  Iteration 6956/35720 Training loss: 1.0911 0.2091 sec/batch\n",
      "Epoch 4/20  Iteration 6957/35720 Training loss: 1.0910 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 6958/35720 Training loss: 1.0910 0.2063 sec/batch\n",
      "Epoch 4/20  Iteration 6959/35720 Training loss: 1.0910 0.2119 sec/batch\n",
      "Epoch 4/20  Iteration 6960/35720 Training loss: 1.0910 0.2055 sec/batch\n",
      "Epoch 4/20  Iteration 6961/35720 Training loss: 1.0909 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 6962/35720 Training loss: 1.0909 0.2130 sec/batch\n",
      "Epoch 4/20  Iteration 6963/35720 Training loss: 1.0908 0.2063 sec/batch\n",
      "Epoch 4/20  Iteration 6964/35720 Training loss: 1.0908 0.2080 sec/batch\n",
      "Epoch 4/20  Iteration 6965/35720 Training loss: 1.0908 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6966/35720 Training loss: 1.0908 0.2087 sec/batch\n",
      "Epoch 4/20  Iteration 6967/35720 Training loss: 1.0907 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 6968/35720 Training loss: 1.0907 0.2298 sec/batch\n",
      "Epoch 4/20  Iteration 6969/35720 Training loss: 1.0906 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 6970/35720 Training loss: 1.0906 0.2107 sec/batch\n",
      "Epoch 4/20  Iteration 6971/35720 Training loss: 1.0905 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 6972/35720 Training loss: 1.0905 0.2203 sec/batch\n",
      "Epoch 4/20  Iteration 6973/35720 Training loss: 1.0905 0.2124 sec/batch\n",
      "Epoch 4/20  Iteration 6974/35720 Training loss: 1.0905 0.2229 sec/batch\n",
      "Epoch 4/20  Iteration 6975/35720 Training loss: 1.0904 0.2252 sec/batch\n",
      "Epoch 4/20  Iteration 6976/35720 Training loss: 1.0904 0.2164 sec/batch\n",
      "Epoch 4/20  Iteration 6977/35720 Training loss: 1.0903 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 6978/35720 Training loss: 1.0903 0.2248 sec/batch\n",
      "Epoch 4/20  Iteration 6979/35720 Training loss: 1.0902 0.2135 sec/batch\n",
      "Epoch 4/20  Iteration 6980/35720 Training loss: 1.0902 0.2277 sec/batch\n",
      "Epoch 4/20  Iteration 6981/35720 Training loss: 1.0902 0.2251 sec/batch\n",
      "Epoch 4/20  Iteration 6982/35720 Training loss: 1.0902 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 6983/35720 Training loss: 1.0902 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 6984/35720 Training loss: 1.0902 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 6985/35720 Training loss: 1.0903 0.2126 sec/batch\n",
      "Epoch 4/20  Iteration 6986/35720 Training loss: 1.0903 0.2123 sec/batch\n",
      "Epoch 4/20  Iteration 6987/35720 Training loss: 1.0902 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 6988/35720 Training loss: 1.0902 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 6989/35720 Training loss: 1.0902 0.2178 sec/batch\n",
      "Epoch 4/20  Iteration 6990/35720 Training loss: 1.0902 0.2162 sec/batch\n",
      "Epoch 4/20  Iteration 6991/35720 Training loss: 1.0901 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 6992/35720 Training loss: 1.0901 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 6993/35720 Training loss: 1.0901 0.2221 sec/batch\n",
      "Epoch 4/20  Iteration 6994/35720 Training loss: 1.0901 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6995/35720 Training loss: 1.0900 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 6996/35720 Training loss: 1.0900 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 6997/35720 Training loss: 1.0900 0.2055 sec/batch\n",
      "Epoch 4/20  Iteration 6998/35720 Training loss: 1.0899 0.2104 sec/batch\n",
      "Epoch 4/20  Iteration 6999/35720 Training loss: 1.0899 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 7000/35720 Training loss: 1.0899 0.2142 sec/batch\n",
      "Validation loss: 1.31651 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 7001/35720 Training loss: 1.0899 0.2087 sec/batch\n",
      "Epoch 4/20  Iteration 7002/35720 Training loss: 1.0899 0.2106 sec/batch\n",
      "Epoch 4/20  Iteration 7003/35720 Training loss: 1.0899 0.2180 sec/batch\n",
      "Epoch 4/20  Iteration 7004/35720 Training loss: 1.0899 0.2157 sec/batch\n",
      "Epoch 4/20  Iteration 7005/35720 Training loss: 1.0899 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 7006/35720 Training loss: 1.0899 0.2185 sec/batch\n",
      "Epoch 4/20  Iteration 7007/35720 Training loss: 1.0899 0.2158 sec/batch\n",
      "Epoch 4/20  Iteration 7008/35720 Training loss: 1.0899 0.2062 sec/batch\n",
      "Epoch 4/20  Iteration 7009/35720 Training loss: 1.0900 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 7010/35720 Training loss: 1.0899 0.2312 sec/batch\n",
      "Epoch 4/20  Iteration 7011/35720 Training loss: 1.0900 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 7012/35720 Training loss: 1.0899 0.2150 sec/batch\n",
      "Epoch 4/20  Iteration 7013/35720 Training loss: 1.0900 0.2097 sec/batch\n",
      "Epoch 4/20  Iteration 7014/35720 Training loss: 1.0900 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 7015/35720 Training loss: 1.0900 0.2197 sec/batch\n",
      "Epoch 4/20  Iteration 7016/35720 Training loss: 1.0900 0.2135 sec/batch\n",
      "Epoch 4/20  Iteration 7017/35720 Training loss: 1.0900 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 7018/35720 Training loss: 1.0899 0.2182 sec/batch\n",
      "Epoch 4/20  Iteration 7019/35720 Training loss: 1.0899 0.2112 sec/batch\n",
      "Epoch 4/20  Iteration 7020/35720 Training loss: 1.0899 0.2108 sec/batch\n",
      "Epoch 4/20  Iteration 7021/35720 Training loss: 1.0899 0.2135 sec/batch\n",
      "Epoch 4/20  Iteration 7022/35720 Training loss: 1.0898 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 7023/35720 Training loss: 1.0899 0.2249 sec/batch\n",
      "Epoch 4/20  Iteration 7024/35720 Training loss: 1.0899 0.2305 sec/batch\n",
      "Epoch 4/20  Iteration 7025/35720 Training loss: 1.0899 0.2077 sec/batch\n",
      "Epoch 4/20  Iteration 7026/35720 Training loss: 1.0900 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 7027/35720 Training loss: 1.0900 0.2235 sec/batch\n",
      "Epoch 4/20  Iteration 7028/35720 Training loss: 1.0900 0.2109 sec/batch\n",
      "Epoch 4/20  Iteration 7029/35720 Training loss: 1.0900 0.2155 sec/batch\n",
      "Epoch 4/20  Iteration 7030/35720 Training loss: 1.0900 0.2159 sec/batch\n",
      "Epoch 4/20  Iteration 7031/35720 Training loss: 1.0900 0.2248 sec/batch\n",
      "Epoch 4/20  Iteration 7032/35720 Training loss: 1.0901 0.2205 sec/batch\n",
      "Epoch 4/20  Iteration 7033/35720 Training loss: 1.0901 0.2250 sec/batch\n",
      "Epoch 4/20  Iteration 7034/35720 Training loss: 1.0901 0.2190 sec/batch\n",
      "Epoch 4/20  Iteration 7035/35720 Training loss: 1.0900 0.2265 sec/batch\n",
      "Epoch 4/20  Iteration 7036/35720 Training loss: 1.0900 0.2129 sec/batch\n",
      "Epoch 4/20  Iteration 7037/35720 Training loss: 1.0899 0.2255 sec/batch\n",
      "Epoch 4/20  Iteration 7038/35720 Training loss: 1.0899 0.2088 sec/batch\n",
      "Epoch 4/20  Iteration 7039/35720 Training loss: 1.0898 0.2146 sec/batch\n",
      "Epoch 4/20  Iteration 7040/35720 Training loss: 1.0898 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 7041/35720 Training loss: 1.0898 0.2250 sec/batch\n",
      "Epoch 4/20  Iteration 7042/35720 Training loss: 1.0898 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 7043/35720 Training loss: 1.0898 0.2151 sec/batch\n",
      "Epoch 4/20  Iteration 7044/35720 Training loss: 1.0898 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 7045/35720 Training loss: 1.0898 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 7046/35720 Training loss: 1.0898 0.2152 sec/batch\n",
      "Epoch 4/20  Iteration 7047/35720 Training loss: 1.0898 0.2093 sec/batch\n",
      "Epoch 4/20  Iteration 7048/35720 Training loss: 1.0897 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 7049/35720 Training loss: 1.0897 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 7050/35720 Training loss: 1.0896 0.2119 sec/batch\n",
      "Epoch 4/20  Iteration 7051/35720 Training loss: 1.0895 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 7052/35720 Training loss: 1.0895 0.2145 sec/batch\n",
      "Epoch 4/20  Iteration 7053/35720 Training loss: 1.0895 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 7054/35720 Training loss: 1.0895 0.2195 sec/batch\n",
      "Epoch 4/20  Iteration 7055/35720 Training loss: 1.0894 0.2092 sec/batch\n",
      "Epoch 4/20  Iteration 7056/35720 Training loss: 1.0894 0.2254 sec/batch\n",
      "Epoch 4/20  Iteration 7057/35720 Training loss: 1.0894 0.2149 sec/batch\n",
      "Epoch 4/20  Iteration 7058/35720 Training loss: 1.0894 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 7059/35720 Training loss: 1.0894 0.2098 sec/batch\n",
      "Epoch 4/20  Iteration 7060/35720 Training loss: 1.0893 0.2261 sec/batch\n",
      "Epoch 4/20  Iteration 7061/35720 Training loss: 1.0893 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 7062/35720 Training loss: 1.0892 0.2248 sec/batch\n",
      "Epoch 4/20  Iteration 7063/35720 Training loss: 1.0892 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 7064/35720 Training loss: 1.0891 0.2259 sec/batch\n",
      "Epoch 4/20  Iteration 7065/35720 Training loss: 1.0891 0.2087 sec/batch\n",
      "Epoch 4/20  Iteration 7066/35720 Training loss: 1.0890 0.2275 sec/batch\n",
      "Epoch 4/20  Iteration 7067/35720 Training loss: 1.0890 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 7068/35720 Training loss: 1.0891 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 7069/35720 Training loss: 1.0891 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 7070/35720 Training loss: 1.0891 0.2236 sec/batch\n",
      "Epoch 4/20  Iteration 7071/35720 Training loss: 1.0891 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 7072/35720 Training loss: 1.0891 0.2103 sec/batch\n",
      "Epoch 4/20  Iteration 7073/35720 Training loss: 1.0892 0.2262 sec/batch\n",
      "Epoch 4/20  Iteration 7074/35720 Training loss: 1.0892 0.2082 sec/batch\n",
      "Epoch 4/20  Iteration 7075/35720 Training loss: 1.0891 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 7076/35720 Training loss: 1.0891 0.2156 sec/batch\n",
      "Epoch 4/20  Iteration 7077/35720 Training loss: 1.0891 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 7078/35720 Training loss: 1.0891 0.2147 sec/batch\n",
      "Epoch 4/20  Iteration 7079/35720 Training loss: 1.0891 0.2220 sec/batch\n",
      "Epoch 4/20  Iteration 7080/35720 Training loss: 1.0891 0.2276 sec/batch\n",
      "Epoch 4/20  Iteration 7081/35720 Training loss: 1.0891 0.2226 sec/batch\n",
      "Epoch 4/20  Iteration 7082/35720 Training loss: 1.0890 0.2175 sec/batch\n",
      "Epoch 4/20  Iteration 7083/35720 Training loss: 1.0890 0.2180 sec/batch\n",
      "Epoch 4/20  Iteration 7084/35720 Training loss: 1.0890 0.2232 sec/batch\n",
      "Epoch 4/20  Iteration 7085/35720 Training loss: 1.0889 0.2136 sec/batch\n",
      "Epoch 4/20  Iteration 7086/35720 Training loss: 1.0889 0.2247 sec/batch\n",
      "Epoch 4/20  Iteration 7087/35720 Training loss: 1.0889 0.2137 sec/batch\n",
      "Epoch 4/20  Iteration 7088/35720 Training loss: 1.0888 0.2171 sec/batch\n",
      "Epoch 4/20  Iteration 7089/35720 Training loss: 1.0888 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 7090/35720 Training loss: 1.0889 0.2192 sec/batch\n",
      "Epoch 4/20  Iteration 7091/35720 Training loss: 1.0888 0.2115 sec/batch\n",
      "Epoch 4/20  Iteration 7092/35720 Training loss: 1.0887 0.2259 sec/batch\n",
      "Epoch 4/20  Iteration 7093/35720 Training loss: 1.0888 0.2085 sec/batch\n",
      "Epoch 4/20  Iteration 7094/35720 Training loss: 1.0887 0.2160 sec/batch\n",
      "Epoch 4/20  Iteration 7095/35720 Training loss: 1.0888 0.2090 sec/batch\n",
      "Epoch 4/20  Iteration 7096/35720 Training loss: 1.0887 0.2288 sec/batch\n",
      "Epoch 4/20  Iteration 7097/35720 Training loss: 1.0887 0.2100 sec/batch\n",
      "Epoch 4/20  Iteration 7098/35720 Training loss: 1.0887 0.2161 sec/batch\n",
      "Epoch 4/20  Iteration 7099/35720 Training loss: 1.0887 0.2238 sec/batch\n",
      "Epoch 4/20  Iteration 7100/35720 Training loss: 1.0887 0.2238 sec/batch\n",
      "Epoch 4/20  Iteration 7101/35720 Training loss: 1.0887 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 7102/35720 Training loss: 1.0887 0.2157 sec/batch\n",
      "Epoch 4/20  Iteration 7103/35720 Training loss: 1.0887 0.2086 sec/batch\n",
      "Epoch 4/20  Iteration 7104/35720 Training loss: 1.0888 0.2253 sec/batch\n",
      "Epoch 4/20  Iteration 7105/35720 Training loss: 1.0888 0.2192 sec/batch\n",
      "Epoch 4/20  Iteration 7106/35720 Training loss: 1.0888 0.2105 sec/batch\n",
      "Epoch 4/20  Iteration 7107/35720 Training loss: 1.0888 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 7108/35720 Training loss: 1.0887 0.2242 sec/batch\n",
      "Epoch 4/20  Iteration 7109/35720 Training loss: 1.0887 0.2109 sec/batch\n",
      "Epoch 4/20  Iteration 7110/35720 Training loss: 1.0886 0.2251 sec/batch\n",
      "Epoch 4/20  Iteration 7111/35720 Training loss: 1.0886 0.2153 sec/batch\n",
      "Epoch 4/20  Iteration 7112/35720 Training loss: 1.0886 0.2066 sec/batch\n",
      "Epoch 4/20  Iteration 7113/35720 Training loss: 1.0886 0.2184 sec/batch\n",
      "Epoch 4/20  Iteration 7114/35720 Training loss: 1.0886 0.2116 sec/batch\n",
      "Epoch 4/20  Iteration 7115/35720 Training loss: 1.0885 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 7116/35720 Training loss: 1.0885 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 7117/35720 Training loss: 1.0885 0.2114 sec/batch\n",
      "Epoch 4/20  Iteration 7118/35720 Training loss: 1.0885 0.2166 sec/batch\n",
      "Epoch 4/20  Iteration 7119/35720 Training loss: 1.0884 0.2128 sec/batch\n",
      "Epoch 4/20  Iteration 7120/35720 Training loss: 1.0884 0.2172 sec/batch\n",
      "Epoch 4/20  Iteration 7121/35720 Training loss: 1.0883 0.2132 sec/batch\n",
      "Epoch 4/20  Iteration 7122/35720 Training loss: 1.0883 0.2217 sec/batch\n",
      "Epoch 4/20  Iteration 7123/35720 Training loss: 1.0883 0.2083 sec/batch\n",
      "Epoch 4/20  Iteration 7124/35720 Training loss: 1.0882 0.2258 sec/batch\n",
      "Epoch 4/20  Iteration 7125/35720 Training loss: 1.0882 0.2163 sec/batch\n",
      "Epoch 4/20  Iteration 7126/35720 Training loss: 1.0882 0.2183 sec/batch\n",
      "Epoch 4/20  Iteration 7127/35720 Training loss: 1.0882 0.2121 sec/batch\n",
      "Epoch 4/20  Iteration 7128/35720 Training loss: 1.0882 0.2167 sec/batch\n",
      "Epoch 4/20  Iteration 7129/35720 Training loss: 1.0882 0.2200 sec/batch\n",
      "Epoch 4/20  Iteration 7130/35720 Training loss: 1.0881 0.2141 sec/batch\n",
      "Epoch 4/20  Iteration 7131/35720 Training loss: 1.0881 0.2193 sec/batch\n",
      "Epoch 4/20  Iteration 7132/35720 Training loss: 1.0880 0.2101 sec/batch\n",
      "Epoch 4/20  Iteration 7133/35720 Training loss: 1.0880 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 7134/35720 Training loss: 1.0880 0.2109 sec/batch\n",
      "Epoch 4/20  Iteration 7135/35720 Training loss: 1.0880 0.2094 sec/batch\n",
      "Epoch 4/20  Iteration 7136/35720 Training loss: 1.0879 0.2220 sec/batch\n",
      "Epoch 4/20  Iteration 7137/35720 Training loss: 1.0878 0.2110 sec/batch\n",
      "Epoch 4/20  Iteration 7138/35720 Training loss: 1.0878 0.2243 sec/batch\n",
      "Epoch 4/20  Iteration 7139/35720 Training loss: 1.0878 0.2134 sec/batch\n",
      "Epoch 4/20  Iteration 7140/35720 Training loss: 1.0877 0.2165 sec/batch\n",
      "Epoch 4/20  Iteration 7141/35720 Training loss: 1.0877 0.2095 sec/batch\n",
      "Epoch 4/20  Iteration 7142/35720 Training loss: 1.0877 0.2198 sec/batch\n",
      "Epoch 4/20  Iteration 7143/35720 Training loss: 1.0876 0.2131 sec/batch\n",
      "Epoch 4/20  Iteration 7144/35720 Training loss: 1.0876 0.2168 sec/batch\n",
      "Epoch 5/20  Iteration 7145/35720 Training loss: 1.1227 0.2183 sec/batch\n",
      "Epoch 5/20  Iteration 7146/35720 Training loss: 1.1035 0.2196 sec/batch\n",
      "Epoch 5/20  Iteration 7147/35720 Training loss: 1.0989 0.2059 sec/batch\n",
      "Epoch 5/20  Iteration 7148/35720 Training loss: 1.0867 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 7149/35720 Training loss: 1.0953 0.2187 sec/batch\n",
      "Epoch 5/20  Iteration 7150/35720 Training loss: 1.0783 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7151/35720 Training loss: 1.0755 0.2210 sec/batch\n",
      "Epoch 5/20  Iteration 7152/35720 Training loss: 1.0593 0.2204 sec/batch\n",
      "Epoch 5/20  Iteration 7153/35720 Training loss: 1.0535 0.2165 sec/batch\n",
      "Epoch 5/20  Iteration 7154/35720 Training loss: 1.0593 0.2177 sec/batch\n",
      "Epoch 5/20  Iteration 7155/35720 Training loss: 1.0619 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 7156/35720 Training loss: 1.0596 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7157/35720 Training loss: 1.0607 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 7158/35720 Training loss: 1.0686 0.2165 sec/batch\n",
      "Epoch 5/20  Iteration 7159/35720 Training loss: 1.0735 0.2168 sec/batch\n",
      "Epoch 5/20  Iteration 7160/35720 Training loss: 1.0748 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7161/35720 Training loss: 1.0764 0.2170 sec/batch\n",
      "Epoch 5/20  Iteration 7162/35720 Training loss: 1.0734 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 7163/35720 Training loss: 1.0718 0.2243 sec/batch\n",
      "Epoch 5/20  Iteration 7164/35720 Training loss: 1.0725 0.2235 sec/batch\n",
      "Epoch 5/20  Iteration 7165/35720 Training loss: 1.0752 0.2117 sec/batch\n",
      "Epoch 5/20  Iteration 7166/35720 Training loss: 1.0719 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7167/35720 Training loss: 1.0725 0.2104 sec/batch\n",
      "Epoch 5/20  Iteration 7168/35720 Training loss: 1.0738 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7169/35720 Training loss: 1.0771 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 7170/35720 Training loss: 1.0763 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 7171/35720 Training loss: 1.0809 0.2169 sec/batch\n",
      "Epoch 5/20  Iteration 7172/35720 Training loss: 1.0816 0.2277 sec/batch\n",
      "Epoch 5/20  Iteration 7173/35720 Training loss: 1.0802 0.2050 sec/batch\n",
      "Epoch 5/20  Iteration 7174/35720 Training loss: 1.0794 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 7175/35720 Training loss: 1.0857 0.2228 sec/batch\n",
      "Epoch 5/20  Iteration 7176/35720 Training loss: 1.0832 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7177/35720 Training loss: 1.0849 0.2230 sec/batch\n",
      "Epoch 5/20  Iteration 7178/35720 Training loss: 1.0873 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 7179/35720 Training loss: 1.0906 0.2153 sec/batch\n",
      "Epoch 5/20  Iteration 7180/35720 Training loss: 1.0905 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7181/35720 Training loss: 1.0894 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 7182/35720 Training loss: 1.0886 0.2139 sec/batch\n",
      "Epoch 5/20  Iteration 7183/35720 Training loss: 1.0856 0.2161 sec/batch\n",
      "Epoch 5/20  Iteration 7184/35720 Training loss: 1.0860 0.2128 sec/batch\n",
      "Epoch 5/20  Iteration 7185/35720 Training loss: 1.0846 0.2164 sec/batch\n",
      "Epoch 5/20  Iteration 7186/35720 Training loss: 1.0826 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 7187/35720 Training loss: 1.0794 0.2181 sec/batch\n",
      "Epoch 5/20  Iteration 7188/35720 Training loss: 1.0777 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 7189/35720 Training loss: 1.0769 0.2167 sec/batch\n",
      "Epoch 5/20  Iteration 7190/35720 Training loss: 1.0753 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 7191/35720 Training loss: 1.0743 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 7192/35720 Training loss: 1.0730 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 7193/35720 Training loss: 1.0739 0.2298 sec/batch\n",
      "Epoch 5/20  Iteration 7194/35720 Training loss: 1.0724 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 7195/35720 Training loss: 1.0725 0.2073 sec/batch\n",
      "Epoch 5/20  Iteration 7196/35720 Training loss: 1.0720 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 7197/35720 Training loss: 1.0714 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 7198/35720 Training loss: 1.0694 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 7199/35720 Training loss: 1.0685 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7200/35720 Training loss: 1.0681 0.2111 sec/batch\n",
      "Validation loss: 1.30188 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 7201/35720 Training loss: 1.0713 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7202/35720 Training loss: 1.0706 0.2059 sec/batch\n",
      "Epoch 5/20  Iteration 7203/35720 Training loss: 1.0690 0.2064 sec/batch\n",
      "Epoch 5/20  Iteration 7204/35720 Training loss: 1.0676 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 7205/35720 Training loss: 1.0658 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 7206/35720 Training loss: 1.0635 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 7207/35720 Training loss: 1.0637 0.2218 sec/batch\n",
      "Epoch 5/20  Iteration 7208/35720 Training loss: 1.0632 0.2235 sec/batch\n",
      "Epoch 5/20  Iteration 7209/35720 Training loss: 1.0638 0.2199 sec/batch\n",
      "Epoch 5/20  Iteration 7210/35720 Training loss: 1.0638 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 7211/35720 Training loss: 1.0636 0.2246 sec/batch\n",
      "Epoch 5/20  Iteration 7212/35720 Training loss: 1.0626 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 7213/35720 Training loss: 1.0633 0.2226 sec/batch\n",
      "Epoch 5/20  Iteration 7214/35720 Training loss: 1.0628 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 7215/35720 Training loss: 1.0632 0.2267 sec/batch\n",
      "Epoch 5/20  Iteration 7216/35720 Training loss: 1.0636 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 7217/35720 Training loss: 1.0639 0.2265 sec/batch\n",
      "Epoch 5/20  Iteration 7218/35720 Training loss: 1.0636 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 7219/35720 Training loss: 1.0620 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 7220/35720 Training loss: 1.0617 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7221/35720 Training loss: 1.0604 0.2258 sec/batch\n",
      "Epoch 5/20  Iteration 7222/35720 Training loss: 1.0614 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 7223/35720 Training loss: 1.0612 0.2234 sec/batch\n",
      "Epoch 5/20  Iteration 7224/35720 Training loss: 1.0636 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 7225/35720 Training loss: 1.0639 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 7226/35720 Training loss: 1.0634 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 7227/35720 Training loss: 1.0632 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 7228/35720 Training loss: 1.0635 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 7229/35720 Training loss: 1.0633 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 7230/35720 Training loss: 1.0631 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7231/35720 Training loss: 1.0632 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 7232/35720 Training loss: 1.0627 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 7233/35720 Training loss: 1.0615 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7234/35720 Training loss: 1.0604 0.2126 sec/batch\n",
      "Epoch 5/20  Iteration 7235/35720 Training loss: 1.0602 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 7236/35720 Training loss: 1.0594 0.2237 sec/batch\n",
      "Epoch 5/20  Iteration 7237/35720 Training loss: 1.0588 0.2128 sec/batch\n",
      "Epoch 5/20  Iteration 7238/35720 Training loss: 1.0584 0.2271 sec/batch\n",
      "Epoch 5/20  Iteration 7239/35720 Training loss: 1.0578 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 7240/35720 Training loss: 1.0569 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 7241/35720 Training loss: 1.0571 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 7242/35720 Training loss: 1.0568 0.2057 sec/batch\n",
      "Epoch 5/20  Iteration 7243/35720 Training loss: 1.0565 0.2173 sec/batch\n",
      "Epoch 5/20  Iteration 7244/35720 Training loss: 1.0561 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 7245/35720 Training loss: 1.0559 0.2208 sec/batch\n",
      "Epoch 5/20  Iteration 7246/35720 Training loss: 1.0562 0.2189 sec/batch\n",
      "Epoch 5/20  Iteration 7247/35720 Training loss: 1.0563 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 7248/35720 Training loss: 1.0562 0.2117 sec/batch\n",
      "Epoch 5/20  Iteration 7249/35720 Training loss: 1.0563 0.2078 sec/batch\n",
      "Epoch 5/20  Iteration 7250/35720 Training loss: 1.0561 0.2237 sec/batch\n",
      "Epoch 5/20  Iteration 7251/35720 Training loss: 1.0564 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7252/35720 Training loss: 1.0565 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 7253/35720 Training loss: 1.0571 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 7254/35720 Training loss: 1.0570 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 7255/35720 Training loss: 1.0572 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 7256/35720 Training loss: 1.0583 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 7257/35720 Training loss: 1.0584 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 7258/35720 Training loss: 1.0592 0.2178 sec/batch\n",
      "Epoch 5/20  Iteration 7259/35720 Training loss: 1.0589 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 7260/35720 Training loss: 1.0592 0.2249 sec/batch\n",
      "Epoch 5/20  Iteration 7261/35720 Training loss: 1.0592 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 7262/35720 Training loss: 1.0600 0.2164 sec/batch\n",
      "Epoch 5/20  Iteration 7263/35720 Training loss: 1.0602 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7264/35720 Training loss: 1.0610 0.2244 sec/batch\n",
      "Epoch 5/20  Iteration 7265/35720 Training loss: 1.0619 0.2112 sec/batch\n",
      "Epoch 5/20  Iteration 7266/35720 Training loss: 1.0617 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 7267/35720 Training loss: 1.0623 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 7268/35720 Training loss: 1.0629 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 7269/35720 Training loss: 1.0622 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 7270/35720 Training loss: 1.0622 0.2077 sec/batch\n",
      "Epoch 5/20  Iteration 7271/35720 Training loss: 1.0622 0.2181 sec/batch\n",
      "Epoch 5/20  Iteration 7272/35720 Training loss: 1.0622 0.2143 sec/batch\n",
      "Epoch 5/20  Iteration 7273/35720 Training loss: 1.0618 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 7274/35720 Training loss: 1.0615 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 7275/35720 Training loss: 1.0611 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 7276/35720 Training loss: 1.0608 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7277/35720 Training loss: 1.0606 0.2074 sec/batch\n",
      "Epoch 5/20  Iteration 7278/35720 Training loss: 1.0607 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 7279/35720 Training loss: 1.0606 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 7280/35720 Training loss: 1.0604 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 7281/35720 Training loss: 1.0615 0.2148 sec/batch\n",
      "Epoch 5/20  Iteration 7282/35720 Training loss: 1.0618 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7283/35720 Training loss: 1.0620 0.2259 sec/batch\n",
      "Epoch 5/20  Iteration 7284/35720 Training loss: 1.0621 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7285/35720 Training loss: 1.0615 0.2259 sec/batch\n",
      "Epoch 5/20  Iteration 7286/35720 Training loss: 1.0608 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7287/35720 Training loss: 1.0600 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 7288/35720 Training loss: 1.0593 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7289/35720 Training loss: 1.0591 0.2256 sec/batch\n",
      "Epoch 5/20  Iteration 7290/35720 Training loss: 1.0594 0.2079 sec/batch\n",
      "Epoch 5/20  Iteration 7291/35720 Training loss: 1.0589 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 7292/35720 Training loss: 1.0589 0.2244 sec/batch\n",
      "Epoch 5/20  Iteration 7293/35720 Training loss: 1.0588 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7294/35720 Training loss: 1.0579 0.2244 sec/batch\n",
      "Epoch 5/20  Iteration 7295/35720 Training loss: 1.0577 0.2207 sec/batch\n",
      "Epoch 5/20  Iteration 7296/35720 Training loss: 1.0578 0.2075 sec/batch\n",
      "Epoch 5/20  Iteration 7297/35720 Training loss: 1.0578 0.2178 sec/batch\n",
      "Epoch 5/20  Iteration 7298/35720 Training loss: 1.0583 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 7299/35720 Training loss: 1.0586 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 7300/35720 Training loss: 1.0588 0.2209 sec/batch\n",
      "Epoch 5/20  Iteration 7301/35720 Training loss: 1.0590 0.2139 sec/batch\n",
      "Epoch 5/20  Iteration 7302/35720 Training loss: 1.0593 0.2274 sec/batch\n",
      "Epoch 5/20  Iteration 7303/35720 Training loss: 1.0590 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 7304/35720 Training loss: 1.0595 0.2254 sec/batch\n",
      "Epoch 5/20  Iteration 7305/35720 Training loss: 1.0591 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 7306/35720 Training loss: 1.0591 0.2337 sec/batch\n",
      "Epoch 5/20  Iteration 7307/35720 Training loss: 1.0592 0.2275 sec/batch\n",
      "Epoch 5/20  Iteration 7308/35720 Training loss: 1.0594 0.2273 sec/batch\n",
      "Epoch 5/20  Iteration 7309/35720 Training loss: 1.0597 0.2233 sec/batch\n",
      "Epoch 5/20  Iteration 7310/35720 Training loss: 1.0600 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7311/35720 Training loss: 1.0601 0.2200 sec/batch\n",
      "Epoch 5/20  Iteration 7312/35720 Training loss: 1.0605 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7313/35720 Training loss: 1.0609 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 7314/35720 Training loss: 1.0611 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 7315/35720 Training loss: 1.0622 0.2170 sec/batch\n",
      "Epoch 5/20  Iteration 7316/35720 Training loss: 1.0627 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7317/35720 Training loss: 1.0629 0.2241 sec/batch\n",
      "Epoch 5/20  Iteration 7318/35720 Training loss: 1.0635 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7319/35720 Training loss: 1.0638 0.2266 sec/batch\n",
      "Epoch 5/20  Iteration 7320/35720 Training loss: 1.0635 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 7321/35720 Training loss: 1.0636 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 7322/35720 Training loss: 1.0634 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 7323/35720 Training loss: 1.0632 0.2227 sec/batch\n",
      "Epoch 5/20  Iteration 7324/35720 Training loss: 1.0629 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7325/35720 Training loss: 1.0632 0.2270 sec/batch\n",
      "Epoch 5/20  Iteration 7326/35720 Training loss: 1.0634 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 7327/35720 Training loss: 1.0635 0.2194 sec/batch\n",
      "Epoch 5/20  Iteration 7328/35720 Training loss: 1.0637 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7329/35720 Training loss: 1.0636 0.2205 sec/batch\n",
      "Epoch 5/20  Iteration 7330/35720 Training loss: 1.0634 0.2208 sec/batch\n",
      "Epoch 5/20  Iteration 7331/35720 Training loss: 1.0631 0.2105 sec/batch\n",
      "Epoch 5/20  Iteration 7332/35720 Training loss: 1.0632 0.2058 sec/batch\n",
      "Epoch 5/20  Iteration 7333/35720 Training loss: 1.0637 0.2112 sec/batch\n",
      "Epoch 5/20  Iteration 7334/35720 Training loss: 1.0637 0.2066 sec/batch\n",
      "Epoch 5/20  Iteration 7335/35720 Training loss: 1.0639 0.2145 sec/batch\n",
      "Epoch 5/20  Iteration 7336/35720 Training loss: 1.0647 0.2180 sec/batch\n",
      "Epoch 5/20  Iteration 7337/35720 Training loss: 1.0650 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 7338/35720 Training loss: 1.0653 0.2261 sec/batch\n",
      "Epoch 5/20  Iteration 7339/35720 Training loss: 1.0652 0.2144 sec/batch\n",
      "Epoch 5/20  Iteration 7340/35720 Training loss: 1.0655 0.2229 sec/batch\n",
      "Epoch 5/20  Iteration 7341/35720 Training loss: 1.0653 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 7342/35720 Training loss: 1.0653 0.2250 sec/batch\n",
      "Epoch 5/20  Iteration 7343/35720 Training loss: 1.0655 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 7344/35720 Training loss: 1.0658 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7345/35720 Training loss: 1.0655 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 7346/35720 Training loss: 1.0653 0.2310 sec/batch\n",
      "Epoch 5/20  Iteration 7347/35720 Training loss: 1.0652 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 7348/35720 Training loss: 1.0650 0.2270 sec/batch\n",
      "Epoch 5/20  Iteration 7349/35720 Training loss: 1.0649 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 7350/35720 Training loss: 1.0648 0.2185 sec/batch\n",
      "Epoch 5/20  Iteration 7351/35720 Training loss: 1.0654 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 7352/35720 Training loss: 1.0657 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 7353/35720 Training loss: 1.0660 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7354/35720 Training loss: 1.0660 0.2208 sec/batch\n",
      "Epoch 5/20  Iteration 7355/35720 Training loss: 1.0663 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 7356/35720 Training loss: 1.0661 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7357/35720 Training loss: 1.0661 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 7358/35720 Training loss: 1.0661 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7359/35720 Training loss: 1.0661 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 7360/35720 Training loss: 1.0661 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 7361/35720 Training loss: 1.0657 0.2055 sec/batch\n",
      "Epoch 5/20  Iteration 7362/35720 Training loss: 1.0656 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7363/35720 Training loss: 1.0656 0.2075 sec/batch\n",
      "Epoch 5/20  Iteration 7364/35720 Training loss: 1.0657 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 7365/35720 Training loss: 1.0657 0.2112 sec/batch\n",
      "Epoch 5/20  Iteration 7366/35720 Training loss: 1.0658 0.2235 sec/batch\n",
      "Epoch 5/20  Iteration 7367/35720 Training loss: 1.0663 0.2226 sec/batch\n",
      "Epoch 5/20  Iteration 7368/35720 Training loss: 1.0666 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 7369/35720 Training loss: 1.0667 0.2072 sec/batch\n",
      "Epoch 5/20  Iteration 7370/35720 Training loss: 1.0666 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 7371/35720 Training loss: 1.0665 0.2440 sec/batch\n",
      "Epoch 5/20  Iteration 7372/35720 Training loss: 1.0661 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 7373/35720 Training loss: 1.0658 0.2316 sec/batch\n",
      "Epoch 5/20  Iteration 7374/35720 Training loss: 1.0662 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7375/35720 Training loss: 1.0664 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7376/35720 Training loss: 1.0663 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 7377/35720 Training loss: 1.0663 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7378/35720 Training loss: 1.0664 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 7379/35720 Training loss: 1.0664 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 7380/35720 Training loss: 1.0662 0.2212 sec/batch\n",
      "Epoch 5/20  Iteration 7381/35720 Training loss: 1.0665 0.2191 sec/batch\n",
      "Epoch 5/20  Iteration 7382/35720 Training loss: 1.0664 0.2073 sec/batch\n",
      "Epoch 5/20  Iteration 7383/35720 Training loss: 1.0661 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 7384/35720 Training loss: 1.0662 0.2191 sec/batch\n",
      "Epoch 5/20  Iteration 7385/35720 Training loss: 1.0659 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 7386/35720 Training loss: 1.0659 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 7387/35720 Training loss: 1.0660 0.2157 sec/batch\n",
      "Epoch 5/20  Iteration 7388/35720 Training loss: 1.0660 0.2190 sec/batch\n",
      "Epoch 5/20  Iteration 7389/35720 Training loss: 1.0657 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7390/35720 Training loss: 1.0657 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7391/35720 Training loss: 1.0658 0.2301 sec/batch\n",
      "Epoch 5/20  Iteration 7392/35720 Training loss: 1.0657 0.2055 sec/batch\n",
      "Epoch 5/20  Iteration 7393/35720 Training loss: 1.0653 0.2168 sec/batch\n",
      "Epoch 5/20  Iteration 7394/35720 Training loss: 1.0652 0.2074 sec/batch\n",
      "Epoch 5/20  Iteration 7395/35720 Training loss: 1.0652 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 7396/35720 Training loss: 1.0650 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 7397/35720 Training loss: 1.0648 0.2144 sec/batch\n",
      "Epoch 5/20  Iteration 7398/35720 Training loss: 1.0650 0.2121 sec/batch\n",
      "Epoch 5/20  Iteration 7399/35720 Training loss: 1.0653 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7400/35720 Training loss: 1.0654 0.2136 sec/batch\n",
      "Validation loss: 1.30649 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 7401/35720 Training loss: 1.0661 0.2079 sec/batch\n",
      "Epoch 5/20  Iteration 7402/35720 Training loss: 1.0662 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 7403/35720 Training loss: 1.0666 0.2081 sec/batch\n",
      "Epoch 5/20  Iteration 7404/35720 Training loss: 1.0664 0.2306 sec/batch\n",
      "Epoch 5/20  Iteration 7405/35720 Training loss: 1.0663 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 7406/35720 Training loss: 1.0662 0.2224 sec/batch\n",
      "Epoch 5/20  Iteration 7407/35720 Training loss: 1.0661 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7408/35720 Training loss: 1.0662 0.2266 sec/batch\n",
      "Epoch 5/20  Iteration 7409/35720 Training loss: 1.0663 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 7410/35720 Training loss: 1.0664 0.2306 sec/batch\n",
      "Epoch 5/20  Iteration 7411/35720 Training loss: 1.0662 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 7412/35720 Training loss: 1.0662 0.2153 sec/batch\n",
      "Epoch 5/20  Iteration 7413/35720 Training loss: 1.0658 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 7414/35720 Training loss: 1.0651 0.2146 sec/batch\n",
      "Epoch 5/20  Iteration 7415/35720 Training loss: 1.0646 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 7416/35720 Training loss: 1.0646 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7417/35720 Training loss: 1.0646 0.2214 sec/batch\n",
      "Epoch 5/20  Iteration 7418/35720 Training loss: 1.0645 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7419/35720 Training loss: 1.0642 0.2185 sec/batch\n",
      "Epoch 5/20  Iteration 7420/35720 Training loss: 1.0641 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7421/35720 Training loss: 1.0637 0.2251 sec/batch\n",
      "Epoch 5/20  Iteration 7422/35720 Training loss: 1.0635 0.2215 sec/batch\n",
      "Epoch 5/20  Iteration 7423/35720 Training loss: 1.0632 0.2257 sec/batch\n",
      "Epoch 5/20  Iteration 7424/35720 Training loss: 1.0632 0.2118 sec/batch\n",
      "Epoch 5/20  Iteration 7425/35720 Training loss: 1.0632 0.2304 sec/batch\n",
      "Epoch 5/20  Iteration 7426/35720 Training loss: 1.0628 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 7427/35720 Training loss: 1.0624 0.2171 sec/batch\n",
      "Epoch 5/20  Iteration 7428/35720 Training loss: 1.0620 0.2127 sec/batch\n",
      "Epoch 5/20  Iteration 7429/35720 Training loss: 1.0623 0.2225 sec/batch\n",
      "Epoch 5/20  Iteration 7430/35720 Training loss: 1.0623 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 7431/35720 Training loss: 1.0618 0.2165 sec/batch\n",
      "Epoch 5/20  Iteration 7432/35720 Training loss: 1.0617 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7433/35720 Training loss: 1.0618 0.2222 sec/batch\n",
      "Epoch 5/20  Iteration 7434/35720 Training loss: 1.0619 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 7435/35720 Training loss: 1.0618 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 7436/35720 Training loss: 1.0618 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 7437/35720 Training loss: 1.0617 0.2166 sec/batch\n",
      "Epoch 5/20  Iteration 7438/35720 Training loss: 1.0616 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 7439/35720 Training loss: 1.0623 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 7440/35720 Training loss: 1.0623 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 7441/35720 Training loss: 1.0623 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7442/35720 Training loss: 1.0625 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 7443/35720 Training loss: 1.0624 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7444/35720 Training loss: 1.0623 0.2175 sec/batch\n",
      "Epoch 5/20  Iteration 7445/35720 Training loss: 1.0624 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7446/35720 Training loss: 1.0621 0.2167 sec/batch\n",
      "Epoch 5/20  Iteration 7447/35720 Training loss: 1.0621 0.2187 sec/batch\n",
      "Epoch 5/20  Iteration 7448/35720 Training loss: 1.0620 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 7449/35720 Training loss: 1.0619 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7450/35720 Training loss: 1.0617 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7451/35720 Training loss: 1.0619 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 7452/35720 Training loss: 1.0621 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 7453/35720 Training loss: 1.0618 0.2080 sec/batch\n",
      "Epoch 5/20  Iteration 7454/35720 Training loss: 1.0616 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7455/35720 Training loss: 1.0614 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 7456/35720 Training loss: 1.0614 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 7457/35720 Training loss: 1.0612 0.2232 sec/batch\n",
      "Epoch 5/20  Iteration 7458/35720 Training loss: 1.0610 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 7459/35720 Training loss: 1.0610 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7460/35720 Training loss: 1.0610 0.2214 sec/batch\n",
      "Epoch 5/20  Iteration 7461/35720 Training loss: 1.0608 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7462/35720 Training loss: 1.0609 0.2239 sec/batch\n",
      "Epoch 5/20  Iteration 7463/35720 Training loss: 1.0610 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 7464/35720 Training loss: 1.0608 0.2230 sec/batch\n",
      "Epoch 5/20  Iteration 7465/35720 Training loss: 1.0609 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 7466/35720 Training loss: 1.0607 0.2171 sec/batch\n",
      "Epoch 5/20  Iteration 7467/35720 Training loss: 1.0609 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 7468/35720 Training loss: 1.0609 0.2193 sec/batch\n",
      "Epoch 5/20  Iteration 7469/35720 Training loss: 1.0608 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 7470/35720 Training loss: 1.0610 0.2216 sec/batch\n",
      "Epoch 5/20  Iteration 7471/35720 Training loss: 1.0610 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 7472/35720 Training loss: 1.0611 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7473/35720 Training loss: 1.0611 0.2189 sec/batch\n",
      "Epoch 5/20  Iteration 7474/35720 Training loss: 1.0611 0.2057 sec/batch\n",
      "Epoch 5/20  Iteration 7475/35720 Training loss: 1.0612 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 7476/35720 Training loss: 1.0614 0.2071 sec/batch\n",
      "Epoch 5/20  Iteration 7477/35720 Training loss: 1.0613 0.2148 sec/batch\n",
      "Epoch 5/20  Iteration 7478/35720 Training loss: 1.0613 0.2064 sec/batch\n",
      "Epoch 5/20  Iteration 7479/35720 Training loss: 1.0611 0.2128 sec/batch\n",
      "Epoch 5/20  Iteration 7480/35720 Training loss: 1.0610 0.2049 sec/batch\n",
      "Epoch 5/20  Iteration 7481/35720 Training loss: 1.0609 0.2105 sec/batch\n",
      "Epoch 5/20  Iteration 7482/35720 Training loss: 1.0608 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 7483/35720 Training loss: 1.0608 0.2235 sec/batch\n",
      "Epoch 5/20  Iteration 7484/35720 Training loss: 1.0609 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7485/35720 Training loss: 1.0607 0.2235 sec/batch\n",
      "Epoch 5/20  Iteration 7486/35720 Training loss: 1.0606 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7487/35720 Training loss: 1.0607 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 7488/35720 Training loss: 1.0607 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7489/35720 Training loss: 1.0604 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 7490/35720 Training loss: 1.0606 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 7491/35720 Training loss: 1.0606 0.2154 sec/batch\n",
      "Epoch 5/20  Iteration 7492/35720 Training loss: 1.0607 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 7493/35720 Training loss: 1.0607 0.2203 sec/batch\n",
      "Epoch 5/20  Iteration 7494/35720 Training loss: 1.0607 0.2206 sec/batch\n",
      "Epoch 5/20  Iteration 7495/35720 Training loss: 1.0608 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7496/35720 Training loss: 1.0607 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7497/35720 Training loss: 1.0607 0.2182 sec/batch\n",
      "Epoch 5/20  Iteration 7498/35720 Training loss: 1.0606 0.2072 sec/batch\n",
      "Epoch 5/20  Iteration 7499/35720 Training loss: 1.0607 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7500/35720 Training loss: 1.0606 0.2212 sec/batch\n",
      "Epoch 5/20  Iteration 7501/35720 Training loss: 1.0607 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7502/35720 Training loss: 1.0607 0.2277 sec/batch\n",
      "Epoch 5/20  Iteration 7503/35720 Training loss: 1.0605 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 7504/35720 Training loss: 1.0604 0.2256 sec/batch\n",
      "Epoch 5/20  Iteration 7505/35720 Training loss: 1.0603 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7506/35720 Training loss: 1.0604 0.2243 sec/batch\n",
      "Epoch 5/20  Iteration 7507/35720 Training loss: 1.0603 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 7508/35720 Training loss: 1.0603 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 7509/35720 Training loss: 1.0602 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7510/35720 Training loss: 1.0601 0.2169 sec/batch\n",
      "Epoch 5/20  Iteration 7511/35720 Training loss: 1.0600 0.2157 sec/batch\n",
      "Epoch 5/20  Iteration 7512/35720 Training loss: 1.0597 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 7513/35720 Training loss: 1.0596 0.2176 sec/batch\n",
      "Epoch 5/20  Iteration 7514/35720 Training loss: 1.0595 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 7515/35720 Training loss: 1.0593 0.2052 sec/batch\n",
      "Epoch 5/20  Iteration 7516/35720 Training loss: 1.0592 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 7517/35720 Training loss: 1.0591 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 7518/35720 Training loss: 1.0590 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7519/35720 Training loss: 1.0589 0.2278 sec/batch\n",
      "Epoch 5/20  Iteration 7520/35720 Training loss: 1.0590 0.2180 sec/batch\n",
      "Epoch 5/20  Iteration 7521/35720 Training loss: 1.0590 0.2237 sec/batch\n",
      "Epoch 5/20  Iteration 7522/35720 Training loss: 1.0589 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7523/35720 Training loss: 1.0587 0.2116 sec/batch\n",
      "Epoch 5/20  Iteration 7524/35720 Training loss: 1.0584 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 7525/35720 Training loss: 1.0583 0.2180 sec/batch\n",
      "Epoch 5/20  Iteration 7526/35720 Training loss: 1.0583 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7527/35720 Training loss: 1.0581 0.2191 sec/batch\n",
      "Epoch 5/20  Iteration 7528/35720 Training loss: 1.0581 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 7529/35720 Training loss: 1.0580 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 7530/35720 Training loss: 1.0579 0.2258 sec/batch\n",
      "Epoch 5/20  Iteration 7531/35720 Training loss: 1.0580 0.2177 sec/batch\n",
      "Epoch 5/20  Iteration 7532/35720 Training loss: 1.0581 0.2079 sec/batch\n",
      "Epoch 5/20  Iteration 7533/35720 Training loss: 1.0582 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 7534/35720 Training loss: 1.0582 0.2720 sec/batch\n",
      "Epoch 5/20  Iteration 7535/35720 Training loss: 1.0582 0.2579 sec/batch\n",
      "Epoch 5/20  Iteration 7536/35720 Training loss: 1.0579 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7537/35720 Training loss: 1.0580 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 7538/35720 Training loss: 1.0577 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 7539/35720 Training loss: 1.0578 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 7540/35720 Training loss: 1.0576 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7541/35720 Training loss: 1.0576 0.2071 sec/batch\n",
      "Epoch 5/20  Iteration 7542/35720 Training loss: 1.0573 0.2186 sec/batch\n",
      "Epoch 5/20  Iteration 7543/35720 Training loss: 1.0572 0.2259 sec/batch\n",
      "Epoch 5/20  Iteration 7544/35720 Training loss: 1.0571 0.2261 sec/batch\n",
      "Epoch 5/20  Iteration 7545/35720 Training loss: 1.0570 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7546/35720 Training loss: 1.0572 0.2246 sec/batch\n",
      "Epoch 5/20  Iteration 7547/35720 Training loss: 1.0574 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7548/35720 Training loss: 1.0573 0.2257 sec/batch\n",
      "Epoch 5/20  Iteration 7549/35720 Training loss: 1.0572 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 7550/35720 Training loss: 1.0571 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 7551/35720 Training loss: 1.0570 0.2071 sec/batch\n",
      "Epoch 5/20  Iteration 7552/35720 Training loss: 1.0569 0.2118 sec/batch\n",
      "Epoch 5/20  Iteration 7553/35720 Training loss: 1.0567 0.2398 sec/batch\n",
      "Epoch 5/20  Iteration 7554/35720 Training loss: 1.0566 0.2145 sec/batch\n",
      "Epoch 5/20  Iteration 7555/35720 Training loss: 1.0567 0.2251 sec/batch\n",
      "Epoch 5/20  Iteration 7556/35720 Training loss: 1.0564 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7557/35720 Training loss: 1.0562 0.2078 sec/batch\n",
      "Epoch 5/20  Iteration 7558/35720 Training loss: 1.0561 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 7559/35720 Training loss: 1.0560 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 7560/35720 Training loss: 1.0560 0.2178 sec/batch\n",
      "Epoch 5/20  Iteration 7561/35720 Training loss: 1.0558 0.2073 sec/batch\n",
      "Epoch 5/20  Iteration 7562/35720 Training loss: 1.0557 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 7563/35720 Training loss: 1.0555 0.2080 sec/batch\n",
      "Epoch 5/20  Iteration 7564/35720 Training loss: 1.0555 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7565/35720 Training loss: 1.0556 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 7566/35720 Training loss: 1.0555 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 7567/35720 Training loss: 1.0557 0.2074 sec/batch\n",
      "Epoch 5/20  Iteration 7568/35720 Training loss: 1.0557 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 7569/35720 Training loss: 1.0558 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 7570/35720 Training loss: 1.0555 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 7571/35720 Training loss: 1.0554 0.2077 sec/batch\n",
      "Epoch 5/20  Iteration 7572/35720 Training loss: 1.0556 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 7573/35720 Training loss: 1.0556 0.2243 sec/batch\n",
      "Epoch 5/20  Iteration 7574/35720 Training loss: 1.0558 0.2075 sec/batch\n",
      "Epoch 5/20  Iteration 7575/35720 Training loss: 1.0559 0.2081 sec/batch\n",
      "Epoch 5/20  Iteration 7576/35720 Training loss: 1.0559 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 7577/35720 Training loss: 1.0562 0.2057 sec/batch\n",
      "Epoch 5/20  Iteration 7578/35720 Training loss: 1.0563 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 7579/35720 Training loss: 1.0564 0.2277 sec/batch\n",
      "Epoch 5/20  Iteration 7580/35720 Training loss: 1.0563 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7581/35720 Training loss: 1.0564 0.2161 sec/batch\n",
      "Epoch 5/20  Iteration 7582/35720 Training loss: 1.0566 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7583/35720 Training loss: 1.0568 0.2148 sec/batch\n",
      "Epoch 5/20  Iteration 7584/35720 Training loss: 1.0567 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 7585/35720 Training loss: 1.0571 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7586/35720 Training loss: 1.0573 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 7587/35720 Training loss: 1.0572 0.2165 sec/batch\n",
      "Epoch 5/20  Iteration 7588/35720 Training loss: 1.0573 0.2307 sec/batch\n",
      "Epoch 5/20  Iteration 7589/35720 Training loss: 1.0572 0.2107 sec/batch\n",
      "Epoch 5/20  Iteration 7590/35720 Training loss: 1.0573 0.2641 sec/batch\n",
      "Epoch 5/20  Iteration 7591/35720 Training loss: 1.0576 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 7592/35720 Training loss: 1.0577 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 7593/35720 Training loss: 1.0578 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 7594/35720 Training loss: 1.0577 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 7595/35720 Training loss: 1.0576 0.2126 sec/batch\n",
      "Epoch 5/20  Iteration 7596/35720 Training loss: 1.0575 0.2268 sec/batch\n",
      "Epoch 5/20  Iteration 7597/35720 Training loss: 1.0575 0.2195 sec/batch\n",
      "Epoch 5/20  Iteration 7598/35720 Training loss: 1.0576 0.2255 sec/batch\n",
      "Epoch 5/20  Iteration 7599/35720 Training loss: 1.0578 0.2215 sec/batch\n",
      "Epoch 5/20  Iteration 7600/35720 Training loss: 1.0579 0.2094 sec/batch\n",
      "Validation loss: 1.30122 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 7601/35720 Training loss: 1.0583 0.2078 sec/batch\n",
      "Epoch 5/20  Iteration 7602/35720 Training loss: 1.0585 0.2218 sec/batch\n",
      "Epoch 5/20  Iteration 7603/35720 Training loss: 1.0583 0.2182 sec/batch\n",
      "Epoch 5/20  Iteration 7604/35720 Training loss: 1.0582 0.2071 sec/batch\n",
      "Epoch 5/20  Iteration 7605/35720 Training loss: 1.0582 0.2074 sec/batch\n",
      "Epoch 5/20  Iteration 7606/35720 Training loss: 1.0582 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 7607/35720 Training loss: 1.0583 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 7608/35720 Training loss: 1.0582 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7609/35720 Training loss: 1.0582 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 7610/35720 Training loss: 1.0581 0.2073 sec/batch\n",
      "Epoch 5/20  Iteration 7611/35720 Training loss: 1.0580 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 7612/35720 Training loss: 1.0579 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7613/35720 Training loss: 1.0578 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7614/35720 Training loss: 1.0576 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 7615/35720 Training loss: 1.0574 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 7616/35720 Training loss: 1.0573 0.2215 sec/batch\n",
      "Epoch 5/20  Iteration 7617/35720 Training loss: 1.0573 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 7618/35720 Training loss: 1.0571 0.2311 sec/batch\n",
      "Epoch 5/20  Iteration 7619/35720 Training loss: 1.0571 0.2112 sec/batch\n",
      "Epoch 5/20  Iteration 7620/35720 Training loss: 1.0570 0.2219 sec/batch\n",
      "Epoch 5/20  Iteration 7621/35720 Training loss: 1.0571 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 7622/35720 Training loss: 1.0570 0.2157 sec/batch\n",
      "Epoch 5/20  Iteration 7623/35720 Training loss: 1.0571 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 7624/35720 Training loss: 1.0569 0.2266 sec/batch\n",
      "Epoch 5/20  Iteration 7625/35720 Training loss: 1.0568 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 7626/35720 Training loss: 1.0567 0.2287 sec/batch\n",
      "Epoch 5/20  Iteration 7627/35720 Training loss: 1.0567 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 7628/35720 Training loss: 1.0567 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7629/35720 Training loss: 1.0566 0.2223 sec/batch\n",
      "Epoch 5/20  Iteration 7630/35720 Training loss: 1.0564 0.2143 sec/batch\n",
      "Epoch 5/20  Iteration 7631/35720 Training loss: 1.0563 0.2344 sec/batch\n",
      "Epoch 5/20  Iteration 7632/35720 Training loss: 1.0562 0.2218 sec/batch\n",
      "Epoch 5/20  Iteration 7633/35720 Training loss: 1.0562 0.2161 sec/batch\n",
      "Epoch 5/20  Iteration 7634/35720 Training loss: 1.0560 0.2238 sec/batch\n",
      "Epoch 5/20  Iteration 7635/35720 Training loss: 1.0559 0.2240 sec/batch\n",
      "Epoch 5/20  Iteration 7636/35720 Training loss: 1.0560 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7637/35720 Training loss: 1.0559 0.2264 sec/batch\n",
      "Epoch 5/20  Iteration 7638/35720 Training loss: 1.0558 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 7639/35720 Training loss: 1.0557 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 7640/35720 Training loss: 1.0555 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 7641/35720 Training loss: 1.0555 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7642/35720 Training loss: 1.0556 0.2073 sec/batch\n",
      "Epoch 5/20  Iteration 7643/35720 Training loss: 1.0555 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 7644/35720 Training loss: 1.0556 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 7645/35720 Training loss: 1.0554 0.2389 sec/batch\n",
      "Epoch 5/20  Iteration 7646/35720 Training loss: 1.0551 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 7647/35720 Training loss: 1.0549 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7648/35720 Training loss: 1.0548 0.2181 sec/batch\n",
      "Epoch 5/20  Iteration 7649/35720 Training loss: 1.0549 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7650/35720 Training loss: 1.0548 0.2154 sec/batch\n",
      "Epoch 5/20  Iteration 7651/35720 Training loss: 1.0547 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7652/35720 Training loss: 1.0547 0.2117 sec/batch\n",
      "Epoch 5/20  Iteration 7653/35720 Training loss: 1.0547 0.2077 sec/batch\n",
      "Epoch 5/20  Iteration 7654/35720 Training loss: 1.0545 0.2183 sec/batch\n",
      "Epoch 5/20  Iteration 7655/35720 Training loss: 1.0543 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 7656/35720 Training loss: 1.0542 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 7657/35720 Training loss: 1.0541 0.2253 sec/batch\n",
      "Epoch 5/20  Iteration 7658/35720 Training loss: 1.0541 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 7659/35720 Training loss: 1.0542 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 7660/35720 Training loss: 1.0541 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7661/35720 Training loss: 1.0541 0.2139 sec/batch\n",
      "Epoch 5/20  Iteration 7662/35720 Training loss: 1.0541 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 7663/35720 Training loss: 1.0540 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7664/35720 Training loss: 1.0541 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 7665/35720 Training loss: 1.0540 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7666/35720 Training loss: 1.0540 0.2207 sec/batch\n",
      "Epoch 5/20  Iteration 7667/35720 Training loss: 1.0540 0.2126 sec/batch\n",
      "Epoch 5/20  Iteration 7668/35720 Training loss: 1.0539 0.2195 sec/batch\n",
      "Epoch 5/20  Iteration 7669/35720 Training loss: 1.0538 0.2075 sec/batch\n",
      "Epoch 5/20  Iteration 7670/35720 Training loss: 1.0538 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 7671/35720 Training loss: 1.0538 0.2107 sec/batch\n",
      "Epoch 5/20  Iteration 7672/35720 Training loss: 1.0536 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7673/35720 Training loss: 1.0536 0.2222 sec/batch\n",
      "Epoch 5/20  Iteration 7674/35720 Training loss: 1.0534 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7675/35720 Training loss: 1.0534 0.2297 sec/batch\n",
      "Epoch 5/20  Iteration 7676/35720 Training loss: 1.0534 0.2127 sec/batch\n",
      "Epoch 5/20  Iteration 7677/35720 Training loss: 1.0533 0.2254 sec/batch\n",
      "Epoch 5/20  Iteration 7678/35720 Training loss: 1.0534 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 7679/35720 Training loss: 1.0532 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 7680/35720 Training loss: 1.0531 0.2077 sec/batch\n",
      "Epoch 5/20  Iteration 7681/35720 Training loss: 1.0529 0.2073 sec/batch\n",
      "Epoch 5/20  Iteration 7682/35720 Training loss: 1.0528 0.2080 sec/batch\n",
      "Epoch 5/20  Iteration 7683/35720 Training loss: 1.0528 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 7684/35720 Training loss: 1.0527 0.2284 sec/batch\n",
      "Epoch 5/20  Iteration 7685/35720 Training loss: 1.0526 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7686/35720 Training loss: 1.0524 0.2251 sec/batch\n",
      "Epoch 5/20  Iteration 7687/35720 Training loss: 1.0523 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 7688/35720 Training loss: 1.0522 0.2240 sec/batch\n",
      "Epoch 5/20  Iteration 7689/35720 Training loss: 1.0523 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 7690/35720 Training loss: 1.0524 0.2242 sec/batch\n",
      "Epoch 5/20  Iteration 7691/35720 Training loss: 1.0523 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 7692/35720 Training loss: 1.0523 0.2179 sec/batch\n",
      "Epoch 5/20  Iteration 7693/35720 Training loss: 1.0521 0.2226 sec/batch\n",
      "Epoch 5/20  Iteration 7694/35720 Training loss: 1.0522 0.2215 sec/batch\n",
      "Epoch 5/20  Iteration 7695/35720 Training loss: 1.0521 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 7696/35720 Training loss: 1.0520 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 7697/35720 Training loss: 1.0519 0.2264 sec/batch\n",
      "Epoch 5/20  Iteration 7698/35720 Training loss: 1.0519 0.2078 sec/batch\n",
      "Epoch 5/20  Iteration 7699/35720 Training loss: 1.0518 0.2210 sec/batch\n",
      "Epoch 5/20  Iteration 7700/35720 Training loss: 1.0518 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7701/35720 Training loss: 1.0518 0.2214 sec/batch\n",
      "Epoch 5/20  Iteration 7702/35720 Training loss: 1.0519 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 7703/35720 Training loss: 1.0518 0.2238 sec/batch\n",
      "Epoch 5/20  Iteration 7704/35720 Training loss: 1.0516 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 7705/35720 Training loss: 1.0517 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 7706/35720 Training loss: 1.0516 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 7707/35720 Training loss: 1.0515 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 7708/35720 Training loss: 1.0514 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 7709/35720 Training loss: 1.0512 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7710/35720 Training loss: 1.0511 0.2054 sec/batch\n",
      "Epoch 5/20  Iteration 7711/35720 Training loss: 1.0512 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7712/35720 Training loss: 1.0510 0.2216 sec/batch\n",
      "Epoch 5/20  Iteration 7713/35720 Training loss: 1.0510 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7714/35720 Training loss: 1.0510 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 7715/35720 Training loss: 1.0510 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 7716/35720 Training loss: 1.0510 0.2184 sec/batch\n",
      "Epoch 5/20  Iteration 7717/35720 Training loss: 1.0512 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 7718/35720 Training loss: 1.0512 0.2266 sec/batch\n",
      "Epoch 5/20  Iteration 7719/35720 Training loss: 1.0512 0.2273 sec/batch\n",
      "Epoch 5/20  Iteration 7720/35720 Training loss: 1.0513 0.2245 sec/batch\n",
      "Epoch 5/20  Iteration 7721/35720 Training loss: 1.0514 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 7722/35720 Training loss: 1.0514 0.2206 sec/batch\n",
      "Epoch 5/20  Iteration 7723/35720 Training loss: 1.0513 0.2181 sec/batch\n",
      "Epoch 5/20  Iteration 7724/35720 Training loss: 1.0512 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 7725/35720 Training loss: 1.0512 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 7726/35720 Training loss: 1.0513 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 7727/35720 Training loss: 1.0513 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7728/35720 Training loss: 1.0513 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 7729/35720 Training loss: 1.0512 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 7730/35720 Training loss: 1.0510 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 7731/35720 Training loss: 1.0508 0.2078 sec/batch\n",
      "Epoch 5/20  Iteration 7732/35720 Training loss: 1.0508 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 7733/35720 Training loss: 1.0506 0.2178 sec/batch\n",
      "Epoch 5/20  Iteration 7734/35720 Training loss: 1.0505 0.2072 sec/batch\n",
      "Epoch 5/20  Iteration 7735/35720 Training loss: 1.0505 0.2145 sec/batch\n",
      "Epoch 5/20  Iteration 7736/35720 Training loss: 1.0505 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7737/35720 Training loss: 1.0505 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7738/35720 Training loss: 1.0505 0.2311 sec/batch\n",
      "Epoch 5/20  Iteration 7739/35720 Training loss: 1.0504 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 7740/35720 Training loss: 1.0503 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7741/35720 Training loss: 1.0502 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7742/35720 Training loss: 1.0504 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 7743/35720 Training loss: 1.0502 0.2146 sec/batch\n",
      "Epoch 5/20  Iteration 7744/35720 Training loss: 1.0500 0.2052 sec/batch\n",
      "Epoch 5/20  Iteration 7745/35720 Training loss: 1.0499 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 7746/35720 Training loss: 1.0497 0.2266 sec/batch\n",
      "Epoch 5/20  Iteration 7747/35720 Training loss: 1.0496 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7748/35720 Training loss: 1.0496 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 7749/35720 Training loss: 1.0496 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 7750/35720 Training loss: 1.0496 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 7751/35720 Training loss: 1.0497 0.2053 sec/batch\n",
      "Epoch 5/20  Iteration 7752/35720 Training loss: 1.0496 0.2194 sec/batch\n",
      "Epoch 5/20  Iteration 7753/35720 Training loss: 1.0498 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 7754/35720 Training loss: 1.0497 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 7755/35720 Training loss: 1.0497 0.2071 sec/batch\n",
      "Epoch 5/20  Iteration 7756/35720 Training loss: 1.0495 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 7757/35720 Training loss: 1.0495 0.2180 sec/batch\n",
      "Epoch 5/20  Iteration 7758/35720 Training loss: 1.0495 0.2210 sec/batch\n",
      "Epoch 5/20  Iteration 7759/35720 Training loss: 1.0495 0.2189 sec/batch\n",
      "Epoch 5/20  Iteration 7760/35720 Training loss: 1.0494 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7761/35720 Training loss: 1.0494 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 7762/35720 Training loss: 1.0493 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 7763/35720 Training loss: 1.0491 0.2139 sec/batch\n",
      "Epoch 5/20  Iteration 7764/35720 Training loss: 1.0490 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 7765/35720 Training loss: 1.0489 0.2117 sec/batch\n",
      "Epoch 5/20  Iteration 7766/35720 Training loss: 1.0489 0.2077 sec/batch\n",
      "Epoch 5/20  Iteration 7767/35720 Training loss: 1.0488 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 7768/35720 Training loss: 1.0487 0.2166 sec/batch\n",
      "Epoch 5/20  Iteration 7769/35720 Training loss: 1.0486 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 7770/35720 Training loss: 1.0486 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 7771/35720 Training loss: 1.0486 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 7772/35720 Training loss: 1.0484 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 7773/35720 Training loss: 1.0484 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7774/35720 Training loss: 1.0483 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 7775/35720 Training loss: 1.0484 0.2209 sec/batch\n",
      "Epoch 5/20  Iteration 7776/35720 Training loss: 1.0483 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 7777/35720 Training loss: 1.0482 0.2176 sec/batch\n",
      "Epoch 5/20  Iteration 7778/35720 Training loss: 1.0482 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7779/35720 Training loss: 1.0482 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 7780/35720 Training loss: 1.0482 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7781/35720 Training loss: 1.0481 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 7782/35720 Training loss: 1.0481 0.2121 sec/batch\n",
      "Epoch 5/20  Iteration 7783/35720 Training loss: 1.0481 0.2179 sec/batch\n",
      "Epoch 5/20  Iteration 7784/35720 Training loss: 1.0481 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 7785/35720 Training loss: 1.0482 0.2165 sec/batch\n",
      "Epoch 5/20  Iteration 7786/35720 Training loss: 1.0483 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 7787/35720 Training loss: 1.0483 0.2235 sec/batch\n",
      "Epoch 5/20  Iteration 7788/35720 Training loss: 1.0483 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 7789/35720 Training loss: 1.0483 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 7790/35720 Training loss: 1.0484 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 7791/35720 Training loss: 1.0483 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 7792/35720 Training loss: 1.0483 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 7793/35720 Training loss: 1.0482 0.2218 sec/batch\n",
      "Epoch 5/20  Iteration 7794/35720 Training loss: 1.0480 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7795/35720 Training loss: 1.0481 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 7796/35720 Training loss: 1.0481 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 7797/35720 Training loss: 1.0481 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 7798/35720 Training loss: 1.0483 0.2238 sec/batch\n",
      "Epoch 5/20  Iteration 7799/35720 Training loss: 1.0483 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 7800/35720 Training loss: 1.0483 0.2118 sec/batch\n",
      "Validation loss: 1.29443 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 7801/35720 Training loss: 1.0487 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7802/35720 Training loss: 1.0489 0.2064 sec/batch\n",
      "Epoch 5/20  Iteration 7803/35720 Training loss: 1.0489 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 7804/35720 Training loss: 1.0489 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 7805/35720 Training loss: 1.0489 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 7806/35720 Training loss: 1.0490 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 7807/35720 Training loss: 1.0490 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7808/35720 Training loss: 1.0490 0.2168 sec/batch\n",
      "Epoch 5/20  Iteration 7809/35720 Training loss: 1.0491 0.2067 sec/batch\n",
      "Epoch 5/20  Iteration 7810/35720 Training loss: 1.0492 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 7811/35720 Training loss: 1.0493 0.2224 sec/batch\n",
      "Epoch 5/20  Iteration 7812/35720 Training loss: 1.0492 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7813/35720 Training loss: 1.0491 0.2348 sec/batch\n",
      "Epoch 5/20  Iteration 7814/35720 Training loss: 1.0491 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 7815/35720 Training loss: 1.0488 0.2186 sec/batch\n",
      "Epoch 5/20  Iteration 7816/35720 Training loss: 1.0489 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 7817/35720 Training loss: 1.0489 0.2276 sec/batch\n",
      "Epoch 5/20  Iteration 7818/35720 Training loss: 1.0488 0.2052 sec/batch\n",
      "Epoch 5/20  Iteration 7819/35720 Training loss: 1.0487 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 7820/35720 Training loss: 1.0486 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 7821/35720 Training loss: 1.0486 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 7822/35720 Training loss: 1.0486 0.2107 sec/batch\n",
      "Epoch 5/20  Iteration 7823/35720 Training loss: 1.0486 0.2079 sec/batch\n",
      "Epoch 5/20  Iteration 7824/35720 Training loss: 1.0485 0.2056 sec/batch\n",
      "Epoch 5/20  Iteration 7825/35720 Training loss: 1.0486 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7826/35720 Training loss: 1.0485 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 7827/35720 Training loss: 1.0484 0.2245 sec/batch\n",
      "Epoch 5/20  Iteration 7828/35720 Training loss: 1.0483 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 7829/35720 Training loss: 1.0485 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 7830/35720 Training loss: 1.0484 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 7831/35720 Training loss: 1.0484 0.2247 sec/batch\n",
      "Epoch 5/20  Iteration 7832/35720 Training loss: 1.0484 0.2056 sec/batch\n",
      "Epoch 5/20  Iteration 7833/35720 Training loss: 1.0484 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 7834/35720 Training loss: 1.0483 0.2245 sec/batch\n",
      "Epoch 5/20  Iteration 7835/35720 Training loss: 1.0483 0.2164 sec/batch\n",
      "Epoch 5/20  Iteration 7836/35720 Training loss: 1.0485 0.2246 sec/batch\n",
      "Epoch 5/20  Iteration 7837/35720 Training loss: 1.0486 0.2181 sec/batch\n",
      "Epoch 5/20  Iteration 7838/35720 Training loss: 1.0486 0.2226 sec/batch\n",
      "Epoch 5/20  Iteration 7839/35720 Training loss: 1.0486 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 7840/35720 Training loss: 1.0486 0.2226 sec/batch\n",
      "Epoch 5/20  Iteration 7841/35720 Training loss: 1.0487 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 7842/35720 Training loss: 1.0486 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 7843/35720 Training loss: 1.0485 0.2178 sec/batch\n",
      "Epoch 5/20  Iteration 7844/35720 Training loss: 1.0485 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 7845/35720 Training loss: 1.0484 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 7846/35720 Training loss: 1.0485 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7847/35720 Training loss: 1.0484 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 7848/35720 Training loss: 1.0484 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 7849/35720 Training loss: 1.0484 0.2247 sec/batch\n",
      "Epoch 5/20  Iteration 7850/35720 Training loss: 1.0484 0.2127 sec/batch\n",
      "Epoch 5/20  Iteration 7851/35720 Training loss: 1.0485 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 7852/35720 Training loss: 1.0485 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 7853/35720 Training loss: 1.0486 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 7854/35720 Training loss: 1.0486 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 7855/35720 Training loss: 1.0487 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7856/35720 Training loss: 1.0487 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 7857/35720 Training loss: 1.0487 0.2112 sec/batch\n",
      "Epoch 5/20  Iteration 7858/35720 Training loss: 1.0486 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7859/35720 Training loss: 1.0487 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 7860/35720 Training loss: 1.0487 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7861/35720 Training loss: 1.0488 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 7862/35720 Training loss: 1.0489 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7863/35720 Training loss: 1.0488 0.2495 sec/batch\n",
      "Epoch 5/20  Iteration 7864/35720 Training loss: 1.0489 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 7865/35720 Training loss: 1.0489 0.2261 sec/batch\n",
      "Epoch 5/20  Iteration 7866/35720 Training loss: 1.0489 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 7867/35720 Training loss: 1.0491 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7868/35720 Training loss: 1.0490 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 7869/35720 Training loss: 1.0490 0.2126 sec/batch\n",
      "Epoch 5/20  Iteration 7870/35720 Training loss: 1.0489 0.2231 sec/batch\n",
      "Epoch 5/20  Iteration 7871/35720 Training loss: 1.0491 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 7872/35720 Training loss: 1.0491 0.2244 sec/batch\n",
      "Epoch 5/20  Iteration 7873/35720 Training loss: 1.0491 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 7874/35720 Training loss: 1.0491 0.2197 sec/batch\n",
      "Epoch 5/20  Iteration 7875/35720 Training loss: 1.0490 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7876/35720 Training loss: 1.0490 0.2199 sec/batch\n",
      "Epoch 5/20  Iteration 7877/35720 Training loss: 1.0490 0.2117 sec/batch\n",
      "Epoch 5/20  Iteration 7878/35720 Training loss: 1.0490 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 7879/35720 Training loss: 1.0490 0.2268 sec/batch\n",
      "Epoch 5/20  Iteration 7880/35720 Training loss: 1.0490 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 7881/35720 Training loss: 1.0491 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 7882/35720 Training loss: 1.0490 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7883/35720 Training loss: 1.0491 0.2151 sec/batch\n",
      "Epoch 5/20  Iteration 7884/35720 Training loss: 1.0492 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 7885/35720 Training loss: 1.0491 0.2267 sec/batch\n",
      "Epoch 5/20  Iteration 7886/35720 Training loss: 1.0491 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 7887/35720 Training loss: 1.0491 0.2199 sec/batch\n",
      "Epoch 5/20  Iteration 7888/35720 Training loss: 1.0491 0.2198 sec/batch\n",
      "Epoch 5/20  Iteration 7889/35720 Training loss: 1.0491 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7890/35720 Training loss: 1.0491 0.2121 sec/batch\n",
      "Epoch 5/20  Iteration 7891/35720 Training loss: 1.0491 0.2177 sec/batch\n",
      "Epoch 5/20  Iteration 7892/35720 Training loss: 1.0490 0.2116 sec/batch\n",
      "Epoch 5/20  Iteration 7893/35720 Training loss: 1.0489 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7894/35720 Training loss: 1.0489 0.2196 sec/batch\n",
      "Epoch 5/20  Iteration 7895/35720 Training loss: 1.0489 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 7896/35720 Training loss: 1.0491 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 7897/35720 Training loss: 1.0489 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7898/35720 Training loss: 1.0489 0.2066 sec/batch\n",
      "Epoch 5/20  Iteration 7899/35720 Training loss: 1.0488 0.2079 sec/batch\n",
      "Epoch 5/20  Iteration 7900/35720 Training loss: 1.0488 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 7901/35720 Training loss: 1.0487 0.2219 sec/batch\n",
      "Epoch 5/20  Iteration 7902/35720 Training loss: 1.0487 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 7903/35720 Training loss: 1.0488 0.2064 sec/batch\n",
      "Epoch 5/20  Iteration 7904/35720 Training loss: 1.0488 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7905/35720 Training loss: 1.0489 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 7906/35720 Training loss: 1.0489 0.2169 sec/batch\n",
      "Epoch 5/20  Iteration 7907/35720 Training loss: 1.0488 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 7908/35720 Training loss: 1.0487 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7909/35720 Training loss: 1.0487 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 7910/35720 Training loss: 1.0488 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 7911/35720 Training loss: 1.0488 0.2232 sec/batch\n",
      "Epoch 5/20  Iteration 7912/35720 Training loss: 1.0487 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 7913/35720 Training loss: 1.0487 0.2263 sec/batch\n",
      "Epoch 5/20  Iteration 7914/35720 Training loss: 1.0489 0.2075 sec/batch\n",
      "Epoch 5/20  Iteration 7915/35720 Training loss: 1.0489 0.2066 sec/batch\n",
      "Epoch 5/20  Iteration 7916/35720 Training loss: 1.0491 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 7917/35720 Training loss: 1.0490 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7918/35720 Training loss: 1.0489 0.2256 sec/batch\n",
      "Epoch 5/20  Iteration 7919/35720 Training loss: 1.0487 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7920/35720 Training loss: 1.0486 0.2464 sec/batch\n",
      "Epoch 5/20  Iteration 7921/35720 Training loss: 1.0486 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 7922/35720 Training loss: 1.0486 0.2244 sec/batch\n",
      "Epoch 5/20  Iteration 7923/35720 Training loss: 1.0487 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 7924/35720 Training loss: 1.0487 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 7925/35720 Training loss: 1.0486 0.2222 sec/batch\n",
      "Epoch 5/20  Iteration 7926/35720 Training loss: 1.0487 0.2146 sec/batch\n",
      "Epoch 5/20  Iteration 7927/35720 Training loss: 1.0487 0.2151 sec/batch\n",
      "Epoch 5/20  Iteration 7928/35720 Training loss: 1.0487 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 7929/35720 Training loss: 1.0486 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 7930/35720 Training loss: 1.0485 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 7931/35720 Training loss: 1.0486 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 7932/35720 Training loss: 1.0485 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7933/35720 Training loss: 1.0486 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7934/35720 Training loss: 1.0486 0.2279 sec/batch\n",
      "Epoch 5/20  Iteration 7935/35720 Training loss: 1.0487 0.2180 sec/batch\n",
      "Epoch 5/20  Iteration 7936/35720 Training loss: 1.0487 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 7937/35720 Training loss: 1.0487 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 7938/35720 Training loss: 1.0486 0.2251 sec/batch\n",
      "Epoch 5/20  Iteration 7939/35720 Training loss: 1.0486 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7940/35720 Training loss: 1.0486 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 7941/35720 Training loss: 1.0486 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 7942/35720 Training loss: 1.0485 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 7943/35720 Training loss: 1.0484 0.2217 sec/batch\n",
      "Epoch 5/20  Iteration 7944/35720 Training loss: 1.0485 0.2199 sec/batch\n",
      "Epoch 5/20  Iteration 7945/35720 Training loss: 1.0484 0.2056 sec/batch\n",
      "Epoch 5/20  Iteration 7946/35720 Training loss: 1.0486 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 7947/35720 Training loss: 1.0486 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7948/35720 Training loss: 1.0488 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 7949/35720 Training loss: 1.0489 0.2226 sec/batch\n",
      "Epoch 5/20  Iteration 7950/35720 Training loss: 1.0489 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 7951/35720 Training loss: 1.0489 0.2166 sec/batch\n",
      "Epoch 5/20  Iteration 7952/35720 Training loss: 1.0489 0.2197 sec/batch\n",
      "Epoch 5/20  Iteration 7953/35720 Training loss: 1.0490 0.2299 sec/batch\n",
      "Epoch 5/20  Iteration 7954/35720 Training loss: 1.0491 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 7955/35720 Training loss: 1.0491 0.2193 sec/batch\n",
      "Epoch 5/20  Iteration 7956/35720 Training loss: 1.0492 0.2054 sec/batch\n",
      "Epoch 5/20  Iteration 7957/35720 Training loss: 1.0493 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 7958/35720 Training loss: 1.0492 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 7959/35720 Training loss: 1.0493 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 7960/35720 Training loss: 1.0492 0.2270 sec/batch\n",
      "Epoch 5/20  Iteration 7961/35720 Training loss: 1.0492 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 7962/35720 Training loss: 1.0493 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 7963/35720 Training loss: 1.0493 0.2144 sec/batch\n",
      "Epoch 5/20  Iteration 7964/35720 Training loss: 1.0493 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 7965/35720 Training loss: 1.0493 0.2183 sec/batch\n",
      "Epoch 5/20  Iteration 7966/35720 Training loss: 1.0492 0.2067 sec/batch\n",
      "Epoch 5/20  Iteration 7967/35720 Training loss: 1.0491 0.2200 sec/batch\n",
      "Epoch 5/20  Iteration 7968/35720 Training loss: 1.0490 0.2057 sec/batch\n",
      "Epoch 5/20  Iteration 7969/35720 Training loss: 1.0489 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 7970/35720 Training loss: 1.0488 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 7971/35720 Training loss: 1.0488 0.2161 sec/batch\n",
      "Epoch 5/20  Iteration 7972/35720 Training loss: 1.0486 0.2164 sec/batch\n",
      "Epoch 5/20  Iteration 7973/35720 Training loss: 1.0486 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 7974/35720 Training loss: 1.0485 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 7975/35720 Training loss: 1.0484 0.2116 sec/batch\n",
      "Epoch 5/20  Iteration 7976/35720 Training loss: 1.0485 0.2173 sec/batch\n",
      "Epoch 5/20  Iteration 7977/35720 Training loss: 1.0487 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 7978/35720 Training loss: 1.0488 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 7979/35720 Training loss: 1.0487 0.2154 sec/batch\n",
      "Epoch 5/20  Iteration 7980/35720 Training loss: 1.0486 0.2212 sec/batch\n",
      "Epoch 5/20  Iteration 7981/35720 Training loss: 1.0487 0.2212 sec/batch\n",
      "Epoch 5/20  Iteration 7982/35720 Training loss: 1.0486 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 7983/35720 Training loss: 1.0487 0.2579 sec/batch\n",
      "Epoch 5/20  Iteration 7984/35720 Training loss: 1.0487 0.2274 sec/batch\n",
      "Epoch 5/20  Iteration 7985/35720 Training loss: 1.0488 0.2208 sec/batch\n",
      "Epoch 5/20  Iteration 7986/35720 Training loss: 1.0488 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 7987/35720 Training loss: 1.0488 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 7988/35720 Training loss: 1.0488 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 7989/35720 Training loss: 1.0488 0.2127 sec/batch\n",
      "Epoch 5/20  Iteration 7990/35720 Training loss: 1.0488 0.2205 sec/batch\n",
      "Epoch 5/20  Iteration 7991/35720 Training loss: 1.0487 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 7992/35720 Training loss: 1.0487 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 7993/35720 Training loss: 1.0486 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 7994/35720 Training loss: 1.0487 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 7995/35720 Training loss: 1.0487 0.2072 sec/batch\n",
      "Epoch 5/20  Iteration 7996/35720 Training loss: 1.0486 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 7997/35720 Training loss: 1.0486 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 7998/35720 Training loss: 1.0486 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 7999/35720 Training loss: 1.0486 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 8000/35720 Training loss: 1.0486 0.2158 sec/batch\n",
      "Validation loss: 1.30174 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 8001/35720 Training loss: 1.0487 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8002/35720 Training loss: 1.0487 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 8003/35720 Training loss: 1.0486 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 8004/35720 Training loss: 1.0486 0.2157 sec/batch\n",
      "Epoch 5/20  Iteration 8005/35720 Training loss: 1.0486 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8006/35720 Training loss: 1.0486 0.2219 sec/batch\n",
      "Epoch 5/20  Iteration 8007/35720 Training loss: 1.0485 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8008/35720 Training loss: 1.0485 0.2232 sec/batch\n",
      "Epoch 5/20  Iteration 8009/35720 Training loss: 1.0484 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 8010/35720 Training loss: 1.0485 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 8011/35720 Training loss: 1.0486 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 8012/35720 Training loss: 1.0486 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 8013/35720 Training loss: 1.0485 0.2055 sec/batch\n",
      "Epoch 5/20  Iteration 8014/35720 Training loss: 1.0485 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 8015/35720 Training loss: 1.0484 0.2279 sec/batch\n",
      "Epoch 5/20  Iteration 8016/35720 Training loss: 1.0483 0.2066 sec/batch\n",
      "Epoch 5/20  Iteration 8017/35720 Training loss: 1.0482 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8018/35720 Training loss: 1.0482 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 8019/35720 Training loss: 1.0481 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 8020/35720 Training loss: 1.0481 0.2154 sec/batch\n",
      "Epoch 5/20  Iteration 8021/35720 Training loss: 1.0480 0.2071 sec/batch\n",
      "Epoch 5/20  Iteration 8022/35720 Training loss: 1.0480 0.2074 sec/batch\n",
      "Epoch 5/20  Iteration 8023/35720 Training loss: 1.0479 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 8024/35720 Training loss: 1.0479 0.2071 sec/batch\n",
      "Epoch 5/20  Iteration 8025/35720 Training loss: 1.0480 0.2169 sec/batch\n",
      "Epoch 5/20  Iteration 8026/35720 Training loss: 1.0479 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8027/35720 Training loss: 1.0480 0.2193 sec/batch\n",
      "Epoch 5/20  Iteration 8028/35720 Training loss: 1.0479 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8029/35720 Training loss: 1.0480 0.2289 sec/batch\n",
      "Epoch 5/20  Iteration 8030/35720 Training loss: 1.0479 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 8031/35720 Training loss: 1.0479 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8032/35720 Training loss: 1.0479 0.2176 sec/batch\n",
      "Epoch 5/20  Iteration 8033/35720 Training loss: 1.0478 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8034/35720 Training loss: 1.0477 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 8035/35720 Training loss: 1.0477 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 8036/35720 Training loss: 1.0476 0.2213 sec/batch\n",
      "Epoch 5/20  Iteration 8037/35720 Training loss: 1.0475 0.2047 sec/batch\n",
      "Epoch 5/20  Iteration 8038/35720 Training loss: 1.0474 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 8039/35720 Training loss: 1.0474 0.2274 sec/batch\n",
      "Epoch 5/20  Iteration 8040/35720 Training loss: 1.0474 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 8041/35720 Training loss: 1.0472 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8042/35720 Training loss: 1.0471 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 8043/35720 Training loss: 1.0470 0.2180 sec/batch\n",
      "Epoch 5/20  Iteration 8044/35720 Training loss: 1.0469 0.2059 sec/batch\n",
      "Epoch 5/20  Iteration 8045/35720 Training loss: 1.0469 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 8046/35720 Training loss: 1.0468 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 8047/35720 Training loss: 1.0467 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 8048/35720 Training loss: 1.0467 0.2192 sec/batch\n",
      "Epoch 5/20  Iteration 8049/35720 Training loss: 1.0466 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 8050/35720 Training loss: 1.0466 0.2286 sec/batch\n",
      "Epoch 5/20  Iteration 8051/35720 Training loss: 1.0466 0.2056 sec/batch\n",
      "Epoch 5/20  Iteration 8052/35720 Training loss: 1.0467 0.2279 sec/batch\n",
      "Epoch 5/20  Iteration 8053/35720 Training loss: 1.0466 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 8054/35720 Training loss: 1.0466 0.2165 sec/batch\n",
      "Epoch 5/20  Iteration 8055/35720 Training loss: 1.0466 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 8056/35720 Training loss: 1.0466 0.2255 sec/batch\n",
      "Epoch 5/20  Iteration 8057/35720 Training loss: 1.0466 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 8058/35720 Training loss: 1.0466 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8059/35720 Training loss: 1.0467 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8060/35720 Training loss: 1.0467 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8061/35720 Training loss: 1.0467 0.2213 sec/batch\n",
      "Epoch 5/20  Iteration 8062/35720 Training loss: 1.0467 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8063/35720 Training loss: 1.0466 0.2271 sec/batch\n",
      "Epoch 5/20  Iteration 8064/35720 Training loss: 1.0466 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 8065/35720 Training loss: 1.0466 0.2303 sec/batch\n",
      "Epoch 5/20  Iteration 8066/35720 Training loss: 1.0465 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8067/35720 Training loss: 1.0465 0.2210 sec/batch\n",
      "Epoch 5/20  Iteration 8068/35720 Training loss: 1.0466 0.2057 sec/batch\n",
      "Epoch 5/20  Iteration 8069/35720 Training loss: 1.0466 0.2121 sec/batch\n",
      "Epoch 5/20  Iteration 8070/35720 Training loss: 1.0466 0.2226 sec/batch\n",
      "Epoch 5/20  Iteration 8071/35720 Training loss: 1.0467 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8072/35720 Training loss: 1.0466 0.2185 sec/batch\n",
      "Epoch 5/20  Iteration 8073/35720 Training loss: 1.0467 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8074/35720 Training loss: 1.0467 0.2283 sec/batch\n",
      "Epoch 5/20  Iteration 8075/35720 Training loss: 1.0468 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 8076/35720 Training loss: 1.0468 0.2199 sec/batch\n",
      "Epoch 5/20  Iteration 8077/35720 Training loss: 1.0468 0.2051 sec/batch\n",
      "Epoch 5/20  Iteration 8078/35720 Training loss: 1.0468 0.2081 sec/batch\n",
      "Epoch 5/20  Iteration 8079/35720 Training loss: 1.0466 0.2248 sec/batch\n",
      "Epoch 5/20  Iteration 8080/35720 Training loss: 1.0465 0.2192 sec/batch\n",
      "Epoch 5/20  Iteration 8081/35720 Training loss: 1.0464 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 8082/35720 Training loss: 1.0463 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 8083/35720 Training loss: 1.0463 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 8084/35720 Training loss: 1.0461 0.2133 sec/batch\n",
      "Epoch 5/20  Iteration 8085/35720 Training loss: 1.0461 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 8086/35720 Training loss: 1.0461 0.2116 sec/batch\n",
      "Epoch 5/20  Iteration 8087/35720 Training loss: 1.0461 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8088/35720 Training loss: 1.0462 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 8089/35720 Training loss: 1.0461 0.2128 sec/batch\n",
      "Epoch 5/20  Iteration 8090/35720 Training loss: 1.0461 0.2246 sec/batch\n",
      "Epoch 5/20  Iteration 8091/35720 Training loss: 1.0461 0.2193 sec/batch\n",
      "Epoch 5/20  Iteration 8092/35720 Training loss: 1.0460 0.2174 sec/batch\n",
      "Epoch 5/20  Iteration 8093/35720 Training loss: 1.0460 0.2057 sec/batch\n",
      "Epoch 5/20  Iteration 8094/35720 Training loss: 1.0459 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 8095/35720 Training loss: 1.0458 0.2200 sec/batch\n",
      "Epoch 5/20  Iteration 8096/35720 Training loss: 1.0457 0.2167 sec/batch\n",
      "Epoch 5/20  Iteration 8097/35720 Training loss: 1.0456 0.2240 sec/batch\n",
      "Epoch 5/20  Iteration 8098/35720 Training loss: 1.0457 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 8099/35720 Training loss: 1.0456 0.2164 sec/batch\n",
      "Epoch 5/20  Iteration 8100/35720 Training loss: 1.0455 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8101/35720 Training loss: 1.0455 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 8102/35720 Training loss: 1.0454 0.2296 sec/batch\n",
      "Epoch 5/20  Iteration 8103/35720 Training loss: 1.0454 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 8104/35720 Training loss: 1.0454 0.2186 sec/batch\n",
      "Epoch 5/20  Iteration 8105/35720 Training loss: 1.0453 0.2105 sec/batch\n",
      "Epoch 5/20  Iteration 8106/35720 Training loss: 1.0453 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8107/35720 Training loss: 1.0452 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 8108/35720 Training loss: 1.0452 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8109/35720 Training loss: 1.0452 0.2081 sec/batch\n",
      "Epoch 5/20  Iteration 8110/35720 Training loss: 1.0451 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 8111/35720 Training loss: 1.0452 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8112/35720 Training loss: 1.0451 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 8113/35720 Training loss: 1.0453 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 8114/35720 Training loss: 1.0455 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8115/35720 Training loss: 1.0454 0.2219 sec/batch\n",
      "Epoch 5/20  Iteration 8116/35720 Training loss: 1.0453 0.2143 sec/batch\n",
      "Epoch 5/20  Iteration 8117/35720 Training loss: 1.0453 0.2246 sec/batch\n",
      "Epoch 5/20  Iteration 8118/35720 Training loss: 1.0452 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 8119/35720 Training loss: 1.0451 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8120/35720 Training loss: 1.0451 0.2112 sec/batch\n",
      "Epoch 5/20  Iteration 8121/35720 Training loss: 1.0451 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 8122/35720 Training loss: 1.0452 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8123/35720 Training loss: 1.0451 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8124/35720 Training loss: 1.0451 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 8125/35720 Training loss: 1.0451 0.2052 sec/batch\n",
      "Epoch 5/20  Iteration 8126/35720 Training loss: 1.0449 0.2143 sec/batch\n",
      "Epoch 5/20  Iteration 8127/35720 Training loss: 1.0448 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 8128/35720 Training loss: 1.0448 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 8129/35720 Training loss: 1.0449 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 8130/35720 Training loss: 1.0448 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 8131/35720 Training loss: 1.0448 0.2243 sec/batch\n",
      "Epoch 5/20  Iteration 8132/35720 Training loss: 1.0448 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8133/35720 Training loss: 1.0448 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 8134/35720 Training loss: 1.0448 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 8135/35720 Training loss: 1.0447 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8136/35720 Training loss: 1.0448 0.2143 sec/batch\n",
      "Epoch 5/20  Iteration 8137/35720 Training loss: 1.0448 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 8138/35720 Training loss: 1.0448 0.2193 sec/batch\n",
      "Epoch 5/20  Iteration 8139/35720 Training loss: 1.0448 0.2067 sec/batch\n",
      "Epoch 5/20  Iteration 8140/35720 Training loss: 1.0448 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8141/35720 Training loss: 1.0449 0.2143 sec/batch\n",
      "Epoch 5/20  Iteration 8142/35720 Training loss: 1.0450 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8143/35720 Training loss: 1.0450 0.2143 sec/batch\n",
      "Epoch 5/20  Iteration 8144/35720 Training loss: 1.0449 0.2064 sec/batch\n",
      "Epoch 5/20  Iteration 8145/35720 Training loss: 1.0448 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 8146/35720 Training loss: 1.0447 0.2299 sec/batch\n",
      "Epoch 5/20  Iteration 8147/35720 Training loss: 1.0446 0.2054 sec/batch\n",
      "Epoch 5/20  Iteration 8148/35720 Training loss: 1.0445 0.2238 sec/batch\n",
      "Epoch 5/20  Iteration 8149/35720 Training loss: 1.0444 0.2126 sec/batch\n",
      "Epoch 5/20  Iteration 8150/35720 Training loss: 1.0444 0.2107 sec/batch\n",
      "Epoch 5/20  Iteration 8151/35720 Training loss: 1.0443 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 8152/35720 Training loss: 1.0442 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8153/35720 Training loss: 1.0441 0.2167 sec/batch\n",
      "Epoch 5/20  Iteration 8154/35720 Training loss: 1.0440 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 8155/35720 Training loss: 1.0440 0.2164 sec/batch\n",
      "Epoch 5/20  Iteration 8156/35720 Training loss: 1.0440 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8157/35720 Training loss: 1.0439 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8158/35720 Training loss: 1.0439 0.2227 sec/batch\n",
      "Epoch 5/20  Iteration 8159/35720 Training loss: 1.0439 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8160/35720 Training loss: 1.0439 0.2157 sec/batch\n",
      "Epoch 5/20  Iteration 8161/35720 Training loss: 1.0439 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8162/35720 Training loss: 1.0439 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 8163/35720 Training loss: 1.0438 0.2145 sec/batch\n",
      "Epoch 5/20  Iteration 8164/35720 Training loss: 1.0438 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8165/35720 Training loss: 1.0437 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 8166/35720 Training loss: 1.0438 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8167/35720 Training loss: 1.0439 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8168/35720 Training loss: 1.0439 0.2180 sec/batch\n",
      "Epoch 5/20  Iteration 8169/35720 Training loss: 1.0440 0.2225 sec/batch\n",
      "Epoch 5/20  Iteration 8170/35720 Training loss: 1.0439 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 8171/35720 Training loss: 1.0438 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8172/35720 Training loss: 1.0438 0.2187 sec/batch\n",
      "Epoch 5/20  Iteration 8173/35720 Training loss: 1.0437 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8174/35720 Training loss: 1.0437 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 8175/35720 Training loss: 1.0437 0.2249 sec/batch\n",
      "Epoch 5/20  Iteration 8176/35720 Training loss: 1.0437 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 8177/35720 Training loss: 1.0437 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 8178/35720 Training loss: 1.0437 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 8179/35720 Training loss: 1.0437 0.2157 sec/batch\n",
      "Epoch 5/20  Iteration 8180/35720 Training loss: 1.0437 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 8181/35720 Training loss: 1.0438 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8182/35720 Training loss: 1.0437 0.2077 sec/batch\n",
      "Epoch 5/20  Iteration 8183/35720 Training loss: 1.0437 0.2261 sec/batch\n",
      "Epoch 5/20  Iteration 8184/35720 Training loss: 1.0437 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 8185/35720 Training loss: 1.0437 0.2365 sec/batch\n",
      "Epoch 5/20  Iteration 8186/35720 Training loss: 1.0437 0.2194 sec/batch\n",
      "Epoch 5/20  Iteration 8187/35720 Training loss: 1.0437 0.2189 sec/batch\n",
      "Epoch 5/20  Iteration 8188/35720 Training loss: 1.0437 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 8189/35720 Training loss: 1.0437 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 8190/35720 Training loss: 1.0437 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 8191/35720 Training loss: 1.0436 0.2153 sec/batch\n",
      "Epoch 5/20  Iteration 8192/35720 Training loss: 1.0436 0.2144 sec/batch\n",
      "Epoch 5/20  Iteration 8193/35720 Training loss: 1.0436 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8194/35720 Training loss: 1.0436 0.2169 sec/batch\n",
      "Epoch 5/20  Iteration 8195/35720 Training loss: 1.0435 0.2075 sec/batch\n",
      "Epoch 5/20  Iteration 8196/35720 Training loss: 1.0436 0.2148 sec/batch\n",
      "Epoch 5/20  Iteration 8197/35720 Training loss: 1.0437 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 8198/35720 Training loss: 1.0437 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 8199/35720 Training loss: 1.0437 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 8200/35720 Training loss: 1.0437 0.2086 sec/batch\n",
      "Validation loss: 1.30723 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 8201/35720 Training loss: 1.0439 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8202/35720 Training loss: 1.0439 0.2146 sec/batch\n",
      "Epoch 5/20  Iteration 8203/35720 Training loss: 1.0439 0.2176 sec/batch\n",
      "Epoch 5/20  Iteration 8204/35720 Training loss: 1.0438 0.2215 sec/batch\n",
      "Epoch 5/20  Iteration 8205/35720 Training loss: 1.0438 0.2072 sec/batch\n",
      "Epoch 5/20  Iteration 8206/35720 Training loss: 1.0438 0.2067 sec/batch\n",
      "Epoch 5/20  Iteration 8207/35720 Training loss: 1.0439 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 8208/35720 Training loss: 1.0438 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8209/35720 Training loss: 1.0438 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 8210/35720 Training loss: 1.0438 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8211/35720 Training loss: 1.0438 0.2116 sec/batch\n",
      "Epoch 5/20  Iteration 8212/35720 Training loss: 1.0437 0.2221 sec/batch\n",
      "Epoch 5/20  Iteration 8213/35720 Training loss: 1.0438 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 8214/35720 Training loss: 1.0438 0.2284 sec/batch\n",
      "Epoch 5/20  Iteration 8215/35720 Training loss: 1.0437 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8216/35720 Training loss: 1.0437 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 8217/35720 Training loss: 1.0437 0.2058 sec/batch\n",
      "Epoch 5/20  Iteration 8218/35720 Training loss: 1.0437 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 8219/35720 Training loss: 1.0437 0.2253 sec/batch\n",
      "Epoch 5/20  Iteration 8220/35720 Training loss: 1.0437 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8221/35720 Training loss: 1.0437 0.2054 sec/batch\n",
      "Epoch 5/20  Iteration 8222/35720 Training loss: 1.0437 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 8223/35720 Training loss: 1.0437 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 8224/35720 Training loss: 1.0436 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 8225/35720 Training loss: 1.0436 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 8226/35720 Training loss: 1.0435 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 8227/35720 Training loss: 1.0435 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 8228/35720 Training loss: 1.0435 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8229/35720 Training loss: 1.0435 0.2220 sec/batch\n",
      "Epoch 5/20  Iteration 8230/35720 Training loss: 1.0435 0.2213 sec/batch\n",
      "Epoch 5/20  Iteration 8231/35720 Training loss: 1.0435 0.2112 sec/batch\n",
      "Epoch 5/20  Iteration 8232/35720 Training loss: 1.0435 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 8233/35720 Training loss: 1.0435 0.2055 sec/batch\n",
      "Epoch 5/20  Iteration 8234/35720 Training loss: 1.0435 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 8235/35720 Training loss: 1.0435 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8236/35720 Training loss: 1.0434 0.2154 sec/batch\n",
      "Epoch 5/20  Iteration 8237/35720 Training loss: 1.0435 0.2067 sec/batch\n",
      "Epoch 5/20  Iteration 8238/35720 Training loss: 1.0435 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8239/35720 Training loss: 1.0434 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8240/35720 Training loss: 1.0434 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8241/35720 Training loss: 1.0433 0.2246 sec/batch\n",
      "Epoch 5/20  Iteration 8242/35720 Training loss: 1.0433 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 8243/35720 Training loss: 1.0434 0.2170 sec/batch\n",
      "Epoch 5/20  Iteration 8244/35720 Training loss: 1.0433 0.2133 sec/batch\n",
      "Epoch 5/20  Iteration 8245/35720 Training loss: 1.0434 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 8246/35720 Training loss: 1.0434 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8247/35720 Training loss: 1.0434 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 8248/35720 Training loss: 1.0434 0.2146 sec/batch\n",
      "Epoch 5/20  Iteration 8249/35720 Training loss: 1.0434 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 8250/35720 Training loss: 1.0434 0.2153 sec/batch\n",
      "Epoch 5/20  Iteration 8251/35720 Training loss: 1.0434 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8252/35720 Training loss: 1.0434 0.2200 sec/batch\n",
      "Epoch 5/20  Iteration 8253/35720 Training loss: 1.0434 0.2080 sec/batch\n",
      "Epoch 5/20  Iteration 8254/35720 Training loss: 1.0434 0.2074 sec/batch\n",
      "Epoch 5/20  Iteration 8255/35720 Training loss: 1.0434 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 8256/35720 Training loss: 1.0433 0.2167 sec/batch\n",
      "Epoch 5/20  Iteration 8257/35720 Training loss: 1.0432 0.2066 sec/batch\n",
      "Epoch 5/20  Iteration 8258/35720 Training loss: 1.0433 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 8259/35720 Training loss: 1.0433 0.2216 sec/batch\n",
      "Epoch 5/20  Iteration 8260/35720 Training loss: 1.0432 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 8261/35720 Training loss: 1.0432 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8262/35720 Training loss: 1.0431 0.2204 sec/batch\n",
      "Epoch 5/20  Iteration 8263/35720 Training loss: 1.0431 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8264/35720 Training loss: 1.0431 0.2258 sec/batch\n",
      "Epoch 5/20  Iteration 8265/35720 Training loss: 1.0432 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 8266/35720 Training loss: 1.0433 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 8267/35720 Training loss: 1.0434 0.2187 sec/batch\n",
      "Epoch 5/20  Iteration 8268/35720 Training loss: 1.0434 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 8269/35720 Training loss: 1.0434 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 8270/35720 Training loss: 1.0433 0.2104 sec/batch\n",
      "Epoch 5/20  Iteration 8271/35720 Training loss: 1.0433 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8272/35720 Training loss: 1.0433 0.2218 sec/batch\n",
      "Epoch 5/20  Iteration 8273/35720 Training loss: 1.0432 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8274/35720 Training loss: 1.0432 0.2048 sec/batch\n",
      "Epoch 5/20  Iteration 8275/35720 Training loss: 1.0432 0.2252 sec/batch\n",
      "Epoch 5/20  Iteration 8276/35720 Training loss: 1.0431 0.2170 sec/batch\n",
      "Epoch 5/20  Iteration 8277/35720 Training loss: 1.0431 0.2218 sec/batch\n",
      "Epoch 5/20  Iteration 8278/35720 Training loss: 1.0432 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 8279/35720 Training loss: 1.0431 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 8280/35720 Training loss: 1.0430 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 8281/35720 Training loss: 1.0430 0.2276 sec/batch\n",
      "Epoch 5/20  Iteration 8282/35720 Training loss: 1.0429 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 8283/35720 Training loss: 1.0428 0.2187 sec/batch\n",
      "Epoch 5/20  Iteration 8284/35720 Training loss: 1.0428 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8285/35720 Training loss: 1.0428 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8286/35720 Training loss: 1.0427 0.2198 sec/batch\n",
      "Epoch 5/20  Iteration 8287/35720 Training loss: 1.0426 0.2066 sec/batch\n",
      "Epoch 5/20  Iteration 8288/35720 Training loss: 1.0426 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8289/35720 Training loss: 1.0425 0.2243 sec/batch\n",
      "Epoch 5/20  Iteration 8290/35720 Training loss: 1.0424 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8291/35720 Training loss: 1.0424 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 8292/35720 Training loss: 1.0423 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 8293/35720 Training loss: 1.0423 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 8294/35720 Training loss: 1.0423 0.2213 sec/batch\n",
      "Epoch 5/20  Iteration 8295/35720 Training loss: 1.0423 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 8296/35720 Training loss: 1.0423 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8297/35720 Training loss: 1.0423 0.2118 sec/batch\n",
      "Epoch 5/20  Iteration 8298/35720 Training loss: 1.0423 0.2256 sec/batch\n",
      "Epoch 5/20  Iteration 8299/35720 Training loss: 1.0423 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 8300/35720 Training loss: 1.0422 0.2186 sec/batch\n",
      "Epoch 5/20  Iteration 8301/35720 Training loss: 1.0422 0.2077 sec/batch\n",
      "Epoch 5/20  Iteration 8302/35720 Training loss: 1.0421 0.2224 sec/batch\n",
      "Epoch 5/20  Iteration 8303/35720 Training loss: 1.0421 0.2253 sec/batch\n",
      "Epoch 5/20  Iteration 8304/35720 Training loss: 1.0421 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8305/35720 Training loss: 1.0420 0.2224 sec/batch\n",
      "Epoch 5/20  Iteration 8306/35720 Training loss: 1.0419 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8307/35720 Training loss: 1.0421 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 8308/35720 Training loss: 1.0421 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8309/35720 Training loss: 1.0421 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 8310/35720 Training loss: 1.0420 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8311/35720 Training loss: 1.0419 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 8312/35720 Training loss: 1.0420 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 8313/35720 Training loss: 1.0420 0.2209 sec/batch\n",
      "Epoch 5/20  Iteration 8314/35720 Training loss: 1.0421 0.2139 sec/batch\n",
      "Epoch 5/20  Iteration 8315/35720 Training loss: 1.0420 0.2144 sec/batch\n",
      "Epoch 5/20  Iteration 8316/35720 Training loss: 1.0420 0.2151 sec/batch\n",
      "Epoch 5/20  Iteration 8317/35720 Training loss: 1.0419 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 8318/35720 Training loss: 1.0419 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 8319/35720 Training loss: 1.0419 0.2253 sec/batch\n",
      "Epoch 5/20  Iteration 8320/35720 Training loss: 1.0420 0.2067 sec/batch\n",
      "Epoch 5/20  Iteration 8321/35720 Training loss: 1.0420 0.2117 sec/batch\n",
      "Epoch 5/20  Iteration 8322/35720 Training loss: 1.0421 0.2261 sec/batch\n",
      "Epoch 5/20  Iteration 8323/35720 Training loss: 1.0420 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8324/35720 Training loss: 1.0420 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 8325/35720 Training loss: 1.0421 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8326/35720 Training loss: 1.0421 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 8327/35720 Training loss: 1.0420 0.2217 sec/batch\n",
      "Epoch 5/20  Iteration 8328/35720 Training loss: 1.0419 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 8329/35720 Training loss: 1.0419 0.2261 sec/batch\n",
      "Epoch 5/20  Iteration 8330/35720 Training loss: 1.0418 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8331/35720 Training loss: 1.0418 0.2279 sec/batch\n",
      "Epoch 5/20  Iteration 8332/35720 Training loss: 1.0418 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8333/35720 Training loss: 1.0417 0.2171 sec/batch\n",
      "Epoch 5/20  Iteration 8334/35720 Training loss: 1.0417 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 8335/35720 Training loss: 1.0417 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 8336/35720 Training loss: 1.0418 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 8337/35720 Training loss: 1.0417 0.2121 sec/batch\n",
      "Epoch 5/20  Iteration 8338/35720 Training loss: 1.0417 0.2133 sec/batch\n",
      "Epoch 5/20  Iteration 8339/35720 Training loss: 1.0418 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 8340/35720 Training loss: 1.0418 0.2306 sec/batch\n",
      "Epoch 5/20  Iteration 8341/35720 Training loss: 1.0418 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 8342/35720 Training loss: 1.0418 0.2240 sec/batch\n",
      "Epoch 5/20  Iteration 8343/35720 Training loss: 1.0418 0.2081 sec/batch\n",
      "Epoch 5/20  Iteration 8344/35720 Training loss: 1.0418 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 8345/35720 Training loss: 1.0417 0.2198 sec/batch\n",
      "Epoch 5/20  Iteration 8346/35720 Training loss: 1.0416 0.2128 sec/batch\n",
      "Epoch 5/20  Iteration 8347/35720 Training loss: 1.0416 0.2249 sec/batch\n",
      "Epoch 5/20  Iteration 8348/35720 Training loss: 1.0415 0.2058 sec/batch\n",
      "Epoch 5/20  Iteration 8349/35720 Training loss: 1.0414 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8350/35720 Training loss: 1.0414 0.2240 sec/batch\n",
      "Epoch 5/20  Iteration 8351/35720 Training loss: 1.0415 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8352/35720 Training loss: 1.0415 0.2266 sec/batch\n",
      "Epoch 5/20  Iteration 8353/35720 Training loss: 1.0414 0.2058 sec/batch\n",
      "Epoch 5/20  Iteration 8354/35720 Training loss: 1.0414 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 8355/35720 Training loss: 1.0414 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 8356/35720 Training loss: 1.0414 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8357/35720 Training loss: 1.0414 0.2148 sec/batch\n",
      "Epoch 5/20  Iteration 8358/35720 Training loss: 1.0414 0.2234 sec/batch\n",
      "Epoch 5/20  Iteration 8359/35720 Training loss: 1.0414 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8360/35720 Training loss: 1.0414 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8361/35720 Training loss: 1.0413 0.2056 sec/batch\n",
      "Epoch 5/20  Iteration 8362/35720 Training loss: 1.0413 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 8363/35720 Training loss: 1.0413 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 8364/35720 Training loss: 1.0412 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 8365/35720 Training loss: 1.0412 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 8366/35720 Training loss: 1.0412 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 8367/35720 Training loss: 1.0412 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 8368/35720 Training loss: 1.0412 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 8369/35720 Training loss: 1.0411 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8370/35720 Training loss: 1.0411 0.2080 sec/batch\n",
      "Epoch 5/20  Iteration 8371/35720 Training loss: 1.0412 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 8372/35720 Training loss: 1.0411 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8373/35720 Training loss: 1.0412 0.2258 sec/batch\n",
      "Epoch 5/20  Iteration 8374/35720 Training loss: 1.0412 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8375/35720 Training loss: 1.0412 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 8376/35720 Training loss: 1.0411 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 8377/35720 Training loss: 1.0411 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 8378/35720 Training loss: 1.0411 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8379/35720 Training loss: 1.0411 0.2332 sec/batch\n",
      "Epoch 5/20  Iteration 8380/35720 Training loss: 1.0410 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8381/35720 Training loss: 1.0410 0.2190 sec/batch\n",
      "Epoch 5/20  Iteration 8382/35720 Training loss: 1.0409 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 8383/35720 Training loss: 1.0408 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 8384/35720 Training loss: 1.0407 0.2168 sec/batch\n",
      "Epoch 5/20  Iteration 8385/35720 Training loss: 1.0407 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 8386/35720 Training loss: 1.0406 0.2217 sec/batch\n",
      "Epoch 5/20  Iteration 8387/35720 Training loss: 1.0406 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8388/35720 Training loss: 1.0405 0.2198 sec/batch\n",
      "Epoch 5/20  Iteration 8389/35720 Training loss: 1.0405 0.2107 sec/batch\n",
      "Epoch 5/20  Iteration 8390/35720 Training loss: 1.0404 0.2133 sec/batch\n",
      "Epoch 5/20  Iteration 8391/35720 Training loss: 1.0404 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 8392/35720 Training loss: 1.0403 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8393/35720 Training loss: 1.0402 0.2166 sec/batch\n",
      "Epoch 5/20  Iteration 8394/35720 Training loss: 1.0402 0.2104 sec/batch\n",
      "Epoch 5/20  Iteration 8395/35720 Training loss: 1.0401 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 8396/35720 Training loss: 1.0401 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8397/35720 Training loss: 1.0401 0.2234 sec/batch\n",
      "Epoch 5/20  Iteration 8398/35720 Training loss: 1.0401 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8399/35720 Training loss: 1.0400 0.2216 sec/batch\n",
      "Epoch 5/20  Iteration 8400/35720 Training loss: 1.0400 0.2114 sec/batch\n",
      "Validation loss: 1.30309 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 8401/35720 Training loss: 1.0401 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8402/35720 Training loss: 1.0400 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 8403/35720 Training loss: 1.0400 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 8404/35720 Training loss: 1.0400 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 8405/35720 Training loss: 1.0400 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8406/35720 Training loss: 1.0400 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 8407/35720 Training loss: 1.0399 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8408/35720 Training loss: 1.0398 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8409/35720 Training loss: 1.0398 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8410/35720 Training loss: 1.0397 0.2166 sec/batch\n",
      "Epoch 5/20  Iteration 8411/35720 Training loss: 1.0397 0.2213 sec/batch\n",
      "Epoch 5/20  Iteration 8412/35720 Training loss: 1.0396 0.2262 sec/batch\n",
      "Epoch 5/20  Iteration 8413/35720 Training loss: 1.0396 0.2209 sec/batch\n",
      "Epoch 5/20  Iteration 8414/35720 Training loss: 1.0396 0.2346 sec/batch\n",
      "Epoch 5/20  Iteration 8415/35720 Training loss: 1.0396 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 8416/35720 Training loss: 1.0395 0.2072 sec/batch\n",
      "Epoch 5/20  Iteration 8417/35720 Training loss: 1.0395 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8418/35720 Training loss: 1.0395 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 8419/35720 Training loss: 1.0394 0.2121 sec/batch\n",
      "Epoch 5/20  Iteration 8420/35720 Training loss: 1.0394 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 8421/35720 Training loss: 1.0393 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 8422/35720 Training loss: 1.0392 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8423/35720 Training loss: 1.0392 0.2148 sec/batch\n",
      "Epoch 5/20  Iteration 8424/35720 Training loss: 1.0391 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 8425/35720 Training loss: 1.0391 0.2176 sec/batch\n",
      "Epoch 5/20  Iteration 8426/35720 Training loss: 1.0390 0.2153 sec/batch\n",
      "Epoch 5/20  Iteration 8427/35720 Training loss: 1.0389 0.2487 sec/batch\n",
      "Epoch 5/20  Iteration 8428/35720 Training loss: 1.0389 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 8429/35720 Training loss: 1.0388 0.2161 sec/batch\n",
      "Epoch 5/20  Iteration 8430/35720 Training loss: 1.0388 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 8431/35720 Training loss: 1.0387 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8432/35720 Training loss: 1.0387 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 8433/35720 Training loss: 1.0387 0.2249 sec/batch\n",
      "Epoch 5/20  Iteration 8434/35720 Training loss: 1.0387 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 8435/35720 Training loss: 1.0387 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 8436/35720 Training loss: 1.0386 0.2291 sec/batch\n",
      "Epoch 5/20  Iteration 8437/35720 Training loss: 1.0386 0.2207 sec/batch\n",
      "Epoch 5/20  Iteration 8438/35720 Training loss: 1.0387 0.2252 sec/batch\n",
      "Epoch 5/20  Iteration 8439/35720 Training loss: 1.0387 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 8440/35720 Training loss: 1.0387 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 8441/35720 Training loss: 1.0387 0.2133 sec/batch\n",
      "Epoch 5/20  Iteration 8442/35720 Training loss: 1.0387 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 8443/35720 Training loss: 1.0386 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 8444/35720 Training loss: 1.0386 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8445/35720 Training loss: 1.0385 0.2200 sec/batch\n",
      "Epoch 5/20  Iteration 8446/35720 Training loss: 1.0385 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 8447/35720 Training loss: 1.0385 0.2205 sec/batch\n",
      "Epoch 5/20  Iteration 8448/35720 Training loss: 1.0384 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8449/35720 Training loss: 1.0384 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8450/35720 Training loss: 1.0384 0.2181 sec/batch\n",
      "Epoch 5/20  Iteration 8451/35720 Training loss: 1.0383 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8452/35720 Training loss: 1.0382 0.2444 sec/batch\n",
      "Epoch 5/20  Iteration 8453/35720 Training loss: 1.0382 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8454/35720 Training loss: 1.0381 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8455/35720 Training loss: 1.0381 0.2064 sec/batch\n",
      "Epoch 5/20  Iteration 8456/35720 Training loss: 1.0381 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 8457/35720 Training loss: 1.0381 0.2082 sec/batch\n",
      "Epoch 5/20  Iteration 8458/35720 Training loss: 1.0380 0.2057 sec/batch\n",
      "Epoch 5/20  Iteration 8459/35720 Training loss: 1.0380 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 8460/35720 Training loss: 1.0380 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 8461/35720 Training loss: 1.0380 0.2164 sec/batch\n",
      "Epoch 5/20  Iteration 8462/35720 Training loss: 1.0379 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8463/35720 Training loss: 1.0379 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8464/35720 Training loss: 1.0379 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 8465/35720 Training loss: 1.0379 0.2127 sec/batch\n",
      "Epoch 5/20  Iteration 8466/35720 Training loss: 1.0379 0.2180 sec/batch\n",
      "Epoch 5/20  Iteration 8467/35720 Training loss: 1.0378 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 8468/35720 Training loss: 1.0379 0.2146 sec/batch\n",
      "Epoch 5/20  Iteration 8469/35720 Training loss: 1.0379 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 8470/35720 Training loss: 1.0379 0.2205 sec/batch\n",
      "Epoch 5/20  Iteration 8471/35720 Training loss: 1.0379 0.2127 sec/batch\n",
      "Epoch 5/20  Iteration 8472/35720 Training loss: 1.0379 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 8473/35720 Training loss: 1.0379 0.2188 sec/batch\n",
      "Epoch 5/20  Iteration 8474/35720 Training loss: 1.0378 0.2091 sec/batch\n",
      "Epoch 5/20  Iteration 8475/35720 Training loss: 1.0377 0.2277 sec/batch\n",
      "Epoch 5/20  Iteration 8476/35720 Training loss: 1.0377 0.2060 sec/batch\n",
      "Epoch 5/20  Iteration 8477/35720 Training loss: 1.0377 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 8478/35720 Training loss: 1.0377 0.2294 sec/batch\n",
      "Epoch 5/20  Iteration 8479/35720 Training loss: 1.0377 0.2126 sec/batch\n",
      "Epoch 5/20  Iteration 8480/35720 Training loss: 1.0378 0.2056 sec/batch\n",
      "Epoch 5/20  Iteration 8481/35720 Training loss: 1.0378 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 8482/35720 Training loss: 1.0378 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 8483/35720 Training loss: 1.0378 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 8484/35720 Training loss: 1.0378 0.2059 sec/batch\n",
      "Epoch 5/20  Iteration 8485/35720 Training loss: 1.0378 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 8486/35720 Training loss: 1.0377 0.2105 sec/batch\n",
      "Epoch 5/20  Iteration 8487/35720 Training loss: 1.0377 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 8488/35720 Training loss: 1.0377 0.2075 sec/batch\n",
      "Epoch 5/20  Iteration 8489/35720 Training loss: 1.0376 0.2066 sec/batch\n",
      "Epoch 5/20  Iteration 8490/35720 Training loss: 1.0376 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8491/35720 Training loss: 1.0376 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8492/35720 Training loss: 1.0376 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 8493/35720 Training loss: 1.0376 0.2197 sec/batch\n",
      "Epoch 5/20  Iteration 8494/35720 Training loss: 1.0377 0.2128 sec/batch\n",
      "Epoch 5/20  Iteration 8495/35720 Training loss: 1.0377 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 8496/35720 Training loss: 1.0378 0.2154 sec/batch\n",
      "Epoch 5/20  Iteration 8497/35720 Training loss: 1.0378 0.2169 sec/batch\n",
      "Epoch 5/20  Iteration 8498/35720 Training loss: 1.0378 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 8499/35720 Training loss: 1.0377 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8500/35720 Training loss: 1.0377 0.2126 sec/batch\n",
      "Epoch 5/20  Iteration 8501/35720 Training loss: 1.0377 0.2198 sec/batch\n",
      "Epoch 5/20  Iteration 8502/35720 Training loss: 1.0377 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 8503/35720 Training loss: 1.0376 0.2251 sec/batch\n",
      "Epoch 5/20  Iteration 8504/35720 Training loss: 1.0375 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8505/35720 Training loss: 1.0375 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 8506/35720 Training loss: 1.0375 0.2212 sec/batch\n",
      "Epoch 5/20  Iteration 8507/35720 Training loss: 1.0375 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 8508/35720 Training loss: 1.0375 0.2263 sec/batch\n",
      "Epoch 5/20  Iteration 8509/35720 Training loss: 1.0374 0.2059 sec/batch\n",
      "Epoch 5/20  Iteration 8510/35720 Training loss: 1.0374 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8511/35720 Training loss: 1.0375 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8512/35720 Training loss: 1.0375 0.2064 sec/batch\n",
      "Epoch 5/20  Iteration 8513/35720 Training loss: 1.0375 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8514/35720 Training loss: 1.0375 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 8515/35720 Training loss: 1.0375 0.2084 sec/batch\n",
      "Epoch 5/20  Iteration 8516/35720 Training loss: 1.0375 0.2268 sec/batch\n",
      "Epoch 5/20  Iteration 8517/35720 Training loss: 1.0375 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8518/35720 Training loss: 1.0375 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 8519/35720 Training loss: 1.0375 0.2248 sec/batch\n",
      "Epoch 5/20  Iteration 8520/35720 Training loss: 1.0374 0.2133 sec/batch\n",
      "Epoch 5/20  Iteration 8521/35720 Training loss: 1.0374 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 8522/35720 Training loss: 1.0374 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8523/35720 Training loss: 1.0374 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8524/35720 Training loss: 1.0374 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 8525/35720 Training loss: 1.0374 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8526/35720 Training loss: 1.0374 0.2221 sec/batch\n",
      "Epoch 5/20  Iteration 8527/35720 Training loss: 1.0374 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 8528/35720 Training loss: 1.0373 0.2177 sec/batch\n",
      "Epoch 5/20  Iteration 8529/35720 Training loss: 1.0374 0.2196 sec/batch\n",
      "Epoch 5/20  Iteration 8530/35720 Training loss: 1.0374 0.2118 sec/batch\n",
      "Epoch 5/20  Iteration 8531/35720 Training loss: 1.0374 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 8532/35720 Training loss: 1.0373 0.2105 sec/batch\n",
      "Epoch 5/20  Iteration 8533/35720 Training loss: 1.0373 0.2221 sec/batch\n",
      "Epoch 5/20  Iteration 8534/35720 Training loss: 1.0373 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8535/35720 Training loss: 1.0372 0.2075 sec/batch\n",
      "Epoch 5/20  Iteration 8536/35720 Training loss: 1.0372 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8537/35720 Training loss: 1.0371 0.2197 sec/batch\n",
      "Epoch 5/20  Iteration 8538/35720 Training loss: 1.0370 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 8539/35720 Training loss: 1.0370 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 8540/35720 Training loss: 1.0369 0.2187 sec/batch\n",
      "Epoch 5/20  Iteration 8541/35720 Training loss: 1.0369 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8542/35720 Training loss: 1.0368 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 8543/35720 Training loss: 1.0367 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 8544/35720 Training loss: 1.0367 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 8545/35720 Training loss: 1.0367 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 8546/35720 Training loss: 1.0366 0.2067 sec/batch\n",
      "Epoch 5/20  Iteration 8547/35720 Training loss: 1.0366 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8548/35720 Training loss: 1.0366 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 8549/35720 Training loss: 1.0366 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8550/35720 Training loss: 1.0366 0.2122 sec/batch\n",
      "Epoch 5/20  Iteration 8551/35720 Training loss: 1.0366 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8552/35720 Training loss: 1.0366 0.2051 sec/batch\n",
      "Epoch 5/20  Iteration 8553/35720 Training loss: 1.0366 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8554/35720 Training loss: 1.0366 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 8555/35720 Training loss: 1.0366 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8556/35720 Training loss: 1.0366 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 8557/35720 Training loss: 1.0366 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8558/35720 Training loss: 1.0366 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 8559/35720 Training loss: 1.0366 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8560/35720 Training loss: 1.0365 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 8561/35720 Training loss: 1.0365 0.2267 sec/batch\n",
      "Epoch 5/20  Iteration 8562/35720 Training loss: 1.0365 0.2173 sec/batch\n",
      "Epoch 5/20  Iteration 8563/35720 Training loss: 1.0366 0.2225 sec/batch\n",
      "Epoch 5/20  Iteration 8564/35720 Training loss: 1.0366 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 8565/35720 Training loss: 1.0366 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 8566/35720 Training loss: 1.0365 0.2251 sec/batch\n",
      "Epoch 5/20  Iteration 8567/35720 Training loss: 1.0365 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8568/35720 Training loss: 1.0365 0.2171 sec/batch\n",
      "Epoch 5/20  Iteration 8569/35720 Training loss: 1.0365 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 8570/35720 Training loss: 1.0366 0.2234 sec/batch\n",
      "Epoch 5/20  Iteration 8571/35720 Training loss: 1.0366 0.2265 sec/batch\n",
      "Epoch 5/20  Iteration 8572/35720 Training loss: 1.0366 0.2179 sec/batch\n",
      "Epoch 5/20  Iteration 8573/35720 Training loss: 1.0366 0.2194 sec/batch\n",
      "Epoch 5/20  Iteration 8574/35720 Training loss: 1.0365 0.2205 sec/batch\n",
      "Epoch 5/20  Iteration 8575/35720 Training loss: 1.0365 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 8576/35720 Training loss: 1.0364 0.2154 sec/batch\n",
      "Epoch 5/20  Iteration 8577/35720 Training loss: 1.0364 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8578/35720 Training loss: 1.0364 0.2198 sec/batch\n",
      "Epoch 5/20  Iteration 8579/35720 Training loss: 1.0364 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 8580/35720 Training loss: 1.0363 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 8581/35720 Training loss: 1.0362 0.2222 sec/batch\n",
      "Epoch 5/20  Iteration 8582/35720 Training loss: 1.0363 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 8583/35720 Training loss: 1.0363 0.2229 sec/batch\n",
      "Epoch 5/20  Iteration 8584/35720 Training loss: 1.0363 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 8585/35720 Training loss: 1.0363 0.2214 sec/batch\n",
      "Epoch 5/20  Iteration 8586/35720 Training loss: 1.0363 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 8587/35720 Training loss: 1.0363 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8588/35720 Training loss: 1.0363 0.2161 sec/batch\n",
      "Epoch 5/20  Iteration 8589/35720 Training loss: 1.0362 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8590/35720 Training loss: 1.0363 0.2205 sec/batch\n",
      "Epoch 5/20  Iteration 8591/35720 Training loss: 1.0362 0.2078 sec/batch\n",
      "Epoch 5/20  Iteration 8592/35720 Training loss: 1.0362 0.2271 sec/batch\n",
      "Epoch 5/20  Iteration 8593/35720 Training loss: 1.0362 0.2199 sec/batch\n",
      "Epoch 5/20  Iteration 8594/35720 Training loss: 1.0361 0.2273 sec/batch\n",
      "Epoch 5/20  Iteration 8595/35720 Training loss: 1.0360 0.2054 sec/batch\n",
      "Epoch 5/20  Iteration 8596/35720 Training loss: 1.0360 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 8597/35720 Training loss: 1.0360 0.2240 sec/batch\n",
      "Epoch 5/20  Iteration 8598/35720 Training loss: 1.0360 0.2114 sec/batch\n",
      "Epoch 5/20  Iteration 8599/35720 Training loss: 1.0359 0.2215 sec/batch\n",
      "Epoch 5/20  Iteration 8600/35720 Training loss: 1.0360 0.2072 sec/batch\n",
      "Validation loss: 1.31062 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 8601/35720 Training loss: 1.0361 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 8602/35720 Training loss: 1.0361 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8603/35720 Training loss: 1.0362 0.2279 sec/batch\n",
      "Epoch 5/20  Iteration 8604/35720 Training loss: 1.0363 0.2607 sec/batch\n",
      "Epoch 5/20  Iteration 8605/35720 Training loss: 1.0363 0.2247 sec/batch\n",
      "Epoch 5/20  Iteration 8606/35720 Training loss: 1.0363 0.2149 sec/batch\n",
      "Epoch 5/20  Iteration 8607/35720 Training loss: 1.0363 0.2193 sec/batch\n",
      "Epoch 5/20  Iteration 8608/35720 Training loss: 1.0363 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 8609/35720 Training loss: 1.0363 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 8610/35720 Training loss: 1.0362 0.2265 sec/batch\n",
      "Epoch 5/20  Iteration 8611/35720 Training loss: 1.0362 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 8612/35720 Training loss: 1.0362 0.2245 sec/batch\n",
      "Epoch 5/20  Iteration 8613/35720 Training loss: 1.0361 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8614/35720 Training loss: 1.0361 0.2086 sec/batch\n",
      "Epoch 5/20  Iteration 8615/35720 Training loss: 1.0362 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 8616/35720 Training loss: 1.0361 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 8617/35720 Training loss: 1.0361 0.2150 sec/batch\n",
      "Epoch 5/20  Iteration 8618/35720 Training loss: 1.0360 0.2070 sec/batch\n",
      "Epoch 5/20  Iteration 8619/35720 Training loss: 1.0359 0.2096 sec/batch\n",
      "Epoch 5/20  Iteration 8620/35720 Training loss: 1.0358 0.2195 sec/batch\n",
      "Epoch 5/20  Iteration 8621/35720 Training loss: 1.0357 0.2055 sec/batch\n",
      "Epoch 5/20  Iteration 8622/35720 Training loss: 1.0356 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8623/35720 Training loss: 1.0356 0.2151 sec/batch\n",
      "Epoch 5/20  Iteration 8624/35720 Training loss: 1.0356 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8625/35720 Training loss: 1.0356 0.2079 sec/batch\n",
      "Epoch 5/20  Iteration 8626/35720 Training loss: 1.0355 0.2051 sec/batch\n",
      "Epoch 5/20  Iteration 8627/35720 Training loss: 1.0354 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 8628/35720 Training loss: 1.0354 0.2130 sec/batch\n",
      "Epoch 5/20  Iteration 8629/35720 Training loss: 1.0353 0.2058 sec/batch\n",
      "Epoch 5/20  Iteration 8630/35720 Training loss: 1.0353 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 8631/35720 Training loss: 1.0353 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 8632/35720 Training loss: 1.0353 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8633/35720 Training loss: 1.0352 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 8634/35720 Training loss: 1.0352 0.2095 sec/batch\n",
      "Epoch 5/20  Iteration 8635/35720 Training loss: 1.0352 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 8636/35720 Training loss: 1.0351 0.2074 sec/batch\n",
      "Epoch 5/20  Iteration 8637/35720 Training loss: 1.0351 0.2117 sec/batch\n",
      "Epoch 5/20  Iteration 8638/35720 Training loss: 1.0351 0.2186 sec/batch\n",
      "Epoch 5/20  Iteration 8639/35720 Training loss: 1.0351 0.2151 sec/batch\n",
      "Epoch 5/20  Iteration 8640/35720 Training loss: 1.0351 0.2174 sec/batch\n",
      "Epoch 5/20  Iteration 8641/35720 Training loss: 1.0350 0.2080 sec/batch\n",
      "Epoch 5/20  Iteration 8642/35720 Training loss: 1.0350 0.2200 sec/batch\n",
      "Epoch 5/20  Iteration 8643/35720 Training loss: 1.0351 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8644/35720 Training loss: 1.0350 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 8645/35720 Training loss: 1.0350 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 8646/35720 Training loss: 1.0350 0.2105 sec/batch\n",
      "Epoch 5/20  Iteration 8647/35720 Training loss: 1.0349 0.2153 sec/batch\n",
      "Epoch 5/20  Iteration 8648/35720 Training loss: 1.0349 0.2058 sec/batch\n",
      "Epoch 5/20  Iteration 8649/35720 Training loss: 1.0349 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8650/35720 Training loss: 1.0350 0.2062 sec/batch\n",
      "Epoch 5/20  Iteration 8651/35720 Training loss: 1.0349 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8652/35720 Training loss: 1.0349 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8653/35720 Training loss: 1.0349 0.2197 sec/batch\n",
      "Epoch 5/20  Iteration 8654/35720 Training loss: 1.0349 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8655/35720 Training loss: 1.0349 0.2088 sec/batch\n",
      "Epoch 5/20  Iteration 8656/35720 Training loss: 1.0348 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 8657/35720 Training loss: 1.0348 0.2202 sec/batch\n",
      "Epoch 5/20  Iteration 8658/35720 Training loss: 1.0348 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 8659/35720 Training loss: 1.0348 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 8660/35720 Training loss: 1.0349 0.2174 sec/batch\n",
      "Epoch 5/20  Iteration 8661/35720 Training loss: 1.0349 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 8662/35720 Training loss: 1.0348 0.2133 sec/batch\n",
      "Epoch 5/20  Iteration 8663/35720 Training loss: 1.0349 0.2131 sec/batch\n",
      "Epoch 5/20  Iteration 8664/35720 Training loss: 1.0348 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8665/35720 Training loss: 1.0348 0.2168 sec/batch\n",
      "Epoch 5/20  Iteration 8666/35720 Training loss: 1.0348 0.2069 sec/batch\n",
      "Epoch 5/20  Iteration 8667/35720 Training loss: 1.0349 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8668/35720 Training loss: 1.0349 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 8669/35720 Training loss: 1.0350 0.2054 sec/batch\n",
      "Epoch 5/20  Iteration 8670/35720 Training loss: 1.0350 0.2098 sec/batch\n",
      "Epoch 5/20  Iteration 8671/35720 Training loss: 1.0350 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 8672/35720 Training loss: 1.0350 0.2065 sec/batch\n",
      "Epoch 5/20  Iteration 8673/35720 Training loss: 1.0350 0.2099 sec/batch\n",
      "Epoch 5/20  Iteration 8674/35720 Training loss: 1.0350 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 8675/35720 Training loss: 1.0351 0.2145 sec/batch\n",
      "Epoch 5/20  Iteration 8676/35720 Training loss: 1.0351 0.2334 sec/batch\n",
      "Epoch 5/20  Iteration 8677/35720 Training loss: 1.0350 0.2112 sec/batch\n",
      "Epoch 5/20  Iteration 8678/35720 Training loss: 1.0350 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 8679/35720 Training loss: 1.0350 0.2083 sec/batch\n",
      "Epoch 5/20  Iteration 8680/35720 Training loss: 1.0350 0.2107 sec/batch\n",
      "Epoch 5/20  Iteration 8681/35720 Training loss: 1.0349 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8682/35720 Training loss: 1.0349 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 8683/35720 Training loss: 1.0349 0.2247 sec/batch\n",
      "Epoch 5/20  Iteration 8684/35720 Training loss: 1.0348 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 8685/35720 Training loss: 1.0347 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 8686/35720 Training loss: 1.0347 0.2055 sec/batch\n",
      "Epoch 5/20  Iteration 8687/35720 Training loss: 1.0347 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 8688/35720 Training loss: 1.0346 0.2127 sec/batch\n",
      "Epoch 5/20  Iteration 8689/35720 Training loss: 1.0346 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 8690/35720 Training loss: 1.0345 0.2081 sec/batch\n",
      "Epoch 5/20  Iteration 8691/35720 Training loss: 1.0345 0.2182 sec/batch\n",
      "Epoch 5/20  Iteration 8692/35720 Training loss: 1.0345 0.2166 sec/batch\n",
      "Epoch 5/20  Iteration 8693/35720 Training loss: 1.0345 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 8694/35720 Training loss: 1.0345 0.2278 sec/batch\n",
      "Epoch 5/20  Iteration 8695/35720 Training loss: 1.0345 0.2109 sec/batch\n",
      "Epoch 5/20  Iteration 8696/35720 Training loss: 1.0344 0.2076 sec/batch\n",
      "Epoch 5/20  Iteration 8697/35720 Training loss: 1.0344 0.2218 sec/batch\n",
      "Epoch 5/20  Iteration 8698/35720 Training loss: 1.0343 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 8699/35720 Training loss: 1.0342 0.2271 sec/batch\n",
      "Epoch 5/20  Iteration 8700/35720 Training loss: 1.0342 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 8701/35720 Training loss: 1.0342 0.2097 sec/batch\n",
      "Epoch 5/20  Iteration 8702/35720 Training loss: 1.0341 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 8703/35720 Training loss: 1.0341 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 8704/35720 Training loss: 1.0340 0.2078 sec/batch\n",
      "Epoch 5/20  Iteration 8705/35720 Training loss: 1.0340 0.2199 sec/batch\n",
      "Epoch 5/20  Iteration 8706/35720 Training loss: 1.0339 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 8707/35720 Training loss: 1.0339 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 8708/35720 Training loss: 1.0339 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 8709/35720 Training loss: 1.0338 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8710/35720 Training loss: 1.0338 0.2134 sec/batch\n",
      "Epoch 5/20  Iteration 8711/35720 Training loss: 1.0338 0.2164 sec/batch\n",
      "Epoch 5/20  Iteration 8712/35720 Training loss: 1.0338 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 8713/35720 Training loss: 1.0338 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 8714/35720 Training loss: 1.0338 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 8715/35720 Training loss: 1.0337 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8716/35720 Training loss: 1.0337 0.2141 sec/batch\n",
      "Epoch 5/20  Iteration 8717/35720 Training loss: 1.0336 0.2119 sec/batch\n",
      "Epoch 5/20  Iteration 8718/35720 Training loss: 1.0336 0.2191 sec/batch\n",
      "Epoch 5/20  Iteration 8719/35720 Training loss: 1.0336 0.2058 sec/batch\n",
      "Epoch 5/20  Iteration 8720/35720 Training loss: 1.0335 0.2110 sec/batch\n",
      "Epoch 5/20  Iteration 8721/35720 Training loss: 1.0335 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 8722/35720 Training loss: 1.0335 0.2093 sec/batch\n",
      "Epoch 5/20  Iteration 8723/35720 Training loss: 1.0335 0.2135 sec/batch\n",
      "Epoch 5/20  Iteration 8724/35720 Training loss: 1.0334 0.2133 sec/batch\n",
      "Epoch 5/20  Iteration 8725/35720 Training loss: 1.0335 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 8726/35720 Training loss: 1.0334 0.2056 sec/batch\n",
      "Epoch 5/20  Iteration 8727/35720 Training loss: 1.0334 0.2100 sec/batch\n",
      "Epoch 5/20  Iteration 8728/35720 Training loss: 1.0333 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 8729/35720 Training loss: 1.0333 0.2061 sec/batch\n",
      "Epoch 5/20  Iteration 8730/35720 Training loss: 1.0332 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8731/35720 Training loss: 1.0332 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 8732/35720 Training loss: 1.0331 0.2103 sec/batch\n",
      "Epoch 5/20  Iteration 8733/35720 Training loss: 1.0331 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 8734/35720 Training loss: 1.0330 0.2199 sec/batch\n",
      "Epoch 5/20  Iteration 8735/35720 Training loss: 1.0330 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 8736/35720 Training loss: 1.0330 0.2234 sec/batch\n",
      "Epoch 5/20  Iteration 8737/35720 Training loss: 1.0330 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8738/35720 Training loss: 1.0330 0.2188 sec/batch\n",
      "Epoch 5/20  Iteration 8739/35720 Training loss: 1.0329 0.2146 sec/batch\n",
      "Epoch 5/20  Iteration 8740/35720 Training loss: 1.0328 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 8741/35720 Training loss: 1.0328 0.2104 sec/batch\n",
      "Epoch 5/20  Iteration 8742/35720 Training loss: 1.0328 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8743/35720 Training loss: 1.0327 0.2116 sec/batch\n",
      "Epoch 5/20  Iteration 8744/35720 Training loss: 1.0327 0.2113 sec/batch\n",
      "Epoch 5/20  Iteration 8745/35720 Training loss: 1.0327 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 8746/35720 Training loss: 1.0327 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 8747/35720 Training loss: 1.0326 0.2295 sec/batch\n",
      "Epoch 5/20  Iteration 8748/35720 Training loss: 1.0326 0.2136 sec/batch\n",
      "Epoch 5/20  Iteration 8749/35720 Training loss: 1.0325 0.2211 sec/batch\n",
      "Epoch 5/20  Iteration 8750/35720 Training loss: 1.0325 0.2058 sec/batch\n",
      "Epoch 5/20  Iteration 8751/35720 Training loss: 1.0325 0.2126 sec/batch\n",
      "Epoch 5/20  Iteration 8752/35720 Training loss: 1.0325 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 8753/35720 Training loss: 1.0324 0.2089 sec/batch\n",
      "Epoch 5/20  Iteration 8754/35720 Training loss: 1.0324 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8755/35720 Training loss: 1.0323 0.2145 sec/batch\n",
      "Epoch 5/20  Iteration 8756/35720 Training loss: 1.0323 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8757/35720 Training loss: 1.0322 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 8758/35720 Training loss: 1.0322 0.2146 sec/batch\n",
      "Epoch 5/20  Iteration 8759/35720 Training loss: 1.0322 0.2138 sec/batch\n",
      "Epoch 5/20  Iteration 8760/35720 Training loss: 1.0322 0.2120 sec/batch\n",
      "Epoch 5/20  Iteration 8761/35720 Training loss: 1.0322 0.2230 sec/batch\n",
      "Epoch 5/20  Iteration 8762/35720 Training loss: 1.0322 0.2347 sec/batch\n",
      "Epoch 5/20  Iteration 8763/35720 Training loss: 1.0321 0.2079 sec/batch\n",
      "Epoch 5/20  Iteration 8764/35720 Training loss: 1.0321 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8765/35720 Training loss: 1.0320 0.2336 sec/batch\n",
      "Epoch 5/20  Iteration 8766/35720 Training loss: 1.0320 0.2117 sec/batch\n",
      "Epoch 5/20  Iteration 8767/35720 Training loss: 1.0320 0.2182 sec/batch\n",
      "Epoch 5/20  Iteration 8768/35720 Training loss: 1.0320 0.2050 sec/batch\n",
      "Epoch 5/20  Iteration 8769/35720 Training loss: 1.0320 0.2147 sec/batch\n",
      "Epoch 5/20  Iteration 8770/35720 Training loss: 1.0320 0.2255 sec/batch\n",
      "Epoch 5/20  Iteration 8771/35720 Training loss: 1.0321 0.2055 sec/batch\n",
      "Epoch 5/20  Iteration 8772/35720 Training loss: 1.0321 0.2165 sec/batch\n",
      "Epoch 5/20  Iteration 8773/35720 Training loss: 1.0320 0.2124 sec/batch\n",
      "Epoch 5/20  Iteration 8774/35720 Training loss: 1.0320 0.2125 sec/batch\n",
      "Epoch 5/20  Iteration 8775/35720 Training loss: 1.0320 0.2321 sec/batch\n",
      "Epoch 5/20  Iteration 8776/35720 Training loss: 1.0320 0.2102 sec/batch\n",
      "Epoch 5/20  Iteration 8777/35720 Training loss: 1.0319 0.2063 sec/batch\n",
      "Epoch 5/20  Iteration 8778/35720 Training loss: 1.0319 0.2047 sec/batch\n",
      "Epoch 5/20  Iteration 8779/35720 Training loss: 1.0319 0.2064 sec/batch\n",
      "Epoch 5/20  Iteration 8780/35720 Training loss: 1.0319 0.2087 sec/batch\n",
      "Epoch 5/20  Iteration 8781/35720 Training loss: 1.0319 0.2054 sec/batch\n",
      "Epoch 5/20  Iteration 8782/35720 Training loss: 1.0318 0.2081 sec/batch\n",
      "Epoch 5/20  Iteration 8783/35720 Training loss: 1.0318 0.2142 sec/batch\n",
      "Epoch 5/20  Iteration 8784/35720 Training loss: 1.0317 0.2203 sec/batch\n",
      "Epoch 5/20  Iteration 8785/35720 Training loss: 1.0317 0.2054 sec/batch\n",
      "Epoch 5/20  Iteration 8786/35720 Training loss: 1.0317 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 8787/35720 Training loss: 1.0317 0.2132 sec/batch\n",
      "Epoch 5/20  Iteration 8788/35720 Training loss: 1.0317 0.2085 sec/batch\n",
      "Epoch 5/20  Iteration 8789/35720 Training loss: 1.0317 0.2236 sec/batch\n",
      "Epoch 5/20  Iteration 8790/35720 Training loss: 1.0317 0.2053 sec/batch\n",
      "Epoch 5/20  Iteration 8791/35720 Training loss: 1.0317 0.2092 sec/batch\n",
      "Epoch 5/20  Iteration 8792/35720 Training loss: 1.0317 0.2068 sec/batch\n",
      "Epoch 5/20  Iteration 8793/35720 Training loss: 1.0317 0.2080 sec/batch\n",
      "Epoch 5/20  Iteration 8794/35720 Training loss: 1.0317 0.2116 sec/batch\n",
      "Epoch 5/20  Iteration 8795/35720 Training loss: 1.0318 0.2111 sec/batch\n",
      "Epoch 5/20  Iteration 8796/35720 Training loss: 1.0318 0.2238 sec/batch\n",
      "Epoch 5/20  Iteration 8797/35720 Training loss: 1.0318 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 8798/35720 Training loss: 1.0317 0.2094 sec/batch\n",
      "Epoch 5/20  Iteration 8799/35720 Training loss: 1.0318 0.2226 sec/batch\n",
      "Epoch 5/20  Iteration 8800/35720 Training loss: 1.0318 0.2074 sec/batch\n",
      "Validation loss: 1.31204 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 8801/35720 Training loss: 1.0319 0.2258 sec/batch\n",
      "Epoch 5/20  Iteration 8802/35720 Training loss: 1.0320 0.2337 sec/batch\n",
      "Epoch 5/20  Iteration 8803/35720 Training loss: 1.0319 0.2262 sec/batch\n",
      "Epoch 5/20  Iteration 8804/35720 Training loss: 1.0319 0.2244 sec/batch\n",
      "Epoch 5/20  Iteration 8805/35720 Training loss: 1.0319 0.2211 sec/batch\n",
      "Epoch 5/20  Iteration 8806/35720 Training loss: 1.0319 0.2345 sec/batch\n",
      "Epoch 5/20  Iteration 8807/35720 Training loss: 1.0319 0.2173 sec/batch\n",
      "Epoch 5/20  Iteration 8808/35720 Training loss: 1.0319 0.2170 sec/batch\n",
      "Epoch 5/20  Iteration 8809/35720 Training loss: 1.0319 0.2249 sec/batch\n",
      "Epoch 5/20  Iteration 8810/35720 Training loss: 1.0319 0.2140 sec/batch\n",
      "Epoch 5/20  Iteration 8811/35720 Training loss: 1.0319 0.2314 sec/batch\n",
      "Epoch 5/20  Iteration 8812/35720 Training loss: 1.0320 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 8813/35720 Training loss: 1.0320 0.2169 sec/batch\n",
      "Epoch 5/20  Iteration 8814/35720 Training loss: 1.0320 0.2155 sec/batch\n",
      "Epoch 5/20  Iteration 8815/35720 Training loss: 1.0321 0.2174 sec/batch\n",
      "Epoch 5/20  Iteration 8816/35720 Training loss: 1.0321 0.2165 sec/batch\n",
      "Epoch 5/20  Iteration 8817/35720 Training loss: 1.0321 0.2270 sec/batch\n",
      "Epoch 5/20  Iteration 8818/35720 Training loss: 1.0321 0.2350 sec/batch\n",
      "Epoch 5/20  Iteration 8819/35720 Training loss: 1.0321 0.2273 sec/batch\n",
      "Epoch 5/20  Iteration 8820/35720 Training loss: 1.0321 0.2168 sec/batch\n",
      "Epoch 5/20  Iteration 8821/35720 Training loss: 1.0321 0.2306 sec/batch\n",
      "Epoch 5/20  Iteration 8822/35720 Training loss: 1.0320 0.2157 sec/batch\n",
      "Epoch 5/20  Iteration 8823/35720 Training loss: 1.0320 0.2206 sec/batch\n",
      "Epoch 5/20  Iteration 8824/35720 Training loss: 1.0320 0.2227 sec/batch\n",
      "Epoch 5/20  Iteration 8825/35720 Training loss: 1.0320 0.2179 sec/batch\n",
      "Epoch 5/20  Iteration 8826/35720 Training loss: 1.0319 0.2373 sec/batch\n",
      "Epoch 5/20  Iteration 8827/35720 Training loss: 1.0319 0.2263 sec/batch\n",
      "Epoch 5/20  Iteration 8828/35720 Training loss: 1.0319 0.2331 sec/batch\n",
      "Epoch 5/20  Iteration 8829/35720 Training loss: 1.0319 0.2300 sec/batch\n",
      "Epoch 5/20  Iteration 8830/35720 Training loss: 1.0320 0.2204 sec/batch\n",
      "Epoch 5/20  Iteration 8831/35720 Training loss: 1.0319 0.2178 sec/batch\n",
      "Epoch 5/20  Iteration 8832/35720 Training loss: 1.0319 0.2158 sec/batch\n",
      "Epoch 5/20  Iteration 8833/35720 Training loss: 1.0319 0.2163 sec/batch\n",
      "Epoch 5/20  Iteration 8834/35720 Training loss: 1.0319 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 8835/35720 Training loss: 1.0318 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 8836/35720 Training loss: 1.0317 0.2159 sec/batch\n",
      "Epoch 5/20  Iteration 8837/35720 Training loss: 1.0317 0.2293 sec/batch\n",
      "Epoch 5/20  Iteration 8838/35720 Training loss: 1.0317 0.2173 sec/batch\n",
      "Epoch 5/20  Iteration 8839/35720 Training loss: 1.0317 0.2220 sec/batch\n",
      "Epoch 5/20  Iteration 8840/35720 Training loss: 1.0316 0.2316 sec/batch\n",
      "Epoch 5/20  Iteration 8841/35720 Training loss: 1.0316 0.2348 sec/batch\n",
      "Epoch 5/20  Iteration 8842/35720 Training loss: 1.0316 0.2304 sec/batch\n",
      "Epoch 5/20  Iteration 8843/35720 Training loss: 1.0316 0.2212 sec/batch\n",
      "Epoch 5/20  Iteration 8844/35720 Training loss: 1.0316 0.2172 sec/batch\n",
      "Epoch 5/20  Iteration 8845/35720 Training loss: 1.0316 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 8846/35720 Training loss: 1.0315 0.2248 sec/batch\n",
      "Epoch 5/20  Iteration 8847/35720 Training loss: 1.0315 0.2300 sec/batch\n",
      "Epoch 5/20  Iteration 8848/35720 Training loss: 1.0314 0.2263 sec/batch\n",
      "Epoch 5/20  Iteration 8849/35720 Training loss: 1.0314 0.2265 sec/batch\n",
      "Epoch 5/20  Iteration 8850/35720 Training loss: 1.0314 0.2489 sec/batch\n",
      "Epoch 5/20  Iteration 8851/35720 Training loss: 1.0313 0.2186 sec/batch\n",
      "Epoch 5/20  Iteration 8852/35720 Training loss: 1.0312 0.2200 sec/batch\n",
      "Epoch 5/20  Iteration 8853/35720 Training loss: 1.0313 0.2176 sec/batch\n",
      "Epoch 5/20  Iteration 8854/35720 Training loss: 1.0313 0.2237 sec/batch\n",
      "Epoch 5/20  Iteration 8855/35720 Training loss: 1.0314 0.2160 sec/batch\n",
      "Epoch 5/20  Iteration 8856/35720 Training loss: 1.0314 0.2171 sec/batch\n",
      "Epoch 5/20  Iteration 8857/35720 Training loss: 1.0314 0.2190 sec/batch\n",
      "Epoch 5/20  Iteration 8858/35720 Training loss: 1.0314 0.2183 sec/batch\n",
      "Epoch 5/20  Iteration 8859/35720 Training loss: 1.0314 0.2266 sec/batch\n",
      "Epoch 5/20  Iteration 8860/35720 Training loss: 1.0314 0.2152 sec/batch\n",
      "Epoch 5/20  Iteration 8861/35720 Training loss: 1.0314 0.2187 sec/batch\n",
      "Epoch 5/20  Iteration 8862/35720 Training loss: 1.0314 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 8863/35720 Training loss: 1.0313 0.2229 sec/batch\n",
      "Epoch 5/20  Iteration 8864/35720 Training loss: 1.0314 0.2234 sec/batch\n",
      "Epoch 5/20  Iteration 8865/35720 Training loss: 1.0313 0.2241 sec/batch\n",
      "Epoch 5/20  Iteration 8866/35720 Training loss: 1.0313 0.2255 sec/batch\n",
      "Epoch 5/20  Iteration 8867/35720 Training loss: 1.0313 0.2261 sec/batch\n",
      "Epoch 5/20  Iteration 8868/35720 Training loss: 1.0313 0.2386 sec/batch\n",
      "Epoch 5/20  Iteration 8869/35720 Training loss: 1.0312 0.2458 sec/batch\n",
      "Epoch 5/20  Iteration 8870/35720 Training loss: 1.0313 0.2211 sec/batch\n",
      "Epoch 5/20  Iteration 8871/35720 Training loss: 1.0312 0.2302 sec/batch\n",
      "Epoch 5/20  Iteration 8872/35720 Training loss: 1.0312 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 8873/35720 Training loss: 1.0312 0.2174 sec/batch\n",
      "Epoch 5/20  Iteration 8874/35720 Training loss: 1.0312 0.2279 sec/batch\n",
      "Epoch 5/20  Iteration 8875/35720 Training loss: 1.0311 0.2105 sec/batch\n",
      "Epoch 5/20  Iteration 8876/35720 Training loss: 1.0312 0.2129 sec/batch\n",
      "Epoch 5/20  Iteration 8877/35720 Training loss: 1.0311 0.2364 sec/batch\n",
      "Epoch 5/20  Iteration 8878/35720 Training loss: 1.0311 0.2515 sec/batch\n",
      "Epoch 5/20  Iteration 8879/35720 Training loss: 1.0311 0.2285 sec/batch\n",
      "Epoch 5/20  Iteration 8880/35720 Training loss: 1.0311 0.2202 sec/batch\n",
      "Epoch 5/20  Iteration 8881/35720 Training loss: 1.0311 0.2325 sec/batch\n",
      "Epoch 5/20  Iteration 8882/35720 Training loss: 1.0311 0.2245 sec/batch\n",
      "Epoch 5/20  Iteration 8883/35720 Training loss: 1.0311 0.2301 sec/batch\n",
      "Epoch 5/20  Iteration 8884/35720 Training loss: 1.0311 0.2324 sec/batch\n",
      "Epoch 5/20  Iteration 8885/35720 Training loss: 1.0311 0.2262 sec/batch\n",
      "Epoch 5/20  Iteration 8886/35720 Training loss: 1.0311 0.2250 sec/batch\n",
      "Epoch 5/20  Iteration 8887/35720 Training loss: 1.0311 0.2300 sec/batch\n",
      "Epoch 5/20  Iteration 8888/35720 Training loss: 1.0311 0.2234 sec/batch\n",
      "Epoch 5/20  Iteration 8889/35720 Training loss: 1.0311 0.2259 sec/batch\n",
      "Epoch 5/20  Iteration 8890/35720 Training loss: 1.0312 0.2388 sec/batch\n",
      "Epoch 5/20  Iteration 8891/35720 Training loss: 1.0312 0.2395 sec/batch\n",
      "Epoch 5/20  Iteration 8892/35720 Training loss: 1.0312 0.2168 sec/batch\n",
      "Epoch 5/20  Iteration 8893/35720 Training loss: 1.0312 0.2374 sec/batch\n",
      "Epoch 5/20  Iteration 8894/35720 Training loss: 1.0311 0.2201 sec/batch\n",
      "Epoch 5/20  Iteration 8895/35720 Training loss: 1.0311 0.2328 sec/batch\n",
      "Epoch 5/20  Iteration 8896/35720 Training loss: 1.0311 0.2300 sec/batch\n",
      "Epoch 5/20  Iteration 8897/35720 Training loss: 1.0311 0.2145 sec/batch\n",
      "Epoch 5/20  Iteration 8898/35720 Training loss: 1.0311 0.2108 sec/batch\n",
      "Epoch 5/20  Iteration 8899/35720 Training loss: 1.0311 0.2090 sec/batch\n",
      "Epoch 5/20  Iteration 8900/35720 Training loss: 1.0310 0.2254 sec/batch\n",
      "Epoch 5/20  Iteration 8901/35720 Training loss: 1.0310 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 8902/35720 Training loss: 1.0310 0.2188 sec/batch\n",
      "Epoch 5/20  Iteration 8903/35720 Training loss: 1.0310 0.2123 sec/batch\n",
      "Epoch 5/20  Iteration 8904/35720 Training loss: 1.0310 0.2166 sec/batch\n",
      "Epoch 5/20  Iteration 8905/35720 Training loss: 1.0309 0.2176 sec/batch\n",
      "Epoch 5/20  Iteration 8906/35720 Training loss: 1.0309 0.2115 sec/batch\n",
      "Epoch 5/20  Iteration 8907/35720 Training loss: 1.0309 0.2300 sec/batch\n",
      "Epoch 5/20  Iteration 8908/35720 Training loss: 1.0308 0.2263 sec/batch\n",
      "Epoch 5/20  Iteration 8909/35720 Training loss: 1.0308 0.2137 sec/batch\n",
      "Epoch 5/20  Iteration 8910/35720 Training loss: 1.0308 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 8911/35720 Training loss: 1.0308 0.2321 sec/batch\n",
      "Epoch 5/20  Iteration 8912/35720 Training loss: 1.0308 0.2081 sec/batch\n",
      "Epoch 5/20  Iteration 8913/35720 Training loss: 1.0308 0.2250 sec/batch\n",
      "Epoch 5/20  Iteration 8914/35720 Training loss: 1.0308 0.2072 sec/batch\n",
      "Epoch 5/20  Iteration 8915/35720 Training loss: 1.0308 0.2116 sec/batch\n",
      "Epoch 5/20  Iteration 8916/35720 Training loss: 1.0307 0.2162 sec/batch\n",
      "Epoch 5/20  Iteration 8917/35720 Training loss: 1.0307 0.2190 sec/batch\n",
      "Epoch 5/20  Iteration 8918/35720 Training loss: 1.0306 0.2214 sec/batch\n",
      "Epoch 5/20  Iteration 8919/35720 Training loss: 1.0306 0.2156 sec/batch\n",
      "Epoch 5/20  Iteration 8920/35720 Training loss: 1.0306 0.2059 sec/batch\n",
      "Epoch 5/20  Iteration 8921/35720 Training loss: 1.0306 0.2101 sec/batch\n",
      "Epoch 5/20  Iteration 8922/35720 Training loss: 1.0306 0.2245 sec/batch\n",
      "Epoch 5/20  Iteration 8923/35720 Training loss: 1.0305 0.2231 sec/batch\n",
      "Epoch 5/20  Iteration 8924/35720 Training loss: 1.0304 0.2213 sec/batch\n",
      "Epoch 5/20  Iteration 8925/35720 Training loss: 1.0304 0.2157 sec/batch\n",
      "Epoch 5/20  Iteration 8926/35720 Training loss: 1.0304 0.2106 sec/batch\n",
      "Epoch 5/20  Iteration 8927/35720 Training loss: 1.0303 0.2195 sec/batch\n",
      "Epoch 5/20  Iteration 8928/35720 Training loss: 1.0303 0.2371 sec/batch\n",
      "Epoch 5/20  Iteration 8929/35720 Training loss: 1.0302 0.2307 sec/batch\n",
      "Epoch 5/20  Iteration 8930/35720 Training loss: 1.0303 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 8931/35720 Training loss: 1.0657 0.2061 sec/batch\n",
      "Epoch 6/20  Iteration 8932/35720 Training loss: 1.0518 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 8933/35720 Training loss: 1.0465 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 8934/35720 Training loss: 1.0338 0.2174 sec/batch\n",
      "Epoch 6/20  Iteration 8935/35720 Training loss: 1.0407 0.2120 sec/batch\n",
      "Epoch 6/20  Iteration 8936/35720 Training loss: 1.0226 0.2170 sec/batch\n",
      "Epoch 6/20  Iteration 8937/35720 Training loss: 1.0227 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 8938/35720 Training loss: 1.0088 0.2074 sec/batch\n",
      "Epoch 6/20  Iteration 8939/35720 Training loss: 1.0042 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 8940/35720 Training loss: 1.0072 0.2265 sec/batch\n",
      "Epoch 6/20  Iteration 8941/35720 Training loss: 1.0099 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 8942/35720 Training loss: 1.0065 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 8943/35720 Training loss: 1.0088 0.2084 sec/batch\n",
      "Epoch 6/20  Iteration 8944/35720 Training loss: 1.0163 0.2190 sec/batch\n",
      "Epoch 6/20  Iteration 8945/35720 Training loss: 1.0199 0.2194 sec/batch\n",
      "Epoch 6/20  Iteration 8946/35720 Training loss: 1.0213 0.2239 sec/batch\n",
      "Epoch 6/20  Iteration 8947/35720 Training loss: 1.0227 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 8948/35720 Training loss: 1.0203 0.2311 sec/batch\n",
      "Epoch 6/20  Iteration 8949/35720 Training loss: 1.0180 0.2221 sec/batch\n",
      "Epoch 6/20  Iteration 8950/35720 Training loss: 1.0182 0.2186 sec/batch\n",
      "Epoch 6/20  Iteration 8951/35720 Training loss: 1.0202 0.2298 sec/batch\n",
      "Epoch 6/20  Iteration 8952/35720 Training loss: 1.0169 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 8953/35720 Training loss: 1.0177 0.2074 sec/batch\n",
      "Epoch 6/20  Iteration 8954/35720 Training loss: 1.0190 0.2086 sec/batch\n",
      "Epoch 6/20  Iteration 8955/35720 Training loss: 1.0219 0.2265 sec/batch\n",
      "Epoch 6/20  Iteration 8956/35720 Training loss: 1.0212 0.2084 sec/batch\n",
      "Epoch 6/20  Iteration 8957/35720 Training loss: 1.0246 0.2240 sec/batch\n",
      "Epoch 6/20  Iteration 8958/35720 Training loss: 1.0254 0.2372 sec/batch\n",
      "Epoch 6/20  Iteration 8959/35720 Training loss: 1.0243 0.2136 sec/batch\n",
      "Epoch 6/20  Iteration 8960/35720 Training loss: 1.0237 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 8961/35720 Training loss: 1.0292 0.2273 sec/batch\n",
      "Epoch 6/20  Iteration 8962/35720 Training loss: 1.0266 0.2218 sec/batch\n",
      "Epoch 6/20  Iteration 8963/35720 Training loss: 1.0284 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 8964/35720 Training loss: 1.0309 0.2071 sec/batch\n",
      "Epoch 6/20  Iteration 8965/35720 Training loss: 1.0343 0.2139 sec/batch\n",
      "Epoch 6/20  Iteration 8966/35720 Training loss: 1.0342 0.2408 sec/batch\n",
      "Epoch 6/20  Iteration 8967/35720 Training loss: 1.0337 0.2293 sec/batch\n",
      "Epoch 6/20  Iteration 8968/35720 Training loss: 1.0335 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 8969/35720 Training loss: 1.0309 0.2257 sec/batch\n",
      "Epoch 6/20  Iteration 8970/35720 Training loss: 1.0315 0.2087 sec/batch\n",
      "Epoch 6/20  Iteration 8971/35720 Training loss: 1.0303 0.2111 sec/batch\n",
      "Epoch 6/20  Iteration 8972/35720 Training loss: 1.0283 0.2268 sec/batch\n",
      "Epoch 6/20  Iteration 8973/35720 Training loss: 1.0253 0.2249 sec/batch\n",
      "Epoch 6/20  Iteration 8974/35720 Training loss: 1.0241 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 8975/35720 Training loss: 1.0234 0.2148 sec/batch\n",
      "Epoch 6/20  Iteration 8976/35720 Training loss: 1.0223 0.2085 sec/batch\n",
      "Epoch 6/20  Iteration 8977/35720 Training loss: 1.0214 0.2435 sec/batch\n",
      "Epoch 6/20  Iteration 8978/35720 Training loss: 1.0203 0.2270 sec/batch\n",
      "Epoch 6/20  Iteration 8979/35720 Training loss: 1.0207 0.2268 sec/batch\n",
      "Epoch 6/20  Iteration 8980/35720 Training loss: 1.0190 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 8981/35720 Training loss: 1.0194 0.2305 sec/batch\n",
      "Epoch 6/20  Iteration 8982/35720 Training loss: 1.0190 0.2278 sec/batch\n",
      "Epoch 6/20  Iteration 8983/35720 Training loss: 1.0184 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 8984/35720 Training loss: 1.0163 0.2208 sec/batch\n",
      "Epoch 6/20  Iteration 8985/35720 Training loss: 1.0152 0.2401 sec/batch\n",
      "Epoch 6/20  Iteration 8986/35720 Training loss: 1.0149 0.2197 sec/batch\n",
      "Epoch 6/20  Iteration 8987/35720 Training loss: 1.0153 0.2060 sec/batch\n",
      "Epoch 6/20  Iteration 8988/35720 Training loss: 1.0146 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 8989/35720 Training loss: 1.0133 0.2202 sec/batch\n",
      "Epoch 6/20  Iteration 8990/35720 Training loss: 1.0120 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 8991/35720 Training loss: 1.0104 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 8992/35720 Training loss: 1.0083 0.2198 sec/batch\n",
      "Epoch 6/20  Iteration 8993/35720 Training loss: 1.0086 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 8994/35720 Training loss: 1.0084 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 8995/35720 Training loss: 1.0090 0.2236 sec/batch\n",
      "Epoch 6/20  Iteration 8996/35720 Training loss: 1.0091 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 8997/35720 Training loss: 1.0089 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 8998/35720 Training loss: 1.0079 0.2335 sec/batch\n",
      "Epoch 6/20  Iteration 8999/35720 Training loss: 1.0087 0.2300 sec/batch\n",
      "Epoch 6/20  Iteration 9000/35720 Training loss: 1.0082 0.2239 sec/batch\n",
      "Validation loss: 1.30154 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 9001/35720 Training loss: 1.0114 0.2087 sec/batch\n",
      "Epoch 6/20  Iteration 9002/35720 Training loss: 1.0119 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 9003/35720 Training loss: 1.0122 0.2311 sec/batch\n",
      "Epoch 6/20  Iteration 9004/35720 Training loss: 1.0121 0.2169 sec/batch\n",
      "Epoch 6/20  Iteration 9005/35720 Training loss: 1.0107 0.2202 sec/batch\n",
      "Epoch 6/20  Iteration 9006/35720 Training loss: 1.0103 0.2131 sec/batch\n",
      "Epoch 6/20  Iteration 9007/35720 Training loss: 1.0091 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 9008/35720 Training loss: 1.0099 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 9009/35720 Training loss: 1.0096 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 9010/35720 Training loss: 1.0119 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9011/35720 Training loss: 1.0122 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 9012/35720 Training loss: 1.0119 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 9013/35720 Training loss: 1.0116 0.2111 sec/batch\n",
      "Epoch 6/20  Iteration 9014/35720 Training loss: 1.0118 0.2245 sec/batch\n",
      "Epoch 6/20  Iteration 9015/35720 Training loss: 1.0117 0.2226 sec/batch\n",
      "Epoch 6/20  Iteration 9016/35720 Training loss: 1.0115 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9017/35720 Training loss: 1.0116 0.2104 sec/batch\n",
      "Epoch 6/20  Iteration 9018/35720 Training loss: 1.0111 0.2088 sec/batch\n",
      "Epoch 6/20  Iteration 9019/35720 Training loss: 1.0099 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9020/35720 Training loss: 1.0088 0.2280 sec/batch\n",
      "Epoch 6/20  Iteration 9021/35720 Training loss: 1.0087 0.2087 sec/batch\n",
      "Epoch 6/20  Iteration 9022/35720 Training loss: 1.0079 0.2248 sec/batch\n",
      "Epoch 6/20  Iteration 9023/35720 Training loss: 1.0074 0.2265 sec/batch\n",
      "Epoch 6/20  Iteration 9024/35720 Training loss: 1.0071 0.2132 sec/batch\n",
      "Epoch 6/20  Iteration 9025/35720 Training loss: 1.0067 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 9026/35720 Training loss: 1.0059 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 9027/35720 Training loss: 1.0061 0.2146 sec/batch\n",
      "Epoch 6/20  Iteration 9028/35720 Training loss: 1.0058 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 9029/35720 Training loss: 1.0056 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9030/35720 Training loss: 1.0051 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 9031/35720 Training loss: 1.0048 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 9032/35720 Training loss: 1.0050 0.2184 sec/batch\n",
      "Epoch 6/20  Iteration 9033/35720 Training loss: 1.0050 0.2247 sec/batch\n",
      "Epoch 6/20  Iteration 9034/35720 Training loss: 1.0048 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 9035/35720 Training loss: 1.0049 0.2080 sec/batch\n",
      "Epoch 6/20  Iteration 9036/35720 Training loss: 1.0046 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9037/35720 Training loss: 1.0049 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 9038/35720 Training loss: 1.0051 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9039/35720 Training loss: 1.0056 0.2225 sec/batch\n",
      "Epoch 6/20  Iteration 9040/35720 Training loss: 1.0055 0.2124 sec/batch\n",
      "Epoch 6/20  Iteration 9041/35720 Training loss: 1.0057 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 9042/35720 Training loss: 1.0068 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9043/35720 Training loss: 1.0068 0.2336 sec/batch\n",
      "Epoch 6/20  Iteration 9044/35720 Training loss: 1.0075 0.2260 sec/batch\n",
      "Epoch 6/20  Iteration 9045/35720 Training loss: 1.0073 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9046/35720 Training loss: 1.0076 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 9047/35720 Training loss: 1.0076 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 9048/35720 Training loss: 1.0083 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9049/35720 Training loss: 1.0085 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9050/35720 Training loss: 1.0094 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 9051/35720 Training loss: 1.0102 0.2200 sec/batch\n",
      "Epoch 6/20  Iteration 9052/35720 Training loss: 1.0100 0.2263 sec/batch\n",
      "Epoch 6/20  Iteration 9053/35720 Training loss: 1.0105 0.2207 sec/batch\n",
      "Epoch 6/20  Iteration 9054/35720 Training loss: 1.0111 0.2152 sec/batch\n",
      "Epoch 6/20  Iteration 9055/35720 Training loss: 1.0105 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 9056/35720 Training loss: 1.0104 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 9057/35720 Training loss: 1.0105 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9058/35720 Training loss: 1.0105 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 9059/35720 Training loss: 1.0100 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 9060/35720 Training loss: 1.0099 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 9061/35720 Training loss: 1.0094 0.2221 sec/batch\n",
      "Epoch 6/20  Iteration 9062/35720 Training loss: 1.0091 0.2247 sec/batch\n",
      "Epoch 6/20  Iteration 9063/35720 Training loss: 1.0090 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 9064/35720 Training loss: 1.0090 0.2176 sec/batch\n",
      "Epoch 6/20  Iteration 9065/35720 Training loss: 1.0090 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 9066/35720 Training loss: 1.0088 0.3802 sec/batch\n",
      "Epoch 6/20  Iteration 9067/35720 Training loss: 1.0098 0.2191 sec/batch\n",
      "Epoch 6/20  Iteration 9068/35720 Training loss: 1.0101 0.2214 sec/batch\n",
      "Epoch 6/20  Iteration 9069/35720 Training loss: 1.0101 0.2196 sec/batch\n",
      "Epoch 6/20  Iteration 9070/35720 Training loss: 1.0102 0.2118 sec/batch\n",
      "Epoch 6/20  Iteration 9071/35720 Training loss: 1.0097 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 9072/35720 Training loss: 1.0090 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 9073/35720 Training loss: 1.0083 0.2370 sec/batch\n",
      "Epoch 6/20  Iteration 9074/35720 Training loss: 1.0076 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 9075/35720 Training loss: 1.0074 0.2111 sec/batch\n",
      "Epoch 6/20  Iteration 9076/35720 Training loss: 1.0078 0.2173 sec/batch\n",
      "Epoch 6/20  Iteration 9077/35720 Training loss: 1.0073 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9078/35720 Training loss: 1.0073 0.2141 sec/batch\n",
      "Epoch 6/20  Iteration 9079/35720 Training loss: 1.0072 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 9080/35720 Training loss: 1.0065 0.2197 sec/batch\n",
      "Epoch 6/20  Iteration 9081/35720 Training loss: 1.0064 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 9082/35720 Training loss: 1.0065 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 9083/35720 Training loss: 1.0065 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 9084/35720 Training loss: 1.0069 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 9085/35720 Training loss: 1.0072 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 9086/35720 Training loss: 1.0074 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 9087/35720 Training loss: 1.0075 0.2231 sec/batch\n",
      "Epoch 6/20  Iteration 9088/35720 Training loss: 1.0078 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 9089/35720 Training loss: 1.0074 0.2302 sec/batch\n",
      "Epoch 6/20  Iteration 9090/35720 Training loss: 1.0079 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9091/35720 Training loss: 1.0076 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 9092/35720 Training loss: 1.0076 0.2302 sec/batch\n",
      "Epoch 6/20  Iteration 9093/35720 Training loss: 1.0077 0.2303 sec/batch\n",
      "Epoch 6/20  Iteration 9094/35720 Training loss: 1.0079 0.2152 sec/batch\n",
      "Epoch 6/20  Iteration 9095/35720 Training loss: 1.0082 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 9096/35720 Training loss: 1.0085 0.2385 sec/batch\n",
      "Epoch 6/20  Iteration 9097/35720 Training loss: 1.0087 0.2282 sec/batch\n",
      "Epoch 6/20  Iteration 9098/35720 Training loss: 1.0090 0.2221 sec/batch\n",
      "Epoch 6/20  Iteration 9099/35720 Training loss: 1.0093 0.2298 sec/batch\n",
      "Epoch 6/20  Iteration 9100/35720 Training loss: 1.0096 0.2214 sec/batch\n",
      "Epoch 6/20  Iteration 9101/35720 Training loss: 1.0106 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 9102/35720 Training loss: 1.0109 0.2149 sec/batch\n",
      "Epoch 6/20  Iteration 9103/35720 Training loss: 1.0112 0.2225 sec/batch\n",
      "Epoch 6/20  Iteration 9104/35720 Training loss: 1.0117 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 9105/35720 Training loss: 1.0120 0.2283 sec/batch\n",
      "Epoch 6/20  Iteration 9106/35720 Training loss: 1.0118 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 9107/35720 Training loss: 1.0119 0.2232 sec/batch\n",
      "Epoch 6/20  Iteration 9108/35720 Training loss: 1.0117 0.2274 sec/batch\n",
      "Epoch 6/20  Iteration 9109/35720 Training loss: 1.0115 0.2278 sec/batch\n",
      "Epoch 6/20  Iteration 9110/35720 Training loss: 1.0112 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9111/35720 Training loss: 1.0115 0.2251 sec/batch\n",
      "Epoch 6/20  Iteration 9112/35720 Training loss: 1.0117 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 9113/35720 Training loss: 1.0118 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9114/35720 Training loss: 1.0120 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 9115/35720 Training loss: 1.0119 0.2225 sec/batch\n",
      "Epoch 6/20  Iteration 9116/35720 Training loss: 1.0117 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9117/35720 Training loss: 1.0115 0.2187 sec/batch\n",
      "Epoch 6/20  Iteration 9118/35720 Training loss: 1.0114 0.2231 sec/batch\n",
      "Epoch 6/20  Iteration 9119/35720 Training loss: 1.0118 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9120/35720 Training loss: 1.0118 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 9121/35720 Training loss: 1.0121 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9122/35720 Training loss: 1.0128 0.2205 sec/batch\n",
      "Epoch 6/20  Iteration 9123/35720 Training loss: 1.0131 0.2239 sec/batch\n",
      "Epoch 6/20  Iteration 9124/35720 Training loss: 1.0134 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 9125/35720 Training loss: 1.0134 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 9126/35720 Training loss: 1.0137 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 9127/35720 Training loss: 1.0134 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 9128/35720 Training loss: 1.0134 0.2306 sec/batch\n",
      "Epoch 6/20  Iteration 9129/35720 Training loss: 1.0136 0.2297 sec/batch\n",
      "Epoch 6/20  Iteration 9130/35720 Training loss: 1.0139 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 9131/35720 Training loss: 1.0137 0.2121 sec/batch\n",
      "Epoch 6/20  Iteration 9132/35720 Training loss: 1.0135 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 9133/35720 Training loss: 1.0134 0.2124 sec/batch\n",
      "Epoch 6/20  Iteration 9134/35720 Training loss: 1.0131 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 9135/35720 Training loss: 1.0131 0.2372 sec/batch\n",
      "Epoch 6/20  Iteration 9136/35720 Training loss: 1.0130 0.2440 sec/batch\n",
      "Epoch 6/20  Iteration 9137/35720 Training loss: 1.0135 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 9138/35720 Training loss: 1.0139 0.2097 sec/batch\n",
      "Epoch 6/20  Iteration 9139/35720 Training loss: 1.0141 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 9140/35720 Training loss: 1.0142 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 9141/35720 Training loss: 1.0144 0.2298 sec/batch\n",
      "Epoch 6/20  Iteration 9142/35720 Training loss: 1.0143 0.2331 sec/batch\n",
      "Epoch 6/20  Iteration 9143/35720 Training loss: 1.0142 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9144/35720 Training loss: 1.0143 0.2244 sec/batch\n",
      "Epoch 6/20  Iteration 9145/35720 Training loss: 1.0143 0.2228 sec/batch\n",
      "Epoch 6/20  Iteration 9146/35720 Training loss: 1.0143 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 9147/35720 Training loss: 1.0140 0.2279 sec/batch\n",
      "Epoch 6/20  Iteration 9148/35720 Training loss: 1.0139 0.2093 sec/batch\n",
      "Epoch 6/20  Iteration 9149/35720 Training loss: 1.0139 0.2118 sec/batch\n",
      "Epoch 6/20  Iteration 9150/35720 Training loss: 1.0139 0.2088 sec/batch\n",
      "Epoch 6/20  Iteration 9151/35720 Training loss: 1.0141 0.2299 sec/batch\n",
      "Epoch 6/20  Iteration 9152/35720 Training loss: 1.0141 0.2093 sec/batch\n",
      "Epoch 6/20  Iteration 9153/35720 Training loss: 1.0146 0.2309 sec/batch\n",
      "Epoch 6/20  Iteration 9154/35720 Training loss: 1.0148 0.2243 sec/batch\n",
      "Epoch 6/20  Iteration 9155/35720 Training loss: 1.0150 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 9156/35720 Training loss: 1.0149 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 9157/35720 Training loss: 1.0148 0.2248 sec/batch\n",
      "Epoch 6/20  Iteration 9158/35720 Training loss: 1.0145 0.2271 sec/batch\n",
      "Epoch 6/20  Iteration 9159/35720 Training loss: 1.0142 0.2378 sec/batch\n",
      "Epoch 6/20  Iteration 9160/35720 Training loss: 1.0145 0.2176 sec/batch\n",
      "Epoch 6/20  Iteration 9161/35720 Training loss: 1.0147 0.2136 sec/batch\n",
      "Epoch 6/20  Iteration 9162/35720 Training loss: 1.0146 0.2312 sec/batch\n",
      "Epoch 6/20  Iteration 9163/35720 Training loss: 1.0147 0.2308 sec/batch\n",
      "Epoch 6/20  Iteration 9164/35720 Training loss: 1.0147 0.2266 sec/batch\n",
      "Epoch 6/20  Iteration 9165/35720 Training loss: 1.0147 0.2149 sec/batch\n",
      "Epoch 6/20  Iteration 9166/35720 Training loss: 1.0146 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9167/35720 Training loss: 1.0149 0.2261 sec/batch\n",
      "Epoch 6/20  Iteration 9168/35720 Training loss: 1.0147 0.2185 sec/batch\n",
      "Epoch 6/20  Iteration 9169/35720 Training loss: 1.0145 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 9170/35720 Training loss: 1.0146 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 9171/35720 Training loss: 1.0144 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 9172/35720 Training loss: 1.0143 0.2081 sec/batch\n",
      "Epoch 6/20  Iteration 9173/35720 Training loss: 1.0144 0.2202 sec/batch\n",
      "Epoch 6/20  Iteration 9174/35720 Training loss: 1.0144 0.2342 sec/batch\n",
      "Epoch 6/20  Iteration 9175/35720 Training loss: 1.0142 0.2309 sec/batch\n",
      "Epoch 6/20  Iteration 9176/35720 Training loss: 1.0142 0.2063 sec/batch\n",
      "Epoch 6/20  Iteration 9177/35720 Training loss: 1.0142 0.2280 sec/batch\n",
      "Epoch 6/20  Iteration 9178/35720 Training loss: 1.0140 0.2227 sec/batch\n",
      "Epoch 6/20  Iteration 9179/35720 Training loss: 1.0137 0.2148 sec/batch\n",
      "Epoch 6/20  Iteration 9180/35720 Training loss: 1.0136 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 9181/35720 Training loss: 1.0136 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 9182/35720 Training loss: 1.0133 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 9183/35720 Training loss: 1.0131 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9184/35720 Training loss: 1.0133 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 9185/35720 Training loss: 1.0136 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 9186/35720 Training loss: 1.0137 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 9187/35720 Training loss: 1.0138 0.2169 sec/batch\n",
      "Epoch 6/20  Iteration 9188/35720 Training loss: 1.0138 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 9189/35720 Training loss: 1.0142 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 9190/35720 Training loss: 1.0140 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 9191/35720 Training loss: 1.0139 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 9192/35720 Training loss: 1.0138 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 9193/35720 Training loss: 1.0137 0.2059 sec/batch\n",
      "Epoch 6/20  Iteration 9194/35720 Training loss: 1.0138 0.2095 sec/batch\n",
      "Epoch 6/20  Iteration 9195/35720 Training loss: 1.0138 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 9196/35720 Training loss: 1.0140 0.2275 sec/batch\n",
      "Epoch 6/20  Iteration 9197/35720 Training loss: 1.0138 0.2376 sec/batch\n",
      "Epoch 6/20  Iteration 9198/35720 Training loss: 1.0138 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 9199/35720 Training loss: 1.0134 0.2301 sec/batch\n",
      "Epoch 6/20  Iteration 9200/35720 Training loss: 1.0128 0.2215 sec/batch\n",
      "Validation loss: 1.30707 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 9201/35720 Training loss: 1.0129 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 9202/35720 Training loss: 1.0129 0.2066 sec/batch\n",
      "Epoch 6/20  Iteration 9203/35720 Training loss: 1.0129 0.2082 sec/batch\n",
      "Epoch 6/20  Iteration 9204/35720 Training loss: 1.0129 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 9205/35720 Training loss: 1.0126 0.2262 sec/batch\n",
      "Epoch 6/20  Iteration 9206/35720 Training loss: 1.0124 0.2295 sec/batch\n",
      "Epoch 6/20  Iteration 9207/35720 Training loss: 1.0121 0.2240 sec/batch\n",
      "Epoch 6/20  Iteration 9208/35720 Training loss: 1.0119 0.2169 sec/batch\n",
      "Epoch 6/20  Iteration 9209/35720 Training loss: 1.0116 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 9210/35720 Training loss: 1.0116 0.2208 sec/batch\n",
      "Epoch 6/20  Iteration 9211/35720 Training loss: 1.0117 0.2066 sec/batch\n",
      "Epoch 6/20  Iteration 9212/35720 Training loss: 1.0113 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 9213/35720 Training loss: 1.0109 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9214/35720 Training loss: 1.0106 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 9215/35720 Training loss: 1.0108 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 9216/35720 Training loss: 1.0108 0.2218 sec/batch\n",
      "Epoch 6/20  Iteration 9217/35720 Training loss: 1.0103 0.2189 sec/batch\n",
      "Epoch 6/20  Iteration 9218/35720 Training loss: 1.0103 0.2264 sec/batch\n",
      "Epoch 6/20  Iteration 9219/35720 Training loss: 1.0104 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 9220/35720 Training loss: 1.0104 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 9221/35720 Training loss: 1.0103 0.2095 sec/batch\n",
      "Epoch 6/20  Iteration 9222/35720 Training loss: 1.0103 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 9223/35720 Training loss: 1.0103 0.2097 sec/batch\n",
      "Epoch 6/20  Iteration 9224/35720 Training loss: 1.0102 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 9225/35720 Training loss: 1.0108 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 9226/35720 Training loss: 1.0107 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 9227/35720 Training loss: 1.0106 0.2229 sec/batch\n",
      "Epoch 6/20  Iteration 9228/35720 Training loss: 1.0108 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9229/35720 Training loss: 1.0107 0.2289 sec/batch\n",
      "Epoch 6/20  Iteration 9230/35720 Training loss: 1.0106 0.2280 sec/batch\n",
      "Epoch 6/20  Iteration 9231/35720 Training loss: 1.0107 0.2122 sec/batch\n",
      "Epoch 6/20  Iteration 9232/35720 Training loss: 1.0104 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 9233/35720 Training loss: 1.0104 0.2386 sec/batch\n",
      "Epoch 6/20  Iteration 9234/35720 Training loss: 1.0103 0.2179 sec/batch\n",
      "Epoch 6/20  Iteration 9235/35720 Training loss: 1.0102 0.2200 sec/batch\n",
      "Epoch 6/20  Iteration 9236/35720 Training loss: 1.0100 0.2128 sec/batch\n",
      "Epoch 6/20  Iteration 9237/35720 Training loss: 1.0102 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 9238/35720 Training loss: 1.0103 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9239/35720 Training loss: 1.0101 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 9240/35720 Training loss: 1.0098 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9241/35720 Training loss: 1.0097 0.2213 sec/batch\n",
      "Epoch 6/20  Iteration 9242/35720 Training loss: 1.0096 0.2258 sec/batch\n",
      "Epoch 6/20  Iteration 9243/35720 Training loss: 1.0095 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 9244/35720 Training loss: 1.0094 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 9245/35720 Training loss: 1.0093 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 9246/35720 Training loss: 1.0092 0.2219 sec/batch\n",
      "Epoch 6/20  Iteration 9247/35720 Training loss: 1.0091 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 9248/35720 Training loss: 1.0092 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 9249/35720 Training loss: 1.0093 0.2087 sec/batch\n",
      "Epoch 6/20  Iteration 9250/35720 Training loss: 1.0092 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 9251/35720 Training loss: 1.0093 0.2202 sec/batch\n",
      "Epoch 6/20  Iteration 9252/35720 Training loss: 1.0092 0.2280 sec/batch\n",
      "Epoch 6/20  Iteration 9253/35720 Training loss: 1.0094 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 9254/35720 Training loss: 1.0094 0.2098 sec/batch\n",
      "Epoch 6/20  Iteration 9255/35720 Training loss: 1.0093 0.2245 sec/batch\n",
      "Epoch 6/20  Iteration 9256/35720 Training loss: 1.0094 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 9257/35720 Training loss: 1.0095 0.2196 sec/batch\n",
      "Epoch 6/20  Iteration 9258/35720 Training loss: 1.0096 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 9259/35720 Training loss: 1.0095 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9260/35720 Training loss: 1.0095 0.2185 sec/batch\n",
      "Epoch 6/20  Iteration 9261/35720 Training loss: 1.0097 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 9262/35720 Training loss: 1.0098 0.2063 sec/batch\n",
      "Epoch 6/20  Iteration 9263/35720 Training loss: 1.0098 0.2128 sec/batch\n",
      "Epoch 6/20  Iteration 9264/35720 Training loss: 1.0097 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9265/35720 Training loss: 1.0096 0.2234 sec/batch\n",
      "Epoch 6/20  Iteration 9266/35720 Training loss: 1.0095 0.2143 sec/batch\n",
      "Epoch 6/20  Iteration 9267/35720 Training loss: 1.0094 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 9268/35720 Training loss: 1.0093 0.2245 sec/batch\n",
      "Epoch 6/20  Iteration 9269/35720 Training loss: 1.0092 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 9270/35720 Training loss: 1.0092 0.2104 sec/batch\n",
      "Epoch 6/20  Iteration 9271/35720 Training loss: 1.0091 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 9272/35720 Training loss: 1.0091 0.2267 sec/batch\n",
      "Epoch 6/20  Iteration 9273/35720 Training loss: 1.0092 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 9274/35720 Training loss: 1.0091 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 9275/35720 Training loss: 1.0088 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 9276/35720 Training loss: 1.0090 0.2082 sec/batch\n",
      "Epoch 6/20  Iteration 9277/35720 Training loss: 1.0090 0.2258 sec/batch\n",
      "Epoch 6/20  Iteration 9278/35720 Training loss: 1.0091 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 9279/35720 Training loss: 1.0092 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9280/35720 Training loss: 1.0092 0.2225 sec/batch\n",
      "Epoch 6/20  Iteration 9281/35720 Training loss: 1.0092 0.2186 sec/batch\n",
      "Epoch 6/20  Iteration 9282/35720 Training loss: 1.0091 0.2086 sec/batch\n",
      "Epoch 6/20  Iteration 9283/35720 Training loss: 1.0091 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 9284/35720 Training loss: 1.0090 0.2293 sec/batch\n",
      "Epoch 6/20  Iteration 9285/35720 Training loss: 1.0091 0.2280 sec/batch\n",
      "Epoch 6/20  Iteration 9286/35720 Training loss: 1.0091 0.2191 sec/batch\n",
      "Epoch 6/20  Iteration 9287/35720 Training loss: 1.0092 0.2794 sec/batch\n",
      "Epoch 6/20  Iteration 9288/35720 Training loss: 1.0092 0.2446 sec/batch\n",
      "Epoch 6/20  Iteration 9289/35720 Training loss: 1.0090 0.2793 sec/batch\n",
      "Epoch 6/20  Iteration 9290/35720 Training loss: 1.0089 0.2264 sec/batch\n",
      "Epoch 6/20  Iteration 9291/35720 Training loss: 1.0088 0.2121 sec/batch\n",
      "Epoch 6/20  Iteration 9292/35720 Training loss: 1.0089 0.2276 sec/batch\n",
      "Epoch 6/20  Iteration 9293/35720 Training loss: 1.0088 0.2274 sec/batch\n",
      "Epoch 6/20  Iteration 9294/35720 Training loss: 1.0088 0.2128 sec/batch\n",
      "Epoch 6/20  Iteration 9295/35720 Training loss: 1.0087 0.2282 sec/batch\n",
      "Epoch 6/20  Iteration 9296/35720 Training loss: 1.0087 0.2301 sec/batch\n",
      "Epoch 6/20  Iteration 9297/35720 Training loss: 1.0086 0.2085 sec/batch\n",
      "Epoch 6/20  Iteration 9298/35720 Training loss: 1.0084 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 9299/35720 Training loss: 1.0082 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 9300/35720 Training loss: 1.0081 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9301/35720 Training loss: 1.0079 0.2076 sec/batch\n",
      "Epoch 6/20  Iteration 9302/35720 Training loss: 1.0078 0.2069 sec/batch\n",
      "Epoch 6/20  Iteration 9303/35720 Training loss: 1.0077 0.2085 sec/batch\n",
      "Epoch 6/20  Iteration 9304/35720 Training loss: 1.0076 0.2200 sec/batch\n",
      "Epoch 6/20  Iteration 9305/35720 Training loss: 1.0076 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 9306/35720 Training loss: 1.0076 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 9307/35720 Training loss: 1.0076 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 9308/35720 Training loss: 1.0075 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 9309/35720 Training loss: 1.0073 0.2386 sec/batch\n",
      "Epoch 6/20  Iteration 9310/35720 Training loss: 1.0071 0.2594 sec/batch\n",
      "Epoch 6/20  Iteration 9311/35720 Training loss: 1.0069 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 9312/35720 Training loss: 1.0069 0.2258 sec/batch\n",
      "Epoch 6/20  Iteration 9313/35720 Training loss: 1.0068 0.2246 sec/batch\n",
      "Epoch 6/20  Iteration 9314/35720 Training loss: 1.0067 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9315/35720 Training loss: 1.0067 0.2221 sec/batch\n",
      "Epoch 6/20  Iteration 9316/35720 Training loss: 1.0067 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 9317/35720 Training loss: 1.0068 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 9318/35720 Training loss: 1.0068 0.2129 sec/batch\n",
      "Epoch 6/20  Iteration 9319/35720 Training loss: 1.0069 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 9320/35720 Training loss: 1.0069 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 9321/35720 Training loss: 1.0069 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 9322/35720 Training loss: 1.0067 0.2219 sec/batch\n",
      "Epoch 6/20  Iteration 9323/35720 Training loss: 1.0067 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 9324/35720 Training loss: 1.0065 0.2270 sec/batch\n",
      "Epoch 6/20  Iteration 9325/35720 Training loss: 1.0065 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 9326/35720 Training loss: 1.0064 0.2360 sec/batch\n",
      "Epoch 6/20  Iteration 9327/35720 Training loss: 1.0064 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 9328/35720 Training loss: 1.0061 0.2170 sec/batch\n",
      "Epoch 6/20  Iteration 9329/35720 Training loss: 1.0059 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 9330/35720 Training loss: 1.0058 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 9331/35720 Training loss: 1.0057 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 9332/35720 Training loss: 1.0059 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 9333/35720 Training loss: 1.0061 0.2072 sec/batch\n",
      "Epoch 6/20  Iteration 9334/35720 Training loss: 1.0060 0.2285 sec/batch\n",
      "Epoch 6/20  Iteration 9335/35720 Training loss: 1.0059 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 9336/35720 Training loss: 1.0058 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 9337/35720 Training loss: 1.0057 0.2078 sec/batch\n",
      "Epoch 6/20  Iteration 9338/35720 Training loss: 1.0056 0.2173 sec/batch\n",
      "Epoch 6/20  Iteration 9339/35720 Training loss: 1.0055 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 9340/35720 Training loss: 1.0054 0.2263 sec/batch\n",
      "Epoch 6/20  Iteration 9341/35720 Training loss: 1.0054 0.2170 sec/batch\n",
      "Epoch 6/20  Iteration 9342/35720 Training loss: 1.0052 0.2230 sec/batch\n",
      "Epoch 6/20  Iteration 9343/35720 Training loss: 1.0049 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9344/35720 Training loss: 1.0049 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 9345/35720 Training loss: 1.0048 0.2143 sec/batch\n",
      "Epoch 6/20  Iteration 9346/35720 Training loss: 1.0047 0.2216 sec/batch\n",
      "Epoch 6/20  Iteration 9347/35720 Training loss: 1.0046 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9348/35720 Training loss: 1.0045 0.2170 sec/batch\n",
      "Epoch 6/20  Iteration 9349/35720 Training loss: 1.0043 0.2085 sec/batch\n",
      "Epoch 6/20  Iteration 9350/35720 Training loss: 1.0043 0.2246 sec/batch\n",
      "Epoch 6/20  Iteration 9351/35720 Training loss: 1.0043 0.2124 sec/batch\n",
      "Epoch 6/20  Iteration 9352/35720 Training loss: 1.0043 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 9353/35720 Training loss: 1.0045 0.2200 sec/batch\n",
      "Epoch 6/20  Iteration 9354/35720 Training loss: 1.0045 0.2184 sec/batch\n",
      "Epoch 6/20  Iteration 9355/35720 Training loss: 1.0045 0.2079 sec/batch\n",
      "Epoch 6/20  Iteration 9356/35720 Training loss: 1.0042 0.2238 sec/batch\n",
      "Epoch 6/20  Iteration 9357/35720 Training loss: 1.0041 0.2311 sec/batch\n",
      "Epoch 6/20  Iteration 9358/35720 Training loss: 1.0042 0.2228 sec/batch\n",
      "Epoch 6/20  Iteration 9359/35720 Training loss: 1.0043 0.2355 sec/batch\n",
      "Epoch 6/20  Iteration 9360/35720 Training loss: 1.0044 0.2276 sec/batch\n",
      "Epoch 6/20  Iteration 9361/35720 Training loss: 1.0045 0.2220 sec/batch\n",
      "Epoch 6/20  Iteration 9362/35720 Training loss: 1.0046 0.2284 sec/batch\n",
      "Epoch 6/20  Iteration 9363/35720 Training loss: 1.0048 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9364/35720 Training loss: 1.0049 0.2288 sec/batch\n",
      "Epoch 6/20  Iteration 9365/35720 Training loss: 1.0050 0.2295 sec/batch\n",
      "Epoch 6/20  Iteration 9366/35720 Training loss: 1.0049 0.2316 sec/batch\n",
      "Epoch 6/20  Iteration 9367/35720 Training loss: 1.0050 0.2261 sec/batch\n",
      "Epoch 6/20  Iteration 9368/35720 Training loss: 1.0052 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 9369/35720 Training loss: 1.0053 0.2134 sec/batch\n",
      "Epoch 6/20  Iteration 9370/35720 Training loss: 1.0053 0.2199 sec/batch\n",
      "Epoch 6/20  Iteration 9371/35720 Training loss: 1.0056 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 9372/35720 Training loss: 1.0058 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 9373/35720 Training loss: 1.0058 0.2239 sec/batch\n",
      "Epoch 6/20  Iteration 9374/35720 Training loss: 1.0058 0.2128 sec/batch\n",
      "Epoch 6/20  Iteration 9375/35720 Training loss: 1.0058 0.2227 sec/batch\n",
      "Epoch 6/20  Iteration 9376/35720 Training loss: 1.0059 0.2098 sec/batch\n",
      "Epoch 6/20  Iteration 9377/35720 Training loss: 1.0062 0.2207 sec/batch\n",
      "Epoch 6/20  Iteration 9378/35720 Training loss: 1.0063 0.2135 sec/batch\n",
      "Epoch 6/20  Iteration 9379/35720 Training loss: 1.0065 0.2241 sec/batch\n",
      "Epoch 6/20  Iteration 9380/35720 Training loss: 1.0064 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 9381/35720 Training loss: 1.0062 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 9382/35720 Training loss: 1.0061 0.2200 sec/batch\n",
      "Epoch 6/20  Iteration 9383/35720 Training loss: 1.0061 0.2242 sec/batch\n",
      "Epoch 6/20  Iteration 9384/35720 Training loss: 1.0062 0.2232 sec/batch\n",
      "Epoch 6/20  Iteration 9385/35720 Training loss: 1.0064 0.2141 sec/batch\n",
      "Epoch 6/20  Iteration 9386/35720 Training loss: 1.0065 0.2277 sec/batch\n",
      "Epoch 6/20  Iteration 9387/35720 Training loss: 1.0067 0.2382 sec/batch\n",
      "Epoch 6/20  Iteration 9388/35720 Training loss: 1.0068 0.2280 sec/batch\n",
      "Epoch 6/20  Iteration 9389/35720 Training loss: 1.0067 0.2102 sec/batch\n",
      "Epoch 6/20  Iteration 9390/35720 Training loss: 1.0065 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 9391/35720 Training loss: 1.0065 0.2136 sec/batch\n",
      "Epoch 6/20  Iteration 9392/35720 Training loss: 1.0066 0.2206 sec/batch\n",
      "Epoch 6/20  Iteration 9393/35720 Training loss: 1.0067 0.2276 sec/batch\n",
      "Epoch 6/20  Iteration 9394/35720 Training loss: 1.0066 0.2330 sec/batch\n",
      "Epoch 6/20  Iteration 9395/35720 Training loss: 1.0065 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 9396/35720 Training loss: 1.0064 0.2207 sec/batch\n",
      "Epoch 6/20  Iteration 9397/35720 Training loss: 1.0064 0.2238 sec/batch\n",
      "Epoch 6/20  Iteration 9398/35720 Training loss: 1.0063 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 9399/35720 Training loss: 1.0061 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 9400/35720 Training loss: 1.0060 0.2203 sec/batch\n",
      "Validation loss: 1.31021 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 9401/35720 Training loss: 1.0062 0.2128 sec/batch\n",
      "Epoch 6/20  Iteration 9402/35720 Training loss: 1.0061 0.2273 sec/batch\n",
      "Epoch 6/20  Iteration 9403/35720 Training loss: 1.0061 0.2078 sec/batch\n",
      "Epoch 6/20  Iteration 9404/35720 Training loss: 1.0060 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9405/35720 Training loss: 1.0060 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 9406/35720 Training loss: 1.0059 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 9407/35720 Training loss: 1.0059 0.2198 sec/batch\n",
      "Epoch 6/20  Iteration 9408/35720 Training loss: 1.0059 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 9409/35720 Training loss: 1.0059 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 9410/35720 Training loss: 1.0058 0.2629 sec/batch\n",
      "Epoch 6/20  Iteration 9411/35720 Training loss: 1.0057 0.2086 sec/batch\n",
      "Epoch 6/20  Iteration 9412/35720 Training loss: 1.0055 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 9413/35720 Training loss: 1.0056 0.2173 sec/batch\n",
      "Epoch 6/20  Iteration 9414/35720 Training loss: 1.0056 0.2202 sec/batch\n",
      "Epoch 6/20  Iteration 9415/35720 Training loss: 1.0055 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 9416/35720 Training loss: 1.0054 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 9417/35720 Training loss: 1.0052 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 9418/35720 Training loss: 1.0052 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 9419/35720 Training loss: 1.0051 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 9420/35720 Training loss: 1.0049 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 9421/35720 Training loss: 1.0049 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 9422/35720 Training loss: 1.0049 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 9423/35720 Training loss: 1.0049 0.2230 sec/batch\n",
      "Epoch 6/20  Iteration 9424/35720 Training loss: 1.0048 0.2348 sec/batch\n",
      "Epoch 6/20  Iteration 9425/35720 Training loss: 1.0048 0.2401 sec/batch\n",
      "Epoch 6/20  Iteration 9426/35720 Training loss: 1.0046 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 9427/35720 Training loss: 1.0046 0.2063 sec/batch\n",
      "Epoch 6/20  Iteration 9428/35720 Training loss: 1.0047 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 9429/35720 Training loss: 1.0046 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9430/35720 Training loss: 1.0046 0.2270 sec/batch\n",
      "Epoch 6/20  Iteration 9431/35720 Training loss: 1.0045 0.2300 sec/batch\n",
      "Epoch 6/20  Iteration 9432/35720 Training loss: 1.0042 0.2381 sec/batch\n",
      "Epoch 6/20  Iteration 9433/35720 Training loss: 1.0040 0.2110 sec/batch\n",
      "Epoch 6/20  Iteration 9434/35720 Training loss: 1.0039 0.2220 sec/batch\n",
      "Epoch 6/20  Iteration 9435/35720 Training loss: 1.0040 0.2292 sec/batch\n",
      "Epoch 6/20  Iteration 9436/35720 Training loss: 1.0039 0.2295 sec/batch\n",
      "Epoch 6/20  Iteration 9437/35720 Training loss: 1.0039 0.2139 sec/batch\n",
      "Epoch 6/20  Iteration 9438/35720 Training loss: 1.0038 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 9439/35720 Training loss: 1.0038 0.2088 sec/batch\n",
      "Epoch 6/20  Iteration 9440/35720 Training loss: 1.0037 0.2197 sec/batch\n",
      "Epoch 6/20  Iteration 9441/35720 Training loss: 1.0035 0.2317 sec/batch\n",
      "Epoch 6/20  Iteration 9442/35720 Training loss: 1.0034 0.2229 sec/batch\n",
      "Epoch 6/20  Iteration 9443/35720 Training loss: 1.0033 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 9444/35720 Training loss: 1.0033 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 9445/35720 Training loss: 1.0034 0.2295 sec/batch\n",
      "Epoch 6/20  Iteration 9446/35720 Training loss: 1.0034 0.2206 sec/batch\n",
      "Epoch 6/20  Iteration 9447/35720 Training loss: 1.0033 0.2185 sec/batch\n",
      "Epoch 6/20  Iteration 9448/35720 Training loss: 1.0034 0.2102 sec/batch\n",
      "Epoch 6/20  Iteration 9449/35720 Training loss: 1.0033 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 9450/35720 Training loss: 1.0034 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 9451/35720 Training loss: 1.0032 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 9452/35720 Training loss: 1.0033 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 9453/35720 Training loss: 1.0032 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9454/35720 Training loss: 1.0031 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 9455/35720 Training loss: 1.0031 0.2110 sec/batch\n",
      "Epoch 6/20  Iteration 9456/35720 Training loss: 1.0031 0.2174 sec/batch\n",
      "Epoch 6/20  Iteration 9457/35720 Training loss: 1.0030 0.2285 sec/batch\n",
      "Epoch 6/20  Iteration 9458/35720 Training loss: 1.0029 0.2226 sec/batch\n",
      "Epoch 6/20  Iteration 9459/35720 Training loss: 1.0029 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 9460/35720 Training loss: 1.0027 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 9461/35720 Training loss: 1.0027 0.2061 sec/batch\n",
      "Epoch 6/20  Iteration 9462/35720 Training loss: 1.0027 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9463/35720 Training loss: 1.0026 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 9464/35720 Training loss: 1.0027 0.2244 sec/batch\n",
      "Epoch 6/20  Iteration 9465/35720 Training loss: 1.0025 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 9466/35720 Training loss: 1.0024 0.2143 sec/batch\n",
      "Epoch 6/20  Iteration 9467/35720 Training loss: 1.0023 0.2225 sec/batch\n",
      "Epoch 6/20  Iteration 9468/35720 Training loss: 1.0022 0.2306 sec/batch\n",
      "Epoch 6/20  Iteration 9469/35720 Training loss: 1.0022 0.2317 sec/batch\n",
      "Epoch 6/20  Iteration 9470/35720 Training loss: 1.0020 0.2322 sec/batch\n",
      "Epoch 6/20  Iteration 9471/35720 Training loss: 1.0020 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 9472/35720 Training loss: 1.0018 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 9473/35720 Training loss: 1.0017 0.2207 sec/batch\n",
      "Epoch 6/20  Iteration 9474/35720 Training loss: 1.0016 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 9475/35720 Training loss: 1.0017 0.2203 sec/batch\n",
      "Epoch 6/20  Iteration 9476/35720 Training loss: 1.0018 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9477/35720 Training loss: 1.0017 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 9478/35720 Training loss: 1.0017 0.2121 sec/batch\n",
      "Epoch 6/20  Iteration 9479/35720 Training loss: 1.0016 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 9480/35720 Training loss: 1.0016 0.2202 sec/batch\n",
      "Epoch 6/20  Iteration 9481/35720 Training loss: 1.0015 0.2174 sec/batch\n",
      "Epoch 6/20  Iteration 9482/35720 Training loss: 1.0015 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9483/35720 Training loss: 1.0014 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 9484/35720 Training loss: 1.0014 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 9485/35720 Training loss: 1.0012 0.2366 sec/batch\n",
      "Epoch 6/20  Iteration 9486/35720 Training loss: 1.0013 0.2245 sec/batch\n",
      "Epoch 6/20  Iteration 9487/35720 Training loss: 1.0012 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 9488/35720 Training loss: 1.0014 0.2350 sec/batch\n",
      "Epoch 6/20  Iteration 9489/35720 Training loss: 1.0013 0.2251 sec/batch\n",
      "Epoch 6/20  Iteration 9490/35720 Training loss: 1.0011 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 9491/35720 Training loss: 1.0012 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 9492/35720 Training loss: 1.0011 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 9493/35720 Training loss: 1.0010 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9494/35720 Training loss: 1.0009 0.2135 sec/batch\n",
      "Epoch 6/20  Iteration 9495/35720 Training loss: 1.0007 0.2399 sec/batch\n",
      "Epoch 6/20  Iteration 9496/35720 Training loss: 1.0006 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9497/35720 Training loss: 1.0007 0.2185 sec/batch\n",
      "Epoch 6/20  Iteration 9498/35720 Training loss: 1.0005 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 9499/35720 Training loss: 1.0005 0.2179 sec/batch\n",
      "Epoch 6/20  Iteration 9500/35720 Training loss: 1.0005 0.2076 sec/batch\n",
      "Epoch 6/20  Iteration 9501/35720 Training loss: 1.0005 0.2155 sec/batch\n",
      "Epoch 6/20  Iteration 9502/35720 Training loss: 1.0005 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 9503/35720 Training loss: 1.0006 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 9504/35720 Training loss: 1.0007 0.2314 sec/batch\n",
      "Epoch 6/20  Iteration 9505/35720 Training loss: 1.0008 0.2609 sec/batch\n",
      "Epoch 6/20  Iteration 9506/35720 Training loss: 1.0008 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9507/35720 Training loss: 1.0009 0.2132 sec/batch\n",
      "Epoch 6/20  Iteration 9508/35720 Training loss: 1.0009 0.2110 sec/batch\n",
      "Epoch 6/20  Iteration 9509/35720 Training loss: 1.0008 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9510/35720 Training loss: 1.0007 0.2095 sec/batch\n",
      "Epoch 6/20  Iteration 9511/35720 Training loss: 1.0007 0.2337 sec/batch\n",
      "Epoch 6/20  Iteration 9512/35720 Training loss: 1.0007 0.2081 sec/batch\n",
      "Epoch 6/20  Iteration 9513/35720 Training loss: 1.0007 0.2213 sec/batch\n",
      "Epoch 6/20  Iteration 9514/35720 Training loss: 1.0008 0.2249 sec/batch\n",
      "Epoch 6/20  Iteration 9515/35720 Training loss: 1.0006 0.2054 sec/batch\n",
      "Epoch 6/20  Iteration 9516/35720 Training loss: 1.0004 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 9517/35720 Training loss: 1.0003 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 9518/35720 Training loss: 1.0002 0.2278 sec/batch\n",
      "Epoch 6/20  Iteration 9519/35720 Training loss: 1.0001 0.2302 sec/batch\n",
      "Epoch 6/20  Iteration 9520/35720 Training loss: 1.0000 0.2225 sec/batch\n",
      "Epoch 6/20  Iteration 9521/35720 Training loss: 1.0001 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 9522/35720 Training loss: 1.0000 0.2173 sec/batch\n",
      "Epoch 6/20  Iteration 9523/35720 Training loss: 1.0000 0.2261 sec/batch\n",
      "Epoch 6/20  Iteration 9524/35720 Training loss: 1.0001 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9525/35720 Training loss: 0.9999 0.2246 sec/batch\n",
      "Epoch 6/20  Iteration 9526/35720 Training loss: 0.9999 0.2074 sec/batch\n",
      "Epoch 6/20  Iteration 9527/35720 Training loss: 0.9998 0.2212 sec/batch\n",
      "Epoch 6/20  Iteration 9528/35720 Training loss: 0.9999 0.2134 sec/batch\n",
      "Epoch 6/20  Iteration 9529/35720 Training loss: 0.9998 0.2281 sec/batch\n",
      "Epoch 6/20  Iteration 9530/35720 Training loss: 0.9996 0.2227 sec/batch\n",
      "Epoch 6/20  Iteration 9531/35720 Training loss: 0.9995 0.2148 sec/batch\n",
      "Epoch 6/20  Iteration 9532/35720 Training loss: 0.9994 0.2282 sec/batch\n",
      "Epoch 6/20  Iteration 9533/35720 Training loss: 0.9993 0.2296 sec/batch\n",
      "Epoch 6/20  Iteration 9534/35720 Training loss: 0.9992 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 9535/35720 Training loss: 0.9992 0.2278 sec/batch\n",
      "Epoch 6/20  Iteration 9536/35720 Training loss: 0.9993 0.2285 sec/batch\n",
      "Epoch 6/20  Iteration 9537/35720 Training loss: 0.9993 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 9538/35720 Training loss: 0.9993 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9539/35720 Training loss: 0.9994 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9540/35720 Training loss: 0.9993 0.2224 sec/batch\n",
      "Epoch 6/20  Iteration 9541/35720 Training loss: 0.9993 0.2185 sec/batch\n",
      "Epoch 6/20  Iteration 9542/35720 Training loss: 0.9992 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 9543/35720 Training loss: 0.9992 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 9544/35720 Training loss: 0.9992 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9545/35720 Training loss: 0.9991 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 9546/35720 Training loss: 0.9990 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9547/35720 Training loss: 0.9990 0.2266 sec/batch\n",
      "Epoch 6/20  Iteration 9548/35720 Training loss: 0.9990 0.2056 sec/batch\n",
      "Epoch 6/20  Iteration 9549/35720 Training loss: 0.9988 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 9550/35720 Training loss: 0.9987 0.2309 sec/batch\n",
      "Epoch 6/20  Iteration 9551/35720 Training loss: 0.9987 0.2218 sec/batch\n",
      "Epoch 6/20  Iteration 9552/35720 Training loss: 0.9987 0.2244 sec/batch\n",
      "Epoch 6/20  Iteration 9553/35720 Training loss: 0.9986 0.2179 sec/batch\n",
      "Epoch 6/20  Iteration 9554/35720 Training loss: 0.9985 0.2093 sec/batch\n",
      "Epoch 6/20  Iteration 9555/35720 Training loss: 0.9984 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 9556/35720 Training loss: 0.9984 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 9557/35720 Training loss: 0.9984 0.2254 sec/batch\n",
      "Epoch 6/20  Iteration 9558/35720 Training loss: 0.9982 0.2216 sec/batch\n",
      "Epoch 6/20  Iteration 9559/35720 Training loss: 0.9982 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9560/35720 Training loss: 0.9981 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 9561/35720 Training loss: 0.9982 0.2254 sec/batch\n",
      "Epoch 6/20  Iteration 9562/35720 Training loss: 0.9981 0.2202 sec/batch\n",
      "Epoch 6/20  Iteration 9563/35720 Training loss: 0.9980 0.2191 sec/batch\n",
      "Epoch 6/20  Iteration 9564/35720 Training loss: 0.9980 0.2058 sec/batch\n",
      "Epoch 6/20  Iteration 9565/35720 Training loss: 0.9980 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 9566/35720 Training loss: 0.9980 0.2110 sec/batch\n",
      "Epoch 6/20  Iteration 9567/35720 Training loss: 0.9980 0.2283 sec/batch\n",
      "Epoch 6/20  Iteration 9568/35720 Training loss: 0.9979 0.2212 sec/batch\n",
      "Epoch 6/20  Iteration 9569/35720 Training loss: 0.9979 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 9570/35720 Training loss: 0.9979 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 9571/35720 Training loss: 0.9981 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 9572/35720 Training loss: 0.9981 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 9573/35720 Training loss: 0.9982 0.2214 sec/batch\n",
      "Epoch 6/20  Iteration 9574/35720 Training loss: 0.9981 0.2148 sec/batch\n",
      "Epoch 6/20  Iteration 9575/35720 Training loss: 0.9981 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 9576/35720 Training loss: 0.9981 0.2222 sec/batch\n",
      "Epoch 6/20  Iteration 9577/35720 Training loss: 0.9981 0.2306 sec/batch\n",
      "Epoch 6/20  Iteration 9578/35720 Training loss: 0.9981 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9579/35720 Training loss: 0.9980 0.2128 sec/batch\n",
      "Epoch 6/20  Iteration 9580/35720 Training loss: 0.9978 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9581/35720 Training loss: 0.9978 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9582/35720 Training loss: 0.9978 0.2227 sec/batch\n",
      "Epoch 6/20  Iteration 9583/35720 Training loss: 0.9979 0.2300 sec/batch\n",
      "Epoch 6/20  Iteration 9584/35720 Training loss: 0.9981 0.2071 sec/batch\n",
      "Epoch 6/20  Iteration 9585/35720 Training loss: 0.9981 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9586/35720 Training loss: 0.9981 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 9587/35720 Training loss: 0.9983 0.2118 sec/batch\n",
      "Epoch 6/20  Iteration 9588/35720 Training loss: 0.9984 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9589/35720 Training loss: 0.9984 0.2203 sec/batch\n",
      "Epoch 6/20  Iteration 9590/35720 Training loss: 0.9984 0.2220 sec/batch\n",
      "Epoch 6/20  Iteration 9591/35720 Training loss: 0.9984 0.2263 sec/batch\n",
      "Epoch 6/20  Iteration 9592/35720 Training loss: 0.9985 0.2128 sec/batch\n",
      "Epoch 6/20  Iteration 9593/35720 Training loss: 0.9985 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 9594/35720 Training loss: 0.9985 0.2284 sec/batch\n",
      "Epoch 6/20  Iteration 9595/35720 Training loss: 0.9986 0.2276 sec/batch\n",
      "Epoch 6/20  Iteration 9596/35720 Training loss: 0.9986 0.2245 sec/batch\n",
      "Epoch 6/20  Iteration 9597/35720 Training loss: 0.9988 0.2067 sec/batch\n",
      "Epoch 6/20  Iteration 9598/35720 Training loss: 0.9987 0.2082 sec/batch\n",
      "Epoch 6/20  Iteration 9599/35720 Training loss: 0.9986 0.2269 sec/batch\n",
      "Epoch 6/20  Iteration 9600/35720 Training loss: 0.9986 0.2117 sec/batch\n",
      "Validation loss: 1.31114 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 9601/35720 Training loss: 0.9988 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 9602/35720 Training loss: 0.9988 0.2065 sec/batch\n",
      "Epoch 6/20  Iteration 9603/35720 Training loss: 0.9989 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 9604/35720 Training loss: 0.9988 0.2253 sec/batch\n",
      "Epoch 6/20  Iteration 9605/35720 Training loss: 0.9987 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9606/35720 Training loss: 0.9987 0.2232 sec/batch\n",
      "Epoch 6/20  Iteration 9607/35720 Training loss: 0.9987 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 9608/35720 Training loss: 0.9987 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 9609/35720 Training loss: 0.9987 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9610/35720 Training loss: 0.9986 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 9611/35720 Training loss: 0.9986 0.2236 sec/batch\n",
      "Epoch 6/20  Iteration 9612/35720 Training loss: 0.9986 0.2253 sec/batch\n",
      "Epoch 6/20  Iteration 9613/35720 Training loss: 0.9985 0.2375 sec/batch\n",
      "Epoch 6/20  Iteration 9614/35720 Training loss: 0.9984 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 9615/35720 Training loss: 0.9986 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 9616/35720 Training loss: 0.9985 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9617/35720 Training loss: 0.9985 0.2233 sec/batch\n",
      "Epoch 6/20  Iteration 9618/35720 Training loss: 0.9985 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 9619/35720 Training loss: 0.9985 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 9620/35720 Training loss: 0.9984 0.2131 sec/batch\n",
      "Epoch 6/20  Iteration 9621/35720 Training loss: 0.9985 0.2121 sec/batch\n",
      "Epoch 6/20  Iteration 9622/35720 Training loss: 0.9986 0.2146 sec/batch\n",
      "Epoch 6/20  Iteration 9623/35720 Training loss: 0.9988 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 9624/35720 Training loss: 0.9988 0.2063 sec/batch\n",
      "Epoch 6/20  Iteration 9625/35720 Training loss: 0.9988 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 9626/35720 Training loss: 0.9988 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 9627/35720 Training loss: 0.9988 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 9628/35720 Training loss: 0.9988 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 9629/35720 Training loss: 0.9987 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 9630/35720 Training loss: 0.9987 0.2069 sec/batch\n",
      "Epoch 6/20  Iteration 9631/35720 Training loss: 0.9986 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 9632/35720 Training loss: 0.9987 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9633/35720 Training loss: 0.9986 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 9634/35720 Training loss: 0.9986 0.2271 sec/batch\n",
      "Epoch 6/20  Iteration 9635/35720 Training loss: 0.9986 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9636/35720 Training loss: 0.9986 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9637/35720 Training loss: 0.9987 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 9638/35720 Training loss: 0.9987 0.2187 sec/batch\n",
      "Epoch 6/20  Iteration 9639/35720 Training loss: 0.9988 0.2208 sec/batch\n",
      "Epoch 6/20  Iteration 9640/35720 Training loss: 0.9989 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 9641/35720 Training loss: 0.9989 0.2093 sec/batch\n",
      "Epoch 6/20  Iteration 9642/35720 Training loss: 0.9989 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 9643/35720 Training loss: 0.9989 0.2299 sec/batch\n",
      "Epoch 6/20  Iteration 9644/35720 Training loss: 0.9989 0.2273 sec/batch\n",
      "Epoch 6/20  Iteration 9645/35720 Training loss: 0.9990 0.2102 sec/batch\n",
      "Epoch 6/20  Iteration 9646/35720 Training loss: 0.9990 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 9647/35720 Training loss: 0.9991 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 9648/35720 Training loss: 0.9991 0.2205 sec/batch\n",
      "Epoch 6/20  Iteration 9649/35720 Training loss: 0.9991 0.2222 sec/batch\n",
      "Epoch 6/20  Iteration 9650/35720 Training loss: 0.9991 0.2202 sec/batch\n",
      "Epoch 6/20  Iteration 9651/35720 Training loss: 0.9991 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9652/35720 Training loss: 0.9992 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 9653/35720 Training loss: 0.9993 0.2148 sec/batch\n",
      "Epoch 6/20  Iteration 9654/35720 Training loss: 0.9992 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 9655/35720 Training loss: 0.9992 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 9656/35720 Training loss: 0.9992 0.2259 sec/batch\n",
      "Epoch 6/20  Iteration 9657/35720 Training loss: 0.9994 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 9658/35720 Training loss: 0.9994 0.2189 sec/batch\n",
      "Epoch 6/20  Iteration 9659/35720 Training loss: 0.9994 0.2220 sec/batch\n",
      "Epoch 6/20  Iteration 9660/35720 Training loss: 0.9994 0.2275 sec/batch\n",
      "Epoch 6/20  Iteration 9661/35720 Training loss: 0.9993 0.2124 sec/batch\n",
      "Epoch 6/20  Iteration 9662/35720 Training loss: 0.9993 0.2073 sec/batch\n",
      "Epoch 6/20  Iteration 9663/35720 Training loss: 0.9993 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 9664/35720 Training loss: 0.9993 0.2329 sec/batch\n",
      "Epoch 6/20  Iteration 9665/35720 Training loss: 0.9992 0.2259 sec/batch\n",
      "Epoch 6/20  Iteration 9666/35720 Training loss: 0.9993 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 9667/35720 Training loss: 0.9994 0.2273 sec/batch\n",
      "Epoch 6/20  Iteration 9668/35720 Training loss: 0.9993 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 9669/35720 Training loss: 0.9994 0.2170 sec/batch\n",
      "Epoch 6/20  Iteration 9670/35720 Training loss: 0.9994 0.2334 sec/batch\n",
      "Epoch 6/20  Iteration 9671/35720 Training loss: 0.9994 0.2280 sec/batch\n",
      "Epoch 6/20  Iteration 9672/35720 Training loss: 0.9994 0.2260 sec/batch\n",
      "Epoch 6/20  Iteration 9673/35720 Training loss: 0.9993 0.2277 sec/batch\n",
      "Epoch 6/20  Iteration 9674/35720 Training loss: 0.9993 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 9675/35720 Training loss: 0.9994 0.2187 sec/batch\n",
      "Epoch 6/20  Iteration 9676/35720 Training loss: 0.9994 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9677/35720 Training loss: 0.9993 0.2287 sec/batch\n",
      "Epoch 6/20  Iteration 9678/35720 Training loss: 0.9993 0.2269 sec/batch\n",
      "Epoch 6/20  Iteration 9679/35720 Training loss: 0.9992 0.2111 sec/batch\n",
      "Epoch 6/20  Iteration 9680/35720 Training loss: 0.9992 0.2190 sec/batch\n",
      "Epoch 6/20  Iteration 9681/35720 Training loss: 0.9992 0.2266 sec/batch\n",
      "Epoch 6/20  Iteration 9682/35720 Training loss: 0.9993 0.2120 sec/batch\n",
      "Epoch 6/20  Iteration 9683/35720 Training loss: 0.9991 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 9684/35720 Training loss: 0.9991 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 9685/35720 Training loss: 0.9991 0.2097 sec/batch\n",
      "Epoch 6/20  Iteration 9686/35720 Training loss: 0.9990 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 9687/35720 Training loss: 0.9989 0.2196 sec/batch\n",
      "Epoch 6/20  Iteration 9688/35720 Training loss: 0.9990 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 9689/35720 Training loss: 0.9991 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9690/35720 Training loss: 0.9991 0.2258 sec/batch\n",
      "Epoch 6/20  Iteration 9691/35720 Training loss: 0.9991 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 9692/35720 Training loss: 0.9991 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 9693/35720 Training loss: 0.9991 0.2271 sec/batch\n",
      "Epoch 6/20  Iteration 9694/35720 Training loss: 0.9990 0.2232 sec/batch\n",
      "Epoch 6/20  Iteration 9695/35720 Training loss: 0.9990 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9696/35720 Training loss: 0.9991 0.2206 sec/batch\n",
      "Epoch 6/20  Iteration 9697/35720 Training loss: 0.9991 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 9698/35720 Training loss: 0.9990 0.2129 sec/batch\n",
      "Epoch 6/20  Iteration 9699/35720 Training loss: 0.9991 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9700/35720 Training loss: 0.9992 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9701/35720 Training loss: 0.9993 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 9702/35720 Training loss: 0.9994 0.2076 sec/batch\n",
      "Epoch 6/20  Iteration 9703/35720 Training loss: 0.9994 0.2274 sec/batch\n",
      "Epoch 6/20  Iteration 9704/35720 Training loss: 0.9993 0.2254 sec/batch\n",
      "Epoch 6/20  Iteration 9705/35720 Training loss: 0.9991 0.2268 sec/batch\n",
      "Epoch 6/20  Iteration 9706/35720 Training loss: 0.9990 0.2080 sec/batch\n",
      "Epoch 6/20  Iteration 9707/35720 Training loss: 0.9990 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 9708/35720 Training loss: 0.9990 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 9709/35720 Training loss: 0.9991 0.2127 sec/batch\n",
      "Epoch 6/20  Iteration 9710/35720 Training loss: 0.9991 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 9711/35720 Training loss: 0.9990 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 9712/35720 Training loss: 0.9991 0.2067 sec/batch\n",
      "Epoch 6/20  Iteration 9713/35720 Training loss: 0.9991 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 9714/35720 Training loss: 0.9991 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 9715/35720 Training loss: 0.9990 0.2314 sec/batch\n",
      "Epoch 6/20  Iteration 9716/35720 Training loss: 0.9989 0.2300 sec/batch\n",
      "Epoch 6/20  Iteration 9717/35720 Training loss: 0.9990 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 9718/35720 Training loss: 0.9989 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 9719/35720 Training loss: 0.9990 0.2362 sec/batch\n",
      "Epoch 6/20  Iteration 9720/35720 Training loss: 0.9990 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 9721/35720 Training loss: 0.9991 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 9722/35720 Training loss: 0.9991 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 9723/35720 Training loss: 0.9991 0.2233 sec/batch\n",
      "Epoch 6/20  Iteration 9724/35720 Training loss: 0.9990 0.2100 sec/batch\n",
      "Epoch 6/20  Iteration 9725/35720 Training loss: 0.9991 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 9726/35720 Training loss: 0.9990 0.2471 sec/batch\n",
      "Epoch 6/20  Iteration 9727/35720 Training loss: 0.9990 0.2623 sec/batch\n",
      "Epoch 6/20  Iteration 9728/35720 Training loss: 0.9989 0.2301 sec/batch\n",
      "Epoch 6/20  Iteration 9729/35720 Training loss: 0.9989 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 9730/35720 Training loss: 0.9989 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 9731/35720 Training loss: 0.9989 0.2131 sec/batch\n",
      "Epoch 6/20  Iteration 9732/35720 Training loss: 0.9990 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 9733/35720 Training loss: 0.9990 0.2190 sec/batch\n",
      "Epoch 6/20  Iteration 9734/35720 Training loss: 0.9992 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 9735/35720 Training loss: 0.9993 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 9736/35720 Training loss: 0.9993 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 9737/35720 Training loss: 0.9993 0.2254 sec/batch\n",
      "Epoch 6/20  Iteration 9738/35720 Training loss: 0.9993 0.2255 sec/batch\n",
      "Epoch 6/20  Iteration 9739/35720 Training loss: 0.9994 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9740/35720 Training loss: 0.9995 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 9741/35720 Training loss: 0.9995 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 9742/35720 Training loss: 0.9996 0.2184 sec/batch\n",
      "Epoch 6/20  Iteration 9743/35720 Training loss: 0.9996 0.2074 sec/batch\n",
      "Epoch 6/20  Iteration 9744/35720 Training loss: 0.9996 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 9745/35720 Training loss: 0.9997 0.2471 sec/batch\n",
      "Epoch 6/20  Iteration 9746/35720 Training loss: 0.9996 0.2392 sec/batch\n",
      "Epoch 6/20  Iteration 9747/35720 Training loss: 0.9996 0.2097 sec/batch\n",
      "Epoch 6/20  Iteration 9748/35720 Training loss: 0.9997 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 9749/35720 Training loss: 0.9996 0.2196 sec/batch\n",
      "Epoch 6/20  Iteration 9750/35720 Training loss: 0.9996 0.2360 sec/batch\n",
      "Epoch 6/20  Iteration 9751/35720 Training loss: 0.9996 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9752/35720 Training loss: 0.9996 0.2209 sec/batch\n",
      "Epoch 6/20  Iteration 9753/35720 Training loss: 0.9994 0.2139 sec/batch\n",
      "Epoch 6/20  Iteration 9754/35720 Training loss: 0.9994 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 9755/35720 Training loss: 0.9993 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 9756/35720 Training loss: 0.9992 0.2309 sec/batch\n",
      "Epoch 6/20  Iteration 9757/35720 Training loss: 0.9992 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 9758/35720 Training loss: 0.9990 0.2095 sec/batch\n",
      "Epoch 6/20  Iteration 9759/35720 Training loss: 0.9990 0.2084 sec/batch\n",
      "Epoch 6/20  Iteration 9760/35720 Training loss: 0.9989 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 9761/35720 Training loss: 0.9988 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 9762/35720 Training loss: 0.9989 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 9763/35720 Training loss: 0.9991 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 9764/35720 Training loss: 0.9991 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 9765/35720 Training loss: 0.9991 0.2276 sec/batch\n",
      "Epoch 6/20  Iteration 9766/35720 Training loss: 0.9990 0.2250 sec/batch\n",
      "Epoch 6/20  Iteration 9767/35720 Training loss: 0.9990 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 9768/35720 Training loss: 0.9990 0.2079 sec/batch\n",
      "Epoch 6/20  Iteration 9769/35720 Training loss: 0.9991 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 9770/35720 Training loss: 0.9991 0.2214 sec/batch\n",
      "Epoch 6/20  Iteration 9771/35720 Training loss: 0.9992 0.2365 sec/batch\n",
      "Epoch 6/20  Iteration 9772/35720 Training loss: 0.9992 0.2080 sec/batch\n",
      "Epoch 6/20  Iteration 9773/35720 Training loss: 0.9991 0.2169 sec/batch\n",
      "Epoch 6/20  Iteration 9774/35720 Training loss: 0.9992 0.2271 sec/batch\n",
      "Epoch 6/20  Iteration 9775/35720 Training loss: 0.9991 0.2297 sec/batch\n",
      "Epoch 6/20  Iteration 9776/35720 Training loss: 0.9991 0.2264 sec/batch\n",
      "Epoch 6/20  Iteration 9777/35720 Training loss: 0.9991 0.2275 sec/batch\n",
      "Epoch 6/20  Iteration 9778/35720 Training loss: 0.9991 0.2118 sec/batch\n",
      "Epoch 6/20  Iteration 9779/35720 Training loss: 0.9990 0.2205 sec/batch\n",
      "Epoch 6/20  Iteration 9780/35720 Training loss: 0.9990 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 9781/35720 Training loss: 0.9990 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 9782/35720 Training loss: 0.9990 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 9783/35720 Training loss: 0.9989 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 9784/35720 Training loss: 0.9989 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 9785/35720 Training loss: 0.9989 0.2233 sec/batch\n",
      "Epoch 6/20  Iteration 9786/35720 Training loss: 0.9989 0.2191 sec/batch\n",
      "Epoch 6/20  Iteration 9787/35720 Training loss: 0.9989 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 9788/35720 Training loss: 0.9989 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9789/35720 Training loss: 0.9988 0.2307 sec/batch\n",
      "Epoch 6/20  Iteration 9790/35720 Training loss: 0.9987 0.2170 sec/batch\n",
      "Epoch 6/20  Iteration 9791/35720 Training loss: 0.9988 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9792/35720 Training loss: 0.9987 0.2267 sec/batch\n",
      "Epoch 6/20  Iteration 9793/35720 Training loss: 0.9988 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 9794/35720 Training loss: 0.9987 0.2127 sec/batch\n",
      "Epoch 6/20  Iteration 9795/35720 Training loss: 0.9986 0.2266 sec/batch\n",
      "Epoch 6/20  Iteration 9796/35720 Training loss: 0.9987 0.2229 sec/batch\n",
      "Epoch 6/20  Iteration 9797/35720 Training loss: 0.9988 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 9798/35720 Training loss: 0.9988 0.2291 sec/batch\n",
      "Epoch 6/20  Iteration 9799/35720 Training loss: 0.9987 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 9800/35720 Training loss: 0.9987 0.2080 sec/batch\n",
      "Validation loss: 1.3055 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 9801/35720 Training loss: 0.9988 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 9802/35720 Training loss: 0.9988 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 9803/35720 Training loss: 0.9987 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 9804/35720 Training loss: 0.9987 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 9805/35720 Training loss: 0.9986 0.2275 sec/batch\n",
      "Epoch 6/20  Iteration 9806/35720 Training loss: 0.9985 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 9807/35720 Training loss: 0.9985 0.2102 sec/batch\n",
      "Epoch 6/20  Iteration 9808/35720 Training loss: 0.9985 0.2253 sec/batch\n",
      "Epoch 6/20  Iteration 9809/35720 Training loss: 0.9984 0.2220 sec/batch\n",
      "Epoch 6/20  Iteration 9810/35720 Training loss: 0.9984 0.2058 sec/batch\n",
      "Epoch 6/20  Iteration 9811/35720 Training loss: 0.9984 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 9812/35720 Training loss: 0.9984 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 9813/35720 Training loss: 0.9985 0.2127 sec/batch\n",
      "Epoch 6/20  Iteration 9814/35720 Training loss: 0.9984 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 9815/35720 Training loss: 0.9984 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 9816/35720 Training loss: 0.9984 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 9817/35720 Training loss: 0.9984 0.2127 sec/batch\n",
      "Epoch 6/20  Iteration 9818/35720 Training loss: 0.9983 0.2246 sec/batch\n",
      "Epoch 6/20  Iteration 9819/35720 Training loss: 0.9983 0.2232 sec/batch\n",
      "Epoch 6/20  Iteration 9820/35720 Training loss: 0.9982 0.2260 sec/batch\n",
      "Epoch 6/20  Iteration 9821/35720 Training loss: 0.9981 0.2072 sec/batch\n",
      "Epoch 6/20  Iteration 9822/35720 Training loss: 0.9980 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9823/35720 Training loss: 0.9979 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 9824/35720 Training loss: 0.9979 0.2261 sec/batch\n",
      "Epoch 6/20  Iteration 9825/35720 Training loss: 0.9978 0.2233 sec/batch\n",
      "Epoch 6/20  Iteration 9826/35720 Training loss: 0.9979 0.2318 sec/batch\n",
      "Epoch 6/20  Iteration 9827/35720 Training loss: 0.9977 0.2194 sec/batch\n",
      "Epoch 6/20  Iteration 9828/35720 Training loss: 0.9976 0.2087 sec/batch\n",
      "Epoch 6/20  Iteration 9829/35720 Training loss: 0.9974 0.2276 sec/batch\n",
      "Epoch 6/20  Iteration 9830/35720 Training loss: 0.9974 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 9831/35720 Training loss: 0.9974 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 9832/35720 Training loss: 0.9973 0.2155 sec/batch\n",
      "Epoch 6/20  Iteration 9833/35720 Training loss: 0.9972 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 9834/35720 Training loss: 0.9971 0.2352 sec/batch\n",
      "Epoch 6/20  Iteration 9835/35720 Training loss: 0.9970 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9836/35720 Training loss: 0.9970 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 9837/35720 Training loss: 0.9970 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9838/35720 Training loss: 0.9971 0.2088 sec/batch\n",
      "Epoch 6/20  Iteration 9839/35720 Training loss: 0.9971 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 9840/35720 Training loss: 0.9971 0.2143 sec/batch\n",
      "Epoch 6/20  Iteration 9841/35720 Training loss: 0.9970 0.2199 sec/batch\n",
      "Epoch 6/20  Iteration 9842/35720 Training loss: 0.9970 0.2353 sec/batch\n",
      "Epoch 6/20  Iteration 9843/35720 Training loss: 0.9971 0.2098 sec/batch\n",
      "Epoch 6/20  Iteration 9844/35720 Training loss: 0.9971 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 9845/35720 Training loss: 0.9971 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 9846/35720 Training loss: 0.9972 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 9847/35720 Training loss: 0.9972 0.2213 sec/batch\n",
      "Epoch 6/20  Iteration 9848/35720 Training loss: 0.9972 0.2281 sec/batch\n",
      "Epoch 6/20  Iteration 9849/35720 Training loss: 0.9971 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 9850/35720 Training loss: 0.9971 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9851/35720 Training loss: 0.9971 0.2169 sec/batch\n",
      "Epoch 6/20  Iteration 9852/35720 Training loss: 0.9970 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 9853/35720 Training loss: 0.9970 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 9854/35720 Training loss: 0.9970 0.2216 sec/batch\n",
      "Epoch 6/20  Iteration 9855/35720 Training loss: 0.9970 0.2075 sec/batch\n",
      "Epoch 6/20  Iteration 9856/35720 Training loss: 0.9970 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 9857/35720 Training loss: 0.9972 0.2084 sec/batch\n",
      "Epoch 6/20  Iteration 9858/35720 Training loss: 0.9970 0.2242 sec/batch\n",
      "Epoch 6/20  Iteration 9859/35720 Training loss: 0.9972 0.2069 sec/batch\n",
      "Epoch 6/20  Iteration 9860/35720 Training loss: 0.9972 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9861/35720 Training loss: 0.9972 0.2056 sec/batch\n",
      "Epoch 6/20  Iteration 9862/35720 Training loss: 0.9973 0.2257 sec/batch\n",
      "Epoch 6/20  Iteration 9863/35720 Training loss: 0.9972 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9864/35720 Training loss: 0.9972 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 9865/35720 Training loss: 0.9971 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 9866/35720 Training loss: 0.9969 0.2179 sec/batch\n",
      "Epoch 6/20  Iteration 9867/35720 Training loss: 0.9969 0.2267 sec/batch\n",
      "Epoch 6/20  Iteration 9868/35720 Training loss: 0.9968 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9869/35720 Training loss: 0.9967 0.2248 sec/batch\n",
      "Epoch 6/20  Iteration 9870/35720 Training loss: 0.9966 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 9871/35720 Training loss: 0.9965 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 9872/35720 Training loss: 0.9966 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 9873/35720 Training loss: 0.9965 0.2194 sec/batch\n",
      "Epoch 6/20  Iteration 9874/35720 Training loss: 0.9966 0.2139 sec/batch\n",
      "Epoch 6/20  Iteration 9875/35720 Training loss: 0.9965 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 9876/35720 Training loss: 0.9966 0.2208 sec/batch\n",
      "Epoch 6/20  Iteration 9877/35720 Training loss: 0.9966 0.2110 sec/batch\n",
      "Epoch 6/20  Iteration 9878/35720 Training loss: 0.9965 0.2270 sec/batch\n",
      "Epoch 6/20  Iteration 9879/35720 Training loss: 0.9965 0.2253 sec/batch\n",
      "Epoch 6/20  Iteration 9880/35720 Training loss: 0.9964 0.2300 sec/batch\n",
      "Epoch 6/20  Iteration 9881/35720 Training loss: 0.9963 0.2236 sec/batch\n",
      "Epoch 6/20  Iteration 9882/35720 Training loss: 0.9962 0.2070 sec/batch\n",
      "Epoch 6/20  Iteration 9883/35720 Training loss: 0.9962 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 9884/35720 Training loss: 0.9962 0.2270 sec/batch\n",
      "Epoch 6/20  Iteration 9885/35720 Training loss: 0.9961 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 9886/35720 Training loss: 0.9961 0.2195 sec/batch\n",
      "Epoch 6/20  Iteration 9887/35720 Training loss: 0.9960 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 9888/35720 Training loss: 0.9960 0.2234 sec/batch\n",
      "Epoch 6/20  Iteration 9889/35720 Training loss: 0.9959 0.2251 sec/batch\n",
      "Epoch 6/20  Iteration 9890/35720 Training loss: 0.9959 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9891/35720 Training loss: 0.9959 0.2277 sec/batch\n",
      "Epoch 6/20  Iteration 9892/35720 Training loss: 0.9958 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 9893/35720 Training loss: 0.9957 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 9894/35720 Training loss: 0.9958 0.2263 sec/batch\n",
      "Epoch 6/20  Iteration 9895/35720 Training loss: 0.9957 0.2080 sec/batch\n",
      "Epoch 6/20  Iteration 9896/35720 Training loss: 0.9956 0.2240 sec/batch\n",
      "Epoch 6/20  Iteration 9897/35720 Training loss: 0.9957 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 9898/35720 Training loss: 0.9956 0.2185 sec/batch\n",
      "Epoch 6/20  Iteration 9899/35720 Training loss: 0.9958 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 9900/35720 Training loss: 0.9960 0.2399 sec/batch\n",
      "Epoch 6/20  Iteration 9901/35720 Training loss: 0.9959 0.2263 sec/batch\n",
      "Epoch 6/20  Iteration 9902/35720 Training loss: 0.9958 0.2231 sec/batch\n",
      "Epoch 6/20  Iteration 9903/35720 Training loss: 0.9958 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 9904/35720 Training loss: 0.9957 0.2234 sec/batch\n",
      "Epoch 6/20  Iteration 9905/35720 Training loss: 0.9957 0.2051 sec/batch\n",
      "Epoch 6/20  Iteration 9906/35720 Training loss: 0.9957 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 9907/35720 Training loss: 0.9957 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 9908/35720 Training loss: 0.9958 0.2152 sec/batch\n",
      "Epoch 6/20  Iteration 9909/35720 Training loss: 0.9957 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 9910/35720 Training loss: 0.9956 0.2247 sec/batch\n",
      "Epoch 6/20  Iteration 9911/35720 Training loss: 0.9956 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 9912/35720 Training loss: 0.9955 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 9913/35720 Training loss: 0.9953 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 9914/35720 Training loss: 0.9953 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 9915/35720 Training loss: 0.9954 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 9916/35720 Training loss: 0.9953 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 9917/35720 Training loss: 0.9953 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 9918/35720 Training loss: 0.9953 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 9919/35720 Training loss: 0.9952 0.2218 sec/batch\n",
      "Epoch 6/20  Iteration 9920/35720 Training loss: 0.9953 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 9921/35720 Training loss: 0.9952 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 9922/35720 Training loss: 0.9952 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 9923/35720 Training loss: 0.9953 0.2364 sec/batch\n",
      "Epoch 6/20  Iteration 9924/35720 Training loss: 0.9953 0.2209 sec/batch\n",
      "Epoch 6/20  Iteration 9925/35720 Training loss: 0.9953 0.2070 sec/batch\n",
      "Epoch 6/20  Iteration 9926/35720 Training loss: 0.9953 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 9927/35720 Training loss: 0.9954 0.2098 sec/batch\n",
      "Epoch 6/20  Iteration 9928/35720 Training loss: 0.9955 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 9929/35720 Training loss: 0.9955 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 9930/35720 Training loss: 0.9954 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 9931/35720 Training loss: 0.9953 0.2068 sec/batch\n",
      "Epoch 6/20  Iteration 9932/35720 Training loss: 0.9952 0.2200 sec/batch\n",
      "Epoch 6/20  Iteration 9933/35720 Training loss: 0.9951 0.2221 sec/batch\n",
      "Epoch 6/20  Iteration 9934/35720 Training loss: 0.9951 0.2100 sec/batch\n",
      "Epoch 6/20  Iteration 9935/35720 Training loss: 0.9950 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 9936/35720 Training loss: 0.9949 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 9937/35720 Training loss: 0.9948 0.2065 sec/batch\n",
      "Epoch 6/20  Iteration 9938/35720 Training loss: 0.9948 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 9939/35720 Training loss: 0.9947 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 9940/35720 Training loss: 0.9946 0.2304 sec/batch\n",
      "Epoch 6/20  Iteration 9941/35720 Training loss: 0.9946 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 9942/35720 Training loss: 0.9946 0.2072 sec/batch\n",
      "Epoch 6/20  Iteration 9943/35720 Training loss: 0.9945 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 9944/35720 Training loss: 0.9945 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 9945/35720 Training loss: 0.9945 0.2082 sec/batch\n",
      "Epoch 6/20  Iteration 9946/35720 Training loss: 0.9945 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9947/35720 Training loss: 0.9945 0.2268 sec/batch\n",
      "Epoch 6/20  Iteration 9948/35720 Training loss: 0.9945 0.2583 sec/batch\n",
      "Epoch 6/20  Iteration 9949/35720 Training loss: 0.9944 0.3180 sec/batch\n",
      "Epoch 6/20  Iteration 9950/35720 Training loss: 0.9944 0.2587 sec/batch\n",
      "Epoch 6/20  Iteration 9951/35720 Training loss: 0.9943 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 9952/35720 Training loss: 0.9944 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 9953/35720 Training loss: 0.9945 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 9954/35720 Training loss: 0.9945 0.2198 sec/batch\n",
      "Epoch 6/20  Iteration 9955/35720 Training loss: 0.9946 0.2197 sec/batch\n",
      "Epoch 6/20  Iteration 9956/35720 Training loss: 0.9945 0.2305 sec/batch\n",
      "Epoch 6/20  Iteration 9957/35720 Training loss: 0.9944 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9958/35720 Training loss: 0.9944 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 9959/35720 Training loss: 0.9943 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9960/35720 Training loss: 0.9943 0.2174 sec/batch\n",
      "Epoch 6/20  Iteration 9961/35720 Training loss: 0.9943 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 9962/35720 Training loss: 0.9943 0.2222 sec/batch\n",
      "Epoch 6/20  Iteration 9963/35720 Training loss: 0.9943 0.2229 sec/batch\n",
      "Epoch 6/20  Iteration 9964/35720 Training loss: 0.9943 0.2052 sec/batch\n",
      "Epoch 6/20  Iteration 9965/35720 Training loss: 0.9943 0.2100 sec/batch\n",
      "Epoch 6/20  Iteration 9966/35720 Training loss: 0.9943 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 9967/35720 Training loss: 0.9944 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 9968/35720 Training loss: 0.9943 0.2236 sec/batch\n",
      "Epoch 6/20  Iteration 9969/35720 Training loss: 0.9943 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 9970/35720 Training loss: 0.9943 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 9971/35720 Training loss: 0.9943 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 9972/35720 Training loss: 0.9943 0.2143 sec/batch\n",
      "Epoch 6/20  Iteration 9973/35720 Training loss: 0.9943 0.2068 sec/batch\n",
      "Epoch 6/20  Iteration 9974/35720 Training loss: 0.9943 0.2337 sec/batch\n",
      "Epoch 6/20  Iteration 9975/35720 Training loss: 0.9943 0.2070 sec/batch\n",
      "Epoch 6/20  Iteration 9976/35720 Training loss: 0.9943 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 9977/35720 Training loss: 0.9943 0.2124 sec/batch\n",
      "Epoch 6/20  Iteration 9978/35720 Training loss: 0.9942 0.2213 sec/batch\n",
      "Epoch 6/20  Iteration 9979/35720 Training loss: 0.9942 0.2205 sec/batch\n",
      "Epoch 6/20  Iteration 9980/35720 Training loss: 0.9942 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 9981/35720 Training loss: 0.9941 0.2088 sec/batch\n",
      "Epoch 6/20  Iteration 9982/35720 Training loss: 0.9943 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 9983/35720 Training loss: 0.9943 0.2242 sec/batch\n",
      "Epoch 6/20  Iteration 9984/35720 Training loss: 0.9943 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 9985/35720 Training loss: 0.9944 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 9986/35720 Training loss: 0.9944 0.2081 sec/batch\n",
      "Epoch 6/20  Iteration 9987/35720 Training loss: 0.9944 0.2148 sec/batch\n",
      "Epoch 6/20  Iteration 9988/35720 Training loss: 0.9944 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 9989/35720 Training loss: 0.9944 0.2244 sec/batch\n",
      "Epoch 6/20  Iteration 9990/35720 Training loss: 0.9943 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 9991/35720 Training loss: 0.9944 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 9992/35720 Training loss: 0.9944 0.2141 sec/batch\n",
      "Epoch 6/20  Iteration 9993/35720 Training loss: 0.9944 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 9994/35720 Training loss: 0.9944 0.2226 sec/batch\n",
      "Epoch 6/20  Iteration 9995/35720 Training loss: 0.9943 0.2139 sec/batch\n",
      "Epoch 6/20  Iteration 9996/35720 Training loss: 0.9943 0.2176 sec/batch\n",
      "Epoch 6/20  Iteration 9997/35720 Training loss: 0.9943 0.2152 sec/batch\n",
      "Epoch 6/20  Iteration 9998/35720 Training loss: 0.9943 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 9999/35720 Training loss: 0.9943 0.2253 sec/batch\n",
      "Epoch 6/20  Iteration 10000/35720 Training loss: 0.9943 0.2104 sec/batch\n",
      "Validation loss: 1.3155 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 10001/35720 Training loss: 0.9945 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 10002/35720 Training loss: 0.9945 0.2139 sec/batch\n",
      "Epoch 6/20  Iteration 10003/35720 Training loss: 0.9944 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 10004/35720 Training loss: 0.9944 0.2405 sec/batch\n",
      "Epoch 6/20  Iteration 10005/35720 Training loss: 0.9944 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 10006/35720 Training loss: 0.9945 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 10007/35720 Training loss: 0.9945 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 10008/35720 Training loss: 0.9945 0.2129 sec/batch\n",
      "Epoch 6/20  Iteration 10009/35720 Training loss: 0.9945 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 10010/35720 Training loss: 0.9944 0.2104 sec/batch\n",
      "Epoch 6/20  Iteration 10011/35720 Training loss: 0.9944 0.2197 sec/batch\n",
      "Epoch 6/20  Iteration 10012/35720 Training loss: 0.9943 0.2260 sec/batch\n",
      "Epoch 6/20  Iteration 10013/35720 Training loss: 0.9943 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 10014/35720 Training loss: 0.9943 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 10015/35720 Training loss: 0.9943 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 10016/35720 Training loss: 0.9943 0.2139 sec/batch\n",
      "Epoch 6/20  Iteration 10017/35720 Training loss: 0.9943 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 10018/35720 Training loss: 0.9943 0.2132 sec/batch\n",
      "Epoch 6/20  Iteration 10019/35720 Training loss: 0.9943 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 10020/35720 Training loss: 0.9943 0.2127 sec/batch\n",
      "Epoch 6/20  Iteration 10021/35720 Training loss: 0.9943 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 10022/35720 Training loss: 0.9943 0.2279 sec/batch\n",
      "Epoch 6/20  Iteration 10023/35720 Training loss: 0.9943 0.2296 sec/batch\n",
      "Epoch 6/20  Iteration 10024/35720 Training loss: 0.9942 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 10025/35720 Training loss: 0.9942 0.2111 sec/batch\n",
      "Epoch 6/20  Iteration 10026/35720 Training loss: 0.9942 0.2141 sec/batch\n",
      "Epoch 6/20  Iteration 10027/35720 Training loss: 0.9941 0.2082 sec/batch\n",
      "Epoch 6/20  Iteration 10028/35720 Training loss: 0.9941 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 10029/35720 Training loss: 0.9941 0.2203 sec/batch\n",
      "Epoch 6/20  Iteration 10030/35720 Training loss: 0.9941 0.2065 sec/batch\n",
      "Epoch 6/20  Iteration 10031/35720 Training loss: 0.9942 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 10032/35720 Training loss: 0.9942 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 10033/35720 Training loss: 0.9942 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 10034/35720 Training loss: 0.9942 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 10035/35720 Training loss: 0.9942 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 10036/35720 Training loss: 0.9941 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 10037/35720 Training loss: 0.9942 0.2205 sec/batch\n",
      "Epoch 6/20  Iteration 10038/35720 Training loss: 0.9942 0.2197 sec/batch\n",
      "Epoch 6/20  Iteration 10039/35720 Training loss: 0.9942 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 10040/35720 Training loss: 0.9942 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10041/35720 Training loss: 0.9942 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 10042/35720 Training loss: 0.9941 0.2134 sec/batch\n",
      "Epoch 6/20  Iteration 10043/35720 Training loss: 0.9940 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 10044/35720 Training loss: 0.9941 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 10045/35720 Training loss: 0.9941 0.2309 sec/batch\n",
      "Epoch 6/20  Iteration 10046/35720 Training loss: 0.9940 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 10047/35720 Training loss: 0.9940 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 10048/35720 Training loss: 0.9939 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 10049/35720 Training loss: 0.9939 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 10050/35720 Training loss: 0.9940 0.2149 sec/batch\n",
      "Epoch 6/20  Iteration 10051/35720 Training loss: 0.9940 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 10052/35720 Training loss: 0.9941 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 10053/35720 Training loss: 0.9942 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 10054/35720 Training loss: 0.9942 0.2329 sec/batch\n",
      "Epoch 6/20  Iteration 10055/35720 Training loss: 0.9941 0.2276 sec/batch\n",
      "Epoch 6/20  Iteration 10056/35720 Training loss: 0.9941 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 10057/35720 Training loss: 0.9940 0.2258 sec/batch\n",
      "Epoch 6/20  Iteration 10058/35720 Training loss: 0.9940 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 10059/35720 Training loss: 0.9939 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 10060/35720 Training loss: 0.9940 0.2129 sec/batch\n",
      "Epoch 6/20  Iteration 10061/35720 Training loss: 0.9939 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 10062/35720 Training loss: 0.9939 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 10063/35720 Training loss: 0.9938 0.2066 sec/batch\n",
      "Epoch 6/20  Iteration 10064/35720 Training loss: 0.9939 0.2104 sec/batch\n",
      "Epoch 6/20  Iteration 10065/35720 Training loss: 0.9938 0.2236 sec/batch\n",
      "Epoch 6/20  Iteration 10066/35720 Training loss: 0.9938 0.2218 sec/batch\n",
      "Epoch 6/20  Iteration 10067/35720 Training loss: 0.9937 0.2273 sec/batch\n",
      "Epoch 6/20  Iteration 10068/35720 Training loss: 0.9936 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 10069/35720 Training loss: 0.9936 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 10070/35720 Training loss: 0.9935 0.2280 sec/batch\n",
      "Epoch 6/20  Iteration 10071/35720 Training loss: 0.9935 0.2302 sec/batch\n",
      "Epoch 6/20  Iteration 10072/35720 Training loss: 0.9935 0.2218 sec/batch\n",
      "Epoch 6/20  Iteration 10073/35720 Training loss: 0.9934 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 10074/35720 Training loss: 0.9934 0.2067 sec/batch\n",
      "Epoch 6/20  Iteration 10075/35720 Training loss: 0.9933 0.2176 sec/batch\n",
      "Epoch 6/20  Iteration 10076/35720 Training loss: 0.9932 0.2254 sec/batch\n",
      "Epoch 6/20  Iteration 10077/35720 Training loss: 0.9932 0.2338 sec/batch\n",
      "Epoch 6/20  Iteration 10078/35720 Training loss: 0.9931 0.2298 sec/batch\n",
      "Epoch 6/20  Iteration 10079/35720 Training loss: 0.9931 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 10080/35720 Training loss: 0.9931 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 10081/35720 Training loss: 0.9931 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 10082/35720 Training loss: 0.9931 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 10083/35720 Training loss: 0.9931 0.2317 sec/batch\n",
      "Epoch 6/20  Iteration 10084/35720 Training loss: 0.9931 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 10085/35720 Training loss: 0.9931 0.2122 sec/batch\n",
      "Epoch 6/20  Iteration 10086/35720 Training loss: 0.9930 0.2293 sec/batch\n",
      "Epoch 6/20  Iteration 10087/35720 Training loss: 0.9930 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 10088/35720 Training loss: 0.9930 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 10089/35720 Training loss: 0.9929 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 10090/35720 Training loss: 0.9929 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 10091/35720 Training loss: 0.9928 0.2186 sec/batch\n",
      "Epoch 6/20  Iteration 10092/35720 Training loss: 0.9928 0.2319 sec/batch\n",
      "Epoch 6/20  Iteration 10093/35720 Training loss: 0.9929 0.2077 sec/batch\n",
      "Epoch 6/20  Iteration 10094/35720 Training loss: 0.9930 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 10095/35720 Training loss: 0.9929 0.2295 sec/batch\n",
      "Epoch 6/20  Iteration 10096/35720 Training loss: 0.9928 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 10097/35720 Training loss: 0.9928 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10098/35720 Training loss: 0.9928 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 10099/35720 Training loss: 0.9928 0.2218 sec/batch\n",
      "Epoch 6/20  Iteration 10100/35720 Training loss: 0.9929 0.2326 sec/batch\n",
      "Epoch 6/20  Iteration 10101/35720 Training loss: 0.9928 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 10102/35720 Training loss: 0.9928 0.2217 sec/batch\n",
      "Epoch 6/20  Iteration 10103/35720 Training loss: 0.9928 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 10104/35720 Training loss: 0.9927 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 10105/35720 Training loss: 0.9928 0.2194 sec/batch\n",
      "Epoch 6/20  Iteration 10106/35720 Training loss: 0.9928 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 10107/35720 Training loss: 0.9928 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 10108/35720 Training loss: 0.9929 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 10109/35720 Training loss: 0.9928 0.2260 sec/batch\n",
      "Epoch 6/20  Iteration 10110/35720 Training loss: 0.9928 0.2136 sec/batch\n",
      "Epoch 6/20  Iteration 10111/35720 Training loss: 0.9929 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 10112/35720 Training loss: 0.9929 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 10113/35720 Training loss: 0.9928 0.2093 sec/batch\n",
      "Epoch 6/20  Iteration 10114/35720 Training loss: 0.9927 0.2272 sec/batch\n",
      "Epoch 6/20  Iteration 10115/35720 Training loss: 0.9927 0.2256 sec/batch\n",
      "Epoch 6/20  Iteration 10116/35720 Training loss: 0.9926 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 10117/35720 Training loss: 0.9926 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 10118/35720 Training loss: 0.9926 0.2145 sec/batch\n",
      "Epoch 6/20  Iteration 10119/35720 Training loss: 0.9925 0.2093 sec/batch\n",
      "Epoch 6/20  Iteration 10120/35720 Training loss: 0.9925 0.2194 sec/batch\n",
      "Epoch 6/20  Iteration 10121/35720 Training loss: 0.9925 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 10122/35720 Training loss: 0.9926 0.2230 sec/batch\n",
      "Epoch 6/20  Iteration 10123/35720 Training loss: 0.9925 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 10124/35720 Training loss: 0.9925 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 10125/35720 Training loss: 0.9926 0.2291 sec/batch\n",
      "Epoch 6/20  Iteration 10126/35720 Training loss: 0.9925 0.2203 sec/batch\n",
      "Epoch 6/20  Iteration 10127/35720 Training loss: 0.9926 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 10128/35720 Training loss: 0.9926 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 10129/35720 Training loss: 0.9926 0.2214 sec/batch\n",
      "Epoch 6/20  Iteration 10130/35720 Training loss: 0.9925 0.2080 sec/batch\n",
      "Epoch 6/20  Iteration 10131/35720 Training loss: 0.9925 0.2223 sec/batch\n",
      "Epoch 6/20  Iteration 10132/35720 Training loss: 0.9924 0.2211 sec/batch\n",
      "Epoch 6/20  Iteration 10133/35720 Training loss: 0.9923 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 10134/35720 Training loss: 0.9923 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 10135/35720 Training loss: 0.9922 0.2257 sec/batch\n",
      "Epoch 6/20  Iteration 10136/35720 Training loss: 0.9922 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 10137/35720 Training loss: 0.9922 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 10138/35720 Training loss: 0.9922 0.2185 sec/batch\n",
      "Epoch 6/20  Iteration 10139/35720 Training loss: 0.9922 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 10140/35720 Training loss: 0.9922 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 10141/35720 Training loss: 0.9922 0.2338 sec/batch\n",
      "Epoch 6/20  Iteration 10142/35720 Training loss: 0.9922 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 10143/35720 Training loss: 0.9921 0.2098 sec/batch\n",
      "Epoch 6/20  Iteration 10144/35720 Training loss: 0.9922 0.2198 sec/batch\n",
      "Epoch 6/20  Iteration 10145/35720 Training loss: 0.9921 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 10146/35720 Training loss: 0.9921 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 10147/35720 Training loss: 0.9921 0.2132 sec/batch\n",
      "Epoch 6/20  Iteration 10148/35720 Training loss: 0.9920 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 10149/35720 Training loss: 0.9920 0.2416 sec/batch\n",
      "Epoch 6/20  Iteration 10150/35720 Training loss: 0.9920 0.2260 sec/batch\n",
      "Epoch 6/20  Iteration 10151/35720 Training loss: 0.9920 0.2063 sec/batch\n",
      "Epoch 6/20  Iteration 10152/35720 Training loss: 0.9920 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 10153/35720 Training loss: 0.9920 0.2198 sec/batch\n",
      "Epoch 6/20  Iteration 10154/35720 Training loss: 0.9919 0.2258 sec/batch\n",
      "Epoch 6/20  Iteration 10155/35720 Training loss: 0.9919 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 10156/35720 Training loss: 0.9919 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 10157/35720 Training loss: 0.9920 0.2136 sec/batch\n",
      "Epoch 6/20  Iteration 10158/35720 Training loss: 0.9919 0.2496 sec/batch\n",
      "Epoch 6/20  Iteration 10159/35720 Training loss: 0.9920 0.2274 sec/batch\n",
      "Epoch 6/20  Iteration 10160/35720 Training loss: 0.9920 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 10161/35720 Training loss: 0.9920 0.2253 sec/batch\n",
      "Epoch 6/20  Iteration 10162/35720 Training loss: 0.9919 0.2129 sec/batch\n",
      "Epoch 6/20  Iteration 10163/35720 Training loss: 0.9919 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 10164/35720 Training loss: 0.9919 0.2285 sec/batch\n",
      "Epoch 6/20  Iteration 10165/35720 Training loss: 0.9919 0.2339 sec/batch\n",
      "Epoch 6/20  Iteration 10166/35720 Training loss: 0.9919 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 10167/35720 Training loss: 0.9918 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 10168/35720 Training loss: 0.9917 0.2287 sec/batch\n",
      "Epoch 6/20  Iteration 10169/35720 Training loss: 0.9916 0.2209 sec/batch\n",
      "Epoch 6/20  Iteration 10170/35720 Training loss: 0.9916 0.2555 sec/batch\n",
      "Epoch 6/20  Iteration 10171/35720 Training loss: 0.9915 0.2362 sec/batch\n",
      "Epoch 6/20  Iteration 10172/35720 Training loss: 0.9915 0.2347 sec/batch\n",
      "Epoch 6/20  Iteration 10173/35720 Training loss: 0.9915 0.2303 sec/batch\n",
      "Epoch 6/20  Iteration 10174/35720 Training loss: 0.9914 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 10175/35720 Training loss: 0.9914 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 10176/35720 Training loss: 0.9913 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 10177/35720 Training loss: 0.9913 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 10178/35720 Training loss: 0.9912 0.2307 sec/batch\n",
      "Epoch 6/20  Iteration 10179/35720 Training loss: 0.9911 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 10180/35720 Training loss: 0.9911 0.2290 sec/batch\n",
      "Epoch 6/20  Iteration 10181/35720 Training loss: 0.9910 0.2284 sec/batch\n",
      "Epoch 6/20  Iteration 10182/35720 Training loss: 0.9910 0.2264 sec/batch\n",
      "Epoch 6/20  Iteration 10183/35720 Training loss: 0.9910 0.2141 sec/batch\n",
      "Epoch 6/20  Iteration 10184/35720 Training loss: 0.9910 0.2131 sec/batch\n",
      "Epoch 6/20  Iteration 10185/35720 Training loss: 0.9909 0.2294 sec/batch\n",
      "Epoch 6/20  Iteration 10186/35720 Training loss: 0.9909 0.2281 sec/batch\n",
      "Epoch 6/20  Iteration 10187/35720 Training loss: 0.9908 0.2312 sec/batch\n",
      "Epoch 6/20  Iteration 10188/35720 Training loss: 0.9908 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 10189/35720 Training loss: 0.9907 0.2275 sec/batch\n",
      "Epoch 6/20  Iteration 10190/35720 Training loss: 0.9908 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 10191/35720 Training loss: 0.9907 0.2252 sec/batch\n",
      "Epoch 6/20  Iteration 10192/35720 Training loss: 0.9907 0.2299 sec/batch\n",
      "Epoch 6/20  Iteration 10193/35720 Training loss: 0.9906 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 10194/35720 Training loss: 0.9906 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 10195/35720 Training loss: 0.9905 0.2098 sec/batch\n",
      "Epoch 6/20  Iteration 10196/35720 Training loss: 0.9904 0.2243 sec/batch\n",
      "Epoch 6/20  Iteration 10197/35720 Training loss: 0.9904 0.2239 sec/batch\n",
      "Epoch 6/20  Iteration 10198/35720 Training loss: 0.9904 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 10199/35720 Training loss: 0.9903 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 10200/35720 Training loss: 0.9904 0.2216 sec/batch\n",
      "Validation loss: 1.31455 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 10201/35720 Training loss: 0.9905 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 10202/35720 Training loss: 0.9904 0.2084 sec/batch\n",
      "Epoch 6/20  Iteration 10203/35720 Training loss: 0.9904 0.2097 sec/batch\n",
      "Epoch 6/20  Iteration 10204/35720 Training loss: 0.9904 0.2111 sec/batch\n",
      "Epoch 6/20  Iteration 10205/35720 Training loss: 0.9904 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 10206/35720 Training loss: 0.9904 0.2176 sec/batch\n",
      "Epoch 6/20  Iteration 10207/35720 Training loss: 0.9903 0.2111 sec/batch\n",
      "Epoch 6/20  Iteration 10208/35720 Training loss: 0.9902 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 10209/35720 Training loss: 0.9902 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 10210/35720 Training loss: 0.9901 0.2058 sec/batch\n",
      "Epoch 6/20  Iteration 10211/35720 Training loss: 0.9901 0.2095 sec/batch\n",
      "Epoch 6/20  Iteration 10212/35720 Training loss: 0.9900 0.2214 sec/batch\n",
      "Epoch 6/20  Iteration 10213/35720 Training loss: 0.9899 0.2187 sec/batch\n",
      "Epoch 6/20  Iteration 10214/35720 Training loss: 0.9899 0.2205 sec/batch\n",
      "Epoch 6/20  Iteration 10215/35720 Training loss: 0.9899 0.2170 sec/batch\n",
      "Epoch 6/20  Iteration 10216/35720 Training loss: 0.9898 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 10217/35720 Training loss: 0.9898 0.2191 sec/batch\n",
      "Epoch 6/20  Iteration 10218/35720 Training loss: 0.9897 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 10219/35720 Training loss: 0.9897 0.2238 sec/batch\n",
      "Epoch 6/20  Iteration 10220/35720 Training loss: 0.9897 0.2236 sec/batch\n",
      "Epoch 6/20  Iteration 10221/35720 Training loss: 0.9897 0.2244 sec/batch\n",
      "Epoch 6/20  Iteration 10222/35720 Training loss: 0.9896 0.2149 sec/batch\n",
      "Epoch 6/20  Iteration 10223/35720 Training loss: 0.9897 0.2245 sec/batch\n",
      "Epoch 6/20  Iteration 10224/35720 Training loss: 0.9898 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 10225/35720 Training loss: 0.9898 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 10226/35720 Training loss: 0.9897 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 10227/35720 Training loss: 0.9898 0.2248 sec/batch\n",
      "Epoch 6/20  Iteration 10228/35720 Training loss: 0.9897 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 10229/35720 Training loss: 0.9897 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 10230/35720 Training loss: 0.9896 0.2247 sec/batch\n",
      "Epoch 6/20  Iteration 10231/35720 Training loss: 0.9895 0.2201 sec/batch\n",
      "Epoch 6/20  Iteration 10232/35720 Training loss: 0.9895 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 10233/35720 Training loss: 0.9895 0.2203 sec/batch\n",
      "Epoch 6/20  Iteration 10234/35720 Training loss: 0.9895 0.2062 sec/batch\n",
      "Epoch 6/20  Iteration 10235/35720 Training loss: 0.9894 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 10236/35720 Training loss: 0.9894 0.2155 sec/batch\n",
      "Epoch 6/20  Iteration 10237/35720 Training loss: 0.9894 0.2264 sec/batch\n",
      "Epoch 6/20  Iteration 10238/35720 Training loss: 0.9893 0.2120 sec/batch\n",
      "Epoch 6/20  Iteration 10239/35720 Training loss: 0.9893 0.2331 sec/batch\n",
      "Epoch 6/20  Iteration 10240/35720 Training loss: 0.9892 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 10241/35720 Training loss: 0.9892 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 10242/35720 Training loss: 0.9892 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 10243/35720 Training loss: 0.9892 0.2095 sec/batch\n",
      "Epoch 6/20  Iteration 10244/35720 Training loss: 0.9892 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 10245/35720 Training loss: 0.9891 0.2176 sec/batch\n",
      "Epoch 6/20  Iteration 10246/35720 Training loss: 0.9891 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 10247/35720 Training loss: 0.9891 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 10248/35720 Training loss: 0.9891 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 10249/35720 Training loss: 0.9890 0.2170 sec/batch\n",
      "Epoch 6/20  Iteration 10250/35720 Training loss: 0.9890 0.2148 sec/batch\n",
      "Epoch 6/20  Iteration 10251/35720 Training loss: 0.9890 0.2119 sec/batch\n",
      "Epoch 6/20  Iteration 10252/35720 Training loss: 0.9890 0.2132 sec/batch\n",
      "Epoch 6/20  Iteration 10253/35720 Training loss: 0.9890 0.2281 sec/batch\n",
      "Epoch 6/20  Iteration 10254/35720 Training loss: 0.9890 0.2124 sec/batch\n",
      "Epoch 6/20  Iteration 10255/35720 Training loss: 0.9890 0.2251 sec/batch\n",
      "Epoch 6/20  Iteration 10256/35720 Training loss: 0.9890 0.2324 sec/batch\n",
      "Epoch 6/20  Iteration 10257/35720 Training loss: 0.9891 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 10258/35720 Training loss: 0.9890 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 10259/35720 Training loss: 0.9890 0.2250 sec/batch\n",
      "Epoch 6/20  Iteration 10260/35720 Training loss: 0.9889 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 10261/35720 Training loss: 0.9889 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 10262/35720 Training loss: 0.9889 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 10263/35720 Training loss: 0.9889 0.2288 sec/batch\n",
      "Epoch 6/20  Iteration 10264/35720 Training loss: 0.9889 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 10265/35720 Training loss: 0.9889 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 10266/35720 Training loss: 0.9889 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 10267/35720 Training loss: 0.9890 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 10268/35720 Training loss: 0.9890 0.2111 sec/batch\n",
      "Epoch 6/20  Iteration 10269/35720 Training loss: 0.9890 0.2169 sec/batch\n",
      "Epoch 6/20  Iteration 10270/35720 Training loss: 0.9890 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 10271/35720 Training loss: 0.9890 0.2135 sec/batch\n",
      "Epoch 6/20  Iteration 10272/35720 Training loss: 0.9889 0.2186 sec/batch\n",
      "Epoch 6/20  Iteration 10273/35720 Training loss: 0.9889 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 10274/35720 Training loss: 0.9889 0.2401 sec/batch\n",
      "Epoch 6/20  Iteration 10275/35720 Training loss: 0.9889 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 10276/35720 Training loss: 0.9889 0.2065 sec/batch\n",
      "Epoch 6/20  Iteration 10277/35720 Training loss: 0.9888 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 10278/35720 Training loss: 0.9889 0.2313 sec/batch\n",
      "Epoch 6/20  Iteration 10279/35720 Training loss: 0.9889 0.2211 sec/batch\n",
      "Epoch 6/20  Iteration 10280/35720 Training loss: 0.9889 0.2132 sec/batch\n",
      "Epoch 6/20  Iteration 10281/35720 Training loss: 0.9889 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 10282/35720 Training loss: 0.9890 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 10283/35720 Training loss: 0.9891 0.2081 sec/batch\n",
      "Epoch 6/20  Iteration 10284/35720 Training loss: 0.9890 0.2365 sec/batch\n",
      "Epoch 6/20  Iteration 10285/35720 Training loss: 0.9890 0.2247 sec/batch\n",
      "Epoch 6/20  Iteration 10286/35720 Training loss: 0.9889 0.2194 sec/batch\n",
      "Epoch 6/20  Iteration 10287/35720 Training loss: 0.9889 0.2118 sec/batch\n",
      "Epoch 6/20  Iteration 10288/35720 Training loss: 0.9889 0.2095 sec/batch\n",
      "Epoch 6/20  Iteration 10289/35720 Training loss: 0.9889 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 10290/35720 Training loss: 0.9888 0.2080 sec/batch\n",
      "Epoch 6/20  Iteration 10291/35720 Training loss: 0.9888 0.2208 sec/batch\n",
      "Epoch 6/20  Iteration 10292/35720 Training loss: 0.9888 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 10293/35720 Training loss: 0.9888 0.2054 sec/batch\n",
      "Epoch 6/20  Iteration 10294/35720 Training loss: 0.9888 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 10295/35720 Training loss: 0.9887 0.2266 sec/batch\n",
      "Epoch 6/20  Iteration 10296/35720 Training loss: 0.9887 0.2246 sec/batch\n",
      "Epoch 6/20  Iteration 10297/35720 Training loss: 0.9887 0.2272 sec/batch\n",
      "Epoch 6/20  Iteration 10298/35720 Training loss: 0.9887 0.2079 sec/batch\n",
      "Epoch 6/20  Iteration 10299/35720 Training loss: 0.9888 0.2288 sec/batch\n",
      "Epoch 6/20  Iteration 10300/35720 Training loss: 0.9888 0.2325 sec/batch\n",
      "Epoch 6/20  Iteration 10301/35720 Training loss: 0.9888 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 10302/35720 Training loss: 0.9888 0.2251 sec/batch\n",
      "Epoch 6/20  Iteration 10303/35720 Training loss: 0.9888 0.2247 sec/batch\n",
      "Epoch 6/20  Iteration 10304/35720 Training loss: 0.9888 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 10305/35720 Training loss: 0.9887 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 10306/35720 Training loss: 0.9887 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 10307/35720 Training loss: 0.9887 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 10308/35720 Training loss: 0.9887 0.2200 sec/batch\n",
      "Epoch 6/20  Iteration 10309/35720 Training loss: 0.9887 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 10310/35720 Training loss: 0.9887 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 10311/35720 Training loss: 0.9887 0.2315 sec/batch\n",
      "Epoch 6/20  Iteration 10312/35720 Training loss: 0.9887 0.2257 sec/batch\n",
      "Epoch 6/20  Iteration 10313/35720 Training loss: 0.9887 0.2244 sec/batch\n",
      "Epoch 6/20  Iteration 10314/35720 Training loss: 0.9886 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 10315/35720 Training loss: 0.9887 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 10316/35720 Training loss: 0.9887 0.2329 sec/batch\n",
      "Epoch 6/20  Iteration 10317/35720 Training loss: 0.9887 0.2095 sec/batch\n",
      "Epoch 6/20  Iteration 10318/35720 Training loss: 0.9887 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 10319/35720 Training loss: 0.9886 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 10320/35720 Training loss: 0.9886 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 10321/35720 Training loss: 0.9886 0.2055 sec/batch\n",
      "Epoch 6/20  Iteration 10322/35720 Training loss: 0.9885 0.2151 sec/batch\n",
      "Epoch 6/20  Iteration 10323/35720 Training loss: 0.9885 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 10324/35720 Training loss: 0.9884 0.2144 sec/batch\n",
      "Epoch 6/20  Iteration 10325/35720 Training loss: 0.9884 0.2411 sec/batch\n",
      "Epoch 6/20  Iteration 10326/35720 Training loss: 0.9883 0.2068 sec/batch\n",
      "Epoch 6/20  Iteration 10327/35720 Training loss: 0.9882 0.2181 sec/batch\n",
      "Epoch 6/20  Iteration 10328/35720 Training loss: 0.9882 0.2316 sec/batch\n",
      "Epoch 6/20  Iteration 10329/35720 Training loss: 0.9881 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 10330/35720 Training loss: 0.9881 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 10331/35720 Training loss: 0.9881 0.2141 sec/batch\n",
      "Epoch 6/20  Iteration 10332/35720 Training loss: 0.9880 0.2262 sec/batch\n",
      "Epoch 6/20  Iteration 10333/35720 Training loss: 0.9881 0.2214 sec/batch\n",
      "Epoch 6/20  Iteration 10334/35720 Training loss: 0.9881 0.2302 sec/batch\n",
      "Epoch 6/20  Iteration 10335/35720 Training loss: 0.9880 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 10336/35720 Training loss: 0.9880 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 10337/35720 Training loss: 0.9880 0.2072 sec/batch\n",
      "Epoch 6/20  Iteration 10338/35720 Training loss: 0.9880 0.2223 sec/batch\n",
      "Epoch 6/20  Iteration 10339/35720 Training loss: 0.9880 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 10340/35720 Training loss: 0.9880 0.2421 sec/batch\n",
      "Epoch 6/20  Iteration 10341/35720 Training loss: 0.9880 0.2282 sec/batch\n",
      "Epoch 6/20  Iteration 10342/35720 Training loss: 0.9880 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 10343/35720 Training loss: 0.9880 0.2086 sec/batch\n",
      "Epoch 6/20  Iteration 10344/35720 Training loss: 0.9880 0.2309 sec/batch\n",
      "Epoch 6/20  Iteration 10345/35720 Training loss: 0.9880 0.2278 sec/batch\n",
      "Epoch 6/20  Iteration 10346/35720 Training loss: 0.9879 0.2302 sec/batch\n",
      "Epoch 6/20  Iteration 10347/35720 Training loss: 0.9879 0.2259 sec/batch\n",
      "Epoch 6/20  Iteration 10348/35720 Training loss: 0.9879 0.2093 sec/batch\n",
      "Epoch 6/20  Iteration 10349/35720 Training loss: 0.9879 0.2373 sec/batch\n",
      "Epoch 6/20  Iteration 10350/35720 Training loss: 0.9879 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 10351/35720 Training loss: 0.9880 0.2285 sec/batch\n",
      "Epoch 6/20  Iteration 10352/35720 Training loss: 0.9879 0.2068 sec/batch\n",
      "Epoch 6/20  Iteration 10353/35720 Training loss: 0.9879 0.2279 sec/batch\n",
      "Epoch 6/20  Iteration 10354/35720 Training loss: 0.9879 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 10355/35720 Training loss: 0.9879 0.2258 sec/batch\n",
      "Epoch 6/20  Iteration 10356/35720 Training loss: 0.9879 0.2326 sec/batch\n",
      "Epoch 6/20  Iteration 10357/35720 Training loss: 0.9879 0.2131 sec/batch\n",
      "Epoch 6/20  Iteration 10358/35720 Training loss: 0.9880 0.2132 sec/batch\n",
      "Epoch 6/20  Iteration 10359/35720 Training loss: 0.9880 0.2085 sec/batch\n",
      "Epoch 6/20  Iteration 10360/35720 Training loss: 0.9879 0.2308 sec/batch\n",
      "Epoch 6/20  Iteration 10361/35720 Training loss: 0.9878 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 10362/35720 Training loss: 0.9878 0.2140 sec/batch\n",
      "Epoch 6/20  Iteration 10363/35720 Training loss: 0.9878 0.2072 sec/batch\n",
      "Epoch 6/20  Iteration 10364/35720 Training loss: 0.9878 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 10365/35720 Training loss: 0.9878 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 10366/35720 Training loss: 0.9877 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 10367/35720 Training loss: 0.9877 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 10368/35720 Training loss: 0.9877 0.2171 sec/batch\n",
      "Epoch 6/20  Iteration 10369/35720 Training loss: 0.9877 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 10370/35720 Training loss: 0.9877 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 10371/35720 Training loss: 0.9877 0.2186 sec/batch\n",
      "Epoch 6/20  Iteration 10372/35720 Training loss: 0.9877 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 10373/35720 Training loss: 0.9877 0.2177 sec/batch\n",
      "Epoch 6/20  Iteration 10374/35720 Training loss: 0.9877 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 10375/35720 Training loss: 0.9877 0.2068 sec/batch\n",
      "Epoch 6/20  Iteration 10376/35720 Training loss: 0.9877 0.2091 sec/batch\n",
      "Epoch 6/20  Iteration 10377/35720 Training loss: 0.9876 0.2271 sec/batch\n",
      "Epoch 6/20  Iteration 10378/35720 Training loss: 0.9876 0.2321 sec/batch\n",
      "Epoch 6/20  Iteration 10379/35720 Training loss: 0.9876 0.2304 sec/batch\n",
      "Epoch 6/20  Iteration 10380/35720 Training loss: 0.9875 0.2079 sec/batch\n",
      "Epoch 6/20  Iteration 10381/35720 Training loss: 0.9874 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 10382/35720 Training loss: 0.9874 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 10383/35720 Training loss: 0.9874 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 10384/35720 Training loss: 0.9874 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 10385/35720 Training loss: 0.9874 0.2194 sec/batch\n",
      "Epoch 6/20  Iteration 10386/35720 Training loss: 0.9874 0.2060 sec/batch\n",
      "Epoch 6/20  Iteration 10387/35720 Training loss: 0.9874 0.2120 sec/batch\n",
      "Epoch 6/20  Iteration 10388/35720 Training loss: 0.9874 0.2281 sec/batch\n",
      "Epoch 6/20  Iteration 10389/35720 Training loss: 0.9875 0.2261 sec/batch\n",
      "Epoch 6/20  Iteration 10390/35720 Training loss: 0.9876 0.2281 sec/batch\n",
      "Epoch 6/20  Iteration 10391/35720 Training loss: 0.9876 0.2298 sec/batch\n",
      "Epoch 6/20  Iteration 10392/35720 Training loss: 0.9876 0.2879 sec/batch\n",
      "Epoch 6/20  Iteration 10393/35720 Training loss: 0.9876 0.2605 sec/batch\n",
      "Epoch 6/20  Iteration 10394/35720 Training loss: 0.9876 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 10395/35720 Training loss: 0.9876 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 10396/35720 Training loss: 0.9876 0.2121 sec/batch\n",
      "Epoch 6/20  Iteration 10397/35720 Training loss: 0.9875 0.2313 sec/batch\n",
      "Epoch 6/20  Iteration 10398/35720 Training loss: 0.9875 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 10399/35720 Training loss: 0.9875 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 10400/35720 Training loss: 0.9875 0.2206 sec/batch\n",
      "Validation loss: 1.31961 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 10401/35720 Training loss: 0.9876 0.2087 sec/batch\n",
      "Epoch 6/20  Iteration 10402/35720 Training loss: 0.9876 0.2093 sec/batch\n",
      "Epoch 6/20  Iteration 10403/35720 Training loss: 0.9876 0.2325 sec/batch\n",
      "Epoch 6/20  Iteration 10404/35720 Training loss: 0.9875 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 10405/35720 Training loss: 0.9874 0.2292 sec/batch\n",
      "Epoch 6/20  Iteration 10406/35720 Training loss: 0.9873 0.2271 sec/batch\n",
      "Epoch 6/20  Iteration 10407/35720 Training loss: 0.9872 0.2198 sec/batch\n",
      "Epoch 6/20  Iteration 10408/35720 Training loss: 0.9872 0.2314 sec/batch\n",
      "Epoch 6/20  Iteration 10409/35720 Training loss: 0.9871 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 10410/35720 Training loss: 0.9871 0.2210 sec/batch\n",
      "Epoch 6/20  Iteration 10411/35720 Training loss: 0.9871 0.2199 sec/batch\n",
      "Epoch 6/20  Iteration 10412/35720 Training loss: 0.9871 0.2205 sec/batch\n",
      "Epoch 6/20  Iteration 10413/35720 Training loss: 0.9870 0.2142 sec/batch\n",
      "Epoch 6/20  Iteration 10414/35720 Training loss: 0.9870 0.2129 sec/batch\n",
      "Epoch 6/20  Iteration 10415/35720 Training loss: 0.9869 0.2158 sec/batch\n",
      "Epoch 6/20  Iteration 10416/35720 Training loss: 0.9869 0.2264 sec/batch\n",
      "Epoch 6/20  Iteration 10417/35720 Training loss: 0.9869 0.2243 sec/batch\n",
      "Epoch 6/20  Iteration 10418/35720 Training loss: 0.9868 0.2120 sec/batch\n",
      "Epoch 6/20  Iteration 10419/35720 Training loss: 0.9868 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10420/35720 Training loss: 0.9868 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 10421/35720 Training loss: 0.9868 0.2262 sec/batch\n",
      "Epoch 6/20  Iteration 10422/35720 Training loss: 0.9867 0.2238 sec/batch\n",
      "Epoch 6/20  Iteration 10423/35720 Training loss: 0.9867 0.2266 sec/batch\n",
      "Epoch 6/20  Iteration 10424/35720 Training loss: 0.9867 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 10425/35720 Training loss: 0.9867 0.2279 sec/batch\n",
      "Epoch 6/20  Iteration 10426/35720 Training loss: 0.9867 0.2286 sec/batch\n",
      "Epoch 6/20  Iteration 10427/35720 Training loss: 0.9866 0.2234 sec/batch\n",
      "Epoch 6/20  Iteration 10428/35720 Training loss: 0.9866 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 10429/35720 Training loss: 0.9867 0.2199 sec/batch\n",
      "Epoch 6/20  Iteration 10430/35720 Training loss: 0.9866 0.2120 sec/batch\n",
      "Epoch 6/20  Iteration 10431/35720 Training loss: 0.9866 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 10432/35720 Training loss: 0.9865 0.2205 sec/batch\n",
      "Epoch 6/20  Iteration 10433/35720 Training loss: 0.9865 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 10434/35720 Training loss: 0.9865 0.2067 sec/batch\n",
      "Epoch 6/20  Iteration 10435/35720 Training loss: 0.9866 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 10436/35720 Training loss: 0.9866 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10437/35720 Training loss: 0.9865 0.2242 sec/batch\n",
      "Epoch 6/20  Iteration 10438/35720 Training loss: 0.9865 0.2248 sec/batch\n",
      "Epoch 6/20  Iteration 10439/35720 Training loss: 0.9865 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10440/35720 Training loss: 0.9865 0.2100 sec/batch\n",
      "Epoch 6/20  Iteration 10441/35720 Training loss: 0.9865 0.2212 sec/batch\n",
      "Epoch 6/20  Iteration 10442/35720 Training loss: 0.9864 0.2134 sec/batch\n",
      "Epoch 6/20  Iteration 10443/35720 Training loss: 0.9864 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 10444/35720 Training loss: 0.9865 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 10445/35720 Training loss: 0.9865 0.2121 sec/batch\n",
      "Epoch 6/20  Iteration 10446/35720 Training loss: 0.9865 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 10447/35720 Training loss: 0.9865 0.2291 sec/batch\n",
      "Epoch 6/20  Iteration 10448/35720 Training loss: 0.9864 0.2311 sec/batch\n",
      "Epoch 6/20  Iteration 10449/35720 Training loss: 0.9865 0.2223 sec/batch\n",
      "Epoch 6/20  Iteration 10450/35720 Training loss: 0.9865 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 10451/35720 Training loss: 0.9865 0.2264 sec/batch\n",
      "Epoch 6/20  Iteration 10452/35720 Training loss: 0.9865 0.2393 sec/batch\n",
      "Epoch 6/20  Iteration 10453/35720 Training loss: 0.9865 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 10454/35720 Training loss: 0.9866 0.2150 sec/batch\n",
      "Epoch 6/20  Iteration 10455/35720 Training loss: 0.9866 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 10456/35720 Training loss: 0.9866 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 10457/35720 Training loss: 0.9866 0.2166 sec/batch\n",
      "Epoch 6/20  Iteration 10458/35720 Training loss: 0.9866 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 10459/35720 Training loss: 0.9866 0.2082 sec/batch\n",
      "Epoch 6/20  Iteration 10460/35720 Training loss: 0.9867 0.2131 sec/batch\n",
      "Epoch 6/20  Iteration 10461/35720 Training loss: 0.9867 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 10462/35720 Training loss: 0.9867 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 10463/35720 Training loss: 0.9867 0.2251 sec/batch\n",
      "Epoch 6/20  Iteration 10464/35720 Training loss: 0.9867 0.2147 sec/batch\n",
      "Epoch 6/20  Iteration 10465/35720 Training loss: 0.9866 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 10466/35720 Training loss: 0.9866 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 10467/35720 Training loss: 0.9865 0.2302 sec/batch\n",
      "Epoch 6/20  Iteration 10468/35720 Training loss: 0.9865 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 10469/35720 Training loss: 0.9865 0.2362 sec/batch\n",
      "Epoch 6/20  Iteration 10470/35720 Training loss: 0.9865 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 10471/35720 Training loss: 0.9864 0.2146 sec/batch\n",
      "Epoch 6/20  Iteration 10472/35720 Training loss: 0.9864 0.2173 sec/batch\n",
      "Epoch 6/20  Iteration 10473/35720 Training loss: 0.9863 0.2155 sec/batch\n",
      "Epoch 6/20  Iteration 10474/35720 Training loss: 0.9863 0.2173 sec/batch\n",
      "Epoch 6/20  Iteration 10475/35720 Training loss: 0.9863 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 10476/35720 Training loss: 0.9862 0.2501 sec/batch\n",
      "Epoch 6/20  Iteration 10477/35720 Training loss: 0.9862 0.2236 sec/batch\n",
      "Epoch 6/20  Iteration 10478/35720 Training loss: 0.9862 0.2061 sec/batch\n",
      "Epoch 6/20  Iteration 10479/35720 Training loss: 0.9862 0.2207 sec/batch\n",
      "Epoch 6/20  Iteration 10480/35720 Training loss: 0.9862 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 10481/35720 Training loss: 0.9862 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 10482/35720 Training loss: 0.9861 0.2212 sec/batch\n",
      "Epoch 6/20  Iteration 10483/35720 Training loss: 0.9861 0.2176 sec/batch\n",
      "Epoch 6/20  Iteration 10484/35720 Training loss: 0.9860 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 10485/35720 Training loss: 0.9859 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 10486/35720 Training loss: 0.9859 0.2113 sec/batch\n",
      "Epoch 6/20  Iteration 10487/35720 Training loss: 0.9859 0.2288 sec/batch\n",
      "Epoch 6/20  Iteration 10488/35720 Training loss: 0.9858 0.2097 sec/batch\n",
      "Epoch 6/20  Iteration 10489/35720 Training loss: 0.9858 0.2105 sec/batch\n",
      "Epoch 6/20  Iteration 10490/35720 Training loss: 0.9857 0.2183 sec/batch\n",
      "Epoch 6/20  Iteration 10491/35720 Training loss: 0.9857 0.2207 sec/batch\n",
      "Epoch 6/20  Iteration 10492/35720 Training loss: 0.9857 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 10493/35720 Training loss: 0.9856 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 10494/35720 Training loss: 0.9856 0.2148 sec/batch\n",
      "Epoch 6/20  Iteration 10495/35720 Training loss: 0.9856 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 10496/35720 Training loss: 0.9855 0.2104 sec/batch\n",
      "Epoch 6/20  Iteration 10497/35720 Training loss: 0.9855 0.2294 sec/batch\n",
      "Epoch 6/20  Iteration 10498/35720 Training loss: 0.9855 0.2287 sec/batch\n",
      "Epoch 6/20  Iteration 10499/35720 Training loss: 0.9855 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 10500/35720 Training loss: 0.9855 0.2071 sec/batch\n",
      "Epoch 6/20  Iteration 10501/35720 Training loss: 0.9854 0.2127 sec/batch\n",
      "Epoch 6/20  Iteration 10502/35720 Training loss: 0.9854 0.2213 sec/batch\n",
      "Epoch 6/20  Iteration 10503/35720 Training loss: 0.9854 0.2195 sec/batch\n",
      "Epoch 6/20  Iteration 10504/35720 Training loss: 0.9853 0.2161 sec/batch\n",
      "Epoch 6/20  Iteration 10505/35720 Training loss: 0.9853 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10506/35720 Training loss: 0.9853 0.2231 sec/batch\n",
      "Epoch 6/20  Iteration 10507/35720 Training loss: 0.9852 0.2185 sec/batch\n",
      "Epoch 6/20  Iteration 10508/35720 Training loss: 0.9852 0.2197 sec/batch\n",
      "Epoch 6/20  Iteration 10509/35720 Training loss: 0.9852 0.2291 sec/batch\n",
      "Epoch 6/20  Iteration 10510/35720 Training loss: 0.9852 0.2212 sec/batch\n",
      "Epoch 6/20  Iteration 10511/35720 Training loss: 0.9852 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 10512/35720 Training loss: 0.9852 0.2246 sec/batch\n",
      "Epoch 6/20  Iteration 10513/35720 Training loss: 0.9851 0.2274 sec/batch\n",
      "Epoch 6/20  Iteration 10514/35720 Training loss: 0.9850 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 10515/35720 Training loss: 0.9851 0.2180 sec/batch\n",
      "Epoch 6/20  Iteration 10516/35720 Training loss: 0.9850 0.2182 sec/batch\n",
      "Epoch 6/20  Iteration 10517/35720 Training loss: 0.9850 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 10518/35720 Training loss: 0.9849 0.2085 sec/batch\n",
      "Epoch 6/20  Iteration 10519/35720 Training loss: 0.9849 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 10520/35720 Training loss: 0.9848 0.2281 sec/batch\n",
      "Epoch 6/20  Iteration 10521/35720 Training loss: 0.9848 0.2227 sec/batch\n",
      "Epoch 6/20  Iteration 10522/35720 Training loss: 0.9848 0.2121 sec/batch\n",
      "Epoch 6/20  Iteration 10523/35720 Training loss: 0.9849 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 10524/35720 Training loss: 0.9848 0.2339 sec/batch\n",
      "Epoch 6/20  Iteration 10525/35720 Training loss: 0.9847 0.2135 sec/batch\n",
      "Epoch 6/20  Iteration 10526/35720 Training loss: 0.9847 0.2169 sec/batch\n",
      "Epoch 6/20  Iteration 10527/35720 Training loss: 0.9847 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 10528/35720 Training loss: 0.9846 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 10529/35720 Training loss: 0.9846 0.2235 sec/batch\n",
      "Epoch 6/20  Iteration 10530/35720 Training loss: 0.9846 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 10531/35720 Training loss: 0.9845 0.2357 sec/batch\n",
      "Epoch 6/20  Iteration 10532/35720 Training loss: 0.9845 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10533/35720 Training loss: 0.9845 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 10534/35720 Training loss: 0.9845 0.2083 sec/batch\n",
      "Epoch 6/20  Iteration 10535/35720 Training loss: 0.9844 0.2275 sec/batch\n",
      "Epoch 6/20  Iteration 10536/35720 Training loss: 0.9844 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 10537/35720 Training loss: 0.9844 0.2204 sec/batch\n",
      "Epoch 6/20  Iteration 10538/35720 Training loss: 0.9844 0.2186 sec/batch\n",
      "Epoch 6/20  Iteration 10539/35720 Training loss: 0.9843 0.2107 sec/batch\n",
      "Epoch 6/20  Iteration 10540/35720 Training loss: 0.9842 0.2221 sec/batch\n",
      "Epoch 6/20  Iteration 10541/35720 Training loss: 0.9842 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 10542/35720 Training loss: 0.9841 0.2191 sec/batch\n",
      "Epoch 6/20  Iteration 10543/35720 Training loss: 0.9841 0.2251 sec/batch\n",
      "Epoch 6/20  Iteration 10544/35720 Training loss: 0.9841 0.2076 sec/batch\n",
      "Epoch 6/20  Iteration 10545/35720 Training loss: 0.9841 0.2094 sec/batch\n",
      "Epoch 6/20  Iteration 10546/35720 Training loss: 0.9841 0.2270 sec/batch\n",
      "Epoch 6/20  Iteration 10547/35720 Training loss: 0.9840 0.2190 sec/batch\n",
      "Epoch 6/20  Iteration 10548/35720 Training loss: 0.9840 0.2423 sec/batch\n",
      "Epoch 6/20  Iteration 10549/35720 Training loss: 0.9840 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 10550/35720 Training loss: 0.9840 0.2089 sec/batch\n",
      "Epoch 6/20  Iteration 10551/35720 Training loss: 0.9839 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 10552/35720 Training loss: 0.9839 0.2254 sec/batch\n",
      "Epoch 6/20  Iteration 10553/35720 Training loss: 0.9839 0.2223 sec/batch\n",
      "Epoch 6/20  Iteration 10554/35720 Training loss: 0.9839 0.2217 sec/batch\n",
      "Epoch 6/20  Iteration 10555/35720 Training loss: 0.9839 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10556/35720 Training loss: 0.9840 0.2174 sec/batch\n",
      "Epoch 6/20  Iteration 10557/35720 Training loss: 0.9840 0.2357 sec/batch\n",
      "Epoch 6/20  Iteration 10558/35720 Training loss: 0.9840 0.2137 sec/batch\n",
      "Epoch 6/20  Iteration 10559/35720 Training loss: 0.9839 0.2168 sec/batch\n",
      "Epoch 6/20  Iteration 10560/35720 Training loss: 0.9839 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 10561/35720 Training loss: 0.9839 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 10562/35720 Training loss: 0.9839 0.2131 sec/batch\n",
      "Epoch 6/20  Iteration 10563/35720 Training loss: 0.9839 0.2338 sec/batch\n",
      "Epoch 6/20  Iteration 10564/35720 Training loss: 0.9838 0.2311 sec/batch\n",
      "Epoch 6/20  Iteration 10565/35720 Training loss: 0.9838 0.2141 sec/batch\n",
      "Epoch 6/20  Iteration 10566/35720 Training loss: 0.9838 0.2209 sec/batch\n",
      "Epoch 6/20  Iteration 10567/35720 Training loss: 0.9838 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 10568/35720 Training loss: 0.9837 0.2198 sec/batch\n",
      "Epoch 6/20  Iteration 10569/35720 Training loss: 0.9838 0.2110 sec/batch\n",
      "Epoch 6/20  Iteration 10570/35720 Training loss: 0.9837 0.2238 sec/batch\n",
      "Epoch 6/20  Iteration 10571/35720 Training loss: 0.9837 0.2133 sec/batch\n",
      "Epoch 6/20  Iteration 10572/35720 Training loss: 0.9837 0.2122 sec/batch\n",
      "Epoch 6/20  Iteration 10573/35720 Training loss: 0.9837 0.2281 sec/batch\n",
      "Epoch 6/20  Iteration 10574/35720 Training loss: 0.9837 0.2320 sec/batch\n",
      "Epoch 6/20  Iteration 10575/35720 Training loss: 0.9837 0.2123 sec/batch\n",
      "Epoch 6/20  Iteration 10576/35720 Training loss: 0.9837 0.2276 sec/batch\n",
      "Epoch 6/20  Iteration 10577/35720 Training loss: 0.9837 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 10578/35720 Training loss: 0.9837 0.2103 sec/batch\n",
      "Epoch 6/20  Iteration 10579/35720 Training loss: 0.9837 0.2199 sec/batch\n",
      "Epoch 6/20  Iteration 10580/35720 Training loss: 0.9837 0.2192 sec/batch\n",
      "Epoch 6/20  Iteration 10581/35720 Training loss: 0.9838 0.2307 sec/batch\n",
      "Epoch 6/20  Iteration 10582/35720 Training loss: 0.9838 0.2243 sec/batch\n",
      "Epoch 6/20  Iteration 10583/35720 Training loss: 0.9838 0.2084 sec/batch\n",
      "Epoch 6/20  Iteration 10584/35720 Training loss: 0.9838 0.2194 sec/batch\n",
      "Epoch 6/20  Iteration 10585/35720 Training loss: 0.9838 0.2284 sec/batch\n",
      "Epoch 6/20  Iteration 10586/35720 Training loss: 0.9838 0.2187 sec/batch\n",
      "Epoch 6/20  Iteration 10587/35720 Training loss: 0.9838 0.2073 sec/batch\n",
      "Epoch 6/20  Iteration 10588/35720 Training loss: 0.9838 0.2146 sec/batch\n",
      "Epoch 6/20  Iteration 10589/35720 Training loss: 0.9838 0.2176 sec/batch\n",
      "Epoch 6/20  Iteration 10590/35720 Training loss: 0.9838 0.2172 sec/batch\n",
      "Epoch 6/20  Iteration 10591/35720 Training loss: 0.9838 0.2101 sec/batch\n",
      "Epoch 6/20  Iteration 10592/35720 Training loss: 0.9838 0.2266 sec/batch\n",
      "Epoch 6/20  Iteration 10593/35720 Training loss: 0.9838 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 10594/35720 Training loss: 0.9838 0.2231 sec/batch\n",
      "Epoch 6/20  Iteration 10595/35720 Training loss: 0.9838 0.2243 sec/batch\n",
      "Epoch 6/20  Iteration 10596/35720 Training loss: 0.9838 0.2283 sec/batch\n",
      "Epoch 6/20  Iteration 10597/35720 Training loss: 0.9838 0.2288 sec/batch\n",
      "Epoch 6/20  Iteration 10598/35720 Training loss: 0.9839 0.2072 sec/batch\n",
      "Epoch 6/20  Iteration 10599/35720 Training loss: 0.9839 0.2136 sec/batch\n",
      "Epoch 6/20  Iteration 10600/35720 Training loss: 0.9839 0.2255 sec/batch\n",
      "Validation loss: 1.31625 Saving checkpoint!\n",
      "Epoch 6/20  Iteration 10601/35720 Training loss: 0.9841 0.2118 sec/batch\n",
      "Epoch 6/20  Iteration 10602/35720 Training loss: 0.9841 0.2226 sec/batch\n",
      "Epoch 6/20  Iteration 10603/35720 Training loss: 0.9841 0.2068 sec/batch\n",
      "Epoch 6/20  Iteration 10604/35720 Training loss: 0.9842 0.2117 sec/batch\n",
      "Epoch 6/20  Iteration 10605/35720 Training loss: 0.9842 0.2294 sec/batch\n",
      "Epoch 6/20  Iteration 10606/35720 Training loss: 0.9842 0.2289 sec/batch\n",
      "Epoch 6/20  Iteration 10607/35720 Training loss: 0.9841 0.2317 sec/batch\n",
      "Epoch 6/20  Iteration 10608/35720 Training loss: 0.9841 0.2069 sec/batch\n",
      "Epoch 6/20  Iteration 10609/35720 Training loss: 0.9841 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 10610/35720 Training loss: 0.9841 0.2087 sec/batch\n",
      "Epoch 6/20  Iteration 10611/35720 Training loss: 0.9840 0.2197 sec/batch\n",
      "Epoch 6/20  Iteration 10612/35720 Training loss: 0.9840 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 10613/35720 Training loss: 0.9840 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 10614/35720 Training loss: 0.9840 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 10615/35720 Training loss: 0.9840 0.2232 sec/batch\n",
      "Epoch 6/20  Iteration 10616/35720 Training loss: 0.9840 0.2120 sec/batch\n",
      "Epoch 6/20  Iteration 10617/35720 Training loss: 0.9840 0.2153 sec/batch\n",
      "Epoch 6/20  Iteration 10618/35720 Training loss: 0.9840 0.2146 sec/batch\n",
      "Epoch 6/20  Iteration 10619/35720 Training loss: 0.9840 0.2154 sec/batch\n",
      "Epoch 6/20  Iteration 10620/35720 Training loss: 0.9839 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 10621/35720 Training loss: 0.9839 0.2167 sec/batch\n",
      "Epoch 6/20  Iteration 10622/35720 Training loss: 0.9838 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 10623/35720 Training loss: 0.9838 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 10624/35720 Training loss: 0.9838 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 10625/35720 Training loss: 0.9838 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 10626/35720 Training loss: 0.9838 0.2064 sec/batch\n",
      "Epoch 6/20  Iteration 10627/35720 Training loss: 0.9837 0.2080 sec/batch\n",
      "Epoch 6/20  Iteration 10628/35720 Training loss: 0.9837 0.2115 sec/batch\n",
      "Epoch 6/20  Iteration 10629/35720 Training loss: 0.9837 0.2272 sec/batch\n",
      "Epoch 6/20  Iteration 10630/35720 Training loss: 0.9837 0.2316 sec/batch\n",
      "Epoch 6/20  Iteration 10631/35720 Training loss: 0.9837 0.2121 sec/batch\n",
      "Epoch 6/20  Iteration 10632/35720 Training loss: 0.9836 0.2085 sec/batch\n",
      "Epoch 6/20  Iteration 10633/35720 Training loss: 0.9836 0.2233 sec/batch\n",
      "Epoch 6/20  Iteration 10634/35720 Training loss: 0.9836 0.2106 sec/batch\n",
      "Epoch 6/20  Iteration 10635/35720 Training loss: 0.9835 0.2268 sec/batch\n",
      "Epoch 6/20  Iteration 10636/35720 Training loss: 0.9835 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10637/35720 Training loss: 0.9834 0.2130 sec/batch\n",
      "Epoch 6/20  Iteration 10638/35720 Training loss: 0.9834 0.2092 sec/batch\n",
      "Epoch 6/20  Iteration 10639/35720 Training loss: 0.9834 0.2300 sec/batch\n",
      "Epoch 6/20  Iteration 10640/35720 Training loss: 0.9835 0.2178 sec/batch\n",
      "Epoch 6/20  Iteration 10641/35720 Training loss: 0.9835 0.2162 sec/batch\n",
      "Epoch 6/20  Iteration 10642/35720 Training loss: 0.9836 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 10643/35720 Training loss: 0.9836 0.2188 sec/batch\n",
      "Epoch 6/20  Iteration 10644/35720 Training loss: 0.9836 0.2159 sec/batch\n",
      "Epoch 6/20  Iteration 10645/35720 Training loss: 0.9836 0.2157 sec/batch\n",
      "Epoch 6/20  Iteration 10646/35720 Training loss: 0.9836 0.2269 sec/batch\n",
      "Epoch 6/20  Iteration 10647/35720 Training loss: 0.9836 0.2221 sec/batch\n",
      "Epoch 6/20  Iteration 10648/35720 Training loss: 0.9835 0.2143 sec/batch\n",
      "Epoch 6/20  Iteration 10649/35720 Training loss: 0.9835 0.2305 sec/batch\n",
      "Epoch 6/20  Iteration 10650/35720 Training loss: 0.9835 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 10651/35720 Training loss: 0.9835 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 10652/35720 Training loss: 0.9835 0.2109 sec/batch\n",
      "Epoch 6/20  Iteration 10653/35720 Training loss: 0.9835 0.2102 sec/batch\n",
      "Epoch 6/20  Iteration 10654/35720 Training loss: 0.9835 0.2090 sec/batch\n",
      "Epoch 6/20  Iteration 10655/35720 Training loss: 0.9834 0.2198 sec/batch\n",
      "Epoch 6/20  Iteration 10656/35720 Training loss: 0.9835 0.2216 sec/batch\n",
      "Epoch 6/20  Iteration 10657/35720 Training loss: 0.9834 0.2305 sec/batch\n",
      "Epoch 6/20  Iteration 10658/35720 Training loss: 0.9834 0.2108 sec/batch\n",
      "Epoch 6/20  Iteration 10659/35720 Training loss: 0.9834 0.2193 sec/batch\n",
      "Epoch 6/20  Iteration 10660/35720 Training loss: 0.9834 0.2135 sec/batch\n",
      "Epoch 6/20  Iteration 10661/35720 Training loss: 0.9834 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 10662/35720 Training loss: 0.9834 0.2138 sec/batch\n",
      "Epoch 6/20  Iteration 10663/35720 Training loss: 0.9834 0.2114 sec/batch\n",
      "Epoch 6/20  Iteration 10664/35720 Training loss: 0.9833 0.2261 sec/batch\n",
      "Epoch 6/20  Iteration 10665/35720 Training loss: 0.9834 0.2074 sec/batch\n",
      "Epoch 6/20  Iteration 10666/35720 Training loss: 0.9834 0.2082 sec/batch\n",
      "Epoch 6/20  Iteration 10667/35720 Training loss: 0.9834 0.2152 sec/batch\n",
      "Epoch 6/20  Iteration 10668/35720 Training loss: 0.9834 0.2214 sec/batch\n",
      "Epoch 6/20  Iteration 10669/35720 Training loss: 0.9834 0.2207 sec/batch\n",
      "Epoch 6/20  Iteration 10670/35720 Training loss: 0.9834 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 10671/35720 Training loss: 0.9834 0.2316 sec/batch\n",
      "Epoch 6/20  Iteration 10672/35720 Training loss: 0.9833 0.2293 sec/batch\n",
      "Epoch 6/20  Iteration 10673/35720 Training loss: 0.9833 0.2208 sec/batch\n",
      "Epoch 6/20  Iteration 10674/35720 Training loss: 0.9834 0.2237 sec/batch\n",
      "Epoch 6/20  Iteration 10675/35720 Training loss: 0.9834 0.2203 sec/batch\n",
      "Epoch 6/20  Iteration 10676/35720 Training loss: 0.9835 0.2125 sec/batch\n",
      "Epoch 6/20  Iteration 10677/35720 Training loss: 0.9835 0.2081 sec/batch\n",
      "Epoch 6/20  Iteration 10678/35720 Training loss: 0.9835 0.2200 sec/batch\n",
      "Epoch 6/20  Iteration 10679/35720 Training loss: 0.9834 0.2327 sec/batch\n",
      "Epoch 6/20  Iteration 10680/35720 Training loss: 0.9834 0.2112 sec/batch\n",
      "Epoch 6/20  Iteration 10681/35720 Training loss: 0.9834 0.2240 sec/batch\n",
      "Epoch 6/20  Iteration 10682/35720 Training loss: 0.9834 0.2096 sec/batch\n",
      "Epoch 6/20  Iteration 10683/35720 Training loss: 0.9834 0.2247 sec/batch\n",
      "Epoch 6/20  Iteration 10684/35720 Training loss: 0.9834 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 10685/35720 Training loss: 0.9834 0.2160 sec/batch\n",
      "Epoch 6/20  Iteration 10686/35720 Training loss: 0.9834 0.2163 sec/batch\n",
      "Epoch 6/20  Iteration 10687/35720 Training loss: 0.9833 0.2084 sec/batch\n",
      "Epoch 6/20  Iteration 10688/35720 Training loss: 0.9833 0.2332 sec/batch\n",
      "Epoch 6/20  Iteration 10689/35720 Training loss: 0.9833 0.2271 sec/batch\n",
      "Epoch 6/20  Iteration 10690/35720 Training loss: 0.9833 0.2164 sec/batch\n",
      "Epoch 6/20  Iteration 10691/35720 Training loss: 0.9833 0.2156 sec/batch\n",
      "Epoch 6/20  Iteration 10692/35720 Training loss: 0.9833 0.2056 sec/batch\n",
      "Epoch 6/20  Iteration 10693/35720 Training loss: 0.9832 0.2129 sec/batch\n",
      "Epoch 6/20  Iteration 10694/35720 Training loss: 0.9832 0.2256 sec/batch\n",
      "Epoch 6/20  Iteration 10695/35720 Training loss: 0.9832 0.2215 sec/batch\n",
      "Epoch 6/20  Iteration 10696/35720 Training loss: 0.9832 0.2324 sec/batch\n",
      "Epoch 6/20  Iteration 10697/35720 Training loss: 0.9832 0.2084 sec/batch\n",
      "Epoch 6/20  Iteration 10698/35720 Training loss: 0.9832 0.2136 sec/batch\n",
      "Epoch 6/20  Iteration 10699/35720 Training loss: 0.9831 0.2141 sec/batch\n",
      "Epoch 6/20  Iteration 10700/35720 Training loss: 0.9832 0.2097 sec/batch\n",
      "Epoch 6/20  Iteration 10701/35720 Training loss: 0.9832 0.2139 sec/batch\n",
      "Epoch 6/20  Iteration 10702/35720 Training loss: 0.9831 0.2203 sec/batch\n",
      "Epoch 6/20  Iteration 10703/35720 Training loss: 0.9831 0.2069 sec/batch\n",
      "Epoch 6/20  Iteration 10704/35720 Training loss: 0.9830 0.2155 sec/batch\n",
      "Epoch 6/20  Iteration 10705/35720 Training loss: 0.9830 0.2126 sec/batch\n",
      "Epoch 6/20  Iteration 10706/35720 Training loss: 0.9830 0.2230 sec/batch\n",
      "Epoch 6/20  Iteration 10707/35720 Training loss: 0.9830 0.2165 sec/batch\n",
      "Epoch 6/20  Iteration 10708/35720 Training loss: 0.9830 0.2099 sec/batch\n",
      "Epoch 6/20  Iteration 10709/35720 Training loss: 0.9829 0.2120 sec/batch\n",
      "Epoch 6/20  Iteration 10710/35720 Training loss: 0.9829 0.2057 sec/batch\n",
      "Epoch 6/20  Iteration 10711/35720 Training loss: 0.9828 0.2132 sec/batch\n",
      "Epoch 6/20  Iteration 10712/35720 Training loss: 0.9828 0.2175 sec/batch\n",
      "Epoch 6/20  Iteration 10713/35720 Training loss: 0.9827 0.2263 sec/batch\n",
      "Epoch 6/20  Iteration 10714/35720 Training loss: 0.9827 0.2116 sec/batch\n",
      "Epoch 6/20  Iteration 10715/35720 Training loss: 0.9826 0.2307 sec/batch\n",
      "Epoch 6/20  Iteration 10716/35720 Training loss: 0.9827 0.2300 sec/batch\n",
      "Epoch 7/20  Iteration 10717/35720 Training loss: 1.0015 0.2081 sec/batch\n",
      "Epoch 7/20  Iteration 10718/35720 Training loss: 0.9931 0.2125 sec/batch\n",
      "Epoch 7/20  Iteration 10719/35720 Training loss: 0.9922 0.2298 sec/batch\n",
      "Epoch 7/20  Iteration 10720/35720 Training loss: 0.9821 0.2201 sec/batch\n",
      "Epoch 7/20  Iteration 10721/35720 Training loss: 0.9900 0.2325 sec/batch\n",
      "Epoch 7/20  Iteration 10722/35720 Training loss: 0.9735 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 10723/35720 Training loss: 0.9730 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 10724/35720 Training loss: 0.9599 0.2183 sec/batch\n",
      "Epoch 7/20  Iteration 10725/35720 Training loss: 0.9567 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 10726/35720 Training loss: 0.9584 0.2220 sec/batch\n",
      "Epoch 7/20  Iteration 10727/35720 Training loss: 0.9599 0.2309 sec/batch\n",
      "Epoch 7/20  Iteration 10728/35720 Training loss: 0.9571 0.2246 sec/batch\n",
      "Epoch 7/20  Iteration 10729/35720 Training loss: 0.9597 0.2232 sec/batch\n",
      "Epoch 7/20  Iteration 10730/35720 Training loss: 0.9670 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 10731/35720 Training loss: 0.9700 0.2079 sec/batch\n",
      "Epoch 7/20  Iteration 10732/35720 Training loss: 0.9703 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 10733/35720 Training loss: 0.9721 0.2118 sec/batch\n",
      "Epoch 7/20  Iteration 10734/35720 Training loss: 0.9703 0.2148 sec/batch\n",
      "Epoch 7/20  Iteration 10735/35720 Training loss: 0.9678 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 10736/35720 Training loss: 0.9679 0.2121 sec/batch\n",
      "Epoch 7/20  Iteration 10737/35720 Training loss: 0.9700 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 10738/35720 Training loss: 0.9666 0.2210 sec/batch\n",
      "Epoch 7/20  Iteration 10739/35720 Training loss: 0.9673 0.2130 sec/batch\n",
      "Epoch 7/20  Iteration 10740/35720 Training loss: 0.9691 0.2151 sec/batch\n",
      "Epoch 7/20  Iteration 10741/35720 Training loss: 0.9715 0.2135 sec/batch\n",
      "Epoch 7/20  Iteration 10742/35720 Training loss: 0.9705 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 10743/35720 Training loss: 0.9738 0.2066 sec/batch\n",
      "Epoch 7/20  Iteration 10744/35720 Training loss: 0.9745 0.2193 sec/batch\n",
      "Epoch 7/20  Iteration 10745/35720 Training loss: 0.9734 0.2200 sec/batch\n",
      "Epoch 7/20  Iteration 10746/35720 Training loss: 0.9735 0.2145 sec/batch\n",
      "Epoch 7/20  Iteration 10747/35720 Training loss: 0.9781 0.2208 sec/batch\n",
      "Epoch 7/20  Iteration 10748/35720 Training loss: 0.9763 0.2089 sec/batch\n",
      "Epoch 7/20  Iteration 10749/35720 Training loss: 0.9786 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 10750/35720 Training loss: 0.9808 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 10751/35720 Training loss: 0.9838 0.2172 sec/batch\n",
      "Epoch 7/20  Iteration 10752/35720 Training loss: 0.9841 0.2066 sec/batch\n",
      "Epoch 7/20  Iteration 10753/35720 Training loss: 0.9843 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 10754/35720 Training loss: 0.9841 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 10755/35720 Training loss: 0.9819 0.2295 sec/batch\n",
      "Epoch 7/20  Iteration 10756/35720 Training loss: 0.9828 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 10757/35720 Training loss: 0.9822 0.2164 sec/batch\n",
      "Epoch 7/20  Iteration 10758/35720 Training loss: 0.9802 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 10759/35720 Training loss: 0.9774 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 10760/35720 Training loss: 0.9767 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 10761/35720 Training loss: 0.9759 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 10762/35720 Training loss: 0.9746 0.2057 sec/batch\n",
      "Epoch 7/20  Iteration 10763/35720 Training loss: 0.9742 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 10764/35720 Training loss: 0.9733 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 10765/35720 Training loss: 0.9736 0.2159 sec/batch\n",
      "Epoch 7/20  Iteration 10766/35720 Training loss: 0.9720 0.2265 sec/batch\n",
      "Epoch 7/20  Iteration 10767/35720 Training loss: 0.9727 0.2238 sec/batch\n",
      "Epoch 7/20  Iteration 10768/35720 Training loss: 0.9721 0.2381 sec/batch\n",
      "Epoch 7/20  Iteration 10769/35720 Training loss: 0.9717 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 10770/35720 Training loss: 0.9698 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 10771/35720 Training loss: 0.9688 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 10772/35720 Training loss: 0.9683 0.2241 sec/batch\n",
      "Epoch 7/20  Iteration 10773/35720 Training loss: 0.9686 0.2207 sec/batch\n",
      "Epoch 7/20  Iteration 10774/35720 Training loss: 0.9680 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 10775/35720 Training loss: 0.9668 0.2262 sec/batch\n",
      "Epoch 7/20  Iteration 10776/35720 Training loss: 0.9656 0.2132 sec/batch\n",
      "Epoch 7/20  Iteration 10777/35720 Training loss: 0.9643 0.2308 sec/batch\n",
      "Epoch 7/20  Iteration 10778/35720 Training loss: 0.9619 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 10779/35720 Training loss: 0.9622 0.2308 sec/batch\n",
      "Epoch 7/20  Iteration 10780/35720 Training loss: 0.9621 0.2061 sec/batch\n",
      "Epoch 7/20  Iteration 10781/35720 Training loss: 0.9628 0.2249 sec/batch\n",
      "Epoch 7/20  Iteration 10782/35720 Training loss: 0.9629 0.2280 sec/batch\n",
      "Epoch 7/20  Iteration 10783/35720 Training loss: 0.9625 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 10784/35720 Training loss: 0.9616 0.2285 sec/batch\n",
      "Epoch 7/20  Iteration 10785/35720 Training loss: 0.9626 0.2131 sec/batch\n",
      "Epoch 7/20  Iteration 10786/35720 Training loss: 0.9622 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 10787/35720 Training loss: 0.9626 0.2083 sec/batch\n",
      "Epoch 7/20  Iteration 10788/35720 Training loss: 0.9630 0.2170 sec/batch\n",
      "Epoch 7/20  Iteration 10789/35720 Training loss: 0.9632 0.2476 sec/batch\n",
      "Epoch 7/20  Iteration 10790/35720 Training loss: 0.9633 0.2247 sec/batch\n",
      "Epoch 7/20  Iteration 10791/35720 Training loss: 0.9621 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 10792/35720 Training loss: 0.9617 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 10793/35720 Training loss: 0.9606 0.2304 sec/batch\n",
      "Epoch 7/20  Iteration 10794/35720 Training loss: 0.9614 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 10795/35720 Training loss: 0.9613 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 10796/35720 Training loss: 0.9634 0.2327 sec/batch\n",
      "Epoch 7/20  Iteration 10797/35720 Training loss: 0.9638 0.2096 sec/batch\n",
      "Epoch 7/20  Iteration 10798/35720 Training loss: 0.9638 0.2086 sec/batch\n",
      "Epoch 7/20  Iteration 10799/35720 Training loss: 0.9637 0.2157 sec/batch\n",
      "Epoch 7/20  Iteration 10800/35720 Training loss: 0.9640 0.2176 sec/batch\n",
      "Validation loss: 1.31895 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 10801/35720 Training loss: 0.9662 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 10802/35720 Training loss: 0.9663 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 10803/35720 Training loss: 0.9663 0.2131 sec/batch\n",
      "Epoch 7/20  Iteration 10804/35720 Training loss: 0.9660 0.2176 sec/batch\n",
      "Epoch 7/20  Iteration 10805/35720 Training loss: 0.9648 0.2219 sec/batch\n",
      "Epoch 7/20  Iteration 10806/35720 Training loss: 0.9639 0.2291 sec/batch\n",
      "Epoch 7/20  Iteration 10807/35720 Training loss: 0.9638 0.2082 sec/batch\n",
      "Epoch 7/20  Iteration 10808/35720 Training loss: 0.9630 0.2199 sec/batch\n",
      "Epoch 7/20  Iteration 10809/35720 Training loss: 0.9627 0.2179 sec/batch\n",
      "Epoch 7/20  Iteration 10810/35720 Training loss: 0.9625 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 10811/35720 Training loss: 0.9624 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 10812/35720 Training loss: 0.9616 0.2176 sec/batch\n",
      "Epoch 7/20  Iteration 10813/35720 Training loss: 0.9617 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 10814/35720 Training loss: 0.9617 0.2293 sec/batch\n",
      "Epoch 7/20  Iteration 10815/35720 Training loss: 0.9613 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 10816/35720 Training loss: 0.9610 0.2225 sec/batch\n",
      "Epoch 7/20  Iteration 10817/35720 Training loss: 0.9608 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 10818/35720 Training loss: 0.9609 0.2055 sec/batch\n",
      "Epoch 7/20  Iteration 10819/35720 Training loss: 0.9609 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 10820/35720 Training loss: 0.9606 0.2177 sec/batch\n",
      "Epoch 7/20  Iteration 10821/35720 Training loss: 0.9607 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 10822/35720 Training loss: 0.9605 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 10823/35720 Training loss: 0.9606 0.2154 sec/batch\n",
      "Epoch 7/20  Iteration 10824/35720 Training loss: 0.9609 0.2080 sec/batch\n",
      "Epoch 7/20  Iteration 10825/35720 Training loss: 0.9614 0.2216 sec/batch\n",
      "Epoch 7/20  Iteration 10826/35720 Training loss: 0.9613 0.2338 sec/batch\n",
      "Epoch 7/20  Iteration 10827/35720 Training loss: 0.9616 0.2342 sec/batch\n",
      "Epoch 7/20  Iteration 10828/35720 Training loss: 0.9624 0.2130 sec/batch\n",
      "Epoch 7/20  Iteration 10829/35720 Training loss: 0.9625 0.2138 sec/batch\n",
      "Epoch 7/20  Iteration 10830/35720 Training loss: 0.9628 0.2127 sec/batch\n",
      "Epoch 7/20  Iteration 10831/35720 Training loss: 0.9627 0.2260 sec/batch\n",
      "Epoch 7/20  Iteration 10832/35720 Training loss: 0.9629 0.2142 sec/batch\n",
      "Epoch 7/20  Iteration 10833/35720 Training loss: 0.9627 0.2070 sec/batch\n",
      "Epoch 7/20  Iteration 10834/35720 Training loss: 0.9632 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 10835/35720 Training loss: 0.9634 0.2083 sec/batch\n",
      "Epoch 7/20  Iteration 10836/35720 Training loss: 0.9643 0.2248 sec/batch\n",
      "Epoch 7/20  Iteration 10837/35720 Training loss: 0.9650 0.2315 sec/batch\n",
      "Epoch 7/20  Iteration 10838/35720 Training loss: 0.9649 0.2221 sec/batch\n",
      "Epoch 7/20  Iteration 10839/35720 Training loss: 0.9653 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 10840/35720 Training loss: 0.9659 0.2122 sec/batch\n",
      "Epoch 7/20  Iteration 10841/35720 Training loss: 0.9654 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 10842/35720 Training loss: 0.9653 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 10843/35720 Training loss: 0.9654 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 10844/35720 Training loss: 0.9654 0.2204 sec/batch\n",
      "Epoch 7/20  Iteration 10845/35720 Training loss: 0.9649 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 10846/35720 Training loss: 0.9648 0.2135 sec/batch\n",
      "Epoch 7/20  Iteration 10847/35720 Training loss: 0.9645 0.2201 sec/batch\n",
      "Epoch 7/20  Iteration 10848/35720 Training loss: 0.9641 0.2303 sec/batch\n",
      "Epoch 7/20  Iteration 10849/35720 Training loss: 0.9640 0.2282 sec/batch\n",
      "Epoch 7/20  Iteration 10850/35720 Training loss: 0.9641 0.2234 sec/batch\n",
      "Epoch 7/20  Iteration 10851/35720 Training loss: 0.9641 0.2317 sec/batch\n",
      "Epoch 7/20  Iteration 10852/35720 Training loss: 0.9640 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 10853/35720 Training loss: 0.9649 0.2297 sec/batch\n",
      "Epoch 7/20  Iteration 10854/35720 Training loss: 0.9652 0.2239 sec/batch\n",
      "Epoch 7/20  Iteration 10855/35720 Training loss: 0.9653 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 10856/35720 Training loss: 0.9655 0.2075 sec/batch\n",
      "Epoch 7/20  Iteration 10857/35720 Training loss: 0.9649 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 10858/35720 Training loss: 0.9643 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 10859/35720 Training loss: 0.9637 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 10860/35720 Training loss: 0.9630 0.2190 sec/batch\n",
      "Epoch 7/20  Iteration 10861/35720 Training loss: 0.9629 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 10862/35720 Training loss: 0.9632 0.2176 sec/batch\n",
      "Epoch 7/20  Iteration 10863/35720 Training loss: 0.9629 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 10864/35720 Training loss: 0.9629 0.2212 sec/batch\n",
      "Epoch 7/20  Iteration 10865/35720 Training loss: 0.9628 0.2165 sec/batch\n",
      "Epoch 7/20  Iteration 10866/35720 Training loss: 0.9622 0.2185 sec/batch\n",
      "Epoch 7/20  Iteration 10867/35720 Training loss: 0.9622 0.2073 sec/batch\n",
      "Epoch 7/20  Iteration 10868/35720 Training loss: 0.9623 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 10869/35720 Training loss: 0.9623 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 10870/35720 Training loss: 0.9627 0.2268 sec/batch\n",
      "Epoch 7/20  Iteration 10871/35720 Training loss: 0.9630 0.2170 sec/batch\n",
      "Epoch 7/20  Iteration 10872/35720 Training loss: 0.9632 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 10873/35720 Training loss: 0.9633 0.2209 sec/batch\n",
      "Epoch 7/20  Iteration 10874/35720 Training loss: 0.9635 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 10875/35720 Training loss: 0.9632 0.2226 sec/batch\n",
      "Epoch 7/20  Iteration 10876/35720 Training loss: 0.9636 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 10877/35720 Training loss: 0.9633 0.2230 sec/batch\n",
      "Epoch 7/20  Iteration 10878/35720 Training loss: 0.9633 0.2139 sec/batch\n",
      "Epoch 7/20  Iteration 10879/35720 Training loss: 0.9635 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 10880/35720 Training loss: 0.9637 0.2303 sec/batch\n",
      "Epoch 7/20  Iteration 10881/35720 Training loss: 0.9640 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 10882/35720 Training loss: 0.9641 0.2297 sec/batch\n",
      "Epoch 7/20  Iteration 10883/35720 Training loss: 0.9643 0.2246 sec/batch\n",
      "Epoch 7/20  Iteration 10884/35720 Training loss: 0.9646 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 10885/35720 Training loss: 0.9650 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 10886/35720 Training loss: 0.9652 0.2229 sec/batch\n",
      "Epoch 7/20  Iteration 10887/35720 Training loss: 0.9659 0.2135 sec/batch\n",
      "Epoch 7/20  Iteration 10888/35720 Training loss: 0.9663 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 10889/35720 Training loss: 0.9666 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 10890/35720 Training loss: 0.9670 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 10891/35720 Training loss: 0.9673 0.2143 sec/batch\n",
      "Epoch 7/20  Iteration 10892/35720 Training loss: 0.9671 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 10893/35720 Training loss: 0.9672 0.2177 sec/batch\n",
      "Epoch 7/20  Iteration 10894/35720 Training loss: 0.9669 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 10895/35720 Training loss: 0.9667 0.2153 sec/batch\n",
      "Epoch 7/20  Iteration 10896/35720 Training loss: 0.9664 0.2279 sec/batch\n",
      "Epoch 7/20  Iteration 10897/35720 Training loss: 0.9666 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 10898/35720 Training loss: 0.9668 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 10899/35720 Training loss: 0.9669 0.2248 sec/batch\n",
      "Epoch 7/20  Iteration 10900/35720 Training loss: 0.9670 0.2082 sec/batch\n",
      "Epoch 7/20  Iteration 10901/35720 Training loss: 0.9670 0.2173 sec/batch\n",
      "Epoch 7/20  Iteration 10902/35720 Training loss: 0.9669 0.2299 sec/batch\n",
      "Epoch 7/20  Iteration 10903/35720 Training loss: 0.9667 0.2224 sec/batch\n",
      "Epoch 7/20  Iteration 10904/35720 Training loss: 0.9667 0.2236 sec/batch\n",
      "Epoch 7/20  Iteration 10905/35720 Training loss: 0.9670 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 10906/35720 Training loss: 0.9670 0.2079 sec/batch\n",
      "Epoch 7/20  Iteration 10907/35720 Training loss: 0.9673 0.2140 sec/batch\n",
      "Epoch 7/20  Iteration 10908/35720 Training loss: 0.9679 0.2256 sec/batch\n",
      "Epoch 7/20  Iteration 10909/35720 Training loss: 0.9682 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 10910/35720 Training loss: 0.9685 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 10911/35720 Training loss: 0.9685 0.2241 sec/batch\n",
      "Epoch 7/20  Iteration 10912/35720 Training loss: 0.9688 0.2140 sec/batch\n",
      "Epoch 7/20  Iteration 10913/35720 Training loss: 0.9684 0.2269 sec/batch\n",
      "Epoch 7/20  Iteration 10914/35720 Training loss: 0.9684 0.2310 sec/batch\n",
      "Epoch 7/20  Iteration 10915/35720 Training loss: 0.9686 0.2266 sec/batch\n",
      "Epoch 7/20  Iteration 10916/35720 Training loss: 0.9689 0.2125 sec/batch\n",
      "Epoch 7/20  Iteration 10917/35720 Training loss: 0.9688 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 10918/35720 Training loss: 0.9686 0.2141 sec/batch\n",
      "Epoch 7/20  Iteration 10919/35720 Training loss: 0.9686 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 10920/35720 Training loss: 0.9684 0.2093 sec/batch\n",
      "Epoch 7/20  Iteration 10921/35720 Training loss: 0.9684 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 10922/35720 Training loss: 0.9682 0.2071 sec/batch\n",
      "Epoch 7/20  Iteration 10923/35720 Training loss: 0.9687 0.2093 sec/batch\n",
      "Epoch 7/20  Iteration 10924/35720 Training loss: 0.9691 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 10925/35720 Training loss: 0.9694 0.2124 sec/batch\n",
      "Epoch 7/20  Iteration 10926/35720 Training loss: 0.9694 0.2212 sec/batch\n",
      "Epoch 7/20  Iteration 10927/35720 Training loss: 0.9696 0.2245 sec/batch\n",
      "Epoch 7/20  Iteration 10928/35720 Training loss: 0.9696 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 10929/35720 Training loss: 0.9695 0.2139 sec/batch\n",
      "Epoch 7/20  Iteration 10930/35720 Training loss: 0.9696 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 10931/35720 Training loss: 0.9696 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 10932/35720 Training loss: 0.9695 0.2273 sec/batch\n",
      "Epoch 7/20  Iteration 10933/35720 Training loss: 0.9692 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 10934/35720 Training loss: 0.9690 0.2089 sec/batch\n",
      "Epoch 7/20  Iteration 10935/35720 Training loss: 0.9690 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 10936/35720 Training loss: 0.9691 0.2308 sec/batch\n",
      "Epoch 7/20  Iteration 10937/35720 Training loss: 0.9692 0.2284 sec/batch\n",
      "Epoch 7/20  Iteration 10938/35720 Training loss: 0.9692 0.2212 sec/batch\n",
      "Epoch 7/20  Iteration 10939/35720 Training loss: 0.9697 0.2230 sec/batch\n",
      "Epoch 7/20  Iteration 10940/35720 Training loss: 0.9700 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 10941/35720 Training loss: 0.9702 0.2311 sec/batch\n",
      "Epoch 7/20  Iteration 10942/35720 Training loss: 0.9701 0.2355 sec/batch\n",
      "Epoch 7/20  Iteration 10943/35720 Training loss: 0.9700 0.2335 sec/batch\n",
      "Epoch 7/20  Iteration 10944/35720 Training loss: 0.9697 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 10945/35720 Training loss: 0.9694 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 10946/35720 Training loss: 0.9696 0.2317 sec/batch\n",
      "Epoch 7/20  Iteration 10947/35720 Training loss: 0.9699 0.2287 sec/batch\n",
      "Epoch 7/20  Iteration 10948/35720 Training loss: 0.9699 0.2364 sec/batch\n",
      "Epoch 7/20  Iteration 10949/35720 Training loss: 0.9699 0.2070 sec/batch\n",
      "Epoch 7/20  Iteration 10950/35720 Training loss: 0.9700 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 10951/35720 Training loss: 0.9700 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 10952/35720 Training loss: 0.9700 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 10953/35720 Training loss: 0.9702 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 10954/35720 Training loss: 0.9701 0.2143 sec/batch\n",
      "Epoch 7/20  Iteration 10955/35720 Training loss: 0.9699 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 10956/35720 Training loss: 0.9699 0.2096 sec/batch\n",
      "Epoch 7/20  Iteration 10957/35720 Training loss: 0.9697 0.2183 sec/batch\n",
      "Epoch 7/20  Iteration 10958/35720 Training loss: 0.9697 0.2263 sec/batch\n",
      "Epoch 7/20  Iteration 10959/35720 Training loss: 0.9697 0.2230 sec/batch\n",
      "Epoch 7/20  Iteration 10960/35720 Training loss: 0.9698 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 10961/35720 Training loss: 0.9695 0.2279 sec/batch\n",
      "Epoch 7/20  Iteration 10962/35720 Training loss: 0.9695 0.2103 sec/batch\n",
      "Epoch 7/20  Iteration 10963/35720 Training loss: 0.9695 0.2133 sec/batch\n",
      "Epoch 7/20  Iteration 10964/35720 Training loss: 0.9694 0.2118 sec/batch\n",
      "Epoch 7/20  Iteration 10965/35720 Training loss: 0.9692 0.2197 sec/batch\n",
      "Epoch 7/20  Iteration 10966/35720 Training loss: 0.9690 0.2157 sec/batch\n",
      "Epoch 7/20  Iteration 10967/35720 Training loss: 0.9690 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 10968/35720 Training loss: 0.9688 0.2161 sec/batch\n",
      "Epoch 7/20  Iteration 10969/35720 Training loss: 0.9686 0.2310 sec/batch\n",
      "Epoch 7/20  Iteration 10970/35720 Training loss: 0.9688 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 10971/35720 Training loss: 0.9691 0.2178 sec/batch\n",
      "Epoch 7/20  Iteration 10972/35720 Training loss: 0.9692 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 10973/35720 Training loss: 0.9692 0.2156 sec/batch\n",
      "Epoch 7/20  Iteration 10974/35720 Training loss: 0.9692 0.2161 sec/batch\n",
      "Epoch 7/20  Iteration 10975/35720 Training loss: 0.9696 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 10976/35720 Training loss: 0.9695 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 10977/35720 Training loss: 0.9694 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 10978/35720 Training loss: 0.9693 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 10979/35720 Training loss: 0.9692 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 10980/35720 Training loss: 0.9692 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 10981/35720 Training loss: 0.9692 0.2089 sec/batch\n",
      "Epoch 7/20  Iteration 10982/35720 Training loss: 0.9694 0.2213 sec/batch\n",
      "Epoch 7/20  Iteration 10983/35720 Training loss: 0.9692 0.2141 sec/batch\n",
      "Epoch 7/20  Iteration 10984/35720 Training loss: 0.9692 0.2125 sec/batch\n",
      "Epoch 7/20  Iteration 10985/35720 Training loss: 0.9689 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 10986/35720 Training loss: 0.9683 0.2190 sec/batch\n",
      "Epoch 7/20  Iteration 10987/35720 Training loss: 0.9679 0.2154 sec/batch\n",
      "Epoch 7/20  Iteration 10988/35720 Training loss: 0.9679 0.2151 sec/batch\n",
      "Epoch 7/20  Iteration 10989/35720 Training loss: 0.9679 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 10990/35720 Training loss: 0.9679 0.2206 sec/batch\n",
      "Epoch 7/20  Iteration 10991/35720 Training loss: 0.9676 0.2246 sec/batch\n",
      "Epoch 7/20  Iteration 10992/35720 Training loss: 0.9675 0.2208 sec/batch\n",
      "Epoch 7/20  Iteration 10993/35720 Training loss: 0.9672 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 10994/35720 Training loss: 0.9670 0.2159 sec/batch\n",
      "Epoch 7/20  Iteration 10995/35720 Training loss: 0.9668 0.2217 sec/batch\n",
      "Epoch 7/20  Iteration 10996/35720 Training loss: 0.9667 0.2312 sec/batch\n",
      "Epoch 7/20  Iteration 10997/35720 Training loss: 0.9667 0.2086 sec/batch\n",
      "Epoch 7/20  Iteration 10998/35720 Training loss: 0.9664 0.2253 sec/batch\n",
      "Epoch 7/20  Iteration 10999/35720 Training loss: 0.9660 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 11000/35720 Training loss: 0.9657 0.2319 sec/batch\n",
      "Validation loss: 1.31944 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 11001/35720 Training loss: 0.9667 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 11002/35720 Training loss: 0.9667 0.2615 sec/batch\n",
      "Epoch 7/20  Iteration 11003/35720 Training loss: 0.9663 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 11004/35720 Training loss: 0.9662 0.2137 sec/batch\n",
      "Epoch 7/20  Iteration 11005/35720 Training loss: 0.9663 0.2245 sec/batch\n",
      "Epoch 7/20  Iteration 11006/35720 Training loss: 0.9663 0.2307 sec/batch\n",
      "Epoch 7/20  Iteration 11007/35720 Training loss: 0.9662 0.2261 sec/batch\n",
      "Epoch 7/20  Iteration 11008/35720 Training loss: 0.9662 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 11009/35720 Training loss: 0.9661 0.2234 sec/batch\n",
      "Epoch 7/20  Iteration 11010/35720 Training loss: 0.9661 0.2209 sec/batch\n",
      "Epoch 7/20  Iteration 11011/35720 Training loss: 0.9665 0.2178 sec/batch\n",
      "Epoch 7/20  Iteration 11012/35720 Training loss: 0.9665 0.2263 sec/batch\n",
      "Epoch 7/20  Iteration 11013/35720 Training loss: 0.9665 0.2286 sec/batch\n",
      "Epoch 7/20  Iteration 11014/35720 Training loss: 0.9667 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 11015/35720 Training loss: 0.9665 0.2206 sec/batch\n",
      "Epoch 7/20  Iteration 11016/35720 Training loss: 0.9665 0.2226 sec/batch\n",
      "Epoch 7/20  Iteration 11017/35720 Training loss: 0.9665 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 11018/35720 Training loss: 0.9662 0.2298 sec/batch\n",
      "Epoch 7/20  Iteration 11019/35720 Training loss: 0.9662 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 11020/35720 Training loss: 0.9662 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 11021/35720 Training loss: 0.9660 0.2139 sec/batch\n",
      "Epoch 7/20  Iteration 11022/35720 Training loss: 0.9659 0.2201 sec/batch\n",
      "Epoch 7/20  Iteration 11023/35720 Training loss: 0.9660 0.2085 sec/batch\n",
      "Epoch 7/20  Iteration 11024/35720 Training loss: 0.9661 0.2074 sec/batch\n",
      "Epoch 7/20  Iteration 11025/35720 Training loss: 0.9659 0.2063 sec/batch\n",
      "Epoch 7/20  Iteration 11026/35720 Training loss: 0.9657 0.2139 sec/batch\n",
      "Epoch 7/20  Iteration 11027/35720 Training loss: 0.9656 0.2147 sec/batch\n",
      "Epoch 7/20  Iteration 11028/35720 Training loss: 0.9655 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 11029/35720 Training loss: 0.9654 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 11030/35720 Training loss: 0.9652 0.2336 sec/batch\n",
      "Epoch 7/20  Iteration 11031/35720 Training loss: 0.9652 0.2069 sec/batch\n",
      "Epoch 7/20  Iteration 11032/35720 Training loss: 0.9651 0.2103 sec/batch\n",
      "Epoch 7/20  Iteration 11033/35720 Training loss: 0.9649 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 11034/35720 Training loss: 0.9651 0.2304 sec/batch\n",
      "Epoch 7/20  Iteration 11035/35720 Training loss: 0.9651 0.2234 sec/batch\n",
      "Epoch 7/20  Iteration 11036/35720 Training loss: 0.9650 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 11037/35720 Training loss: 0.9651 0.2131 sec/batch\n",
      "Epoch 7/20  Iteration 11038/35720 Training loss: 0.9651 0.2291 sec/batch\n",
      "Epoch 7/20  Iteration 11039/35720 Training loss: 0.9653 0.2183 sec/batch\n",
      "Epoch 7/20  Iteration 11040/35720 Training loss: 0.9654 0.2058 sec/batch\n",
      "Epoch 7/20  Iteration 11041/35720 Training loss: 0.9652 0.2171 sec/batch\n",
      "Epoch 7/20  Iteration 11042/35720 Training loss: 0.9654 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 11043/35720 Training loss: 0.9655 0.2307 sec/batch\n",
      "Epoch 7/20  Iteration 11044/35720 Training loss: 0.9656 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 11045/35720 Training loss: 0.9656 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 11046/35720 Training loss: 0.9656 0.2121 sec/batch\n",
      "Epoch 7/20  Iteration 11047/35720 Training loss: 0.9657 0.2124 sec/batch\n",
      "Epoch 7/20  Iteration 11048/35720 Training loss: 0.9659 0.2136 sec/batch\n",
      "Epoch 7/20  Iteration 11049/35720 Training loss: 0.9658 0.2181 sec/batch\n",
      "Epoch 7/20  Iteration 11050/35720 Training loss: 0.9658 0.2334 sec/batch\n",
      "Epoch 7/20  Iteration 11051/35720 Training loss: 0.9658 0.2287 sec/batch\n",
      "Epoch 7/20  Iteration 11052/35720 Training loss: 0.9656 0.2114 sec/batch\n",
      "Epoch 7/20  Iteration 11053/35720 Training loss: 0.9656 0.2213 sec/batch\n",
      "Epoch 7/20  Iteration 11054/35720 Training loss: 0.9654 0.2220 sec/batch\n",
      "Epoch 7/20  Iteration 11055/35720 Training loss: 0.9654 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 11056/35720 Training loss: 0.9654 0.2265 sec/batch\n",
      "Epoch 7/20  Iteration 11057/35720 Training loss: 0.9653 0.2165 sec/batch\n",
      "Epoch 7/20  Iteration 11058/35720 Training loss: 0.9653 0.2069 sec/batch\n",
      "Epoch 7/20  Iteration 11059/35720 Training loss: 0.9654 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 11060/35720 Training loss: 0.9653 0.2217 sec/batch\n",
      "Epoch 7/20  Iteration 11061/35720 Training loss: 0.9650 0.2179 sec/batch\n",
      "Epoch 7/20  Iteration 11062/35720 Training loss: 0.9653 0.2172 sec/batch\n",
      "Epoch 7/20  Iteration 11063/35720 Training loss: 0.9653 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11064/35720 Training loss: 0.9655 0.2130 sec/batch\n",
      "Epoch 7/20  Iteration 11065/35720 Training loss: 0.9655 0.2287 sec/batch\n",
      "Epoch 7/20  Iteration 11066/35720 Training loss: 0.9656 0.2140 sec/batch\n",
      "Epoch 7/20  Iteration 11067/35720 Training loss: 0.9657 0.2236 sec/batch\n",
      "Epoch 7/20  Iteration 11068/35720 Training loss: 0.9656 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 11069/35720 Training loss: 0.9656 0.2058 sec/batch\n",
      "Epoch 7/20  Iteration 11070/35720 Training loss: 0.9656 0.2213 sec/batch\n",
      "Epoch 7/20  Iteration 11071/35720 Training loss: 0.9657 0.2208 sec/batch\n",
      "Epoch 7/20  Iteration 11072/35720 Training loss: 0.9656 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 11073/35720 Training loss: 0.9657 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 11074/35720 Training loss: 0.9657 0.2085 sec/batch\n",
      "Epoch 7/20  Iteration 11075/35720 Training loss: 0.9656 0.2121 sec/batch\n",
      "Epoch 7/20  Iteration 11076/35720 Training loss: 0.9655 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11077/35720 Training loss: 0.9654 0.2215 sec/batch\n",
      "Epoch 7/20  Iteration 11078/35720 Training loss: 0.9656 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 11079/35720 Training loss: 0.9655 0.2165 sec/batch\n",
      "Epoch 7/20  Iteration 11080/35720 Training loss: 0.9655 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 11081/35720 Training loss: 0.9655 0.2142 sec/batch\n",
      "Epoch 7/20  Iteration 11082/35720 Training loss: 0.9654 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 11083/35720 Training loss: 0.9654 0.2249 sec/batch\n",
      "Epoch 7/20  Iteration 11084/35720 Training loss: 0.9651 0.2237 sec/batch\n",
      "Epoch 7/20  Iteration 11085/35720 Training loss: 0.9650 0.2212 sec/batch\n",
      "Epoch 7/20  Iteration 11086/35720 Training loss: 0.9649 0.2061 sec/batch\n",
      "Epoch 7/20  Iteration 11087/35720 Training loss: 0.9647 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 11088/35720 Training loss: 0.9646 0.2188 sec/batch\n",
      "Epoch 7/20  Iteration 11089/35720 Training loss: 0.9646 0.2279 sec/batch\n",
      "Epoch 7/20  Iteration 11090/35720 Training loss: 0.9645 0.2259 sec/batch\n",
      "Epoch 7/20  Iteration 11091/35720 Training loss: 0.9645 0.2101 sec/batch\n",
      "Epoch 7/20  Iteration 11092/35720 Training loss: 0.9645 0.2215 sec/batch\n",
      "Epoch 7/20  Iteration 11093/35720 Training loss: 0.9646 0.2165 sec/batch\n",
      "Epoch 7/20  Iteration 11094/35720 Training loss: 0.9645 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 11095/35720 Training loss: 0.9643 0.2131 sec/batch\n",
      "Epoch 7/20  Iteration 11096/35720 Training loss: 0.9641 0.2137 sec/batch\n",
      "Epoch 7/20  Iteration 11097/35720 Training loss: 0.9640 0.2205 sec/batch\n",
      "Epoch 7/20  Iteration 11098/35720 Training loss: 0.9639 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 11099/35720 Training loss: 0.9639 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 11100/35720 Training loss: 0.9639 0.2136 sec/batch\n",
      "Epoch 7/20  Iteration 11101/35720 Training loss: 0.9638 0.2176 sec/batch\n",
      "Epoch 7/20  Iteration 11102/35720 Training loss: 0.9638 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 11103/35720 Training loss: 0.9639 0.2252 sec/batch\n",
      "Epoch 7/20  Iteration 11104/35720 Training loss: 0.9640 0.2293 sec/batch\n",
      "Epoch 7/20  Iteration 11105/35720 Training loss: 0.9641 0.2141 sec/batch\n",
      "Epoch 7/20  Iteration 11106/35720 Training loss: 0.9640 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 11107/35720 Training loss: 0.9640 0.2205 sec/batch\n",
      "Epoch 7/20  Iteration 11108/35720 Training loss: 0.9637 0.2054 sec/batch\n",
      "Epoch 7/20  Iteration 11109/35720 Training loss: 0.9638 0.2144 sec/batch\n",
      "Epoch 7/20  Iteration 11110/35720 Training loss: 0.9636 0.2252 sec/batch\n",
      "Epoch 7/20  Iteration 11111/35720 Training loss: 0.9636 0.2255 sec/batch\n",
      "Epoch 7/20  Iteration 11112/35720 Training loss: 0.9634 0.2112 sec/batch\n",
      "Epoch 7/20  Iteration 11113/35720 Training loss: 0.9634 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 11114/35720 Training loss: 0.9631 0.2096 sec/batch\n",
      "Epoch 7/20  Iteration 11115/35720 Training loss: 0.9629 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 11116/35720 Training loss: 0.9629 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 11117/35720 Training loss: 0.9628 0.2209 sec/batch\n",
      "Epoch 7/20  Iteration 11118/35720 Training loss: 0.9630 0.2138 sec/batch\n",
      "Epoch 7/20  Iteration 11119/35720 Training loss: 0.9631 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 11120/35720 Training loss: 0.9630 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 11121/35720 Training loss: 0.9629 0.2343 sec/batch\n",
      "Epoch 7/20  Iteration 11122/35720 Training loss: 0.9628 0.2232 sec/batch\n",
      "Epoch 7/20  Iteration 11123/35720 Training loss: 0.9627 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 11124/35720 Training loss: 0.9627 0.2216 sec/batch\n",
      "Epoch 7/20  Iteration 11125/35720 Training loss: 0.9625 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 11126/35720 Training loss: 0.9623 0.2210 sec/batch\n",
      "Epoch 7/20  Iteration 11127/35720 Training loss: 0.9624 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11128/35720 Training loss: 0.9622 0.2205 sec/batch\n",
      "Epoch 7/20  Iteration 11129/35720 Training loss: 0.9619 0.2179 sec/batch\n",
      "Epoch 7/20  Iteration 11130/35720 Training loss: 0.9619 0.2071 sec/batch\n",
      "Epoch 7/20  Iteration 11131/35720 Training loss: 0.9618 0.2096 sec/batch\n",
      "Epoch 7/20  Iteration 11132/35720 Training loss: 0.9618 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 11133/35720 Training loss: 0.9616 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 11134/35720 Training loss: 0.9615 0.2220 sec/batch\n",
      "Epoch 7/20  Iteration 11135/35720 Training loss: 0.9613 0.2210 sec/batch\n",
      "Epoch 7/20  Iteration 11136/35720 Training loss: 0.9613 0.2083 sec/batch\n",
      "Epoch 7/20  Iteration 11137/35720 Training loss: 0.9614 0.2198 sec/batch\n",
      "Epoch 7/20  Iteration 11138/35720 Training loss: 0.9613 0.2140 sec/batch\n",
      "Epoch 7/20  Iteration 11139/35720 Training loss: 0.9614 0.2166 sec/batch\n",
      "Epoch 7/20  Iteration 11140/35720 Training loss: 0.9615 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 11141/35720 Training loss: 0.9615 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 11142/35720 Training loss: 0.9612 0.2238 sec/batch\n",
      "Epoch 7/20  Iteration 11143/35720 Training loss: 0.9611 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 11144/35720 Training loss: 0.9612 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11145/35720 Training loss: 0.9612 0.2103 sec/batch\n",
      "Epoch 7/20  Iteration 11146/35720 Training loss: 0.9613 0.2156 sec/batch\n",
      "Epoch 7/20  Iteration 11147/35720 Training loss: 0.9615 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 11148/35720 Training loss: 0.9615 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 11149/35720 Training loss: 0.9617 0.2185 sec/batch\n",
      "Epoch 7/20  Iteration 11150/35720 Training loss: 0.9618 0.2237 sec/batch\n",
      "Epoch 7/20  Iteration 11151/35720 Training loss: 0.9619 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 11152/35720 Training loss: 0.9618 0.2130 sec/batch\n",
      "Epoch 7/20  Iteration 11153/35720 Training loss: 0.9619 0.2362 sec/batch\n",
      "Epoch 7/20  Iteration 11154/35720 Training loss: 0.9620 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 11155/35720 Training loss: 0.9622 0.2098 sec/batch\n",
      "Epoch 7/20  Iteration 11156/35720 Training loss: 0.9622 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 11157/35720 Training loss: 0.9624 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 11158/35720 Training loss: 0.9626 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 11159/35720 Training loss: 0.9626 0.2295 sec/batch\n",
      "Epoch 7/20  Iteration 11160/35720 Training loss: 0.9626 0.2238 sec/batch\n",
      "Epoch 7/20  Iteration 11161/35720 Training loss: 0.9626 0.2224 sec/batch\n",
      "Epoch 7/20  Iteration 11162/35720 Training loss: 0.9627 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 11163/35720 Training loss: 0.9630 0.2201 sec/batch\n",
      "Epoch 7/20  Iteration 11164/35720 Training loss: 0.9631 0.2101 sec/batch\n",
      "Epoch 7/20  Iteration 11165/35720 Training loss: 0.9633 0.2273 sec/batch\n",
      "Epoch 7/20  Iteration 11166/35720 Training loss: 0.9632 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 11167/35720 Training loss: 0.9631 0.2292 sec/batch\n",
      "Epoch 7/20  Iteration 11168/35720 Training loss: 0.9630 0.2239 sec/batch\n",
      "Epoch 7/20  Iteration 11169/35720 Training loss: 0.9630 0.2119 sec/batch\n",
      "Epoch 7/20  Iteration 11170/35720 Training loss: 0.9631 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 11171/35720 Training loss: 0.9632 0.2245 sec/batch\n",
      "Epoch 7/20  Iteration 11172/35720 Training loss: 0.9634 0.2266 sec/batch\n",
      "Epoch 7/20  Iteration 11173/35720 Training loss: 0.9636 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11174/35720 Training loss: 0.9637 0.2171 sec/batch\n",
      "Epoch 7/20  Iteration 11175/35720 Training loss: 0.9636 0.2131 sec/batch\n",
      "Epoch 7/20  Iteration 11176/35720 Training loss: 0.9635 0.2217 sec/batch\n",
      "Epoch 7/20  Iteration 11177/35720 Training loss: 0.9635 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 11178/35720 Training loss: 0.9635 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 11179/35720 Training loss: 0.9636 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11180/35720 Training loss: 0.9635 0.2056 sec/batch\n",
      "Epoch 7/20  Iteration 11181/35720 Training loss: 0.9635 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 11182/35720 Training loss: 0.9634 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 11183/35720 Training loss: 0.9634 0.2125 sec/batch\n",
      "Epoch 7/20  Iteration 11184/35720 Training loss: 0.9633 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 11185/35720 Training loss: 0.9632 0.2081 sec/batch\n",
      "Epoch 7/20  Iteration 11186/35720 Training loss: 0.9630 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 11187/35720 Training loss: 0.9629 0.2190 sec/batch\n",
      "Epoch 7/20  Iteration 11188/35720 Training loss: 0.9629 0.2244 sec/batch\n",
      "Epoch 7/20  Iteration 11189/35720 Training loss: 0.9629 0.2236 sec/batch\n",
      "Epoch 7/20  Iteration 11190/35720 Training loss: 0.9627 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 11191/35720 Training loss: 0.9628 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 11192/35720 Training loss: 0.9627 0.2133 sec/batch\n",
      "Epoch 7/20  Iteration 11193/35720 Training loss: 0.9628 0.2244 sec/batch\n",
      "Epoch 7/20  Iteration 11194/35720 Training loss: 0.9627 0.2191 sec/batch\n",
      "Epoch 7/20  Iteration 11195/35720 Training loss: 0.9627 0.2271 sec/batch\n",
      "Epoch 7/20  Iteration 11196/35720 Training loss: 0.9626 0.2132 sec/batch\n",
      "Epoch 7/20  Iteration 11197/35720 Training loss: 0.9624 0.2231 sec/batch\n",
      "Epoch 7/20  Iteration 11198/35720 Training loss: 0.9623 0.2310 sec/batch\n",
      "Epoch 7/20  Iteration 11199/35720 Training loss: 0.9623 0.2316 sec/batch\n",
      "Epoch 7/20  Iteration 11200/35720 Training loss: 0.9623 0.2313 sec/batch\n",
      "Validation loss: 1.32995 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 11201/35720 Training loss: 0.9628 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11202/35720 Training loss: 0.9627 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 11203/35720 Training loss: 0.9626 0.2282 sec/batch\n",
      "Epoch 7/20  Iteration 11204/35720 Training loss: 0.9626 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 11205/35720 Training loss: 0.9625 0.2159 sec/batch\n",
      "Epoch 7/20  Iteration 11206/35720 Training loss: 0.9624 0.2233 sec/batch\n",
      "Epoch 7/20  Iteration 11207/35720 Training loss: 0.9623 0.2137 sec/batch\n",
      "Epoch 7/20  Iteration 11208/35720 Training loss: 0.9624 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 11209/35720 Training loss: 0.9624 0.2279 sec/batch\n",
      "Epoch 7/20  Iteration 11210/35720 Training loss: 0.9623 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 11211/35720 Training loss: 0.9623 0.2198 sec/batch\n",
      "Epoch 7/20  Iteration 11212/35720 Training loss: 0.9621 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 11213/35720 Training loss: 0.9622 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 11214/35720 Training loss: 0.9622 0.2076 sec/batch\n",
      "Epoch 7/20  Iteration 11215/35720 Training loss: 0.9621 0.2151 sec/batch\n",
      "Epoch 7/20  Iteration 11216/35720 Training loss: 0.9622 0.2156 sec/batch\n",
      "Epoch 7/20  Iteration 11217/35720 Training loss: 0.9620 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 11218/35720 Training loss: 0.9619 0.2242 sec/batch\n",
      "Epoch 7/20  Iteration 11219/35720 Training loss: 0.9616 0.2098 sec/batch\n",
      "Epoch 7/20  Iteration 11220/35720 Training loss: 0.9616 0.2278 sec/batch\n",
      "Epoch 7/20  Iteration 11221/35720 Training loss: 0.9616 0.2173 sec/batch\n",
      "Epoch 7/20  Iteration 11222/35720 Training loss: 0.9616 0.2182 sec/batch\n",
      "Epoch 7/20  Iteration 11223/35720 Training loss: 0.9615 0.2333 sec/batch\n",
      "Epoch 7/20  Iteration 11224/35720 Training loss: 0.9615 0.2181 sec/batch\n",
      "Epoch 7/20  Iteration 11225/35720 Training loss: 0.9615 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 11226/35720 Training loss: 0.9614 0.2208 sec/batch\n",
      "Epoch 7/20  Iteration 11227/35720 Training loss: 0.9612 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 11228/35720 Training loss: 0.9611 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 11229/35720 Training loss: 0.9610 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 11230/35720 Training loss: 0.9611 0.2143 sec/batch\n",
      "Epoch 7/20  Iteration 11231/35720 Training loss: 0.9611 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 11232/35720 Training loss: 0.9611 0.2205 sec/batch\n",
      "Epoch 7/20  Iteration 11233/35720 Training loss: 0.9611 0.2278 sec/batch\n",
      "Epoch 7/20  Iteration 11234/35720 Training loss: 0.9611 0.2063 sec/batch\n",
      "Epoch 7/20  Iteration 11235/35720 Training loss: 0.9610 0.2254 sec/batch\n",
      "Epoch 7/20  Iteration 11236/35720 Training loss: 0.9611 0.2276 sec/batch\n",
      "Epoch 7/20  Iteration 11237/35720 Training loss: 0.9609 0.2317 sec/batch\n",
      "Epoch 7/20  Iteration 11238/35720 Training loss: 0.9610 0.2306 sec/batch\n",
      "Epoch 7/20  Iteration 11239/35720 Training loss: 0.9609 0.2246 sec/batch\n",
      "Epoch 7/20  Iteration 11240/35720 Training loss: 0.9607 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 11241/35720 Training loss: 0.9607 0.2182 sec/batch\n",
      "Epoch 7/20  Iteration 11242/35720 Training loss: 0.9607 0.2225 sec/batch\n",
      "Epoch 7/20  Iteration 11243/35720 Training loss: 0.9607 0.2303 sec/batch\n",
      "Epoch 7/20  Iteration 11244/35720 Training loss: 0.9606 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 11245/35720 Training loss: 0.9606 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 11246/35720 Training loss: 0.9604 0.2253 sec/batch\n",
      "Epoch 7/20  Iteration 11247/35720 Training loss: 0.9604 0.2140 sec/batch\n",
      "Epoch 7/20  Iteration 11248/35720 Training loss: 0.9604 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 11249/35720 Training loss: 0.9604 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 11250/35720 Training loss: 0.9604 0.2138 sec/batch\n",
      "Epoch 7/20  Iteration 11251/35720 Training loss: 0.9602 0.2203 sec/batch\n",
      "Epoch 7/20  Iteration 11252/35720 Training loss: 0.9601 0.2185 sec/batch\n",
      "Epoch 7/20  Iteration 11253/35720 Training loss: 0.9600 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 11254/35720 Training loss: 0.9599 0.2164 sec/batch\n",
      "Epoch 7/20  Iteration 11255/35720 Training loss: 0.9599 0.2079 sec/batch\n",
      "Epoch 7/20  Iteration 11256/35720 Training loss: 0.9598 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11257/35720 Training loss: 0.9597 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 11258/35720 Training loss: 0.9595 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 11259/35720 Training loss: 0.9594 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11260/35720 Training loss: 0.9593 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 11261/35720 Training loss: 0.9594 0.2250 sec/batch\n",
      "Epoch 7/20  Iteration 11262/35720 Training loss: 0.9595 0.2058 sec/batch\n",
      "Epoch 7/20  Iteration 11263/35720 Training loss: 0.9595 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 11264/35720 Training loss: 0.9595 0.2177 sec/batch\n",
      "Epoch 7/20  Iteration 11265/35720 Training loss: 0.9594 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 11266/35720 Training loss: 0.9595 0.2155 sec/batch\n",
      "Epoch 7/20  Iteration 11267/35720 Training loss: 0.9594 0.2113 sec/batch\n",
      "Epoch 7/20  Iteration 11268/35720 Training loss: 0.9593 0.2172 sec/batch\n",
      "Epoch 7/20  Iteration 11269/35720 Training loss: 0.9592 0.2132 sec/batch\n",
      "Epoch 7/20  Iteration 11270/35720 Training loss: 0.9592 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11271/35720 Training loss: 0.9591 0.2122 sec/batch\n",
      "Epoch 7/20  Iteration 11272/35720 Training loss: 0.9591 0.2243 sec/batch\n",
      "Epoch 7/20  Iteration 11273/35720 Training loss: 0.9591 0.2060 sec/batch\n",
      "Epoch 7/20  Iteration 11274/35720 Training loss: 0.9593 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 11275/35720 Training loss: 0.9592 0.2294 sec/batch\n",
      "Epoch 7/20  Iteration 11276/35720 Training loss: 0.9590 0.2282 sec/batch\n",
      "Epoch 7/20  Iteration 11277/35720 Training loss: 0.9591 0.2261 sec/batch\n",
      "Epoch 7/20  Iteration 11278/35720 Training loss: 0.9590 0.2201 sec/batch\n",
      "Epoch 7/20  Iteration 11279/35720 Training loss: 0.9590 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 11280/35720 Training loss: 0.9589 0.2262 sec/batch\n",
      "Epoch 7/20  Iteration 11281/35720 Training loss: 0.9587 0.2229 sec/batch\n",
      "Epoch 7/20  Iteration 11282/35720 Training loss: 0.9587 0.2275 sec/batch\n",
      "Epoch 7/20  Iteration 11283/35720 Training loss: 0.9587 0.2261 sec/batch\n",
      "Epoch 7/20  Iteration 11284/35720 Training loss: 0.9586 0.2061 sec/batch\n",
      "Epoch 7/20  Iteration 11285/35720 Training loss: 0.9585 0.2096 sec/batch\n",
      "Epoch 7/20  Iteration 11286/35720 Training loss: 0.9585 0.2315 sec/batch\n",
      "Epoch 7/20  Iteration 11287/35720 Training loss: 0.9585 0.2213 sec/batch\n",
      "Epoch 7/20  Iteration 11288/35720 Training loss: 0.9585 0.2247 sec/batch\n",
      "Epoch 7/20  Iteration 11289/35720 Training loss: 0.9586 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 11290/35720 Training loss: 0.9586 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11291/35720 Training loss: 0.9587 0.2295 sec/batch\n",
      "Epoch 7/20  Iteration 11292/35720 Training loss: 0.9588 0.2122 sec/batch\n",
      "Epoch 7/20  Iteration 11293/35720 Training loss: 0.9588 0.2165 sec/batch\n",
      "Epoch 7/20  Iteration 11294/35720 Training loss: 0.9588 0.2262 sec/batch\n",
      "Epoch 7/20  Iteration 11295/35720 Training loss: 0.9587 0.2140 sec/batch\n",
      "Epoch 7/20  Iteration 11296/35720 Training loss: 0.9586 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 11297/35720 Training loss: 0.9586 0.2290 sec/batch\n",
      "Epoch 7/20  Iteration 11298/35720 Training loss: 0.9586 0.2191 sec/batch\n",
      "Epoch 7/20  Iteration 11299/35720 Training loss: 0.9586 0.2134 sec/batch\n",
      "Epoch 7/20  Iteration 11300/35720 Training loss: 0.9586 0.2068 sec/batch\n",
      "Epoch 7/20  Iteration 11301/35720 Training loss: 0.9585 0.2139 sec/batch\n",
      "Epoch 7/20  Iteration 11302/35720 Training loss: 0.9583 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 11303/35720 Training loss: 0.9582 0.2233 sec/batch\n",
      "Epoch 7/20  Iteration 11304/35720 Training loss: 0.9581 0.2209 sec/batch\n",
      "Epoch 7/20  Iteration 11305/35720 Training loss: 0.9580 0.2114 sec/batch\n",
      "Epoch 7/20  Iteration 11306/35720 Training loss: 0.9579 0.2204 sec/batch\n",
      "Epoch 7/20  Iteration 11307/35720 Training loss: 0.9580 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 11308/35720 Training loss: 0.9580 0.2140 sec/batch\n",
      "Epoch 7/20  Iteration 11309/35720 Training loss: 0.9580 0.2078 sec/batch\n",
      "Epoch 7/20  Iteration 11310/35720 Training loss: 0.9581 0.2190 sec/batch\n",
      "Epoch 7/20  Iteration 11311/35720 Training loss: 0.9579 0.2196 sec/batch\n",
      "Epoch 7/20  Iteration 11312/35720 Training loss: 0.9579 0.2103 sec/batch\n",
      "Epoch 7/20  Iteration 11313/35720 Training loss: 0.9578 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 11314/35720 Training loss: 0.9580 0.2311 sec/batch\n",
      "Epoch 7/20  Iteration 11315/35720 Training loss: 0.9578 0.2279 sec/batch\n",
      "Epoch 7/20  Iteration 11316/35720 Training loss: 0.9576 0.2207 sec/batch\n",
      "Epoch 7/20  Iteration 11317/35720 Training loss: 0.9576 0.2058 sec/batch\n",
      "Epoch 7/20  Iteration 11318/35720 Training loss: 0.9574 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 11319/35720 Training loss: 0.9573 0.2272 sec/batch\n",
      "Epoch 7/20  Iteration 11320/35720 Training loss: 0.9573 0.2066 sec/batch\n",
      "Epoch 7/20  Iteration 11321/35720 Training loss: 0.9573 0.2199 sec/batch\n",
      "Epoch 7/20  Iteration 11322/35720 Training loss: 0.9574 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11323/35720 Training loss: 0.9574 0.2141 sec/batch\n",
      "Epoch 7/20  Iteration 11324/35720 Training loss: 0.9574 0.2247 sec/batch\n",
      "Epoch 7/20  Iteration 11325/35720 Training loss: 0.9575 0.2299 sec/batch\n",
      "Epoch 7/20  Iteration 11326/35720 Training loss: 0.9574 0.2256 sec/batch\n",
      "Epoch 7/20  Iteration 11327/35720 Training loss: 0.9574 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 11328/35720 Training loss: 0.9573 0.2272 sec/batch\n",
      "Epoch 7/20  Iteration 11329/35720 Training loss: 0.9573 0.2205 sec/batch\n",
      "Epoch 7/20  Iteration 11330/35720 Training loss: 0.9573 0.2220 sec/batch\n",
      "Epoch 7/20  Iteration 11331/35720 Training loss: 0.9572 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 11332/35720 Training loss: 0.9571 0.2277 sec/batch\n",
      "Epoch 7/20  Iteration 11333/35720 Training loss: 0.9571 0.2081 sec/batch\n",
      "Epoch 7/20  Iteration 11334/35720 Training loss: 0.9570 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 11335/35720 Training loss: 0.9569 0.2265 sec/batch\n",
      "Epoch 7/20  Iteration 11336/35720 Training loss: 0.9568 0.2351 sec/batch\n",
      "Epoch 7/20  Iteration 11337/35720 Training loss: 0.9568 0.2278 sec/batch\n",
      "Epoch 7/20  Iteration 11338/35720 Training loss: 0.9568 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 11339/35720 Training loss: 0.9567 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 11340/35720 Training loss: 0.9566 0.2244 sec/batch\n",
      "Epoch 7/20  Iteration 11341/35720 Training loss: 0.9565 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 11342/35720 Training loss: 0.9565 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 11343/35720 Training loss: 0.9565 0.2331 sec/batch\n",
      "Epoch 7/20  Iteration 11344/35720 Training loss: 0.9564 0.2119 sec/batch\n",
      "Epoch 7/20  Iteration 11345/35720 Training loss: 0.9563 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 11346/35720 Training loss: 0.9562 0.2113 sec/batch\n",
      "Epoch 7/20  Iteration 11347/35720 Training loss: 0.9564 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 11348/35720 Training loss: 0.9562 0.2204 sec/batch\n",
      "Epoch 7/20  Iteration 11349/35720 Training loss: 0.9562 0.2157 sec/batch\n",
      "Epoch 7/20  Iteration 11350/35720 Training loss: 0.9562 0.2061 sec/batch\n",
      "Epoch 7/20  Iteration 11351/35720 Training loss: 0.9561 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11352/35720 Training loss: 0.9561 0.2113 sec/batch\n",
      "Epoch 7/20  Iteration 11353/35720 Training loss: 0.9561 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 11354/35720 Training loss: 0.9560 0.2255 sec/batch\n",
      "Epoch 7/20  Iteration 11355/35720 Training loss: 0.9560 0.2156 sec/batch\n",
      "Epoch 7/20  Iteration 11356/35720 Training loss: 0.9560 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 11357/35720 Training loss: 0.9561 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 11358/35720 Training loss: 0.9562 0.2086 sec/batch\n",
      "Epoch 7/20  Iteration 11359/35720 Training loss: 0.9562 0.2214 sec/batch\n",
      "Epoch 7/20  Iteration 11360/35720 Training loss: 0.9561 0.2271 sec/batch\n",
      "Epoch 7/20  Iteration 11361/35720 Training loss: 0.9561 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 11362/35720 Training loss: 0.9562 0.2233 sec/batch\n",
      "Epoch 7/20  Iteration 11363/35720 Training loss: 0.9561 0.2145 sec/batch\n",
      "Epoch 7/20  Iteration 11364/35720 Training loss: 0.9561 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 11365/35720 Training loss: 0.9560 0.2228 sec/batch\n",
      "Epoch 7/20  Iteration 11366/35720 Training loss: 0.9558 0.2134 sec/batch\n",
      "Epoch 7/20  Iteration 11367/35720 Training loss: 0.9559 0.2080 sec/batch\n",
      "Epoch 7/20  Iteration 11368/35720 Training loss: 0.9559 0.2381 sec/batch\n",
      "Epoch 7/20  Iteration 11369/35720 Training loss: 0.9559 0.2276 sec/batch\n",
      "Epoch 7/20  Iteration 11370/35720 Training loss: 0.9560 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 11371/35720 Training loss: 0.9560 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 11372/35720 Training loss: 0.9561 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 11373/35720 Training loss: 0.9562 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11374/35720 Training loss: 0.9563 0.2151 sec/batch\n",
      "Epoch 7/20  Iteration 11375/35720 Training loss: 0.9563 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 11376/35720 Training loss: 0.9563 0.2151 sec/batch\n",
      "Epoch 7/20  Iteration 11377/35720 Training loss: 0.9564 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 11378/35720 Training loss: 0.9564 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 11379/35720 Training loss: 0.9564 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 11380/35720 Training loss: 0.9565 0.2323 sec/batch\n",
      "Epoch 7/20  Iteration 11381/35720 Training loss: 0.9565 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 11382/35720 Training loss: 0.9566 0.2112 sec/batch\n",
      "Epoch 7/20  Iteration 11383/35720 Training loss: 0.9567 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 11384/35720 Training loss: 0.9566 0.2185 sec/batch\n",
      "Epoch 7/20  Iteration 11385/35720 Training loss: 0.9565 0.2276 sec/batch\n",
      "Epoch 7/20  Iteration 11386/35720 Training loss: 0.9565 0.2177 sec/batch\n",
      "Epoch 7/20  Iteration 11387/35720 Training loss: 0.9563 0.2200 sec/batch\n",
      "Epoch 7/20  Iteration 11388/35720 Training loss: 0.9563 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 11389/35720 Training loss: 0.9564 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11390/35720 Training loss: 0.9563 0.2230 sec/batch\n",
      "Epoch 7/20  Iteration 11391/35720 Training loss: 0.9563 0.2206 sec/batch\n",
      "Epoch 7/20  Iteration 11392/35720 Training loss: 0.9562 0.2272 sec/batch\n",
      "Epoch 7/20  Iteration 11393/35720 Training loss: 0.9562 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 11394/35720 Training loss: 0.9562 0.2089 sec/batch\n",
      "Epoch 7/20  Iteration 11395/35720 Training loss: 0.9562 0.2170 sec/batch\n",
      "Epoch 7/20  Iteration 11396/35720 Training loss: 0.9561 0.2216 sec/batch\n",
      "Epoch 7/20  Iteration 11397/35720 Training loss: 0.9561 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 11398/35720 Training loss: 0.9561 0.2276 sec/batch\n",
      "Epoch 7/20  Iteration 11399/35720 Training loss: 0.9560 0.2059 sec/batch\n",
      "Epoch 7/20  Iteration 11400/35720 Training loss: 0.9559 0.2097 sec/batch\n",
      "Validation loss: 1.33072 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 11401/35720 Training loss: 0.9564 0.2125 sec/batch\n",
      "Epoch 7/20  Iteration 11402/35720 Training loss: 0.9564 0.2329 sec/batch\n",
      "Epoch 7/20  Iteration 11403/35720 Training loss: 0.9563 0.2085 sec/batch\n",
      "Epoch 7/20  Iteration 11404/35720 Training loss: 0.9563 0.2132 sec/batch\n",
      "Epoch 7/20  Iteration 11405/35720 Training loss: 0.9563 0.2269 sec/batch\n",
      "Epoch 7/20  Iteration 11406/35720 Training loss: 0.9563 0.2320 sec/batch\n",
      "Epoch 7/20  Iteration 11407/35720 Training loss: 0.9563 0.2083 sec/batch\n",
      "Epoch 7/20  Iteration 11408/35720 Training loss: 0.9564 0.2164 sec/batch\n",
      "Epoch 7/20  Iteration 11409/35720 Training loss: 0.9566 0.2259 sec/batch\n",
      "Epoch 7/20  Iteration 11410/35720 Training loss: 0.9565 0.2055 sec/batch\n",
      "Epoch 7/20  Iteration 11411/35720 Training loss: 0.9565 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 11412/35720 Training loss: 0.9565 0.2098 sec/batch\n",
      "Epoch 7/20  Iteration 11413/35720 Training loss: 0.9566 0.2125 sec/batch\n",
      "Epoch 7/20  Iteration 11414/35720 Training loss: 0.9566 0.2391 sec/batch\n",
      "Epoch 7/20  Iteration 11415/35720 Training loss: 0.9565 0.2069 sec/batch\n",
      "Epoch 7/20  Iteration 11416/35720 Training loss: 0.9565 0.2130 sec/batch\n",
      "Epoch 7/20  Iteration 11417/35720 Training loss: 0.9564 0.2221 sec/batch\n",
      "Epoch 7/20  Iteration 11418/35720 Training loss: 0.9564 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 11419/35720 Training loss: 0.9564 0.2164 sec/batch\n",
      "Epoch 7/20  Iteration 11420/35720 Training loss: 0.9564 0.2103 sec/batch\n",
      "Epoch 7/20  Iteration 11421/35720 Training loss: 0.9564 0.2155 sec/batch\n",
      "Epoch 7/20  Iteration 11422/35720 Training loss: 0.9564 0.2131 sec/batch\n",
      "Epoch 7/20  Iteration 11423/35720 Training loss: 0.9565 0.2216 sec/batch\n",
      "Epoch 7/20  Iteration 11424/35720 Training loss: 0.9565 0.2371 sec/batch\n",
      "Epoch 7/20  Iteration 11425/35720 Training loss: 0.9566 0.2232 sec/batch\n",
      "Epoch 7/20  Iteration 11426/35720 Training loss: 0.9567 0.2173 sec/batch\n",
      "Epoch 7/20  Iteration 11427/35720 Training loss: 0.9568 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 11428/35720 Training loss: 0.9567 0.2347 sec/batch\n",
      "Epoch 7/20  Iteration 11429/35720 Training loss: 0.9568 0.2147 sec/batch\n",
      "Epoch 7/20  Iteration 11430/35720 Training loss: 0.9567 0.2323 sec/batch\n",
      "Epoch 7/20  Iteration 11431/35720 Training loss: 0.9568 0.2073 sec/batch\n",
      "Epoch 7/20  Iteration 11432/35720 Training loss: 0.9568 0.2218 sec/batch\n",
      "Epoch 7/20  Iteration 11433/35720 Training loss: 0.9569 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 11434/35720 Training loss: 0.9570 0.2199 sec/batch\n",
      "Epoch 7/20  Iteration 11435/35720 Training loss: 0.9569 0.2196 sec/batch\n",
      "Epoch 7/20  Iteration 11436/35720 Training loss: 0.9570 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 11437/35720 Training loss: 0.9570 0.2070 sec/batch\n",
      "Epoch 7/20  Iteration 11438/35720 Training loss: 0.9570 0.2136 sec/batch\n",
      "Epoch 7/20  Iteration 11439/35720 Training loss: 0.9571 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 11440/35720 Training loss: 0.9570 0.2242 sec/batch\n",
      "Epoch 7/20  Iteration 11441/35720 Training loss: 0.9570 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 11442/35720 Training loss: 0.9570 0.2118 sec/batch\n",
      "Epoch 7/20  Iteration 11443/35720 Training loss: 0.9572 0.2190 sec/batch\n",
      "Epoch 7/20  Iteration 11444/35720 Training loss: 0.9571 0.2353 sec/batch\n",
      "Epoch 7/20  Iteration 11445/35720 Training loss: 0.9572 0.2334 sec/batch\n",
      "Epoch 7/20  Iteration 11446/35720 Training loss: 0.9572 0.2230 sec/batch\n",
      "Epoch 7/20  Iteration 11447/35720 Training loss: 0.9571 0.2525 sec/batch\n",
      "Epoch 7/20  Iteration 11448/35720 Training loss: 0.9571 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 11449/35720 Training loss: 0.9571 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11450/35720 Training loss: 0.9571 0.2153 sec/batch\n",
      "Epoch 7/20  Iteration 11451/35720 Training loss: 0.9571 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 11452/35720 Training loss: 0.9571 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 11453/35720 Training loss: 0.9572 0.2126 sec/batch\n",
      "Epoch 7/20  Iteration 11454/35720 Training loss: 0.9571 0.2083 sec/batch\n",
      "Epoch 7/20  Iteration 11455/35720 Training loss: 0.9572 0.2136 sec/batch\n",
      "Epoch 7/20  Iteration 11456/35720 Training loss: 0.9572 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 11457/35720 Training loss: 0.9572 0.2440 sec/batch\n",
      "Epoch 7/20  Iteration 11458/35720 Training loss: 0.9572 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 11459/35720 Training loss: 0.9571 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11460/35720 Training loss: 0.9571 0.2227 sec/batch\n",
      "Epoch 7/20  Iteration 11461/35720 Training loss: 0.9572 0.2221 sec/batch\n",
      "Epoch 7/20  Iteration 11462/35720 Training loss: 0.9572 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 11463/35720 Training loss: 0.9572 0.2175 sec/batch\n",
      "Epoch 7/20  Iteration 11464/35720 Training loss: 0.9571 0.2315 sec/batch\n",
      "Epoch 7/20  Iteration 11465/35720 Training loss: 0.9570 0.2083 sec/batch\n",
      "Epoch 7/20  Iteration 11466/35720 Training loss: 0.9570 0.2057 sec/batch\n",
      "Epoch 7/20  Iteration 11467/35720 Training loss: 0.9570 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 11468/35720 Training loss: 0.9571 0.2139 sec/batch\n",
      "Epoch 7/20  Iteration 11469/35720 Training loss: 0.9569 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 11470/35720 Training loss: 0.9569 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 11471/35720 Training loss: 0.9569 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11472/35720 Training loss: 0.9568 0.2124 sec/batch\n",
      "Epoch 7/20  Iteration 11473/35720 Training loss: 0.9567 0.2101 sec/batch\n",
      "Epoch 7/20  Iteration 11474/35720 Training loss: 0.9568 0.2194 sec/batch\n",
      "Epoch 7/20  Iteration 11475/35720 Training loss: 0.9569 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 11476/35720 Training loss: 0.9569 0.2152 sec/batch\n",
      "Epoch 7/20  Iteration 11477/35720 Training loss: 0.9569 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 11478/35720 Training loss: 0.9569 0.2173 sec/batch\n",
      "Epoch 7/20  Iteration 11479/35720 Training loss: 0.9569 0.2230 sec/batch\n",
      "Epoch 7/20  Iteration 11480/35720 Training loss: 0.9568 0.2170 sec/batch\n",
      "Epoch 7/20  Iteration 11481/35720 Training loss: 0.9568 0.2063 sec/batch\n",
      "Epoch 7/20  Iteration 11482/35720 Training loss: 0.9569 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 11483/35720 Training loss: 0.9569 0.2210 sec/batch\n",
      "Epoch 7/20  Iteration 11484/35720 Training loss: 0.9568 0.2183 sec/batch\n",
      "Epoch 7/20  Iteration 11485/35720 Training loss: 0.9569 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 11486/35720 Training loss: 0.9570 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 11487/35720 Training loss: 0.9571 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 11488/35720 Training loss: 0.9572 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 11489/35720 Training loss: 0.9572 0.2218 sec/batch\n",
      "Epoch 7/20  Iteration 11490/35720 Training loss: 0.9571 0.2473 sec/batch\n",
      "Epoch 7/20  Iteration 11491/35720 Training loss: 0.9570 0.2236 sec/batch\n",
      "Epoch 7/20  Iteration 11492/35720 Training loss: 0.9569 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 11493/35720 Training loss: 0.9569 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 11494/35720 Training loss: 0.9569 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 11495/35720 Training loss: 0.9569 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 11496/35720 Training loss: 0.9569 0.2060 sec/batch\n",
      "Epoch 7/20  Iteration 11497/35720 Training loss: 0.9569 0.2069 sec/batch\n",
      "Epoch 7/20  Iteration 11498/35720 Training loss: 0.9569 0.2182 sec/batch\n",
      "Epoch 7/20  Iteration 11499/35720 Training loss: 0.9570 0.2228 sec/batch\n",
      "Epoch 7/20  Iteration 11500/35720 Training loss: 0.9570 0.2077 sec/batch\n",
      "Epoch 7/20  Iteration 11501/35720 Training loss: 0.9569 0.2291 sec/batch\n",
      "Epoch 7/20  Iteration 11502/35720 Training loss: 0.9568 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 11503/35720 Training loss: 0.9568 0.2069 sec/batch\n",
      "Epoch 7/20  Iteration 11504/35720 Training loss: 0.9568 0.2152 sec/batch\n",
      "Epoch 7/20  Iteration 11505/35720 Training loss: 0.9568 0.2214 sec/batch\n",
      "Epoch 7/20  Iteration 11506/35720 Training loss: 0.9568 0.2262 sec/batch\n",
      "Epoch 7/20  Iteration 11507/35720 Training loss: 0.9569 0.2458 sec/batch\n",
      "Epoch 7/20  Iteration 11508/35720 Training loss: 0.9569 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 11509/35720 Training loss: 0.9568 0.2086 sec/batch\n",
      "Epoch 7/20  Iteration 11510/35720 Training loss: 0.9568 0.2153 sec/batch\n",
      "Epoch 7/20  Iteration 11511/35720 Training loss: 0.9568 0.2300 sec/batch\n",
      "Epoch 7/20  Iteration 11512/35720 Training loss: 0.9568 0.2247 sec/batch\n",
      "Epoch 7/20  Iteration 11513/35720 Training loss: 0.9568 0.2277 sec/batch\n",
      "Epoch 7/20  Iteration 11514/35720 Training loss: 0.9567 0.2075 sec/batch\n",
      "Epoch 7/20  Iteration 11515/35720 Training loss: 0.9566 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 11516/35720 Training loss: 0.9567 0.2223 sec/batch\n",
      "Epoch 7/20  Iteration 11517/35720 Training loss: 0.9567 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 11518/35720 Training loss: 0.9568 0.2224 sec/batch\n",
      "Epoch 7/20  Iteration 11519/35720 Training loss: 0.9568 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 11520/35720 Training loss: 0.9570 0.2197 sec/batch\n",
      "Epoch 7/20  Iteration 11521/35720 Training loss: 0.9570 0.2157 sec/batch\n",
      "Epoch 7/20  Iteration 11522/35720 Training loss: 0.9570 0.2234 sec/batch\n",
      "Epoch 7/20  Iteration 11523/35720 Training loss: 0.9571 0.2298 sec/batch\n",
      "Epoch 7/20  Iteration 11524/35720 Training loss: 0.9571 0.2255 sec/batch\n",
      "Epoch 7/20  Iteration 11525/35720 Training loss: 0.9572 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 11526/35720 Training loss: 0.9573 0.2289 sec/batch\n",
      "Epoch 7/20  Iteration 11527/35720 Training loss: 0.9573 0.2229 sec/batch\n",
      "Epoch 7/20  Iteration 11528/35720 Training loss: 0.9573 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 11529/35720 Training loss: 0.9574 0.2252 sec/batch\n",
      "Epoch 7/20  Iteration 11530/35720 Training loss: 0.9574 0.2155 sec/batch\n",
      "Epoch 7/20  Iteration 11531/35720 Training loss: 0.9575 0.2059 sec/batch\n",
      "Epoch 7/20  Iteration 11532/35720 Training loss: 0.9575 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 11533/35720 Training loss: 0.9574 0.2181 sec/batch\n",
      "Epoch 7/20  Iteration 11534/35720 Training loss: 0.9575 0.2305 sec/batch\n",
      "Epoch 7/20  Iteration 11535/35720 Training loss: 0.9575 0.2233 sec/batch\n",
      "Epoch 7/20  Iteration 11536/35720 Training loss: 0.9575 0.2132 sec/batch\n",
      "Epoch 7/20  Iteration 11537/35720 Training loss: 0.9575 0.2271 sec/batch\n",
      "Epoch 7/20  Iteration 11538/35720 Training loss: 0.9574 0.2241 sec/batch\n",
      "Epoch 7/20  Iteration 11539/35720 Training loss: 0.9573 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 11540/35720 Training loss: 0.9572 0.2342 sec/batch\n",
      "Epoch 7/20  Iteration 11541/35720 Training loss: 0.9571 0.2261 sec/batch\n",
      "Epoch 7/20  Iteration 11542/35720 Training loss: 0.9571 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 11543/35720 Training loss: 0.9570 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 11544/35720 Training loss: 0.9569 0.2209 sec/batch\n",
      "Epoch 7/20  Iteration 11545/35720 Training loss: 0.9569 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 11546/35720 Training loss: 0.9568 0.2122 sec/batch\n",
      "Epoch 7/20  Iteration 11547/35720 Training loss: 0.9567 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 11548/35720 Training loss: 0.9567 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 11549/35720 Training loss: 0.9569 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 11550/35720 Training loss: 0.9569 0.2196 sec/batch\n",
      "Epoch 7/20  Iteration 11551/35720 Training loss: 0.9569 0.2275 sec/batch\n",
      "Epoch 7/20  Iteration 11552/35720 Training loss: 0.9568 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 11553/35720 Training loss: 0.9568 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11554/35720 Training loss: 0.9567 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 11555/35720 Training loss: 0.9568 0.2330 sec/batch\n",
      "Epoch 7/20  Iteration 11556/35720 Training loss: 0.9568 0.2407 sec/batch\n",
      "Epoch 7/20  Iteration 11557/35720 Training loss: 0.9569 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 11558/35720 Training loss: 0.9569 0.2069 sec/batch\n",
      "Epoch 7/20  Iteration 11559/35720 Training loss: 0.9568 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 11560/35720 Training loss: 0.9569 0.2242 sec/batch\n",
      "Epoch 7/20  Iteration 11561/35720 Training loss: 0.9569 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 11562/35720 Training loss: 0.9568 0.2188 sec/batch\n",
      "Epoch 7/20  Iteration 11563/35720 Training loss: 0.9568 0.2147 sec/batch\n",
      "Epoch 7/20  Iteration 11564/35720 Training loss: 0.9568 0.2066 sec/batch\n",
      "Epoch 7/20  Iteration 11565/35720 Training loss: 0.9567 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 11566/35720 Training loss: 0.9568 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 11567/35720 Training loss: 0.9567 0.2209 sec/batch\n",
      "Epoch 7/20  Iteration 11568/35720 Training loss: 0.9567 0.2103 sec/batch\n",
      "Epoch 7/20  Iteration 11569/35720 Training loss: 0.9566 0.2069 sec/batch\n",
      "Epoch 7/20  Iteration 11570/35720 Training loss: 0.9566 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 11571/35720 Training loss: 0.9567 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 11572/35720 Training loss: 0.9566 0.2119 sec/batch\n",
      "Epoch 7/20  Iteration 11573/35720 Training loss: 0.9566 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 11574/35720 Training loss: 0.9566 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11575/35720 Training loss: 0.9565 0.2320 sec/batch\n",
      "Epoch 7/20  Iteration 11576/35720 Training loss: 0.9564 0.2152 sec/batch\n",
      "Epoch 7/20  Iteration 11577/35720 Training loss: 0.9565 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 11578/35720 Training loss: 0.9564 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 11579/35720 Training loss: 0.9564 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 11580/35720 Training loss: 0.9564 0.2145 sec/batch\n",
      "Epoch 7/20  Iteration 11581/35720 Training loss: 0.9563 0.2134 sec/batch\n",
      "Epoch 7/20  Iteration 11582/35720 Training loss: 0.9564 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 11583/35720 Training loss: 0.9565 0.2256 sec/batch\n",
      "Epoch 7/20  Iteration 11584/35720 Training loss: 0.9565 0.2237 sec/batch\n",
      "Epoch 7/20  Iteration 11585/35720 Training loss: 0.9564 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 11586/35720 Training loss: 0.9564 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 11587/35720 Training loss: 0.9564 0.2175 sec/batch\n",
      "Epoch 7/20  Iteration 11588/35720 Training loss: 0.9563 0.2278 sec/batch\n",
      "Epoch 7/20  Iteration 11589/35720 Training loss: 0.9562 0.2082 sec/batch\n",
      "Epoch 7/20  Iteration 11590/35720 Training loss: 0.9562 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 11591/35720 Training loss: 0.9561 0.2231 sec/batch\n",
      "Epoch 7/20  Iteration 11592/35720 Training loss: 0.9561 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 11593/35720 Training loss: 0.9560 0.2299 sec/batch\n",
      "Epoch 7/20  Iteration 11594/35720 Training loss: 0.9560 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 11595/35720 Training loss: 0.9560 0.2238 sec/batch\n",
      "Epoch 7/20  Iteration 11596/35720 Training loss: 0.9560 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 11597/35720 Training loss: 0.9560 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 11598/35720 Training loss: 0.9560 0.2292 sec/batch\n",
      "Epoch 7/20  Iteration 11599/35720 Training loss: 0.9560 0.2276 sec/batch\n",
      "Epoch 7/20  Iteration 11600/35720 Training loss: 0.9560 0.2200 sec/batch\n",
      "Validation loss: 1.32574 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 11601/35720 Training loss: 0.9563 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 11602/35720 Training loss: 0.9562 0.2135 sec/batch\n",
      "Epoch 7/20  Iteration 11603/35720 Training loss: 0.9562 0.2178 sec/batch\n",
      "Epoch 7/20  Iteration 11604/35720 Training loss: 0.9562 0.2178 sec/batch\n",
      "Epoch 7/20  Iteration 11605/35720 Training loss: 0.9561 0.2328 sec/batch\n",
      "Epoch 7/20  Iteration 11606/35720 Training loss: 0.9560 0.2231 sec/batch\n",
      "Epoch 7/20  Iteration 11607/35720 Training loss: 0.9559 0.2139 sec/batch\n",
      "Epoch 7/20  Iteration 11608/35720 Training loss: 0.9558 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 11609/35720 Training loss: 0.9558 0.2265 sec/batch\n",
      "Epoch 7/20  Iteration 11610/35720 Training loss: 0.9557 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 11611/35720 Training loss: 0.9557 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 11612/35720 Training loss: 0.9557 0.2164 sec/batch\n",
      "Epoch 7/20  Iteration 11613/35720 Training loss: 0.9556 0.2083 sec/batch\n",
      "Epoch 7/20  Iteration 11614/35720 Training loss: 0.9554 0.2304 sec/batch\n",
      "Epoch 7/20  Iteration 11615/35720 Training loss: 0.9553 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 11616/35720 Training loss: 0.9553 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 11617/35720 Training loss: 0.9552 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 11618/35720 Training loss: 0.9551 0.2070 sec/batch\n",
      "Epoch 7/20  Iteration 11619/35720 Training loss: 0.9550 0.2140 sec/batch\n",
      "Epoch 7/20  Iteration 11620/35720 Training loss: 0.9550 0.2157 sec/batch\n",
      "Epoch 7/20  Iteration 11621/35720 Training loss: 0.9549 0.2188 sec/batch\n",
      "Epoch 7/20  Iteration 11622/35720 Training loss: 0.9549 0.2444 sec/batch\n",
      "Epoch 7/20  Iteration 11623/35720 Training loss: 0.9549 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 11624/35720 Training loss: 0.9550 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11625/35720 Training loss: 0.9550 0.2071 sec/batch\n",
      "Epoch 7/20  Iteration 11626/35720 Training loss: 0.9549 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11627/35720 Training loss: 0.9549 0.2190 sec/batch\n",
      "Epoch 7/20  Iteration 11628/35720 Training loss: 0.9549 0.2154 sec/batch\n",
      "Epoch 7/20  Iteration 11629/35720 Training loss: 0.9549 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 11630/35720 Training loss: 0.9549 0.2130 sec/batch\n",
      "Epoch 7/20  Iteration 11631/35720 Training loss: 0.9550 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 11632/35720 Training loss: 0.9550 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 11633/35720 Training loss: 0.9551 0.2207 sec/batch\n",
      "Epoch 7/20  Iteration 11634/35720 Training loss: 0.9550 0.2070 sec/batch\n",
      "Epoch 7/20  Iteration 11635/35720 Training loss: 0.9550 0.2061 sec/batch\n",
      "Epoch 7/20  Iteration 11636/35720 Training loss: 0.9549 0.2076 sec/batch\n",
      "Epoch 7/20  Iteration 11637/35720 Training loss: 0.9549 0.2242 sec/batch\n",
      "Epoch 7/20  Iteration 11638/35720 Training loss: 0.9548 0.2134 sec/batch\n",
      "Epoch 7/20  Iteration 11639/35720 Training loss: 0.9549 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 11640/35720 Training loss: 0.9549 0.2261 sec/batch\n",
      "Epoch 7/20  Iteration 11641/35720 Training loss: 0.9549 0.2133 sec/batch\n",
      "Epoch 7/20  Iteration 11642/35720 Training loss: 0.9549 0.2238 sec/batch\n",
      "Epoch 7/20  Iteration 11643/35720 Training loss: 0.9550 0.2303 sec/batch\n",
      "Epoch 7/20  Iteration 11644/35720 Training loss: 0.9549 0.2275 sec/batch\n",
      "Epoch 7/20  Iteration 11645/35720 Training loss: 0.9550 0.2178 sec/batch\n",
      "Epoch 7/20  Iteration 11646/35720 Training loss: 0.9550 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 11647/35720 Training loss: 0.9551 0.2332 sec/batch\n",
      "Epoch 7/20  Iteration 11648/35720 Training loss: 0.9551 0.2147 sec/batch\n",
      "Epoch 7/20  Iteration 11649/35720 Training loss: 0.9551 0.2173 sec/batch\n",
      "Epoch 7/20  Iteration 11650/35720 Training loss: 0.9551 0.2193 sec/batch\n",
      "Epoch 7/20  Iteration 11651/35720 Training loss: 0.9549 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 11652/35720 Training loss: 0.9548 0.2182 sec/batch\n",
      "Epoch 7/20  Iteration 11653/35720 Training loss: 0.9547 0.2154 sec/batch\n",
      "Epoch 7/20  Iteration 11654/35720 Training loss: 0.9547 0.2333 sec/batch\n",
      "Epoch 7/20  Iteration 11655/35720 Training loss: 0.9546 0.2392 sec/batch\n",
      "Epoch 7/20  Iteration 11656/35720 Training loss: 0.9545 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 11657/35720 Training loss: 0.9544 0.2247 sec/batch\n",
      "Epoch 7/20  Iteration 11658/35720 Training loss: 0.9544 0.2144 sec/batch\n",
      "Epoch 7/20  Iteration 11659/35720 Training loss: 0.9544 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 11660/35720 Training loss: 0.9545 0.2073 sec/batch\n",
      "Epoch 7/20  Iteration 11661/35720 Training loss: 0.9544 0.2175 sec/batch\n",
      "Epoch 7/20  Iteration 11662/35720 Training loss: 0.9544 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 11663/35720 Training loss: 0.9544 0.2305 sec/batch\n",
      "Epoch 7/20  Iteration 11664/35720 Training loss: 0.9543 0.2246 sec/batch\n",
      "Epoch 7/20  Iteration 11665/35720 Training loss: 0.9543 0.2219 sec/batch\n",
      "Epoch 7/20  Iteration 11666/35720 Training loss: 0.9542 0.2137 sec/batch\n",
      "Epoch 7/20  Iteration 11667/35720 Training loss: 0.9541 0.2147 sec/batch\n",
      "Epoch 7/20  Iteration 11668/35720 Training loss: 0.9540 0.2343 sec/batch\n",
      "Epoch 7/20  Iteration 11669/35720 Training loss: 0.9540 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 11670/35720 Training loss: 0.9540 0.2290 sec/batch\n",
      "Epoch 7/20  Iteration 11671/35720 Training loss: 0.9539 0.2294 sec/batch\n",
      "Epoch 7/20  Iteration 11672/35720 Training loss: 0.9539 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 11673/35720 Training loss: 0.9538 0.2241 sec/batch\n",
      "Epoch 7/20  Iteration 11674/35720 Training loss: 0.9538 0.2191 sec/batch\n",
      "Epoch 7/20  Iteration 11675/35720 Training loss: 0.9537 0.2119 sec/batch\n",
      "Epoch 7/20  Iteration 11676/35720 Training loss: 0.9537 0.2206 sec/batch\n",
      "Epoch 7/20  Iteration 11677/35720 Training loss: 0.9536 0.2166 sec/batch\n",
      "Epoch 7/20  Iteration 11678/35720 Training loss: 0.9536 0.2063 sec/batch\n",
      "Epoch 7/20  Iteration 11679/35720 Training loss: 0.9535 0.2142 sec/batch\n",
      "Epoch 7/20  Iteration 11680/35720 Training loss: 0.9535 0.2346 sec/batch\n",
      "Epoch 7/20  Iteration 11681/35720 Training loss: 0.9534 0.2288 sec/batch\n",
      "Epoch 7/20  Iteration 11682/35720 Training loss: 0.9534 0.2334 sec/batch\n",
      "Epoch 7/20  Iteration 11683/35720 Training loss: 0.9534 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 11684/35720 Training loss: 0.9533 0.2058 sec/batch\n",
      "Epoch 7/20  Iteration 11685/35720 Training loss: 0.9535 0.2171 sec/batch\n",
      "Epoch 7/20  Iteration 11686/35720 Training loss: 0.9537 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 11687/35720 Training loss: 0.9536 0.2078 sec/batch\n",
      "Epoch 7/20  Iteration 11688/35720 Training loss: 0.9535 0.2232 sec/batch\n",
      "Epoch 7/20  Iteration 11689/35720 Training loss: 0.9535 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 11690/35720 Training loss: 0.9534 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 11691/35720 Training loss: 0.9534 0.2126 sec/batch\n",
      "Epoch 7/20  Iteration 11692/35720 Training loss: 0.9534 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 11693/35720 Training loss: 0.9534 0.2068 sec/batch\n",
      "Epoch 7/20  Iteration 11694/35720 Training loss: 0.9535 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11695/35720 Training loss: 0.9534 0.2125 sec/batch\n",
      "Epoch 7/20  Iteration 11696/35720 Training loss: 0.9533 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 11697/35720 Training loss: 0.9533 0.2148 sec/batch\n",
      "Epoch 7/20  Iteration 11698/35720 Training loss: 0.9532 0.2253 sec/batch\n",
      "Epoch 7/20  Iteration 11699/35720 Training loss: 0.9531 0.2239 sec/batch\n",
      "Epoch 7/20  Iteration 11700/35720 Training loss: 0.9531 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 11701/35720 Training loss: 0.9531 0.2263 sec/batch\n",
      "Epoch 7/20  Iteration 11702/35720 Training loss: 0.9530 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 11703/35720 Training loss: 0.9530 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 11704/35720 Training loss: 0.9530 0.2233 sec/batch\n",
      "Epoch 7/20  Iteration 11705/35720 Training loss: 0.9529 0.2342 sec/batch\n",
      "Epoch 7/20  Iteration 11706/35720 Training loss: 0.9529 0.2054 sec/batch\n",
      "Epoch 7/20  Iteration 11707/35720 Training loss: 0.9529 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 11708/35720 Training loss: 0.9529 0.2277 sec/batch\n",
      "Epoch 7/20  Iteration 11709/35720 Training loss: 0.9529 0.2272 sec/batch\n",
      "Epoch 7/20  Iteration 11710/35720 Training loss: 0.9529 0.2256 sec/batch\n",
      "Epoch 7/20  Iteration 11711/35720 Training loss: 0.9530 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 11712/35720 Training loss: 0.9530 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 11713/35720 Training loss: 0.9530 0.2135 sec/batch\n",
      "Epoch 7/20  Iteration 11714/35720 Training loss: 0.9531 0.2141 sec/batch\n",
      "Epoch 7/20  Iteration 11715/35720 Training loss: 0.9531 0.2246 sec/batch\n",
      "Epoch 7/20  Iteration 11716/35720 Training loss: 0.9530 0.2270 sec/batch\n",
      "Epoch 7/20  Iteration 11717/35720 Training loss: 0.9529 0.2080 sec/batch\n",
      "Epoch 7/20  Iteration 11718/35720 Training loss: 0.9528 0.2188 sec/batch\n",
      "Epoch 7/20  Iteration 11719/35720 Training loss: 0.9527 0.2133 sec/batch\n",
      "Epoch 7/20  Iteration 11720/35720 Training loss: 0.9527 0.2098 sec/batch\n",
      "Epoch 7/20  Iteration 11721/35720 Training loss: 0.9526 0.2245 sec/batch\n",
      "Epoch 7/20  Iteration 11722/35720 Training loss: 0.9525 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 11723/35720 Training loss: 0.9525 0.2121 sec/batch\n",
      "Epoch 7/20  Iteration 11724/35720 Training loss: 0.9524 0.2066 sec/batch\n",
      "Epoch 7/20  Iteration 11725/35720 Training loss: 0.9523 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 11726/35720 Training loss: 0.9522 0.2188 sec/batch\n",
      "Epoch 7/20  Iteration 11727/35720 Training loss: 0.9522 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 11728/35720 Training loss: 0.9522 0.2057 sec/batch\n",
      "Epoch 7/20  Iteration 11729/35720 Training loss: 0.9522 0.2093 sec/batch\n",
      "Epoch 7/20  Iteration 11730/35720 Training loss: 0.9522 0.2124 sec/batch\n",
      "Epoch 7/20  Iteration 11731/35720 Training loss: 0.9521 0.2468 sec/batch\n",
      "Epoch 7/20  Iteration 11732/35720 Training loss: 0.9522 0.2224 sec/batch\n",
      "Epoch 7/20  Iteration 11733/35720 Training loss: 0.9521 0.2074 sec/batch\n",
      "Epoch 7/20  Iteration 11734/35720 Training loss: 0.9521 0.2113 sec/batch\n",
      "Epoch 7/20  Iteration 11735/35720 Training loss: 0.9520 0.2275 sec/batch\n",
      "Epoch 7/20  Iteration 11736/35720 Training loss: 0.9520 0.2260 sec/batch\n",
      "Epoch 7/20  Iteration 11737/35720 Training loss: 0.9519 0.2173 sec/batch\n",
      "Epoch 7/20  Iteration 11738/35720 Training loss: 0.9520 0.2101 sec/batch\n",
      "Epoch 7/20  Iteration 11739/35720 Training loss: 0.9521 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 11740/35720 Training loss: 0.9521 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 11741/35720 Training loss: 0.9521 0.2113 sec/batch\n",
      "Epoch 7/20  Iteration 11742/35720 Training loss: 0.9521 0.2188 sec/batch\n",
      "Epoch 7/20  Iteration 11743/35720 Training loss: 0.9520 0.2273 sec/batch\n",
      "Epoch 7/20  Iteration 11744/35720 Training loss: 0.9520 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 11745/35720 Training loss: 0.9519 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 11746/35720 Training loss: 0.9519 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 11747/35720 Training loss: 0.9519 0.2197 sec/batch\n",
      "Epoch 7/20  Iteration 11748/35720 Training loss: 0.9519 0.2418 sec/batch\n",
      "Epoch 7/20  Iteration 11749/35720 Training loss: 0.9519 0.2179 sec/batch\n",
      "Epoch 7/20  Iteration 11750/35720 Training loss: 0.9519 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11751/35720 Training loss: 0.9519 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 11752/35720 Training loss: 0.9519 0.2216 sec/batch\n",
      "Epoch 7/20  Iteration 11753/35720 Training loss: 0.9520 0.2221 sec/batch\n",
      "Epoch 7/20  Iteration 11754/35720 Training loss: 0.9520 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 11755/35720 Training loss: 0.9519 0.2220 sec/batch\n",
      "Epoch 7/20  Iteration 11756/35720 Training loss: 0.9519 0.2061 sec/batch\n",
      "Epoch 7/20  Iteration 11757/35720 Training loss: 0.9520 0.2080 sec/batch\n",
      "Epoch 7/20  Iteration 11758/35720 Training loss: 0.9520 0.2175 sec/batch\n",
      "Epoch 7/20  Iteration 11759/35720 Training loss: 0.9519 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 11760/35720 Training loss: 0.9520 0.2145 sec/batch\n",
      "Epoch 7/20  Iteration 11761/35720 Training loss: 0.9520 0.2170 sec/batch\n",
      "Epoch 7/20  Iteration 11762/35720 Training loss: 0.9520 0.2265 sec/batch\n",
      "Epoch 7/20  Iteration 11763/35720 Training loss: 0.9519 0.2299 sec/batch\n",
      "Epoch 7/20  Iteration 11764/35720 Training loss: 0.9519 0.2089 sec/batch\n",
      "Epoch 7/20  Iteration 11765/35720 Training loss: 0.9519 0.2270 sec/batch\n",
      "Epoch 7/20  Iteration 11766/35720 Training loss: 0.9519 0.2225 sec/batch\n",
      "Epoch 7/20  Iteration 11767/35720 Training loss: 0.9518 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 11768/35720 Training loss: 0.9519 0.2338 sec/batch\n",
      "Epoch 7/20  Iteration 11769/35720 Training loss: 0.9520 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11770/35720 Training loss: 0.9520 0.2218 sec/batch\n",
      "Epoch 7/20  Iteration 11771/35720 Training loss: 0.9520 0.2073 sec/batch\n",
      "Epoch 7/20  Iteration 11772/35720 Training loss: 0.9520 0.2134 sec/batch\n",
      "Epoch 7/20  Iteration 11773/35720 Training loss: 0.9520 0.2072 sec/batch\n",
      "Epoch 7/20  Iteration 11774/35720 Training loss: 0.9520 0.2347 sec/batch\n",
      "Epoch 7/20  Iteration 11775/35720 Training loss: 0.9520 0.2096 sec/batch\n",
      "Epoch 7/20  Iteration 11776/35720 Training loss: 0.9519 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 11777/35720 Training loss: 0.9520 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 11778/35720 Training loss: 0.9520 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 11779/35720 Training loss: 0.9521 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 11780/35720 Training loss: 0.9520 0.2253 sec/batch\n",
      "Epoch 7/20  Iteration 11781/35720 Training loss: 0.9520 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 11782/35720 Training loss: 0.9519 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 11783/35720 Training loss: 0.9519 0.2155 sec/batch\n",
      "Epoch 7/20  Iteration 11784/35720 Training loss: 0.9520 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 11785/35720 Training loss: 0.9520 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 11786/35720 Training loss: 0.9520 0.2288 sec/batch\n",
      "Epoch 7/20  Iteration 11787/35720 Training loss: 0.9519 0.2212 sec/batch\n",
      "Epoch 7/20  Iteration 11788/35720 Training loss: 0.9519 0.2127 sec/batch\n",
      "Epoch 7/20  Iteration 11789/35720 Training loss: 0.9519 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 11790/35720 Training loss: 0.9519 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 11791/35720 Training loss: 0.9519 0.2244 sec/batch\n",
      "Epoch 7/20  Iteration 11792/35720 Training loss: 0.9519 0.2155 sec/batch\n",
      "Epoch 7/20  Iteration 11793/35720 Training loss: 0.9519 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 11794/35720 Training loss: 0.9519 0.2260 sec/batch\n",
      "Epoch 7/20  Iteration 11795/35720 Training loss: 0.9519 0.2085 sec/batch\n",
      "Epoch 7/20  Iteration 11796/35720 Training loss: 0.9519 0.2217 sec/batch\n",
      "Epoch 7/20  Iteration 11797/35720 Training loss: 0.9518 0.2073 sec/batch\n",
      "Epoch 7/20  Iteration 11798/35720 Training loss: 0.9518 0.2172 sec/batch\n",
      "Epoch 7/20  Iteration 11799/35720 Training loss: 0.9518 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 11800/35720 Training loss: 0.9517 0.2117 sec/batch\n",
      "Validation loss: 1.33115 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 11801/35720 Training loss: 0.9520 0.2122 sec/batch\n",
      "Epoch 7/20  Iteration 11802/35720 Training loss: 0.9520 0.2144 sec/batch\n",
      "Epoch 7/20  Iteration 11803/35720 Training loss: 0.9520 0.2179 sec/batch\n",
      "Epoch 7/20  Iteration 11804/35720 Training loss: 0.9521 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 11805/35720 Training loss: 0.9520 0.2152 sec/batch\n",
      "Epoch 7/20  Iteration 11806/35720 Training loss: 0.9520 0.2086 sec/batch\n",
      "Epoch 7/20  Iteration 11807/35720 Training loss: 0.9520 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 11808/35720 Training loss: 0.9520 0.2144 sec/batch\n",
      "Epoch 7/20  Iteration 11809/35720 Training loss: 0.9520 0.2210 sec/batch\n",
      "Epoch 7/20  Iteration 11810/35720 Training loss: 0.9520 0.2119 sec/batch\n",
      "Epoch 7/20  Iteration 11811/35720 Training loss: 0.9520 0.2103 sec/batch\n",
      "Epoch 7/20  Iteration 11812/35720 Training loss: 0.9519 0.2192 sec/batch\n",
      "Epoch 7/20  Iteration 11813/35720 Training loss: 0.9519 0.2373 sec/batch\n",
      "Epoch 7/20  Iteration 11814/35720 Training loss: 0.9519 0.2304 sec/batch\n",
      "Epoch 7/20  Iteration 11815/35720 Training loss: 0.9519 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 11816/35720 Training loss: 0.9519 0.2226 sec/batch\n",
      "Epoch 7/20  Iteration 11817/35720 Training loss: 0.9520 0.2093 sec/batch\n",
      "Epoch 7/20  Iteration 11818/35720 Training loss: 0.9519 0.2231 sec/batch\n",
      "Epoch 7/20  Iteration 11819/35720 Training loss: 0.9520 0.2255 sec/batch\n",
      "Epoch 7/20  Iteration 11820/35720 Training loss: 0.9520 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 11821/35720 Training loss: 0.9520 0.2077 sec/batch\n",
      "Epoch 7/20  Iteration 11822/35720 Training loss: 0.9519 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 11823/35720 Training loss: 0.9519 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 11824/35720 Training loss: 0.9520 0.2290 sec/batch\n",
      "Epoch 7/20  Iteration 11825/35720 Training loss: 0.9520 0.2273 sec/batch\n",
      "Epoch 7/20  Iteration 11826/35720 Training loss: 0.9520 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 11827/35720 Training loss: 0.9519 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 11828/35720 Training loss: 0.9518 0.2086 sec/batch\n",
      "Epoch 7/20  Iteration 11829/35720 Training loss: 0.9518 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 11830/35720 Training loss: 0.9518 0.2181 sec/batch\n",
      "Epoch 7/20  Iteration 11831/35720 Training loss: 0.9518 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 11832/35720 Training loss: 0.9518 0.2133 sec/batch\n",
      "Epoch 7/20  Iteration 11833/35720 Training loss: 0.9518 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11834/35720 Training loss: 0.9517 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 11835/35720 Training loss: 0.9517 0.2229 sec/batch\n",
      "Epoch 7/20  Iteration 11836/35720 Training loss: 0.9518 0.2232 sec/batch\n",
      "Epoch 7/20  Iteration 11837/35720 Training loss: 0.9518 0.2313 sec/batch\n",
      "Epoch 7/20  Iteration 11838/35720 Training loss: 0.9519 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 11839/35720 Training loss: 0.9520 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 11840/35720 Training loss: 0.9520 0.2192 sec/batch\n",
      "Epoch 7/20  Iteration 11841/35720 Training loss: 0.9520 0.2188 sec/batch\n",
      "Epoch 7/20  Iteration 11842/35720 Training loss: 0.9519 0.2207 sec/batch\n",
      "Epoch 7/20  Iteration 11843/35720 Training loss: 0.9519 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 11844/35720 Training loss: 0.9519 0.2193 sec/batch\n",
      "Epoch 7/20  Iteration 11845/35720 Training loss: 0.9518 0.2240 sec/batch\n",
      "Epoch 7/20  Iteration 11846/35720 Training loss: 0.9518 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 11847/35720 Training loss: 0.9518 0.2269 sec/batch\n",
      "Epoch 7/20  Iteration 11848/35720 Training loss: 0.9517 0.2161 sec/batch\n",
      "Epoch 7/20  Iteration 11849/35720 Training loss: 0.9517 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 11850/35720 Training loss: 0.9518 0.2137 sec/batch\n",
      "Epoch 7/20  Iteration 11851/35720 Training loss: 0.9517 0.2126 sec/batch\n",
      "Epoch 7/20  Iteration 11852/35720 Training loss: 0.9517 0.2175 sec/batch\n",
      "Epoch 7/20  Iteration 11853/35720 Training loss: 0.9516 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 11854/35720 Training loss: 0.9515 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11855/35720 Training loss: 0.9515 0.2267 sec/batch\n",
      "Epoch 7/20  Iteration 11856/35720 Training loss: 0.9515 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 11857/35720 Training loss: 0.9515 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 11858/35720 Training loss: 0.9514 0.2176 sec/batch\n",
      "Epoch 7/20  Iteration 11859/35720 Training loss: 0.9513 0.2257 sec/batch\n",
      "Epoch 7/20  Iteration 11860/35720 Training loss: 0.9513 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 11861/35720 Training loss: 0.9513 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 11862/35720 Training loss: 0.9512 0.2166 sec/batch\n",
      "Epoch 7/20  Iteration 11863/35720 Training loss: 0.9512 0.2217 sec/batch\n",
      "Epoch 7/20  Iteration 11864/35720 Training loss: 0.9511 0.2179 sec/batch\n",
      "Epoch 7/20  Iteration 11865/35720 Training loss: 0.9511 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 11866/35720 Training loss: 0.9510 0.2074 sec/batch\n",
      "Epoch 7/20  Iteration 11867/35720 Training loss: 0.9511 0.2240 sec/batch\n",
      "Epoch 7/20  Iteration 11868/35720 Training loss: 0.9511 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 11869/35720 Training loss: 0.9511 0.2220 sec/batch\n",
      "Epoch 7/20  Iteration 11870/35720 Training loss: 0.9511 0.2269 sec/batch\n",
      "Epoch 7/20  Iteration 11871/35720 Training loss: 0.9510 0.2059 sec/batch\n",
      "Epoch 7/20  Iteration 11872/35720 Training loss: 0.9510 0.2197 sec/batch\n",
      "Epoch 7/20  Iteration 11873/35720 Training loss: 0.9510 0.2263 sec/batch\n",
      "Epoch 7/20  Iteration 11874/35720 Training loss: 0.9509 0.2278 sec/batch\n",
      "Epoch 7/20  Iteration 11875/35720 Training loss: 0.9509 0.2236 sec/batch\n",
      "Epoch 7/20  Iteration 11876/35720 Training loss: 0.9509 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 11877/35720 Training loss: 0.9508 0.2252 sec/batch\n",
      "Epoch 7/20  Iteration 11878/35720 Training loss: 0.9508 0.2271 sec/batch\n",
      "Epoch 7/20  Iteration 11879/35720 Training loss: 0.9509 0.2260 sec/batch\n",
      "Epoch 7/20  Iteration 11880/35720 Training loss: 0.9509 0.2250 sec/batch\n",
      "Epoch 7/20  Iteration 11881/35720 Training loss: 0.9508 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 11882/35720 Training loss: 0.9508 0.2131 sec/batch\n",
      "Epoch 7/20  Iteration 11883/35720 Training loss: 0.9507 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 11884/35720 Training loss: 0.9508 0.2248 sec/batch\n",
      "Epoch 7/20  Iteration 11885/35720 Training loss: 0.9508 0.2079 sec/batch\n",
      "Epoch 7/20  Iteration 11886/35720 Training loss: 0.9509 0.2223 sec/batch\n",
      "Epoch 7/20  Iteration 11887/35720 Training loss: 0.9508 0.2178 sec/batch\n",
      "Epoch 7/20  Iteration 11888/35720 Training loss: 0.9508 0.2246 sec/batch\n",
      "Epoch 7/20  Iteration 11889/35720 Training loss: 0.9508 0.2404 sec/batch\n",
      "Epoch 7/20  Iteration 11890/35720 Training loss: 0.9507 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 11891/35720 Training loss: 0.9508 0.2079 sec/batch\n",
      "Epoch 7/20  Iteration 11892/35720 Training loss: 0.9508 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 11893/35720 Training loss: 0.9508 0.2066 sec/batch\n",
      "Epoch 7/20  Iteration 11894/35720 Training loss: 0.9509 0.2096 sec/batch\n",
      "Epoch 7/20  Iteration 11895/35720 Training loss: 0.9508 0.2161 sec/batch\n",
      "Epoch 7/20  Iteration 11896/35720 Training loss: 0.9508 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 11897/35720 Training loss: 0.9509 0.2204 sec/batch\n",
      "Epoch 7/20  Iteration 11898/35720 Training loss: 0.9509 0.2161 sec/batch\n",
      "Epoch 7/20  Iteration 11899/35720 Training loss: 0.9508 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 11900/35720 Training loss: 0.9507 0.2308 sec/batch\n",
      "Epoch 7/20  Iteration 11901/35720 Training loss: 0.9507 0.2191 sec/batch\n",
      "Epoch 7/20  Iteration 11902/35720 Training loss: 0.9506 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 11903/35720 Training loss: 0.9506 0.2132 sec/batch\n",
      "Epoch 7/20  Iteration 11904/35720 Training loss: 0.9506 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 11905/35720 Training loss: 0.9505 0.2138 sec/batch\n",
      "Epoch 7/20  Iteration 11906/35720 Training loss: 0.9505 0.2274 sec/batch\n",
      "Epoch 7/20  Iteration 11907/35720 Training loss: 0.9506 0.2282 sec/batch\n",
      "Epoch 7/20  Iteration 11908/35720 Training loss: 0.9506 0.2292 sec/batch\n",
      "Epoch 7/20  Iteration 11909/35720 Training loss: 0.9505 0.2072 sec/batch\n",
      "Epoch 7/20  Iteration 11910/35720 Training loss: 0.9505 0.2192 sec/batch\n",
      "Epoch 7/20  Iteration 11911/35720 Training loss: 0.9506 0.2151 sec/batch\n",
      "Epoch 7/20  Iteration 11912/35720 Training loss: 0.9505 0.2093 sec/batch\n",
      "Epoch 7/20  Iteration 11913/35720 Training loss: 0.9506 0.2322 sec/batch\n",
      "Epoch 7/20  Iteration 11914/35720 Training loss: 0.9506 0.2069 sec/batch\n",
      "Epoch 7/20  Iteration 11915/35720 Training loss: 0.9506 0.2057 sec/batch\n",
      "Epoch 7/20  Iteration 11916/35720 Training loss: 0.9505 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11917/35720 Training loss: 0.9505 0.2178 sec/batch\n",
      "Epoch 7/20  Iteration 11918/35720 Training loss: 0.9504 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 11919/35720 Training loss: 0.9503 0.2076 sec/batch\n",
      "Epoch 7/20  Iteration 11920/35720 Training loss: 0.9503 0.2325 sec/batch\n",
      "Epoch 7/20  Iteration 11921/35720 Training loss: 0.9502 0.2082 sec/batch\n",
      "Epoch 7/20  Iteration 11922/35720 Training loss: 0.9502 0.2137 sec/batch\n",
      "Epoch 7/20  Iteration 11923/35720 Training loss: 0.9502 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 11924/35720 Training loss: 0.9502 0.2271 sec/batch\n",
      "Epoch 7/20  Iteration 11925/35720 Training loss: 0.9502 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 11926/35720 Training loss: 0.9502 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 11927/35720 Training loss: 0.9501 0.2119 sec/batch\n",
      "Epoch 7/20  Iteration 11928/35720 Training loss: 0.9501 0.2287 sec/batch\n",
      "Epoch 7/20  Iteration 11929/35720 Training loss: 0.9501 0.2329 sec/batch\n",
      "Epoch 7/20  Iteration 11930/35720 Training loss: 0.9501 0.2289 sec/batch\n",
      "Epoch 7/20  Iteration 11931/35720 Training loss: 0.9501 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 11932/35720 Training loss: 0.9501 0.2078 sec/batch\n",
      "Epoch 7/20  Iteration 11933/35720 Training loss: 0.9500 0.2197 sec/batch\n",
      "Epoch 7/20  Iteration 11934/35720 Training loss: 0.9500 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 11935/35720 Training loss: 0.9500 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 11936/35720 Training loss: 0.9499 0.2203 sec/batch\n",
      "Epoch 7/20  Iteration 11937/35720 Training loss: 0.9499 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 11938/35720 Training loss: 0.9499 0.2164 sec/batch\n",
      "Epoch 7/20  Iteration 11939/35720 Training loss: 0.9499 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 11940/35720 Training loss: 0.9499 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 11941/35720 Training loss: 0.9498 0.2207 sec/batch\n",
      "Epoch 7/20  Iteration 11942/35720 Training loss: 0.9498 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 11943/35720 Training loss: 0.9499 0.2134 sec/batch\n",
      "Epoch 7/20  Iteration 11944/35720 Training loss: 0.9498 0.2121 sec/batch\n",
      "Epoch 7/20  Iteration 11945/35720 Training loss: 0.9499 0.2176 sec/batch\n",
      "Epoch 7/20  Iteration 11946/35720 Training loss: 0.9499 0.2228 sec/batch\n",
      "Epoch 7/20  Iteration 11947/35720 Training loss: 0.9499 0.2205 sec/batch\n",
      "Epoch 7/20  Iteration 11948/35720 Training loss: 0.9499 0.2078 sec/batch\n",
      "Epoch 7/20  Iteration 11949/35720 Training loss: 0.9498 0.2272 sec/batch\n",
      "Epoch 7/20  Iteration 11950/35720 Training loss: 0.9498 0.2225 sec/batch\n",
      "Epoch 7/20  Iteration 11951/35720 Training loss: 0.9498 0.2086 sec/batch\n",
      "Epoch 7/20  Iteration 11952/35720 Training loss: 0.9498 0.2214 sec/batch\n",
      "Epoch 7/20  Iteration 11953/35720 Training loss: 0.9497 0.2214 sec/batch\n",
      "Epoch 7/20  Iteration 11954/35720 Training loss: 0.9497 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 11955/35720 Training loss: 0.9496 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 11956/35720 Training loss: 0.9495 0.2144 sec/batch\n",
      "Epoch 7/20  Iteration 11957/35720 Training loss: 0.9495 0.2270 sec/batch\n",
      "Epoch 7/20  Iteration 11958/35720 Training loss: 0.9495 0.2136 sec/batch\n",
      "Epoch 7/20  Iteration 11959/35720 Training loss: 0.9494 0.2068 sec/batch\n",
      "Epoch 7/20  Iteration 11960/35720 Training loss: 0.9494 0.2154 sec/batch\n",
      "Epoch 7/20  Iteration 11961/35720 Training loss: 0.9493 0.2221 sec/batch\n",
      "Epoch 7/20  Iteration 11962/35720 Training loss: 0.9493 0.2144 sec/batch\n",
      "Epoch 7/20  Iteration 11963/35720 Training loss: 0.9492 0.2191 sec/batch\n",
      "Epoch 7/20  Iteration 11964/35720 Training loss: 0.9492 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 11965/35720 Training loss: 0.9491 0.2112 sec/batch\n",
      "Epoch 7/20  Iteration 11966/35720 Training loss: 0.9491 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 11967/35720 Training loss: 0.9490 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 11968/35720 Training loss: 0.9490 0.2385 sec/batch\n",
      "Epoch 7/20  Iteration 11969/35720 Training loss: 0.9490 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 11970/35720 Training loss: 0.9490 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 11971/35720 Training loss: 0.9489 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 11972/35720 Training loss: 0.9489 0.2286 sec/batch\n",
      "Epoch 7/20  Iteration 11973/35720 Training loss: 0.9488 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 11974/35720 Training loss: 0.9488 0.2231 sec/batch\n",
      "Epoch 7/20  Iteration 11975/35720 Training loss: 0.9488 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 11976/35720 Training loss: 0.9488 0.2101 sec/batch\n",
      "Epoch 7/20  Iteration 11977/35720 Training loss: 0.9487 0.2253 sec/batch\n",
      "Epoch 7/20  Iteration 11978/35720 Training loss: 0.9487 0.2268 sec/batch\n",
      "Epoch 7/20  Iteration 11979/35720 Training loss: 0.9486 0.2124 sec/batch\n",
      "Epoch 7/20  Iteration 11980/35720 Training loss: 0.9486 0.2243 sec/batch\n",
      "Epoch 7/20  Iteration 11981/35720 Training loss: 0.9485 0.2247 sec/batch\n",
      "Epoch 7/20  Iteration 11982/35720 Training loss: 0.9485 0.2121 sec/batch\n",
      "Epoch 7/20  Iteration 11983/35720 Training loss: 0.9484 0.2236 sec/batch\n",
      "Epoch 7/20  Iteration 11984/35720 Training loss: 0.9484 0.2126 sec/batch\n",
      "Epoch 7/20  Iteration 11985/35720 Training loss: 0.9484 0.2089 sec/batch\n",
      "Epoch 7/20  Iteration 11986/35720 Training loss: 0.9484 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 11987/35720 Training loss: 0.9483 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 11988/35720 Training loss: 0.9483 0.2249 sec/batch\n",
      "Epoch 7/20  Iteration 11989/35720 Training loss: 0.9482 0.2143 sec/batch\n",
      "Epoch 7/20  Iteration 11990/35720 Training loss: 0.9482 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 11991/35720 Training loss: 0.9482 0.2157 sec/batch\n",
      "Epoch 7/20  Iteration 11992/35720 Training loss: 0.9482 0.2075 sec/batch\n",
      "Epoch 7/20  Iteration 11993/35720 Training loss: 0.9481 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 11994/35720 Training loss: 0.9480 0.2172 sec/batch\n",
      "Epoch 7/20  Iteration 11995/35720 Training loss: 0.9480 0.2081 sec/batch\n",
      "Epoch 7/20  Iteration 11996/35720 Training loss: 0.9479 0.2261 sec/batch\n",
      "Epoch 7/20  Iteration 11997/35720 Training loss: 0.9479 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 11998/35720 Training loss: 0.9479 0.2053 sec/batch\n",
      "Epoch 7/20  Iteration 11999/35720 Training loss: 0.9478 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 12000/35720 Training loss: 0.9478 0.2200 sec/batch\n",
      "Validation loss: 1.33473 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 12001/35720 Training loss: 0.9479 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 12002/35720 Training loss: 0.9478 0.2062 sec/batch\n",
      "Epoch 7/20  Iteration 12003/35720 Training loss: 0.9478 0.2062 sec/batch\n",
      "Epoch 7/20  Iteration 12004/35720 Training loss: 0.9477 0.2148 sec/batch\n",
      "Epoch 7/20  Iteration 12005/35720 Training loss: 0.9477 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 12006/35720 Training loss: 0.9478 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 12007/35720 Training loss: 0.9477 0.2183 sec/batch\n",
      "Epoch 7/20  Iteration 12008/35720 Training loss: 0.9477 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 12009/35720 Training loss: 0.9477 0.2249 sec/batch\n",
      "Epoch 7/20  Iteration 12010/35720 Training loss: 0.9478 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 12011/35720 Training loss: 0.9478 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 12012/35720 Training loss: 0.9477 0.2219 sec/batch\n",
      "Epoch 7/20  Iteration 12013/35720 Training loss: 0.9478 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 12014/35720 Training loss: 0.9477 0.2082 sec/batch\n",
      "Epoch 7/20  Iteration 12015/35720 Training loss: 0.9477 0.2142 sec/batch\n",
      "Epoch 7/20  Iteration 12016/35720 Training loss: 0.9476 0.2259 sec/batch\n",
      "Epoch 7/20  Iteration 12017/35720 Training loss: 0.9475 0.2400 sec/batch\n",
      "Epoch 7/20  Iteration 12018/35720 Training loss: 0.9476 0.2263 sec/batch\n",
      "Epoch 7/20  Iteration 12019/35720 Training loss: 0.9475 0.2118 sec/batch\n",
      "Epoch 7/20  Iteration 12020/35720 Training loss: 0.9475 0.2114 sec/batch\n",
      "Epoch 7/20  Iteration 12021/35720 Training loss: 0.9475 0.2248 sec/batch\n",
      "Epoch 7/20  Iteration 12022/35720 Training loss: 0.9474 0.2320 sec/batch\n",
      "Epoch 7/20  Iteration 12023/35720 Training loss: 0.9474 0.2212 sec/batch\n",
      "Epoch 7/20  Iteration 12024/35720 Training loss: 0.9474 0.2271 sec/batch\n",
      "Epoch 7/20  Iteration 12025/35720 Training loss: 0.9474 0.2112 sec/batch\n",
      "Epoch 7/20  Iteration 12026/35720 Training loss: 0.9473 0.2122 sec/batch\n",
      "Epoch 7/20  Iteration 12027/35720 Training loss: 0.9473 0.2322 sec/batch\n",
      "Epoch 7/20  Iteration 12028/35720 Training loss: 0.9473 0.2214 sec/batch\n",
      "Epoch 7/20  Iteration 12029/35720 Training loss: 0.9473 0.2273 sec/batch\n",
      "Epoch 7/20  Iteration 12030/35720 Training loss: 0.9473 0.2062 sec/batch\n",
      "Epoch 7/20  Iteration 12031/35720 Training loss: 0.9472 0.2098 sec/batch\n",
      "Epoch 7/20  Iteration 12032/35720 Training loss: 0.9472 0.2253 sec/batch\n",
      "Epoch 7/20  Iteration 12033/35720 Training loss: 0.9472 0.2282 sec/batch\n",
      "Epoch 7/20  Iteration 12034/35720 Training loss: 0.9472 0.2335 sec/batch\n",
      "Epoch 7/20  Iteration 12035/35720 Training loss: 0.9472 0.2159 sec/batch\n",
      "Epoch 7/20  Iteration 12036/35720 Training loss: 0.9471 0.2199 sec/batch\n",
      "Epoch 7/20  Iteration 12037/35720 Training loss: 0.9471 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 12038/35720 Training loss: 0.9471 0.2257 sec/batch\n",
      "Epoch 7/20  Iteration 12039/35720 Training loss: 0.9471 0.2275 sec/batch\n",
      "Epoch 7/20  Iteration 12040/35720 Training loss: 0.9471 0.2233 sec/batch\n",
      "Epoch 7/20  Iteration 12041/35720 Training loss: 0.9471 0.2135 sec/batch\n",
      "Epoch 7/20  Iteration 12042/35720 Training loss: 0.9472 0.2216 sec/batch\n",
      "Epoch 7/20  Iteration 12043/35720 Training loss: 0.9472 0.2177 sec/batch\n",
      "Epoch 7/20  Iteration 12044/35720 Training loss: 0.9471 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 12045/35720 Training loss: 0.9471 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 12046/35720 Training loss: 0.9470 0.2059 sec/batch\n",
      "Epoch 7/20  Iteration 12047/35720 Training loss: 0.9470 0.2071 sec/batch\n",
      "Epoch 7/20  Iteration 12048/35720 Training loss: 0.9470 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 12049/35720 Training loss: 0.9470 0.2334 sec/batch\n",
      "Epoch 7/20  Iteration 12050/35720 Training loss: 0.9470 0.2278 sec/batch\n",
      "Epoch 7/20  Iteration 12051/35720 Training loss: 0.9470 0.2244 sec/batch\n",
      "Epoch 7/20  Iteration 12052/35720 Training loss: 0.9471 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 12053/35720 Training loss: 0.9471 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 12054/35720 Training loss: 0.9471 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 12055/35720 Training loss: 0.9471 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 12056/35720 Training loss: 0.9471 0.2186 sec/batch\n",
      "Epoch 7/20  Iteration 12057/35720 Training loss: 0.9471 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 12058/35720 Training loss: 0.9471 0.2056 sec/batch\n",
      "Epoch 7/20  Iteration 12059/35720 Training loss: 0.9470 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 12060/35720 Training loss: 0.9470 0.2205 sec/batch\n",
      "Epoch 7/20  Iteration 12061/35720 Training loss: 0.9470 0.2356 sec/batch\n",
      "Epoch 7/20  Iteration 12062/35720 Training loss: 0.9470 0.2257 sec/batch\n",
      "Epoch 7/20  Iteration 12063/35720 Training loss: 0.9470 0.2116 sec/batch\n",
      "Epoch 7/20  Iteration 12064/35720 Training loss: 0.9470 0.2113 sec/batch\n",
      "Epoch 7/20  Iteration 12065/35720 Training loss: 0.9470 0.2254 sec/batch\n",
      "Epoch 7/20  Iteration 12066/35720 Training loss: 0.9471 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 12067/35720 Training loss: 0.9471 0.2246 sec/batch\n",
      "Epoch 7/20  Iteration 12068/35720 Training loss: 0.9472 0.2082 sec/batch\n",
      "Epoch 7/20  Iteration 12069/35720 Training loss: 0.9473 0.2060 sec/batch\n",
      "Epoch 7/20  Iteration 12070/35720 Training loss: 0.9472 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 12071/35720 Training loss: 0.9471 0.2279 sec/batch\n",
      "Epoch 7/20  Iteration 12072/35720 Training loss: 0.9471 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 12073/35720 Training loss: 0.9471 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 12074/35720 Training loss: 0.9471 0.2223 sec/batch\n",
      "Epoch 7/20  Iteration 12075/35720 Training loss: 0.9471 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 12076/35720 Training loss: 0.9470 0.2282 sec/batch\n",
      "Epoch 7/20  Iteration 12077/35720 Training loss: 0.9470 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 12078/35720 Training loss: 0.9470 0.2182 sec/batch\n",
      "Epoch 7/20  Iteration 12079/35720 Training loss: 0.9470 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 12080/35720 Training loss: 0.9469 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 12081/35720 Training loss: 0.9469 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 12082/35720 Training loss: 0.9469 0.2252 sec/batch\n",
      "Epoch 7/20  Iteration 12083/35720 Training loss: 0.9469 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 12084/35720 Training loss: 0.9469 0.2181 sec/batch\n",
      "Epoch 7/20  Iteration 12085/35720 Training loss: 0.9469 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 12086/35720 Training loss: 0.9470 0.2085 sec/batch\n",
      "Epoch 7/20  Iteration 12087/35720 Training loss: 0.9470 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 12088/35720 Training loss: 0.9470 0.2151 sec/batch\n",
      "Epoch 7/20  Iteration 12089/35720 Training loss: 0.9470 0.2166 sec/batch\n",
      "Epoch 7/20  Iteration 12090/35720 Training loss: 0.9469 0.2147 sec/batch\n",
      "Epoch 7/20  Iteration 12091/35720 Training loss: 0.9469 0.2101 sec/batch\n",
      "Epoch 7/20  Iteration 12092/35720 Training loss: 0.9469 0.2307 sec/batch\n",
      "Epoch 7/20  Iteration 12093/35720 Training loss: 0.9469 0.2159 sec/batch\n",
      "Epoch 7/20  Iteration 12094/35720 Training loss: 0.9469 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 12095/35720 Training loss: 0.9469 0.2242 sec/batch\n",
      "Epoch 7/20  Iteration 12096/35720 Training loss: 0.9469 0.2166 sec/batch\n",
      "Epoch 7/20  Iteration 12097/35720 Training loss: 0.9469 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 12098/35720 Training loss: 0.9469 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 12099/35720 Training loss: 0.9469 0.2156 sec/batch\n",
      "Epoch 7/20  Iteration 12100/35720 Training loss: 0.9468 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 12101/35720 Training loss: 0.9469 0.2233 sec/batch\n",
      "Epoch 7/20  Iteration 12102/35720 Training loss: 0.9469 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 12103/35720 Training loss: 0.9469 0.2284 sec/batch\n",
      "Epoch 7/20  Iteration 12104/35720 Training loss: 0.9469 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 12105/35720 Training loss: 0.9468 0.2156 sec/batch\n",
      "Epoch 7/20  Iteration 12106/35720 Training loss: 0.9468 0.2395 sec/batch\n",
      "Epoch 7/20  Iteration 12107/35720 Training loss: 0.9468 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 12108/35720 Training loss: 0.9467 0.2112 sec/batch\n",
      "Epoch 7/20  Iteration 12109/35720 Training loss: 0.9467 0.2773 sec/batch\n",
      "Epoch 7/20  Iteration 12110/35720 Training loss: 0.9466 0.3569 sec/batch\n",
      "Epoch 7/20  Iteration 12111/35720 Training loss: 0.9466 0.3010 sec/batch\n",
      "Epoch 7/20  Iteration 12112/35720 Training loss: 0.9465 0.2213 sec/batch\n",
      "Epoch 7/20  Iteration 12113/35720 Training loss: 0.9465 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 12114/35720 Training loss: 0.9464 0.2262 sec/batch\n",
      "Epoch 7/20  Iteration 12115/35720 Training loss: 0.9464 0.2196 sec/batch\n",
      "Epoch 7/20  Iteration 12116/35720 Training loss: 0.9463 0.2201 sec/batch\n",
      "Epoch 7/20  Iteration 12117/35720 Training loss: 0.9463 0.2112 sec/batch\n",
      "Epoch 7/20  Iteration 12118/35720 Training loss: 0.9463 0.2194 sec/batch\n",
      "Epoch 7/20  Iteration 12119/35720 Training loss: 0.9463 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 12120/35720 Training loss: 0.9463 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 12121/35720 Training loss: 0.9462 0.2075 sec/batch\n",
      "Epoch 7/20  Iteration 12122/35720 Training loss: 0.9462 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 12123/35720 Training loss: 0.9462 0.2077 sec/batch\n",
      "Epoch 7/20  Iteration 12124/35720 Training loss: 0.9462 0.2171 sec/batch\n",
      "Epoch 7/20  Iteration 12125/35720 Training loss: 0.9462 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 12126/35720 Training loss: 0.9462 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 12127/35720 Training loss: 0.9462 0.2165 sec/batch\n",
      "Epoch 7/20  Iteration 12128/35720 Training loss: 0.9462 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 12129/35720 Training loss: 0.9462 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 12130/35720 Training loss: 0.9462 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 12131/35720 Training loss: 0.9462 0.2118 sec/batch\n",
      "Epoch 7/20  Iteration 12132/35720 Training loss: 0.9461 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 12133/35720 Training loss: 0.9461 0.2190 sec/batch\n",
      "Epoch 7/20  Iteration 12134/35720 Training loss: 0.9461 0.2062 sec/batch\n",
      "Epoch 7/20  Iteration 12135/35720 Training loss: 0.9461 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 12136/35720 Training loss: 0.9462 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 12137/35720 Training loss: 0.9462 0.2270 sec/batch\n",
      "Epoch 7/20  Iteration 12138/35720 Training loss: 0.9461 0.2267 sec/batch\n",
      "Epoch 7/20  Iteration 12139/35720 Training loss: 0.9461 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 12140/35720 Training loss: 0.9461 0.2204 sec/batch\n",
      "Epoch 7/20  Iteration 12141/35720 Training loss: 0.9461 0.2148 sec/batch\n",
      "Epoch 7/20  Iteration 12142/35720 Training loss: 0.9461 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 12143/35720 Training loss: 0.9461 0.2298 sec/batch\n",
      "Epoch 7/20  Iteration 12144/35720 Training loss: 0.9462 0.2068 sec/batch\n",
      "Epoch 7/20  Iteration 12145/35720 Training loss: 0.9462 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 12146/35720 Training loss: 0.9461 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 12147/35720 Training loss: 0.9460 0.2190 sec/batch\n",
      "Epoch 7/20  Iteration 12148/35720 Training loss: 0.9460 0.2185 sec/batch\n",
      "Epoch 7/20  Iteration 12149/35720 Training loss: 0.9460 0.2077 sec/batch\n",
      "Epoch 7/20  Iteration 12150/35720 Training loss: 0.9460 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 12151/35720 Training loss: 0.9460 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 12152/35720 Training loss: 0.9459 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 12153/35720 Training loss: 0.9459 0.2171 sec/batch\n",
      "Epoch 7/20  Iteration 12154/35720 Training loss: 0.9459 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 12155/35720 Training loss: 0.9459 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 12156/35720 Training loss: 0.9459 0.2078 sec/batch\n",
      "Epoch 7/20  Iteration 12157/35720 Training loss: 0.9460 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 12158/35720 Training loss: 0.9459 0.2236 sec/batch\n",
      "Epoch 7/20  Iteration 12159/35720 Training loss: 0.9459 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 12160/35720 Training loss: 0.9459 0.2146 sec/batch\n",
      "Epoch 7/20  Iteration 12161/35720 Training loss: 0.9459 0.2265 sec/batch\n",
      "Epoch 7/20  Iteration 12162/35720 Training loss: 0.9459 0.2070 sec/batch\n",
      "Epoch 7/20  Iteration 12163/35720 Training loss: 0.9458 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 12164/35720 Training loss: 0.9458 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 12165/35720 Training loss: 0.9458 0.2241 sec/batch\n",
      "Epoch 7/20  Iteration 12166/35720 Training loss: 0.9457 0.2245 sec/batch\n",
      "Epoch 7/20  Iteration 12167/35720 Training loss: 0.9456 0.2086 sec/batch\n",
      "Epoch 7/20  Iteration 12168/35720 Training loss: 0.9456 0.2093 sec/batch\n",
      "Epoch 7/20  Iteration 12169/35720 Training loss: 0.9456 0.2175 sec/batch\n",
      "Epoch 7/20  Iteration 12170/35720 Training loss: 0.9456 0.2215 sec/batch\n",
      "Epoch 7/20  Iteration 12171/35720 Training loss: 0.9455 0.2211 sec/batch\n",
      "Epoch 7/20  Iteration 12172/35720 Training loss: 0.9456 0.2136 sec/batch\n",
      "Epoch 7/20  Iteration 12173/35720 Training loss: 0.9456 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 12174/35720 Training loss: 0.9456 0.2204 sec/batch\n",
      "Epoch 7/20  Iteration 12175/35720 Training loss: 0.9456 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 12176/35720 Training loss: 0.9457 0.2336 sec/batch\n",
      "Epoch 7/20  Iteration 12177/35720 Training loss: 0.9457 0.2286 sec/batch\n",
      "Epoch 7/20  Iteration 12178/35720 Training loss: 0.9457 0.2141 sec/batch\n",
      "Epoch 7/20  Iteration 12179/35720 Training loss: 0.9457 0.2216 sec/batch\n",
      "Epoch 7/20  Iteration 12180/35720 Training loss: 0.9457 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 12181/35720 Training loss: 0.9457 0.2097 sec/batch\n",
      "Epoch 7/20  Iteration 12182/35720 Training loss: 0.9457 0.2085 sec/batch\n",
      "Epoch 7/20  Iteration 12183/35720 Training loss: 0.9456 0.2183 sec/batch\n",
      "Epoch 7/20  Iteration 12184/35720 Training loss: 0.9456 0.2143 sec/batch\n",
      "Epoch 7/20  Iteration 12185/35720 Training loss: 0.9456 0.2093 sec/batch\n",
      "Epoch 7/20  Iteration 12186/35720 Training loss: 0.9456 0.2249 sec/batch\n",
      "Epoch 7/20  Iteration 12187/35720 Training loss: 0.9456 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 12188/35720 Training loss: 0.9455 0.2159 sec/batch\n",
      "Epoch 7/20  Iteration 12189/35720 Training loss: 0.9455 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 12190/35720 Training loss: 0.9454 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 12191/35720 Training loss: 0.9453 0.2346 sec/batch\n",
      "Epoch 7/20  Iteration 12192/35720 Training loss: 0.9452 0.2154 sec/batch\n",
      "Epoch 7/20  Iteration 12193/35720 Training loss: 0.9452 0.2172 sec/batch\n",
      "Epoch 7/20  Iteration 12194/35720 Training loss: 0.9451 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 12195/35720 Training loss: 0.9451 0.2107 sec/batch\n",
      "Epoch 7/20  Iteration 12196/35720 Training loss: 0.9451 0.2181 sec/batch\n",
      "Epoch 7/20  Iteration 12197/35720 Training loss: 0.9451 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 12198/35720 Training loss: 0.9450 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 12199/35720 Training loss: 0.9450 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 12200/35720 Training loss: 0.9449 0.2108 sec/batch\n",
      "Validation loss: 1.34527 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 12201/35720 Training loss: 0.9451 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 12202/35720 Training loss: 0.9450 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 12203/35720 Training loss: 0.9450 0.2371 sec/batch\n",
      "Epoch 7/20  Iteration 12204/35720 Training loss: 0.9450 0.2062 sec/batch\n",
      "Epoch 7/20  Iteration 12205/35720 Training loss: 0.9450 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 12206/35720 Training loss: 0.9449 0.2274 sec/batch\n",
      "Epoch 7/20  Iteration 12207/35720 Training loss: 0.9449 0.2295 sec/batch\n",
      "Epoch 7/20  Iteration 12208/35720 Training loss: 0.9448 0.2083 sec/batch\n",
      "Epoch 7/20  Iteration 12209/35720 Training loss: 0.9449 0.2197 sec/batch\n",
      "Epoch 7/20  Iteration 12210/35720 Training loss: 0.9449 0.2267 sec/batch\n",
      "Epoch 7/20  Iteration 12211/35720 Training loss: 0.9448 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 12212/35720 Training loss: 0.9448 0.2284 sec/batch\n",
      "Epoch 7/20  Iteration 12213/35720 Training loss: 0.9448 0.2077 sec/batch\n",
      "Epoch 7/20  Iteration 12214/35720 Training loss: 0.9448 0.2284 sec/batch\n",
      "Epoch 7/20  Iteration 12215/35720 Training loss: 0.9448 0.2196 sec/batch\n",
      "Epoch 7/20  Iteration 12216/35720 Training loss: 0.9447 0.2143 sec/batch\n",
      "Epoch 7/20  Iteration 12217/35720 Training loss: 0.9447 0.2213 sec/batch\n",
      "Epoch 7/20  Iteration 12218/35720 Training loss: 0.9447 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 12219/35720 Training loss: 0.9446 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 12220/35720 Training loss: 0.9446 0.2214 sec/batch\n",
      "Epoch 7/20  Iteration 12221/35720 Training loss: 0.9447 0.2136 sec/batch\n",
      "Epoch 7/20  Iteration 12222/35720 Training loss: 0.9447 0.2062 sec/batch\n",
      "Epoch 7/20  Iteration 12223/35720 Training loss: 0.9446 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 12224/35720 Training loss: 0.9446 0.2229 sec/batch\n",
      "Epoch 7/20  Iteration 12225/35720 Training loss: 0.9446 0.2218 sec/batch\n",
      "Epoch 7/20  Iteration 12226/35720 Training loss: 0.9446 0.2093 sec/batch\n",
      "Epoch 7/20  Iteration 12227/35720 Training loss: 0.9446 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 12228/35720 Training loss: 0.9446 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 12229/35720 Training loss: 0.9445 0.2224 sec/batch\n",
      "Epoch 7/20  Iteration 12230/35720 Training loss: 0.9446 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 12231/35720 Training loss: 0.9446 0.2198 sec/batch\n",
      "Epoch 7/20  Iteration 12232/35720 Training loss: 0.9446 0.2262 sec/batch\n",
      "Epoch 7/20  Iteration 12233/35720 Training loss: 0.9446 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 12234/35720 Training loss: 0.9446 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 12235/35720 Training loss: 0.9446 0.2280 sec/batch\n",
      "Epoch 7/20  Iteration 12236/35720 Training loss: 0.9446 0.2273 sec/batch\n",
      "Epoch 7/20  Iteration 12237/35720 Training loss: 0.9446 0.2284 sec/batch\n",
      "Epoch 7/20  Iteration 12238/35720 Training loss: 0.9446 0.2054 sec/batch\n",
      "Epoch 7/20  Iteration 12239/35720 Training loss: 0.9447 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 12240/35720 Training loss: 0.9447 0.2182 sec/batch\n",
      "Epoch 7/20  Iteration 12241/35720 Training loss: 0.9447 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 12242/35720 Training loss: 0.9447 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 12243/35720 Training loss: 0.9447 0.2183 sec/batch\n",
      "Epoch 7/20  Iteration 12244/35720 Training loss: 0.9447 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 12245/35720 Training loss: 0.9448 0.2135 sec/batch\n",
      "Epoch 7/20  Iteration 12246/35720 Training loss: 0.9448 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 12247/35720 Training loss: 0.9448 0.2158 sec/batch\n",
      "Epoch 7/20  Iteration 12248/35720 Training loss: 0.9448 0.2195 sec/batch\n",
      "Epoch 7/20  Iteration 12249/35720 Training loss: 0.9448 0.2061 sec/batch\n",
      "Epoch 7/20  Iteration 12250/35720 Training loss: 0.9448 0.2114 sec/batch\n",
      "Epoch 7/20  Iteration 12251/35720 Training loss: 0.9448 0.2129 sec/batch\n",
      "Epoch 7/20  Iteration 12252/35720 Training loss: 0.9448 0.2154 sec/batch\n",
      "Epoch 7/20  Iteration 12253/35720 Training loss: 0.9447 0.2174 sec/batch\n",
      "Epoch 7/20  Iteration 12254/35720 Training loss: 0.9447 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 12255/35720 Training loss: 0.9447 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 12256/35720 Training loss: 0.9446 0.2153 sec/batch\n",
      "Epoch 7/20  Iteration 12257/35720 Training loss: 0.9445 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 12258/35720 Training loss: 0.9446 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 12259/35720 Training loss: 0.9445 0.2224 sec/batch\n",
      "Epoch 7/20  Iteration 12260/35720 Training loss: 0.9445 0.2224 sec/batch\n",
      "Epoch 7/20  Iteration 12261/35720 Training loss: 0.9445 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 12262/35720 Training loss: 0.9444 0.2091 sec/batch\n",
      "Epoch 7/20  Iteration 12263/35720 Training loss: 0.9444 0.2234 sec/batch\n",
      "Epoch 7/20  Iteration 12264/35720 Training loss: 0.9444 0.2237 sec/batch\n",
      "Epoch 7/20  Iteration 12265/35720 Training loss: 0.9444 0.2199 sec/batch\n",
      "Epoch 7/20  Iteration 12266/35720 Training loss: 0.9444 0.2147 sec/batch\n",
      "Epoch 7/20  Iteration 12267/35720 Training loss: 0.9444 0.2332 sec/batch\n",
      "Epoch 7/20  Iteration 12268/35720 Training loss: 0.9443 0.2268 sec/batch\n",
      "Epoch 7/20  Iteration 12269/35720 Training loss: 0.9443 0.2098 sec/batch\n",
      "Epoch 7/20  Iteration 12270/35720 Training loss: 0.9443 0.2112 sec/batch\n",
      "Epoch 7/20  Iteration 12271/35720 Training loss: 0.9442 0.2275 sec/batch\n",
      "Epoch 7/20  Iteration 12272/35720 Training loss: 0.9442 0.2057 sec/batch\n",
      "Epoch 7/20  Iteration 12273/35720 Training loss: 0.9441 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 12274/35720 Training loss: 0.9441 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 12275/35720 Training loss: 0.9440 0.2287 sec/batch\n",
      "Epoch 7/20  Iteration 12276/35720 Training loss: 0.9440 0.2252 sec/batch\n",
      "Epoch 7/20  Iteration 12277/35720 Training loss: 0.9440 0.2176 sec/batch\n",
      "Epoch 7/20  Iteration 12278/35720 Training loss: 0.9439 0.2106 sec/batch\n",
      "Epoch 7/20  Iteration 12279/35720 Training loss: 0.9439 0.2310 sec/batch\n",
      "Epoch 7/20  Iteration 12280/35720 Training loss: 0.9439 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 12281/35720 Training loss: 0.9438 0.2283 sec/batch\n",
      "Epoch 7/20  Iteration 12282/35720 Training loss: 0.9438 0.2257 sec/batch\n",
      "Epoch 7/20  Iteration 12283/35720 Training loss: 0.9438 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 12284/35720 Training loss: 0.9438 0.2206 sec/batch\n",
      "Epoch 7/20  Iteration 12285/35720 Training loss: 0.9438 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 12286/35720 Training loss: 0.9438 0.2293 sec/batch\n",
      "Epoch 7/20  Iteration 12287/35720 Training loss: 0.9437 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 12288/35720 Training loss: 0.9437 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 12289/35720 Training loss: 0.9437 0.2215 sec/batch\n",
      "Epoch 7/20  Iteration 12290/35720 Training loss: 0.9436 0.2301 sec/batch\n",
      "Epoch 7/20  Iteration 12291/35720 Training loss: 0.9436 0.2128 sec/batch\n",
      "Epoch 7/20  Iteration 12292/35720 Training loss: 0.9436 0.2310 sec/batch\n",
      "Epoch 7/20  Iteration 12293/35720 Training loss: 0.9435 0.2288 sec/batch\n",
      "Epoch 7/20  Iteration 12294/35720 Training loss: 0.9435 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 12295/35720 Training loss: 0.9436 0.2259 sec/batch\n",
      "Epoch 7/20  Iteration 12296/35720 Training loss: 0.9435 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 12297/35720 Training loss: 0.9436 0.2227 sec/batch\n",
      "Epoch 7/20  Iteration 12298/35720 Training loss: 0.9435 0.2234 sec/batch\n",
      "Epoch 7/20  Iteration 12299/35720 Training loss: 0.9435 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 12300/35720 Training loss: 0.9434 0.2103 sec/batch\n",
      "Epoch 7/20  Iteration 12301/35720 Training loss: 0.9434 0.2299 sec/batch\n",
      "Epoch 7/20  Iteration 12302/35720 Training loss: 0.9434 0.2092 sec/batch\n",
      "Epoch 7/20  Iteration 12303/35720 Training loss: 0.9433 0.2148 sec/batch\n",
      "Epoch 7/20  Iteration 12304/35720 Training loss: 0.9433 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 12305/35720 Training loss: 0.9432 0.2217 sec/batch\n",
      "Epoch 7/20  Iteration 12306/35720 Training loss: 0.9432 0.2175 sec/batch\n",
      "Epoch 7/20  Iteration 12307/35720 Training loss: 0.9432 0.2136 sec/batch\n",
      "Epoch 7/20  Iteration 12308/35720 Training loss: 0.9432 0.2203 sec/batch\n",
      "Epoch 7/20  Iteration 12309/35720 Training loss: 0.9432 0.2269 sec/batch\n",
      "Epoch 7/20  Iteration 12310/35720 Training loss: 0.9432 0.2060 sec/batch\n",
      "Epoch 7/20  Iteration 12311/35720 Training loss: 0.9431 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 12312/35720 Training loss: 0.9430 0.2217 sec/batch\n",
      "Epoch 7/20  Iteration 12313/35720 Training loss: 0.9430 0.2181 sec/batch\n",
      "Epoch 7/20  Iteration 12314/35720 Training loss: 0.9430 0.2172 sec/batch\n",
      "Epoch 7/20  Iteration 12315/35720 Training loss: 0.9430 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 12316/35720 Training loss: 0.9430 0.2210 sec/batch\n",
      "Epoch 7/20  Iteration 12317/35720 Training loss: 0.9429 0.2189 sec/batch\n",
      "Epoch 7/20  Iteration 12318/35720 Training loss: 0.9429 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 12319/35720 Training loss: 0.9429 0.2341 sec/batch\n",
      "Epoch 7/20  Iteration 12320/35720 Training loss: 0.9429 0.2228 sec/batch\n",
      "Epoch 7/20  Iteration 12321/35720 Training loss: 0.9428 0.2288 sec/batch\n",
      "Epoch 7/20  Iteration 12322/35720 Training loss: 0.9428 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 12323/35720 Training loss: 0.9428 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 12324/35720 Training loss: 0.9428 0.2323 sec/batch\n",
      "Epoch 7/20  Iteration 12325/35720 Training loss: 0.9427 0.2229 sec/batch\n",
      "Epoch 7/20  Iteration 12326/35720 Training loss: 0.9427 0.2070 sec/batch\n",
      "Epoch 7/20  Iteration 12327/35720 Training loss: 0.9426 0.2100 sec/batch\n",
      "Epoch 7/20  Iteration 12328/35720 Training loss: 0.9426 0.2646 sec/batch\n",
      "Epoch 7/20  Iteration 12329/35720 Training loss: 0.9425 0.2372 sec/batch\n",
      "Epoch 7/20  Iteration 12330/35720 Training loss: 0.9425 0.2416 sec/batch\n",
      "Epoch 7/20  Iteration 12331/35720 Training loss: 0.9425 0.2090 sec/batch\n",
      "Epoch 7/20  Iteration 12332/35720 Training loss: 0.9425 0.2258 sec/batch\n",
      "Epoch 7/20  Iteration 12333/35720 Training loss: 0.9425 0.2303 sec/batch\n",
      "Epoch 7/20  Iteration 12334/35720 Training loss: 0.9425 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 12335/35720 Training loss: 0.9424 0.2147 sec/batch\n",
      "Epoch 7/20  Iteration 12336/35720 Training loss: 0.9424 0.2108 sec/batch\n",
      "Epoch 7/20  Iteration 12337/35720 Training loss: 0.9424 0.2177 sec/batch\n",
      "Epoch 7/20  Iteration 12338/35720 Training loss: 0.9424 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 12339/35720 Training loss: 0.9424 0.2294 sec/batch\n",
      "Epoch 7/20  Iteration 12340/35720 Training loss: 0.9423 0.2141 sec/batch\n",
      "Epoch 7/20  Iteration 12341/35720 Training loss: 0.9424 0.2264 sec/batch\n",
      "Epoch 7/20  Iteration 12342/35720 Training loss: 0.9424 0.2067 sec/batch\n",
      "Epoch 7/20  Iteration 12343/35720 Training loss: 0.9424 0.2173 sec/batch\n",
      "Epoch 7/20  Iteration 12344/35720 Training loss: 0.9424 0.2251 sec/batch\n",
      "Epoch 7/20  Iteration 12345/35720 Training loss: 0.9424 0.2207 sec/batch\n",
      "Epoch 7/20  Iteration 12346/35720 Training loss: 0.9424 0.2165 sec/batch\n",
      "Epoch 7/20  Iteration 12347/35720 Training loss: 0.9424 0.2117 sec/batch\n",
      "Epoch 7/20  Iteration 12348/35720 Training loss: 0.9423 0.2133 sec/batch\n",
      "Epoch 7/20  Iteration 12349/35720 Training loss: 0.9423 0.2081 sec/batch\n",
      "Epoch 7/20  Iteration 12350/35720 Training loss: 0.9423 0.2142 sec/batch\n",
      "Epoch 7/20  Iteration 12351/35720 Training loss: 0.9423 0.2308 sec/batch\n",
      "Epoch 7/20  Iteration 12352/35720 Training loss: 0.9424 0.2263 sec/batch\n",
      "Epoch 7/20  Iteration 12353/35720 Training loss: 0.9423 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 12354/35720 Training loss: 0.9423 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 12355/35720 Training loss: 0.9423 0.2324 sec/batch\n",
      "Epoch 7/20  Iteration 12356/35720 Training loss: 0.9423 0.2226 sec/batch\n",
      "Epoch 7/20  Iteration 12357/35720 Training loss: 0.9422 0.2224 sec/batch\n",
      "Epoch 7/20  Iteration 12358/35720 Training loss: 0.9422 0.2121 sec/batch\n",
      "Epoch 7/20  Iteration 12359/35720 Training loss: 0.9422 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 12360/35720 Training loss: 0.9422 0.2193 sec/batch\n",
      "Epoch 7/20  Iteration 12361/35720 Training loss: 0.9423 0.2163 sec/batch\n",
      "Epoch 7/20  Iteration 12362/35720 Training loss: 0.9423 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 12363/35720 Training loss: 0.9423 0.2110 sec/batch\n",
      "Epoch 7/20  Iteration 12364/35720 Training loss: 0.9423 0.2152 sec/batch\n",
      "Epoch 7/20  Iteration 12365/35720 Training loss: 0.9423 0.2228 sec/batch\n",
      "Epoch 7/20  Iteration 12366/35720 Training loss: 0.9423 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 12367/35720 Training loss: 0.9424 0.2191 sec/batch\n",
      "Epoch 7/20  Iteration 12368/35720 Training loss: 0.9423 0.2317 sec/batch\n",
      "Epoch 7/20  Iteration 12369/35720 Training loss: 0.9424 0.2255 sec/batch\n",
      "Epoch 7/20  Iteration 12370/35720 Training loss: 0.9423 0.2126 sec/batch\n",
      "Epoch 7/20  Iteration 12371/35720 Training loss: 0.9424 0.2289 sec/batch\n",
      "Epoch 7/20  Iteration 12372/35720 Training loss: 0.9424 0.2257 sec/batch\n",
      "Epoch 7/20  Iteration 12373/35720 Training loss: 0.9424 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 12374/35720 Training loss: 0.9424 0.2306 sec/batch\n",
      "Epoch 7/20  Iteration 12375/35720 Training loss: 0.9424 0.2227 sec/batch\n",
      "Epoch 7/20  Iteration 12376/35720 Training loss: 0.9424 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 12377/35720 Training loss: 0.9424 0.2234 sec/batch\n",
      "Epoch 7/20  Iteration 12378/35720 Training loss: 0.9424 0.2327 sec/batch\n",
      "Epoch 7/20  Iteration 12379/35720 Training loss: 0.9424 0.2251 sec/batch\n",
      "Epoch 7/20  Iteration 12380/35720 Training loss: 0.9424 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 12381/35720 Training loss: 0.9424 0.2184 sec/batch\n",
      "Epoch 7/20  Iteration 12382/35720 Training loss: 0.9424 0.2134 sec/batch\n",
      "Epoch 7/20  Iteration 12383/35720 Training loss: 0.9424 0.2145 sec/batch\n",
      "Epoch 7/20  Iteration 12384/35720 Training loss: 0.9425 0.2126 sec/batch\n",
      "Epoch 7/20  Iteration 12385/35720 Training loss: 0.9425 0.2187 sec/batch\n",
      "Epoch 7/20  Iteration 12386/35720 Training loss: 0.9425 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 12387/35720 Training loss: 0.9426 0.2066 sec/batch\n",
      "Epoch 7/20  Iteration 12388/35720 Training loss: 0.9426 0.2085 sec/batch\n",
      "Epoch 7/20  Iteration 12389/35720 Training loss: 0.9426 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 12390/35720 Training loss: 0.9426 0.2154 sec/batch\n",
      "Epoch 7/20  Iteration 12391/35720 Training loss: 0.9426 0.2270 sec/batch\n",
      "Epoch 7/20  Iteration 12392/35720 Training loss: 0.9426 0.2062 sec/batch\n",
      "Epoch 7/20  Iteration 12393/35720 Training loss: 0.9426 0.2084 sec/batch\n",
      "Epoch 7/20  Iteration 12394/35720 Training loss: 0.9426 0.2173 sec/batch\n",
      "Epoch 7/20  Iteration 12395/35720 Training loss: 0.9426 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 12396/35720 Training loss: 0.9425 0.2160 sec/batch\n",
      "Epoch 7/20  Iteration 12397/35720 Training loss: 0.9425 0.2068 sec/batch\n",
      "Epoch 7/20  Iteration 12398/35720 Training loss: 0.9424 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 12399/35720 Training loss: 0.9425 0.2166 sec/batch\n",
      "Epoch 7/20  Iteration 12400/35720 Training loss: 0.9425 0.2197 sec/batch\n",
      "Validation loss: 1.33425 Saving checkpoint!\n",
      "Epoch 7/20  Iteration 12401/35720 Training loss: 0.9426 0.2101 sec/batch\n",
      "Epoch 7/20  Iteration 12402/35720 Training loss: 0.9427 0.2149 sec/batch\n",
      "Epoch 7/20  Iteration 12403/35720 Training loss: 0.9427 0.2262 sec/batch\n",
      "Epoch 7/20  Iteration 12404/35720 Training loss: 0.9426 0.2281 sec/batch\n",
      "Epoch 7/20  Iteration 12405/35720 Training loss: 0.9427 0.2150 sec/batch\n",
      "Epoch 7/20  Iteration 12406/35720 Training loss: 0.9426 0.2309 sec/batch\n",
      "Epoch 7/20  Iteration 12407/35720 Training loss: 0.9426 0.2252 sec/batch\n",
      "Epoch 7/20  Iteration 12408/35720 Training loss: 0.9425 0.2087 sec/batch\n",
      "Epoch 7/20  Iteration 12409/35720 Training loss: 0.9425 0.2289 sec/batch\n",
      "Epoch 7/20  Iteration 12410/35720 Training loss: 0.9425 0.2161 sec/batch\n",
      "Epoch 7/20  Iteration 12411/35720 Training loss: 0.9425 0.2213 sec/batch\n",
      "Epoch 7/20  Iteration 12412/35720 Training loss: 0.9425 0.2114 sec/batch\n",
      "Epoch 7/20  Iteration 12413/35720 Training loss: 0.9424 0.2285 sec/batch\n",
      "Epoch 7/20  Iteration 12414/35720 Training loss: 0.9424 0.2130 sec/batch\n",
      "Epoch 7/20  Iteration 12415/35720 Training loss: 0.9424 0.2255 sec/batch\n",
      "Epoch 7/20  Iteration 12416/35720 Training loss: 0.9424 0.2259 sec/batch\n",
      "Epoch 7/20  Iteration 12417/35720 Training loss: 0.9424 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 12418/35720 Training loss: 0.9424 0.2071 sec/batch\n",
      "Epoch 7/20  Iteration 12419/35720 Training loss: 0.9424 0.2105 sec/batch\n",
      "Epoch 7/20  Iteration 12420/35720 Training loss: 0.9423 0.2139 sec/batch\n",
      "Epoch 7/20  Iteration 12421/35720 Training loss: 0.9423 0.2274 sec/batch\n",
      "Epoch 7/20  Iteration 12422/35720 Training loss: 0.9423 0.2198 sec/batch\n",
      "Epoch 7/20  Iteration 12423/35720 Training loss: 0.9422 0.2138 sec/batch\n",
      "Epoch 7/20  Iteration 12424/35720 Training loss: 0.9422 0.2213 sec/batch\n",
      "Epoch 7/20  Iteration 12425/35720 Training loss: 0.9422 0.2145 sec/batch\n",
      "Epoch 7/20  Iteration 12426/35720 Training loss: 0.9422 0.2254 sec/batch\n",
      "Epoch 7/20  Iteration 12427/35720 Training loss: 0.9423 0.2214 sec/batch\n",
      "Epoch 7/20  Iteration 12428/35720 Training loss: 0.9423 0.2118 sec/batch\n",
      "Epoch 7/20  Iteration 12429/35720 Training loss: 0.9423 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 12430/35720 Training loss: 0.9423 0.2204 sec/batch\n",
      "Epoch 7/20  Iteration 12431/35720 Training loss: 0.9423 0.2300 sec/batch\n",
      "Epoch 7/20  Iteration 12432/35720 Training loss: 0.9423 0.2290 sec/batch\n",
      "Epoch 7/20  Iteration 12433/35720 Training loss: 0.9423 0.2254 sec/batch\n",
      "Epoch 7/20  Iteration 12434/35720 Training loss: 0.9423 0.2063 sec/batch\n",
      "Epoch 7/20  Iteration 12435/35720 Training loss: 0.9422 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 12436/35720 Training loss: 0.9423 0.2111 sec/batch\n",
      "Epoch 7/20  Iteration 12437/35720 Training loss: 0.9423 0.2193 sec/batch\n",
      "Epoch 7/20  Iteration 12438/35720 Training loss: 0.9423 0.2279 sec/batch\n",
      "Epoch 7/20  Iteration 12439/35720 Training loss: 0.9423 0.2338 sec/batch\n",
      "Epoch 7/20  Iteration 12440/35720 Training loss: 0.9422 0.2059 sec/batch\n",
      "Epoch 7/20  Iteration 12441/35720 Training loss: 0.9422 0.2066 sec/batch\n",
      "Epoch 7/20  Iteration 12442/35720 Training loss: 0.9422 0.2080 sec/batch\n",
      "Epoch 7/20  Iteration 12443/35720 Training loss: 0.9422 0.2191 sec/batch\n",
      "Epoch 7/20  Iteration 12444/35720 Training loss: 0.9422 0.2191 sec/batch\n",
      "Epoch 7/20  Iteration 12445/35720 Training loss: 0.9422 0.2115 sec/batch\n",
      "Epoch 7/20  Iteration 12446/35720 Training loss: 0.9422 0.2137 sec/batch\n",
      "Epoch 7/20  Iteration 12447/35720 Training loss: 0.9421 0.2109 sec/batch\n",
      "Epoch 7/20  Iteration 12448/35720 Training loss: 0.9422 0.2250 sec/batch\n",
      "Epoch 7/20  Iteration 12449/35720 Training loss: 0.9422 0.2094 sec/batch\n",
      "Epoch 7/20  Iteration 12450/35720 Training loss: 0.9421 0.2182 sec/batch\n",
      "Epoch 7/20  Iteration 12451/35720 Training loss: 0.9422 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 12452/35720 Training loss: 0.9422 0.2222 sec/batch\n",
      "Epoch 7/20  Iteration 12453/35720 Training loss: 0.9422 0.2095 sec/batch\n",
      "Epoch 7/20  Iteration 12454/35720 Training loss: 0.9421 0.2169 sec/batch\n",
      "Epoch 7/20  Iteration 12455/35720 Training loss: 0.9422 0.2170 sec/batch\n",
      "Epoch 7/20  Iteration 12456/35720 Training loss: 0.9421 0.2175 sec/batch\n",
      "Epoch 7/20  Iteration 12457/35720 Training loss: 0.9422 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 12458/35720 Training loss: 0.9421 0.2331 sec/batch\n",
      "Epoch 7/20  Iteration 12459/35720 Training loss: 0.9421 0.2286 sec/batch\n",
      "Epoch 7/20  Iteration 12460/35720 Training loss: 0.9422 0.2155 sec/batch\n",
      "Epoch 7/20  Iteration 12461/35720 Training loss: 0.9422 0.2231 sec/batch\n",
      "Epoch 7/20  Iteration 12462/35720 Training loss: 0.9423 0.2145 sec/batch\n",
      "Epoch 7/20  Iteration 12463/35720 Training loss: 0.9422 0.2055 sec/batch\n",
      "Epoch 7/20  Iteration 12464/35720 Training loss: 0.9423 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 12465/35720 Training loss: 0.9422 0.2261 sec/batch\n",
      "Epoch 7/20  Iteration 12466/35720 Training loss: 0.9422 0.2132 sec/batch\n",
      "Epoch 7/20  Iteration 12467/35720 Training loss: 0.9422 0.2235 sec/batch\n",
      "Epoch 7/20  Iteration 12468/35720 Training loss: 0.9421 0.2123 sec/batch\n",
      "Epoch 7/20  Iteration 12469/35720 Training loss: 0.9421 0.2088 sec/batch\n",
      "Epoch 7/20  Iteration 12470/35720 Training loss: 0.9421 0.2168 sec/batch\n",
      "Epoch 7/20  Iteration 12471/35720 Training loss: 0.9421 0.2081 sec/batch\n",
      "Epoch 7/20  Iteration 12472/35720 Training loss: 0.9421 0.2286 sec/batch\n",
      "Epoch 7/20  Iteration 12473/35720 Training loss: 0.9421 0.2269 sec/batch\n",
      "Epoch 7/20  Iteration 12474/35720 Training loss: 0.9421 0.2072 sec/batch\n",
      "Epoch 7/20  Iteration 12475/35720 Training loss: 0.9421 0.2133 sec/batch\n",
      "Epoch 7/20  Iteration 12476/35720 Training loss: 0.9421 0.2180 sec/batch\n",
      "Epoch 7/20  Iteration 12477/35720 Training loss: 0.9421 0.2152 sec/batch\n",
      "Epoch 7/20  Iteration 12478/35720 Training loss: 0.9420 0.2162 sec/batch\n",
      "Epoch 7/20  Iteration 12479/35720 Training loss: 0.9420 0.2102 sec/batch\n",
      "Epoch 7/20  Iteration 12480/35720 Training loss: 0.9420 0.2183 sec/batch\n",
      "Epoch 7/20  Iteration 12481/35720 Training loss: 0.9420 0.2145 sec/batch\n",
      "Epoch 7/20  Iteration 12482/35720 Training loss: 0.9420 0.2141 sec/batch\n",
      "Epoch 7/20  Iteration 12483/35720 Training loss: 0.9420 0.2218 sec/batch\n",
      "Epoch 7/20  Iteration 12484/35720 Training loss: 0.9420 0.2166 sec/batch\n",
      "Epoch 7/20  Iteration 12485/35720 Training loss: 0.9420 0.2080 sec/batch\n",
      "Epoch 7/20  Iteration 12486/35720 Training loss: 0.9420 0.2185 sec/batch\n",
      "Epoch 7/20  Iteration 12487/35720 Training loss: 0.9420 0.2185 sec/batch\n",
      "Epoch 7/20  Iteration 12488/35720 Training loss: 0.9419 0.2167 sec/batch\n",
      "Epoch 7/20  Iteration 12489/35720 Training loss: 0.9419 0.2151 sec/batch\n",
      "Epoch 7/20  Iteration 12490/35720 Training loss: 0.9419 0.2200 sec/batch\n",
      "Epoch 7/20  Iteration 12491/35720 Training loss: 0.9418 0.2206 sec/batch\n",
      "Epoch 7/20  Iteration 12492/35720 Training loss: 0.9418 0.2075 sec/batch\n",
      "Epoch 7/20  Iteration 12493/35720 Training loss: 0.9418 0.2099 sec/batch\n",
      "Epoch 7/20  Iteration 12494/35720 Training loss: 0.9418 0.2120 sec/batch\n",
      "Epoch 7/20  Iteration 12495/35720 Training loss: 0.9417 0.2063 sec/batch\n",
      "Epoch 7/20  Iteration 12496/35720 Training loss: 0.9417 0.2073 sec/batch\n",
      "Epoch 7/20  Iteration 12497/35720 Training loss: 0.9416 0.2127 sec/batch\n",
      "Epoch 7/20  Iteration 12498/35720 Training loss: 0.9416 0.2064 sec/batch\n",
      "Epoch 7/20  Iteration 12499/35720 Training loss: 0.9416 0.2202 sec/batch\n",
      "Epoch 7/20  Iteration 12500/35720 Training loss: 0.9415 0.2065 sec/batch\n",
      "Epoch 7/20  Iteration 12501/35720 Training loss: 0.9415 0.2104 sec/batch\n",
      "Epoch 7/20  Iteration 12502/35720 Training loss: 0.9415 0.2803 sec/batch\n",
      "Epoch 8/20  Iteration 12503/35720 Training loss: 0.9612 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 12504/35720 Training loss: 0.9527 0.2191 sec/batch\n",
      "Epoch 8/20  Iteration 12505/35720 Training loss: 0.9469 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 12506/35720 Training loss: 0.9384 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 12507/35720 Training loss: 0.9461 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 12508/35720 Training loss: 0.9319 0.2073 sec/batch\n",
      "Epoch 8/20  Iteration 12509/35720 Training loss: 0.9327 0.2073 sec/batch\n",
      "Epoch 8/20  Iteration 12510/35720 Training loss: 0.9224 0.2247 sec/batch\n",
      "Epoch 8/20  Iteration 12511/35720 Training loss: 0.9184 0.2234 sec/batch\n",
      "Epoch 8/20  Iteration 12512/35720 Training loss: 0.9201 0.2237 sec/batch\n",
      "Epoch 8/20  Iteration 12513/35720 Training loss: 0.9211 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 12514/35720 Training loss: 0.9181 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 12515/35720 Training loss: 0.9203 0.2279 sec/batch\n",
      "Epoch 8/20  Iteration 12516/35720 Training loss: 0.9268 0.2207 sec/batch\n",
      "Epoch 8/20  Iteration 12517/35720 Training loss: 0.9296 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 12518/35720 Training loss: 0.9283 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 12519/35720 Training loss: 0.9298 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 12520/35720 Training loss: 0.9281 0.2192 sec/batch\n",
      "Epoch 8/20  Iteration 12521/35720 Training loss: 0.9265 0.2241 sec/batch\n",
      "Epoch 8/20  Iteration 12522/35720 Training loss: 0.9274 0.2282 sec/batch\n",
      "Epoch 8/20  Iteration 12523/35720 Training loss: 0.9293 0.2081 sec/batch\n",
      "Epoch 8/20  Iteration 12524/35720 Training loss: 0.9261 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 12525/35720 Training loss: 0.9262 0.2302 sec/batch\n",
      "Epoch 8/20  Iteration 12526/35720 Training loss: 0.9275 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 12527/35720 Training loss: 0.9299 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 12528/35720 Training loss: 0.9291 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 12529/35720 Training loss: 0.9318 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 12530/35720 Training loss: 0.9327 0.2206 sec/batch\n",
      "Epoch 8/20  Iteration 12531/35720 Training loss: 0.9317 0.2292 sec/batch\n",
      "Epoch 8/20  Iteration 12532/35720 Training loss: 0.9317 0.2106 sec/batch\n",
      "Epoch 8/20  Iteration 12533/35720 Training loss: 0.9355 0.2306 sec/batch\n",
      "Epoch 8/20  Iteration 12534/35720 Training loss: 0.9340 0.2476 sec/batch\n",
      "Epoch 8/20  Iteration 12535/35720 Training loss: 0.9360 0.2215 sec/batch\n",
      "Epoch 8/20  Iteration 12536/35720 Training loss: 0.9385 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 12537/35720 Training loss: 0.9417 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 12538/35720 Training loss: 0.9420 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 12539/35720 Training loss: 0.9421 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 12540/35720 Training loss: 0.9420 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 12541/35720 Training loss: 0.9402 0.2296 sec/batch\n",
      "Epoch 8/20  Iteration 12542/35720 Training loss: 0.9414 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 12543/35720 Training loss: 0.9408 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 12544/35720 Training loss: 0.9391 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 12545/35720 Training loss: 0.9368 0.2106 sec/batch\n",
      "Epoch 8/20  Iteration 12546/35720 Training loss: 0.9361 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 12547/35720 Training loss: 0.9352 0.2155 sec/batch\n",
      "Epoch 8/20  Iteration 12548/35720 Training loss: 0.9341 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 12549/35720 Training loss: 0.9339 0.2205 sec/batch\n",
      "Epoch 8/20  Iteration 12550/35720 Training loss: 0.9334 0.2204 sec/batch\n",
      "Epoch 8/20  Iteration 12551/35720 Training loss: 0.9334 0.2496 sec/batch\n",
      "Epoch 8/20  Iteration 12552/35720 Training loss: 0.9321 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 12553/35720 Training loss: 0.9327 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 12554/35720 Training loss: 0.9323 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 12555/35720 Training loss: 0.9318 0.2122 sec/batch\n",
      "Epoch 8/20  Iteration 12556/35720 Training loss: 0.9299 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 12557/35720 Training loss: 0.9289 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 12558/35720 Training loss: 0.9282 0.2151 sec/batch\n",
      "Epoch 8/20  Iteration 12559/35720 Training loss: 0.9283 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 12560/35720 Training loss: 0.9277 0.2152 sec/batch\n",
      "Epoch 8/20  Iteration 12561/35720 Training loss: 0.9265 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 12562/35720 Training loss: 0.9252 0.2258 sec/batch\n",
      "Epoch 8/20  Iteration 12563/35720 Training loss: 0.9242 0.2214 sec/batch\n",
      "Epoch 8/20  Iteration 12564/35720 Training loss: 0.9221 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 12565/35720 Training loss: 0.9226 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 12566/35720 Training loss: 0.9225 0.2479 sec/batch\n",
      "Epoch 8/20  Iteration 12567/35720 Training loss: 0.9231 0.2228 sec/batch\n",
      "Epoch 8/20  Iteration 12568/35720 Training loss: 0.9231 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 12569/35720 Training loss: 0.9226 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 12570/35720 Training loss: 0.9217 0.2210 sec/batch\n",
      "Epoch 8/20  Iteration 12571/35720 Training loss: 0.9227 0.2265 sec/batch\n",
      "Epoch 8/20  Iteration 12572/35720 Training loss: 0.9220 0.2062 sec/batch\n",
      "Epoch 8/20  Iteration 12573/35720 Training loss: 0.9224 0.2058 sec/batch\n",
      "Epoch 8/20  Iteration 12574/35720 Training loss: 0.9227 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 12575/35720 Training loss: 0.9229 0.2280 sec/batch\n",
      "Epoch 8/20  Iteration 12576/35720 Training loss: 0.9231 0.2291 sec/batch\n",
      "Epoch 8/20  Iteration 12577/35720 Training loss: 0.9220 0.2255 sec/batch\n",
      "Epoch 8/20  Iteration 12578/35720 Training loss: 0.9218 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 12579/35720 Training loss: 0.9209 0.2307 sec/batch\n",
      "Epoch 8/20  Iteration 12580/35720 Training loss: 0.9216 0.2224 sec/batch\n",
      "Epoch 8/20  Iteration 12581/35720 Training loss: 0.9215 0.2130 sec/batch\n",
      "Epoch 8/20  Iteration 12582/35720 Training loss: 0.9233 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 12583/35720 Training loss: 0.9236 0.2330 sec/batch\n",
      "Epoch 8/20  Iteration 12584/35720 Training loss: 0.9236 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 12585/35720 Training loss: 0.9235 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 12586/35720 Training loss: 0.9237 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 12587/35720 Training loss: 0.9237 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 12588/35720 Training loss: 0.9237 0.2156 sec/batch\n",
      "Epoch 8/20  Iteration 12589/35720 Training loss: 0.9237 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 12590/35720 Training loss: 0.9234 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 12591/35720 Training loss: 0.9224 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 12592/35720 Training loss: 0.9215 0.2231 sec/batch\n",
      "Epoch 8/20  Iteration 12593/35720 Training loss: 0.9216 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 12594/35720 Training loss: 0.9211 0.2148 sec/batch\n",
      "Epoch 8/20  Iteration 12595/35720 Training loss: 0.9209 0.2252 sec/batch\n",
      "Epoch 8/20  Iteration 12596/35720 Training loss: 0.9210 0.2144 sec/batch\n",
      "Epoch 8/20  Iteration 12597/35720 Training loss: 0.9207 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 12598/35720 Training loss: 0.9198 0.2084 sec/batch\n",
      "Epoch 8/20  Iteration 12599/35720 Training loss: 0.9201 0.2213 sec/batch\n",
      "Epoch 8/20  Iteration 12600/35720 Training loss: 0.9201 0.2149 sec/batch\n",
      "Validation loss: 1.34349 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 12601/35720 Training loss: 0.9223 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 12602/35720 Training loss: 0.9221 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 12603/35720 Training loss: 0.9219 0.2142 sec/batch\n",
      "Epoch 8/20  Iteration 12604/35720 Training loss: 0.9219 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 12605/35720 Training loss: 0.9217 0.2231 sec/batch\n",
      "Epoch 8/20  Iteration 12606/35720 Training loss: 0.9215 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 12607/35720 Training loss: 0.9215 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 12608/35720 Training loss: 0.9212 0.2254 sec/batch\n",
      "Epoch 8/20  Iteration 12609/35720 Training loss: 0.9213 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 12610/35720 Training loss: 0.9217 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 12611/35720 Training loss: 0.9222 0.2107 sec/batch\n",
      "Epoch 8/20  Iteration 12612/35720 Training loss: 0.9222 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 12613/35720 Training loss: 0.9226 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 12614/35720 Training loss: 0.9235 0.2200 sec/batch\n",
      "Epoch 8/20  Iteration 12615/35720 Training loss: 0.9235 0.2216 sec/batch\n",
      "Epoch 8/20  Iteration 12616/35720 Training loss: 0.9237 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 12617/35720 Training loss: 0.9235 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 12618/35720 Training loss: 0.9237 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 12619/35720 Training loss: 0.9235 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 12620/35720 Training loss: 0.9239 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 12621/35720 Training loss: 0.9241 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 12622/35720 Training loss: 0.9249 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 12623/35720 Training loss: 0.9256 0.2260 sec/batch\n",
      "Epoch 8/20  Iteration 12624/35720 Training loss: 0.9254 0.2274 sec/batch\n",
      "Epoch 8/20  Iteration 12625/35720 Training loss: 0.9257 0.2148 sec/batch\n",
      "Epoch 8/20  Iteration 12626/35720 Training loss: 0.9262 0.2287 sec/batch\n",
      "Epoch 8/20  Iteration 12627/35720 Training loss: 0.9257 0.2080 sec/batch\n",
      "Epoch 8/20  Iteration 12628/35720 Training loss: 0.9257 0.2066 sec/batch\n",
      "Epoch 8/20  Iteration 12629/35720 Training loss: 0.9258 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 12630/35720 Training loss: 0.9257 0.2192 sec/batch\n",
      "Epoch 8/20  Iteration 12631/35720 Training loss: 0.9252 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 12632/35720 Training loss: 0.9252 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 12633/35720 Training loss: 0.9249 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 12634/35720 Training loss: 0.9245 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 12635/35720 Training loss: 0.9244 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 12636/35720 Training loss: 0.9245 0.2233 sec/batch\n",
      "Epoch 8/20  Iteration 12637/35720 Training loss: 0.9244 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 12638/35720 Training loss: 0.9244 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 12639/35720 Training loss: 0.9252 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 12640/35720 Training loss: 0.9254 0.2123 sec/batch\n",
      "Epoch 8/20  Iteration 12641/35720 Training loss: 0.9255 0.2326 sec/batch\n",
      "Epoch 8/20  Iteration 12642/35720 Training loss: 0.9257 0.2277 sec/batch\n",
      "Epoch 8/20  Iteration 12643/35720 Training loss: 0.9252 0.2223 sec/batch\n",
      "Epoch 8/20  Iteration 12644/35720 Training loss: 0.9247 0.2123 sec/batch\n",
      "Epoch 8/20  Iteration 12645/35720 Training loss: 0.9240 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 12646/35720 Training loss: 0.9233 0.2266 sec/batch\n",
      "Epoch 8/20  Iteration 12647/35720 Training loss: 0.9233 0.2240 sec/batch\n",
      "Epoch 8/20  Iteration 12648/35720 Training loss: 0.9237 0.2279 sec/batch\n",
      "Epoch 8/20  Iteration 12649/35720 Training loss: 0.9233 0.2236 sec/batch\n",
      "Epoch 8/20  Iteration 12650/35720 Training loss: 0.9232 0.2056 sec/batch\n",
      "Epoch 8/20  Iteration 12651/35720 Training loss: 0.9233 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 12652/35720 Training loss: 0.9226 0.2274 sec/batch\n",
      "Epoch 8/20  Iteration 12653/35720 Training loss: 0.9225 0.2227 sec/batch\n",
      "Epoch 8/20  Iteration 12654/35720 Training loss: 0.9226 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 12655/35720 Training loss: 0.9226 0.2066 sec/batch\n",
      "Epoch 8/20  Iteration 12656/35720 Training loss: 0.9230 0.2174 sec/batch\n",
      "Epoch 8/20  Iteration 12657/35720 Training loss: 0.9233 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 12658/35720 Training loss: 0.9237 0.2357 sec/batch\n",
      "Epoch 8/20  Iteration 12659/35720 Training loss: 0.9238 0.2123 sec/batch\n",
      "Epoch 8/20  Iteration 12660/35720 Training loss: 0.9239 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 12661/35720 Training loss: 0.9236 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 12662/35720 Training loss: 0.9238 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 12663/35720 Training loss: 0.9235 0.2145 sec/batch\n",
      "Epoch 8/20  Iteration 12664/35720 Training loss: 0.9237 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 12665/35720 Training loss: 0.9238 0.2143 sec/batch\n",
      "Epoch 8/20  Iteration 12666/35720 Training loss: 0.9240 0.2252 sec/batch\n",
      "Epoch 8/20  Iteration 12667/35720 Training loss: 0.9243 0.2078 sec/batch\n",
      "Epoch 8/20  Iteration 12668/35720 Training loss: 0.9245 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 12669/35720 Training loss: 0.9246 0.2205 sec/batch\n",
      "Epoch 8/20  Iteration 12670/35720 Training loss: 0.9249 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 12671/35720 Training loss: 0.9253 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 12672/35720 Training loss: 0.9255 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 12673/35720 Training loss: 0.9262 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 12674/35720 Training loss: 0.9266 0.2130 sec/batch\n",
      "Epoch 8/20  Iteration 12675/35720 Training loss: 0.9269 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 12676/35720 Training loss: 0.9274 0.2128 sec/batch\n",
      "Epoch 8/20  Iteration 12677/35720 Training loss: 0.9277 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 12678/35720 Training loss: 0.9276 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 12679/35720 Training loss: 0.9277 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 12680/35720 Training loss: 0.9274 0.2134 sec/batch\n",
      "Epoch 8/20  Iteration 12681/35720 Training loss: 0.9272 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 12682/35720 Training loss: 0.9269 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 12683/35720 Training loss: 0.9271 0.2063 sec/batch\n",
      "Epoch 8/20  Iteration 12684/35720 Training loss: 0.9273 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 12685/35720 Training loss: 0.9274 0.2370 sec/batch\n",
      "Epoch 8/20  Iteration 12686/35720 Training loss: 0.9275 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 12687/35720 Training loss: 0.9274 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 12688/35720 Training loss: 0.9273 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 12689/35720 Training loss: 0.9270 0.2170 sec/batch\n",
      "Epoch 8/20  Iteration 12690/35720 Training loss: 0.9271 0.2148 sec/batch\n",
      "Epoch 8/20  Iteration 12691/35720 Training loss: 0.9273 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 12692/35720 Training loss: 0.9274 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 12693/35720 Training loss: 0.9276 0.2177 sec/batch\n",
      "Epoch 8/20  Iteration 12694/35720 Training loss: 0.9280 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 12695/35720 Training loss: 0.9283 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 12696/35720 Training loss: 0.9286 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 12697/35720 Training loss: 0.9286 0.2302 sec/batch\n",
      "Epoch 8/20  Iteration 12698/35720 Training loss: 0.9289 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 12699/35720 Training loss: 0.9286 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 12700/35720 Training loss: 0.9286 0.2156 sec/batch\n",
      "Epoch 8/20  Iteration 12701/35720 Training loss: 0.9288 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 12702/35720 Training loss: 0.9290 0.2212 sec/batch\n",
      "Epoch 8/20  Iteration 12703/35720 Training loss: 0.9289 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 12704/35720 Training loss: 0.9287 0.2197 sec/batch\n",
      "Epoch 8/20  Iteration 12705/35720 Training loss: 0.9288 0.2074 sec/batch\n",
      "Epoch 8/20  Iteration 12706/35720 Training loss: 0.9286 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 12707/35720 Training loss: 0.9286 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 12708/35720 Training loss: 0.9284 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 12709/35720 Training loss: 0.9289 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 12710/35720 Training loss: 0.9293 0.2141 sec/batch\n",
      "Epoch 8/20  Iteration 12711/35720 Training loss: 0.9296 0.2249 sec/batch\n",
      "Epoch 8/20  Iteration 12712/35720 Training loss: 0.9297 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 12713/35720 Training loss: 0.9299 0.2278 sec/batch\n",
      "Epoch 8/20  Iteration 12714/35720 Training loss: 0.9298 0.2314 sec/batch\n",
      "Epoch 8/20  Iteration 12715/35720 Training loss: 0.9297 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 12716/35720 Training loss: 0.9298 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 12717/35720 Training loss: 0.9298 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 12718/35720 Training loss: 0.9297 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 12719/35720 Training loss: 0.9296 0.2292 sec/batch\n",
      "Epoch 8/20  Iteration 12720/35720 Training loss: 0.9294 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 12721/35720 Training loss: 0.9295 0.2144 sec/batch\n",
      "Epoch 8/20  Iteration 12722/35720 Training loss: 0.9296 0.2177 sec/batch\n",
      "Epoch 8/20  Iteration 12723/35720 Training loss: 0.9296 0.2086 sec/batch\n",
      "Epoch 8/20  Iteration 12724/35720 Training loss: 0.9296 0.2321 sec/batch\n",
      "Epoch 8/20  Iteration 12725/35720 Training loss: 0.9300 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 12726/35720 Training loss: 0.9303 0.2204 sec/batch\n",
      "Epoch 8/20  Iteration 12727/35720 Training loss: 0.9304 0.2275 sec/batch\n",
      "Epoch 8/20  Iteration 12728/35720 Training loss: 0.9304 0.2192 sec/batch\n",
      "Epoch 8/20  Iteration 12729/35720 Training loss: 0.9301 0.2270 sec/batch\n",
      "Epoch 8/20  Iteration 12730/35720 Training loss: 0.9299 0.2194 sec/batch\n",
      "Epoch 8/20  Iteration 12731/35720 Training loss: 0.9296 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 12732/35720 Training loss: 0.9298 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 12733/35720 Training loss: 0.9301 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 12734/35720 Training loss: 0.9300 0.2084 sec/batch\n",
      "Epoch 8/20  Iteration 12735/35720 Training loss: 0.9301 0.2170 sec/batch\n",
      "Epoch 8/20  Iteration 12736/35720 Training loss: 0.9302 0.2138 sec/batch\n",
      "Epoch 8/20  Iteration 12737/35720 Training loss: 0.9303 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 12738/35720 Training loss: 0.9302 0.2275 sec/batch\n",
      "Epoch 8/20  Iteration 12739/35720 Training loss: 0.9304 0.2205 sec/batch\n",
      "Epoch 8/20  Iteration 12740/35720 Training loss: 0.9303 0.2383 sec/batch\n",
      "Epoch 8/20  Iteration 12741/35720 Training loss: 0.9301 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 12742/35720 Training loss: 0.9302 0.2309 sec/batch\n",
      "Epoch 8/20  Iteration 12743/35720 Training loss: 0.9300 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 12744/35720 Training loss: 0.9299 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 12745/35720 Training loss: 0.9300 0.2128 sec/batch\n",
      "Epoch 8/20  Iteration 12746/35720 Training loss: 0.9300 0.2198 sec/batch\n",
      "Epoch 8/20  Iteration 12747/35720 Training loss: 0.9298 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 12748/35720 Training loss: 0.9298 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 12749/35720 Training loss: 0.9298 0.2254 sec/batch\n",
      "Epoch 8/20  Iteration 12750/35720 Training loss: 0.9297 0.2127 sec/batch\n",
      "Epoch 8/20  Iteration 12751/35720 Training loss: 0.9296 0.2264 sec/batch\n",
      "Epoch 8/20  Iteration 12752/35720 Training loss: 0.9293 0.2125 sec/batch\n",
      "Epoch 8/20  Iteration 12753/35720 Training loss: 0.9293 0.3087 sec/batch\n",
      "Epoch 8/20  Iteration 12754/35720 Training loss: 0.9292 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 12755/35720 Training loss: 0.9289 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 12756/35720 Training loss: 0.9292 0.2151 sec/batch\n",
      "Epoch 8/20  Iteration 12757/35720 Training loss: 0.9295 0.2188 sec/batch\n",
      "Epoch 8/20  Iteration 12758/35720 Training loss: 0.9295 0.2239 sec/batch\n",
      "Epoch 8/20  Iteration 12759/35720 Training loss: 0.9297 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 12760/35720 Training loss: 0.9297 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 12761/35720 Training loss: 0.9301 0.2297 sec/batch\n",
      "Epoch 8/20  Iteration 12762/35720 Training loss: 0.9300 0.2333 sec/batch\n",
      "Epoch 8/20  Iteration 12763/35720 Training loss: 0.9299 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 12764/35720 Training loss: 0.9297 0.2217 sec/batch\n",
      "Epoch 8/20  Iteration 12765/35720 Training loss: 0.9296 0.2108 sec/batch\n",
      "Epoch 8/20  Iteration 12766/35720 Training loss: 0.9297 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 12767/35720 Training loss: 0.9297 0.2344 sec/batch\n",
      "Epoch 8/20  Iteration 12768/35720 Training loss: 0.9298 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 12769/35720 Training loss: 0.9297 0.2293 sec/batch\n",
      "Epoch 8/20  Iteration 12770/35720 Training loss: 0.9297 0.2282 sec/batch\n",
      "Epoch 8/20  Iteration 12771/35720 Training loss: 0.9295 0.2119 sec/batch\n",
      "Epoch 8/20  Iteration 12772/35720 Training loss: 0.9290 0.2177 sec/batch\n",
      "Epoch 8/20  Iteration 12773/35720 Training loss: 0.9287 0.2632 sec/batch\n",
      "Epoch 8/20  Iteration 12774/35720 Training loss: 0.9286 0.2459 sec/batch\n",
      "Epoch 8/20  Iteration 12775/35720 Training loss: 0.9287 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 12776/35720 Training loss: 0.9287 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 12777/35720 Training loss: 0.9285 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 12778/35720 Training loss: 0.9283 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 12779/35720 Training loss: 0.9281 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 12780/35720 Training loss: 0.9279 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 12781/35720 Training loss: 0.9277 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 12782/35720 Training loss: 0.9278 0.2206 sec/batch\n",
      "Epoch 8/20  Iteration 12783/35720 Training loss: 0.9277 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 12784/35720 Training loss: 0.9274 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 12785/35720 Training loss: 0.9270 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 12786/35720 Training loss: 0.9267 0.2194 sec/batch\n",
      "Epoch 8/20  Iteration 12787/35720 Training loss: 0.9270 0.2214 sec/batch\n",
      "Epoch 8/20  Iteration 12788/35720 Training loss: 0.9270 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 12789/35720 Training loss: 0.9265 0.2271 sec/batch\n",
      "Epoch 8/20  Iteration 12790/35720 Training loss: 0.9265 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 12791/35720 Training loss: 0.9265 0.2224 sec/batch\n",
      "Epoch 8/20  Iteration 12792/35720 Training loss: 0.9265 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 12793/35720 Training loss: 0.9264 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 12794/35720 Training loss: 0.9264 0.2127 sec/batch\n",
      "Epoch 8/20  Iteration 12795/35720 Training loss: 0.9264 0.2206 sec/batch\n",
      "Epoch 8/20  Iteration 12796/35720 Training loss: 0.9264 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 12797/35720 Training loss: 0.9268 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 12798/35720 Training loss: 0.9268 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 12799/35720 Training loss: 0.9267 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 12800/35720 Training loss: 0.9269 0.2132 sec/batch\n",
      "Validation loss: 1.33558 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 12801/35720 Training loss: 0.9275 0.2107 sec/batch\n",
      "Epoch 8/20  Iteration 12802/35720 Training loss: 0.9276 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 12803/35720 Training loss: 0.9276 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 12804/35720 Training loss: 0.9274 0.2077 sec/batch\n",
      "Epoch 8/20  Iteration 12805/35720 Training loss: 0.9274 0.2288 sec/batch\n",
      "Epoch 8/20  Iteration 12806/35720 Training loss: 0.9274 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 12807/35720 Training loss: 0.9273 0.2272 sec/batch\n",
      "Epoch 8/20  Iteration 12808/35720 Training loss: 0.9271 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 12809/35720 Training loss: 0.9271 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 12810/35720 Training loss: 0.9272 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 12811/35720 Training loss: 0.9270 0.2245 sec/batch\n",
      "Epoch 8/20  Iteration 12812/35720 Training loss: 0.9268 0.2155 sec/batch\n",
      "Epoch 8/20  Iteration 12813/35720 Training loss: 0.9267 0.2210 sec/batch\n",
      "Epoch 8/20  Iteration 12814/35720 Training loss: 0.9267 0.2317 sec/batch\n",
      "Epoch 8/20  Iteration 12815/35720 Training loss: 0.9266 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 12816/35720 Training loss: 0.9265 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 12817/35720 Training loss: 0.9264 0.2108 sec/batch\n",
      "Epoch 8/20  Iteration 12818/35720 Training loss: 0.9263 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 12819/35720 Training loss: 0.9261 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 12820/35720 Training loss: 0.9262 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 12821/35720 Training loss: 0.9263 0.2247 sec/batch\n",
      "Epoch 8/20  Iteration 12822/35720 Training loss: 0.9262 0.2210 sec/batch\n",
      "Epoch 8/20  Iteration 12823/35720 Training loss: 0.9263 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 12824/35720 Training loss: 0.9263 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 12825/35720 Training loss: 0.9264 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 12826/35720 Training loss: 0.9265 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 12827/35720 Training loss: 0.9263 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 12828/35720 Training loss: 0.9265 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 12829/35720 Training loss: 0.9265 0.2332 sec/batch\n",
      "Epoch 8/20  Iteration 12830/35720 Training loss: 0.9266 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 12831/35720 Training loss: 0.9266 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 12832/35720 Training loss: 0.9266 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 12833/35720 Training loss: 0.9267 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 12834/35720 Training loss: 0.9268 0.2212 sec/batch\n",
      "Epoch 8/20  Iteration 12835/35720 Training loss: 0.9268 0.2205 sec/batch\n",
      "Epoch 8/20  Iteration 12836/35720 Training loss: 0.9268 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 12837/35720 Training loss: 0.9267 0.2230 sec/batch\n",
      "Epoch 8/20  Iteration 12838/35720 Training loss: 0.9265 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 12839/35720 Training loss: 0.9265 0.2219 sec/batch\n",
      "Epoch 8/20  Iteration 12840/35720 Training loss: 0.9263 0.2205 sec/batch\n",
      "Epoch 8/20  Iteration 12841/35720 Training loss: 0.9263 0.2068 sec/batch\n",
      "Epoch 8/20  Iteration 12842/35720 Training loss: 0.9264 0.2073 sec/batch\n",
      "Epoch 8/20  Iteration 12843/35720 Training loss: 0.9263 0.2131 sec/batch\n",
      "Epoch 8/20  Iteration 12844/35720 Training loss: 0.9262 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 12845/35720 Training loss: 0.9262 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 12846/35720 Training loss: 0.9262 0.2340 sec/batch\n",
      "Epoch 8/20  Iteration 12847/35720 Training loss: 0.9260 0.2225 sec/batch\n",
      "Epoch 8/20  Iteration 12848/35720 Training loss: 0.9261 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 12849/35720 Training loss: 0.9262 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 12850/35720 Training loss: 0.9263 0.2265 sec/batch\n",
      "Epoch 8/20  Iteration 12851/35720 Training loss: 0.9264 0.2202 sec/batch\n",
      "Epoch 8/20  Iteration 12852/35720 Training loss: 0.9264 0.2196 sec/batch\n",
      "Epoch 8/20  Iteration 12853/35720 Training loss: 0.9264 0.2175 sec/batch\n",
      "Epoch 8/20  Iteration 12854/35720 Training loss: 0.9264 0.2133 sec/batch\n",
      "Epoch 8/20  Iteration 12855/35720 Training loss: 0.9264 0.2397 sec/batch\n",
      "Epoch 8/20  Iteration 12856/35720 Training loss: 0.9264 0.2260 sec/batch\n",
      "Epoch 8/20  Iteration 12857/35720 Training loss: 0.9265 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 12858/35720 Training loss: 0.9265 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 12859/35720 Training loss: 0.9265 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 12860/35720 Training loss: 0.9265 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 12861/35720 Training loss: 0.9264 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 12862/35720 Training loss: 0.9263 0.2280 sec/batch\n",
      "Epoch 8/20  Iteration 12863/35720 Training loss: 0.9262 0.2202 sec/batch\n",
      "Epoch 8/20  Iteration 12864/35720 Training loss: 0.9263 0.2108 sec/batch\n",
      "Epoch 8/20  Iteration 12865/35720 Training loss: 0.9262 0.2285 sec/batch\n",
      "Epoch 8/20  Iteration 12866/35720 Training loss: 0.9262 0.2331 sec/batch\n",
      "Epoch 8/20  Iteration 12867/35720 Training loss: 0.9261 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 12868/35720 Training loss: 0.9261 0.2280 sec/batch\n",
      "Epoch 8/20  Iteration 12869/35720 Training loss: 0.9261 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 12870/35720 Training loss: 0.9259 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 12871/35720 Training loss: 0.9257 0.2233 sec/batch\n",
      "Epoch 8/20  Iteration 12872/35720 Training loss: 0.9256 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 12873/35720 Training loss: 0.9255 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 12874/35720 Training loss: 0.9254 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 12875/35720 Training loss: 0.9253 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 12876/35720 Training loss: 0.9253 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 12877/35720 Training loss: 0.9253 0.2224 sec/batch\n",
      "Epoch 8/20  Iteration 12878/35720 Training loss: 0.9253 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 12879/35720 Training loss: 0.9253 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 12880/35720 Training loss: 0.9253 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 12881/35720 Training loss: 0.9251 0.2412 sec/batch\n",
      "Epoch 8/20  Iteration 12882/35720 Training loss: 0.9250 0.2218 sec/batch\n",
      "Epoch 8/20  Iteration 12883/35720 Training loss: 0.9249 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 12884/35720 Training loss: 0.9249 0.2198 sec/batch\n",
      "Epoch 8/20  Iteration 12885/35720 Training loss: 0.9248 0.2197 sec/batch\n",
      "Epoch 8/20  Iteration 12886/35720 Training loss: 0.9249 0.2074 sec/batch\n",
      "Epoch 8/20  Iteration 12887/35720 Training loss: 0.9248 0.2139 sec/batch\n",
      "Epoch 8/20  Iteration 12888/35720 Training loss: 0.9248 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 12889/35720 Training loss: 0.9249 0.2277 sec/batch\n",
      "Epoch 8/20  Iteration 12890/35720 Training loss: 0.9249 0.2250 sec/batch\n",
      "Epoch 8/20  Iteration 12891/35720 Training loss: 0.9250 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 12892/35720 Training loss: 0.9249 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 12893/35720 Training loss: 0.9248 0.2131 sec/batch\n",
      "Epoch 8/20  Iteration 12894/35720 Training loss: 0.9246 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 12895/35720 Training loss: 0.9247 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 12896/35720 Training loss: 0.9244 0.2076 sec/batch\n",
      "Epoch 8/20  Iteration 12897/35720 Training loss: 0.9244 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 12898/35720 Training loss: 0.9243 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 12899/35720 Training loss: 0.9243 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 12900/35720 Training loss: 0.9240 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 12901/35720 Training loss: 0.9239 0.2175 sec/batch\n",
      "Epoch 8/20  Iteration 12902/35720 Training loss: 0.9238 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 12903/35720 Training loss: 0.9237 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 12904/35720 Training loss: 0.9239 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 12905/35720 Training loss: 0.9240 0.2290 sec/batch\n",
      "Epoch 8/20  Iteration 12906/35720 Training loss: 0.9239 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 12907/35720 Training loss: 0.9237 0.2259 sec/batch\n",
      "Epoch 8/20  Iteration 12908/35720 Training loss: 0.9237 0.2056 sec/batch\n",
      "Epoch 8/20  Iteration 12909/35720 Training loss: 0.9236 0.2125 sec/batch\n",
      "Epoch 8/20  Iteration 12910/35720 Training loss: 0.9235 0.2171 sec/batch\n",
      "Epoch 8/20  Iteration 12911/35720 Training loss: 0.9234 0.2211 sec/batch\n",
      "Epoch 8/20  Iteration 12912/35720 Training loss: 0.9232 0.2217 sec/batch\n",
      "Epoch 8/20  Iteration 12913/35720 Training loss: 0.9232 0.2290 sec/batch\n",
      "Epoch 8/20  Iteration 12914/35720 Training loss: 0.9231 0.2100 sec/batch\n",
      "Epoch 8/20  Iteration 12915/35720 Training loss: 0.9229 0.2229 sec/batch\n",
      "Epoch 8/20  Iteration 12916/35720 Training loss: 0.9228 0.2252 sec/batch\n",
      "Epoch 8/20  Iteration 12917/35720 Training loss: 0.9227 0.2194 sec/batch\n",
      "Epoch 8/20  Iteration 12918/35720 Training loss: 0.9227 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 12919/35720 Training loss: 0.9226 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 12920/35720 Training loss: 0.9225 0.2107 sec/batch\n",
      "Epoch 8/20  Iteration 12921/35720 Training loss: 0.9223 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 12922/35720 Training loss: 0.9223 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 12923/35720 Training loss: 0.9223 0.2359 sec/batch\n",
      "Epoch 8/20  Iteration 12924/35720 Training loss: 0.9223 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 12925/35720 Training loss: 0.9224 0.2122 sec/batch\n",
      "Epoch 8/20  Iteration 12926/35720 Training loss: 0.9225 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 12927/35720 Training loss: 0.9224 0.2207 sec/batch\n",
      "Epoch 8/20  Iteration 12928/35720 Training loss: 0.9222 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 12929/35720 Training loss: 0.9221 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 12930/35720 Training loss: 0.9222 0.2054 sec/batch\n",
      "Epoch 8/20  Iteration 12931/35720 Training loss: 0.9222 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 12932/35720 Training loss: 0.9223 0.2281 sec/batch\n",
      "Epoch 8/20  Iteration 12933/35720 Training loss: 0.9224 0.2151 sec/batch\n",
      "Epoch 8/20  Iteration 12934/35720 Training loss: 0.9224 0.2267 sec/batch\n",
      "Epoch 8/20  Iteration 12935/35720 Training loss: 0.9226 0.2083 sec/batch\n",
      "Epoch 8/20  Iteration 12936/35720 Training loss: 0.9227 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 12937/35720 Training loss: 0.9228 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 12938/35720 Training loss: 0.9226 0.2273 sec/batch\n",
      "Epoch 8/20  Iteration 12939/35720 Training loss: 0.9228 0.2138 sec/batch\n",
      "Epoch 8/20  Iteration 12940/35720 Training loss: 0.9229 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 12941/35720 Training loss: 0.9230 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 12942/35720 Training loss: 0.9230 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 12943/35720 Training loss: 0.9232 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 12944/35720 Training loss: 0.9234 0.2341 sec/batch\n",
      "Epoch 8/20  Iteration 12945/35720 Training loss: 0.9234 0.2277 sec/batch\n",
      "Epoch 8/20  Iteration 12946/35720 Training loss: 0.9234 0.2258 sec/batch\n",
      "Epoch 8/20  Iteration 12947/35720 Training loss: 0.9234 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 12948/35720 Training loss: 0.9235 0.2299 sec/batch\n",
      "Epoch 8/20  Iteration 12949/35720 Training loss: 0.9237 0.2303 sec/batch\n",
      "Epoch 8/20  Iteration 12950/35720 Training loss: 0.9239 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 12951/35720 Training loss: 0.9240 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 12952/35720 Training loss: 0.9239 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 12953/35720 Training loss: 0.9238 0.2061 sec/batch\n",
      "Epoch 8/20  Iteration 12954/35720 Training loss: 0.9237 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 12955/35720 Training loss: 0.9237 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 12956/35720 Training loss: 0.9238 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 12957/35720 Training loss: 0.9239 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 12958/35720 Training loss: 0.9241 0.2130 sec/batch\n",
      "Epoch 8/20  Iteration 12959/35720 Training loss: 0.9243 0.2247 sec/batch\n",
      "Epoch 8/20  Iteration 12960/35720 Training loss: 0.9244 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 12961/35720 Training loss: 0.9243 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 12962/35720 Training loss: 0.9241 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 12963/35720 Training loss: 0.9242 0.2133 sec/batch\n",
      "Epoch 8/20  Iteration 12964/35720 Training loss: 0.9242 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 12965/35720 Training loss: 0.9243 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 12966/35720 Training loss: 0.9242 0.2107 sec/batch\n",
      "Epoch 8/20  Iteration 12967/35720 Training loss: 0.9242 0.2223 sec/batch\n",
      "Epoch 8/20  Iteration 12968/35720 Training loss: 0.9242 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 12969/35720 Training loss: 0.9242 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 12970/35720 Training loss: 0.9241 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 12971/35720 Training loss: 0.9240 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 12972/35720 Training loss: 0.9238 0.2186 sec/batch\n",
      "Epoch 8/20  Iteration 12973/35720 Training loss: 0.9237 0.2123 sec/batch\n",
      "Epoch 8/20  Iteration 12974/35720 Training loss: 0.9237 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 12975/35720 Training loss: 0.9236 0.2205 sec/batch\n",
      "Epoch 8/20  Iteration 12976/35720 Training loss: 0.9235 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 12977/35720 Training loss: 0.9236 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 12978/35720 Training loss: 0.9235 0.2276 sec/batch\n",
      "Epoch 8/20  Iteration 12979/35720 Training loss: 0.9236 0.2139 sec/batch\n",
      "Epoch 8/20  Iteration 12980/35720 Training loss: 0.9236 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 12981/35720 Training loss: 0.9236 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 12982/35720 Training loss: 0.9235 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 12983/35720 Training loss: 0.9233 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 12984/35720 Training loss: 0.9233 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 12985/35720 Training loss: 0.9233 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 12986/35720 Training loss: 0.9233 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 12987/35720 Training loss: 0.9233 0.2190 sec/batch\n",
      "Epoch 8/20  Iteration 12988/35720 Training loss: 0.9232 0.2236 sec/batch\n",
      "Epoch 8/20  Iteration 12989/35720 Training loss: 0.9231 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 12990/35720 Training loss: 0.9230 0.2235 sec/batch\n",
      "Epoch 8/20  Iteration 12991/35720 Training loss: 0.9230 0.2285 sec/batch\n",
      "Epoch 8/20  Iteration 12992/35720 Training loss: 0.9229 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 12993/35720 Training loss: 0.9228 0.2143 sec/batch\n",
      "Epoch 8/20  Iteration 12994/35720 Training loss: 0.9229 0.2216 sec/batch\n",
      "Epoch 8/20  Iteration 12995/35720 Training loss: 0.9228 0.2280 sec/batch\n",
      "Epoch 8/20  Iteration 12996/35720 Training loss: 0.9228 0.2281 sec/batch\n",
      "Epoch 8/20  Iteration 12997/35720 Training loss: 0.9228 0.2269 sec/batch\n",
      "Epoch 8/20  Iteration 12998/35720 Training loss: 0.9226 0.2190 sec/batch\n",
      "Epoch 8/20  Iteration 12999/35720 Training loss: 0.9227 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 13000/35720 Training loss: 0.9227 0.2202 sec/batch\n",
      "Validation loss: 1.34378 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 13001/35720 Training loss: 0.9232 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13002/35720 Training loss: 0.9232 0.2081 sec/batch\n",
      "Epoch 8/20  Iteration 13003/35720 Training loss: 0.9231 0.2141 sec/batch\n",
      "Epoch 8/20  Iteration 13004/35720 Training loss: 0.9229 0.2243 sec/batch\n",
      "Epoch 8/20  Iteration 13005/35720 Training loss: 0.9227 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 13006/35720 Training loss: 0.9227 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 13007/35720 Training loss: 0.9227 0.2076 sec/batch\n",
      "Epoch 8/20  Iteration 13008/35720 Training loss: 0.9227 0.2100 sec/batch\n",
      "Epoch 8/20  Iteration 13009/35720 Training loss: 0.9226 0.2330 sec/batch\n",
      "Epoch 8/20  Iteration 13010/35720 Training loss: 0.9226 0.2268 sec/batch\n",
      "Epoch 8/20  Iteration 13011/35720 Training loss: 0.9226 0.2191 sec/batch\n",
      "Epoch 8/20  Iteration 13012/35720 Training loss: 0.9225 0.2151 sec/batch\n",
      "Epoch 8/20  Iteration 13013/35720 Training loss: 0.9224 0.2228 sec/batch\n",
      "Epoch 8/20  Iteration 13014/35720 Training loss: 0.9223 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 13015/35720 Training loss: 0.9223 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 13016/35720 Training loss: 0.9223 0.2203 sec/batch\n",
      "Epoch 8/20  Iteration 13017/35720 Training loss: 0.9223 0.2271 sec/batch\n",
      "Epoch 8/20  Iteration 13018/35720 Training loss: 0.9223 0.2285 sec/batch\n",
      "Epoch 8/20  Iteration 13019/35720 Training loss: 0.9223 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 13020/35720 Training loss: 0.9223 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 13021/35720 Training loss: 0.9222 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 13022/35720 Training loss: 0.9223 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13023/35720 Training loss: 0.9221 0.2057 sec/batch\n",
      "Epoch 8/20  Iteration 13024/35720 Training loss: 0.9222 0.2138 sec/batch\n",
      "Epoch 8/20  Iteration 13025/35720 Training loss: 0.9221 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 13026/35720 Training loss: 0.9220 0.2282 sec/batch\n",
      "Epoch 8/20  Iteration 13027/35720 Training loss: 0.9220 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 13028/35720 Training loss: 0.9219 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 13029/35720 Training loss: 0.9219 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 13030/35720 Training loss: 0.9218 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 13031/35720 Training loss: 0.9218 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13032/35720 Training loss: 0.9217 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13033/35720 Training loss: 0.9217 0.2053 sec/batch\n",
      "Epoch 8/20  Iteration 13034/35720 Training loss: 0.9217 0.2247 sec/batch\n",
      "Epoch 8/20  Iteration 13035/35720 Training loss: 0.9217 0.2156 sec/batch\n",
      "Epoch 8/20  Iteration 13036/35720 Training loss: 0.9217 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 13037/35720 Training loss: 0.9215 0.2263 sec/batch\n",
      "Epoch 8/20  Iteration 13038/35720 Training loss: 0.9214 0.2253 sec/batch\n",
      "Epoch 8/20  Iteration 13039/35720 Training loss: 0.9213 0.2211 sec/batch\n",
      "Epoch 8/20  Iteration 13040/35720 Training loss: 0.9212 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 13041/35720 Training loss: 0.9213 0.2083 sec/batch\n",
      "Epoch 8/20  Iteration 13042/35720 Training loss: 0.9212 0.2107 sec/batch\n",
      "Epoch 8/20  Iteration 13043/35720 Training loss: 0.9211 0.2281 sec/batch\n",
      "Epoch 8/20  Iteration 13044/35720 Training loss: 0.9209 0.2319 sec/batch\n",
      "Epoch 8/20  Iteration 13045/35720 Training loss: 0.9208 0.2257 sec/batch\n",
      "Epoch 8/20  Iteration 13046/35720 Training loss: 0.9207 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13047/35720 Training loss: 0.9208 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 13048/35720 Training loss: 0.9209 0.2307 sec/batch\n",
      "Epoch 8/20  Iteration 13049/35720 Training loss: 0.9209 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 13050/35720 Training loss: 0.9209 0.2258 sec/batch\n",
      "Epoch 8/20  Iteration 13051/35720 Training loss: 0.9208 0.2248 sec/batch\n",
      "Epoch 8/20  Iteration 13052/35720 Training loss: 0.9208 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13053/35720 Training loss: 0.9208 0.2118 sec/batch\n",
      "Epoch 8/20  Iteration 13054/35720 Training loss: 0.9208 0.2220 sec/batch\n",
      "Epoch 8/20  Iteration 13055/35720 Training loss: 0.9207 0.2190 sec/batch\n",
      "Epoch 8/20  Iteration 13056/35720 Training loss: 0.9207 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 13057/35720 Training loss: 0.9206 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13058/35720 Training loss: 0.9207 0.2188 sec/batch\n",
      "Epoch 8/20  Iteration 13059/35720 Training loss: 0.9206 0.2134 sec/batch\n",
      "Epoch 8/20  Iteration 13060/35720 Training loss: 0.9207 0.2235 sec/batch\n",
      "Epoch 8/20  Iteration 13061/35720 Training loss: 0.9207 0.2240 sec/batch\n",
      "Epoch 8/20  Iteration 13062/35720 Training loss: 0.9206 0.2057 sec/batch\n",
      "Epoch 8/20  Iteration 13063/35720 Training loss: 0.9207 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 13064/35720 Training loss: 0.9205 0.2123 sec/batch\n",
      "Epoch 8/20  Iteration 13065/35720 Training loss: 0.9205 0.2253 sec/batch\n",
      "Epoch 8/20  Iteration 13066/35720 Training loss: 0.9205 0.2421 sec/batch\n",
      "Epoch 8/20  Iteration 13067/35720 Training loss: 0.9203 0.2174 sec/batch\n",
      "Epoch 8/20  Iteration 13068/35720 Training loss: 0.9203 0.2063 sec/batch\n",
      "Epoch 8/20  Iteration 13069/35720 Training loss: 0.9203 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13070/35720 Training loss: 0.9201 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 13071/35720 Training loss: 0.9201 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 13072/35720 Training loss: 0.9200 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13073/35720 Training loss: 0.9200 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 13074/35720 Training loss: 0.9201 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 13075/35720 Training loss: 0.9202 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 13076/35720 Training loss: 0.9202 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 13077/35720 Training loss: 0.9203 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 13078/35720 Training loss: 0.9203 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13079/35720 Training loss: 0.9203 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 13080/35720 Training loss: 0.9203 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 13081/35720 Training loss: 0.9203 0.2276 sec/batch\n",
      "Epoch 8/20  Iteration 13082/35720 Training loss: 0.9202 0.2141 sec/batch\n",
      "Epoch 8/20  Iteration 13083/35720 Training loss: 0.9202 0.2251 sec/batch\n",
      "Epoch 8/20  Iteration 13084/35720 Training loss: 0.9203 0.2194 sec/batch\n",
      "Epoch 8/20  Iteration 13085/35720 Training loss: 0.9202 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 13086/35720 Training loss: 0.9202 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 13087/35720 Training loss: 0.9201 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 13088/35720 Training loss: 0.9199 0.2206 sec/batch\n",
      "Epoch 8/20  Iteration 13089/35720 Training loss: 0.9198 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 13090/35720 Training loss: 0.9198 0.2272 sec/batch\n",
      "Epoch 8/20  Iteration 13091/35720 Training loss: 0.9196 0.2263 sec/batch\n",
      "Epoch 8/20  Iteration 13092/35720 Training loss: 0.9196 0.2251 sec/batch\n",
      "Epoch 8/20  Iteration 13093/35720 Training loss: 0.9196 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 13094/35720 Training loss: 0.9196 0.2200 sec/batch\n",
      "Epoch 8/20  Iteration 13095/35720 Training loss: 0.9196 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 13096/35720 Training loss: 0.9196 0.2068 sec/batch\n",
      "Epoch 8/20  Iteration 13097/35720 Training loss: 0.9195 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 13098/35720 Training loss: 0.9195 0.2246 sec/batch\n",
      "Epoch 8/20  Iteration 13099/35720 Training loss: 0.9194 0.2279 sec/batch\n",
      "Epoch 8/20  Iteration 13100/35720 Training loss: 0.9195 0.2172 sec/batch\n",
      "Epoch 8/20  Iteration 13101/35720 Training loss: 0.9194 0.2211 sec/batch\n",
      "Epoch 8/20  Iteration 13102/35720 Training loss: 0.9193 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 13103/35720 Training loss: 0.9192 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 13104/35720 Training loss: 0.9191 0.2130 sec/batch\n",
      "Epoch 8/20  Iteration 13105/35720 Training loss: 0.9190 0.2229 sec/batch\n",
      "Epoch 8/20  Iteration 13106/35720 Training loss: 0.9190 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 13107/35720 Training loss: 0.9190 0.2070 sec/batch\n",
      "Epoch 8/20  Iteration 13108/35720 Training loss: 0.9190 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 13109/35720 Training loss: 0.9190 0.2335 sec/batch\n",
      "Epoch 8/20  Iteration 13110/35720 Training loss: 0.9190 0.2136 sec/batch\n",
      "Epoch 8/20  Iteration 13111/35720 Training loss: 0.9191 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 13112/35720 Training loss: 0.9190 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 13113/35720 Training loss: 0.9190 0.2138 sec/batch\n",
      "Epoch 8/20  Iteration 13114/35720 Training loss: 0.9188 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 13115/35720 Training loss: 0.9188 0.2332 sec/batch\n",
      "Epoch 8/20  Iteration 13116/35720 Training loss: 0.9188 0.2118 sec/batch\n",
      "Epoch 8/20  Iteration 13117/35720 Training loss: 0.9188 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 13118/35720 Training loss: 0.9187 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 13119/35720 Training loss: 0.9187 0.2295 sec/batch\n",
      "Epoch 8/20  Iteration 13120/35720 Training loss: 0.9186 0.2258 sec/batch\n",
      "Epoch 8/20  Iteration 13121/35720 Training loss: 0.9185 0.2174 sec/batch\n",
      "Epoch 8/20  Iteration 13122/35720 Training loss: 0.9184 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 13123/35720 Training loss: 0.9184 0.2074 sec/batch\n",
      "Epoch 8/20  Iteration 13124/35720 Training loss: 0.9185 0.2106 sec/batch\n",
      "Epoch 8/20  Iteration 13125/35720 Training loss: 0.9184 0.2254 sec/batch\n",
      "Epoch 8/20  Iteration 13126/35720 Training loss: 0.9182 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 13127/35720 Training loss: 0.9181 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 13128/35720 Training loss: 0.9181 0.2268 sec/batch\n",
      "Epoch 8/20  Iteration 13129/35720 Training loss: 0.9182 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 13130/35720 Training loss: 0.9180 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 13131/35720 Training loss: 0.9180 0.2224 sec/batch\n",
      "Epoch 8/20  Iteration 13132/35720 Training loss: 0.9179 0.2197 sec/batch\n",
      "Epoch 8/20  Iteration 13133/35720 Training loss: 0.9180 0.2247 sec/batch\n",
      "Epoch 8/20  Iteration 13134/35720 Training loss: 0.9179 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 13135/35720 Training loss: 0.9178 0.2071 sec/batch\n",
      "Epoch 8/20  Iteration 13136/35720 Training loss: 0.9178 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 13137/35720 Training loss: 0.9178 0.2311 sec/batch\n",
      "Epoch 8/20  Iteration 13138/35720 Training loss: 0.9178 0.2230 sec/batch\n",
      "Epoch 8/20  Iteration 13139/35720 Training loss: 0.9177 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13140/35720 Training loss: 0.9176 0.2069 sec/batch\n",
      "Epoch 8/20  Iteration 13141/35720 Training loss: 0.9176 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 13142/35720 Training loss: 0.9177 0.2141 sec/batch\n",
      "Epoch 8/20  Iteration 13143/35720 Training loss: 0.9178 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 13144/35720 Training loss: 0.9178 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 13145/35720 Training loss: 0.9179 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 13146/35720 Training loss: 0.9178 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 13147/35720 Training loss: 0.9178 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 13148/35720 Training loss: 0.9178 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 13149/35720 Training loss: 0.9177 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 13150/35720 Training loss: 0.9177 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 13151/35720 Training loss: 0.9176 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 13152/35720 Training loss: 0.9175 0.2080 sec/batch\n",
      "Epoch 8/20  Iteration 13153/35720 Training loss: 0.9175 0.2249 sec/batch\n",
      "Epoch 8/20  Iteration 13154/35720 Training loss: 0.9175 0.2207 sec/batch\n",
      "Epoch 8/20  Iteration 13155/35720 Training loss: 0.9175 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 13156/35720 Training loss: 0.9176 0.2070 sec/batch\n",
      "Epoch 8/20  Iteration 13157/35720 Training loss: 0.9176 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 13158/35720 Training loss: 0.9176 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 13159/35720 Training loss: 0.9178 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 13160/35720 Training loss: 0.9178 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 13161/35720 Training loss: 0.9178 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 13162/35720 Training loss: 0.9178 0.2224 sec/batch\n",
      "Epoch 8/20  Iteration 13163/35720 Training loss: 0.9179 0.2252 sec/batch\n",
      "Epoch 8/20  Iteration 13164/35720 Training loss: 0.9179 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 13165/35720 Training loss: 0.9179 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 13166/35720 Training loss: 0.9179 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 13167/35720 Training loss: 0.9179 0.2203 sec/batch\n",
      "Epoch 8/20  Iteration 13168/35720 Training loss: 0.9180 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 13169/35720 Training loss: 0.9181 0.2118 sec/batch\n",
      "Epoch 8/20  Iteration 13170/35720 Training loss: 0.9180 0.2246 sec/batch\n",
      "Epoch 8/20  Iteration 13171/35720 Training loss: 0.9179 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 13172/35720 Training loss: 0.9179 0.2202 sec/batch\n",
      "Epoch 8/20  Iteration 13173/35720 Training loss: 0.9177 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 13174/35720 Training loss: 0.9177 0.2183 sec/batch\n",
      "Epoch 8/20  Iteration 13175/35720 Training loss: 0.9178 0.2229 sec/batch\n",
      "Epoch 8/20  Iteration 13176/35720 Training loss: 0.9177 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 13177/35720 Training loss: 0.9177 0.2337 sec/batch\n",
      "Epoch 8/20  Iteration 13178/35720 Training loss: 0.9176 0.2220 sec/batch\n",
      "Epoch 8/20  Iteration 13179/35720 Training loss: 0.9176 0.2056 sec/batch\n",
      "Epoch 8/20  Iteration 13180/35720 Training loss: 0.9176 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 13181/35720 Training loss: 0.9176 0.2212 sec/batch\n",
      "Epoch 8/20  Iteration 13182/35720 Training loss: 0.9175 0.2183 sec/batch\n",
      "Epoch 8/20  Iteration 13183/35720 Training loss: 0.9175 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 13184/35720 Training loss: 0.9174 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 13185/35720 Training loss: 0.9174 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 13186/35720 Training loss: 0.9173 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 13187/35720 Training loss: 0.9174 0.2186 sec/batch\n",
      "Epoch 8/20  Iteration 13188/35720 Training loss: 0.9174 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13189/35720 Training loss: 0.9174 0.2243 sec/batch\n",
      "Epoch 8/20  Iteration 13190/35720 Training loss: 0.9174 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 13191/35720 Training loss: 0.9173 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 13192/35720 Training loss: 0.9173 0.2192 sec/batch\n",
      "Epoch 8/20  Iteration 13193/35720 Training loss: 0.9173 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13194/35720 Training loss: 0.9174 0.2253 sec/batch\n",
      "Epoch 8/20  Iteration 13195/35720 Training loss: 0.9176 0.2233 sec/batch\n",
      "Epoch 8/20  Iteration 13196/35720 Training loss: 0.9176 0.2056 sec/batch\n",
      "Epoch 8/20  Iteration 13197/35720 Training loss: 0.9176 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 13198/35720 Training loss: 0.9176 0.2207 sec/batch\n",
      "Epoch 8/20  Iteration 13199/35720 Training loss: 0.9176 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 13200/35720 Training loss: 0.9176 0.2141 sec/batch\n",
      "Validation loss: 1.34283 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 13201/35720 Training loss: 0.9179 0.2108 sec/batch\n",
      "Epoch 8/20  Iteration 13202/35720 Training loss: 0.9179 0.2143 sec/batch\n",
      "Epoch 8/20  Iteration 13203/35720 Training loss: 0.9177 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 13204/35720 Training loss: 0.9178 0.2263 sec/batch\n",
      "Epoch 8/20  Iteration 13205/35720 Training loss: 0.9177 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 13206/35720 Training loss: 0.9177 0.2124 sec/batch\n",
      "Epoch 8/20  Iteration 13207/35720 Training loss: 0.9177 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 13208/35720 Training loss: 0.9178 0.2307 sec/batch\n",
      "Epoch 8/20  Iteration 13209/35720 Training loss: 0.9179 0.2258 sec/batch\n",
      "Epoch 8/20  Iteration 13210/35720 Training loss: 0.9179 0.2265 sec/batch\n",
      "Epoch 8/20  Iteration 13211/35720 Training loss: 0.9180 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 13212/35720 Training loss: 0.9180 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 13213/35720 Training loss: 0.9181 0.2279 sec/batch\n",
      "Epoch 8/20  Iteration 13214/35720 Training loss: 0.9181 0.2305 sec/batch\n",
      "Epoch 8/20  Iteration 13215/35720 Training loss: 0.9181 0.2177 sec/batch\n",
      "Epoch 8/20  Iteration 13216/35720 Training loss: 0.9181 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 13217/35720 Training loss: 0.9182 0.2278 sec/batch\n",
      "Epoch 8/20  Iteration 13218/35720 Training loss: 0.9182 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 13219/35720 Training loss: 0.9184 0.2125 sec/batch\n",
      "Epoch 8/20  Iteration 13220/35720 Training loss: 0.9184 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13221/35720 Training loss: 0.9183 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13222/35720 Training loss: 0.9184 0.2146 sec/batch\n",
      "Epoch 8/20  Iteration 13223/35720 Training loss: 0.9183 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 13224/35720 Training loss: 0.9184 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 13225/35720 Training loss: 0.9185 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13226/35720 Training loss: 0.9184 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 13227/35720 Training loss: 0.9184 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 13228/35720 Training loss: 0.9184 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 13229/35720 Training loss: 0.9185 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 13230/35720 Training loss: 0.9186 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 13231/35720 Training loss: 0.9186 0.2200 sec/batch\n",
      "Epoch 8/20  Iteration 13232/35720 Training loss: 0.9186 0.2127 sec/batch\n",
      "Epoch 8/20  Iteration 13233/35720 Training loss: 0.9185 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13234/35720 Training loss: 0.9185 0.2146 sec/batch\n",
      "Epoch 8/20  Iteration 13235/35720 Training loss: 0.9185 0.2100 sec/batch\n",
      "Epoch 8/20  Iteration 13236/35720 Training loss: 0.9185 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 13237/35720 Training loss: 0.9184 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 13238/35720 Training loss: 0.9185 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13239/35720 Training loss: 0.9185 0.2212 sec/batch\n",
      "Epoch 8/20  Iteration 13240/35720 Training loss: 0.9185 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 13241/35720 Training loss: 0.9185 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 13242/35720 Training loss: 0.9186 0.2108 sec/batch\n",
      "Epoch 8/20  Iteration 13243/35720 Training loss: 0.9185 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 13244/35720 Training loss: 0.9185 0.2281 sec/batch\n",
      "Epoch 8/20  Iteration 13245/35720 Training loss: 0.9185 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 13246/35720 Training loss: 0.9185 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 13247/35720 Training loss: 0.9185 0.2131 sec/batch\n",
      "Epoch 8/20  Iteration 13248/35720 Training loss: 0.9186 0.2254 sec/batch\n",
      "Epoch 8/20  Iteration 13249/35720 Training loss: 0.9186 0.2296 sec/batch\n",
      "Epoch 8/20  Iteration 13250/35720 Training loss: 0.9185 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 13251/35720 Training loss: 0.9185 0.2119 sec/batch\n",
      "Epoch 8/20  Iteration 13252/35720 Training loss: 0.9185 0.2171 sec/batch\n",
      "Epoch 8/20  Iteration 13253/35720 Training loss: 0.9184 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 13254/35720 Training loss: 0.9186 0.2240 sec/batch\n",
      "Epoch 8/20  Iteration 13255/35720 Training loss: 0.9184 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 13256/35720 Training loss: 0.9184 0.2155 sec/batch\n",
      "Epoch 8/20  Iteration 13257/35720 Training loss: 0.9183 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 13258/35720 Training loss: 0.9183 0.2311 sec/batch\n",
      "Epoch 8/20  Iteration 13259/35720 Training loss: 0.9182 0.2203 sec/batch\n",
      "Epoch 8/20  Iteration 13260/35720 Training loss: 0.9183 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13261/35720 Training loss: 0.9184 0.2047 sec/batch\n",
      "Epoch 8/20  Iteration 13262/35720 Training loss: 0.9184 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 13263/35720 Training loss: 0.9185 0.2212 sec/batch\n",
      "Epoch 8/20  Iteration 13264/35720 Training loss: 0.9185 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 13265/35720 Training loss: 0.9184 0.2353 sec/batch\n",
      "Epoch 8/20  Iteration 13266/35720 Training loss: 0.9184 0.2119 sec/batch\n",
      "Epoch 8/20  Iteration 13267/35720 Training loss: 0.9184 0.2226 sec/batch\n",
      "Epoch 8/20  Iteration 13268/35720 Training loss: 0.9185 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 13269/35720 Training loss: 0.9185 0.2281 sec/batch\n",
      "Epoch 8/20  Iteration 13270/35720 Training loss: 0.9185 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 13271/35720 Training loss: 0.9185 0.2278 sec/batch\n",
      "Epoch 8/20  Iteration 13272/35720 Training loss: 0.9187 0.2238 sec/batch\n",
      "Epoch 8/20  Iteration 13273/35720 Training loss: 0.9187 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 13274/35720 Training loss: 0.9189 0.2236 sec/batch\n",
      "Epoch 8/20  Iteration 13275/35720 Training loss: 0.9189 0.2195 sec/batch\n",
      "Epoch 8/20  Iteration 13276/35720 Training loss: 0.9188 0.2208 sec/batch\n",
      "Epoch 8/20  Iteration 13277/35720 Training loss: 0.9187 0.2070 sec/batch\n",
      "Epoch 8/20  Iteration 13278/35720 Training loss: 0.9186 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 13279/35720 Training loss: 0.9186 0.2128 sec/batch\n",
      "Epoch 8/20  Iteration 13280/35720 Training loss: 0.9187 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 13281/35720 Training loss: 0.9187 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 13282/35720 Training loss: 0.9187 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 13283/35720 Training loss: 0.9187 0.2124 sec/batch\n",
      "Epoch 8/20  Iteration 13284/35720 Training loss: 0.9187 0.2142 sec/batch\n",
      "Epoch 8/20  Iteration 13285/35720 Training loss: 0.9187 0.2243 sec/batch\n",
      "Epoch 8/20  Iteration 13286/35720 Training loss: 0.9187 0.2124 sec/batch\n",
      "Epoch 8/20  Iteration 13287/35720 Training loss: 0.9187 0.2183 sec/batch\n",
      "Epoch 8/20  Iteration 13288/35720 Training loss: 0.9186 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 13289/35720 Training loss: 0.9186 0.2210 sec/batch\n",
      "Epoch 8/20  Iteration 13290/35720 Training loss: 0.9186 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 13291/35720 Training loss: 0.9187 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13292/35720 Training loss: 0.9186 0.2125 sec/batch\n",
      "Epoch 8/20  Iteration 13293/35720 Training loss: 0.9187 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 13294/35720 Training loss: 0.9187 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13295/35720 Training loss: 0.9187 0.2207 sec/batch\n",
      "Epoch 8/20  Iteration 13296/35720 Training loss: 0.9186 0.2190 sec/batch\n",
      "Epoch 8/20  Iteration 13297/35720 Training loss: 0.9187 0.2198 sec/batch\n",
      "Epoch 8/20  Iteration 13298/35720 Training loss: 0.9187 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 13299/35720 Training loss: 0.9186 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 13300/35720 Training loss: 0.9185 0.2237 sec/batch\n",
      "Epoch 8/20  Iteration 13301/35720 Training loss: 0.9185 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 13302/35720 Training loss: 0.9185 0.2174 sec/batch\n",
      "Epoch 8/20  Iteration 13303/35720 Training loss: 0.9185 0.2136 sec/batch\n",
      "Epoch 8/20  Iteration 13304/35720 Training loss: 0.9186 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 13305/35720 Training loss: 0.9186 0.2079 sec/batch\n",
      "Epoch 8/20  Iteration 13306/35720 Training loss: 0.9187 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 13307/35720 Training loss: 0.9188 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13308/35720 Training loss: 0.9188 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 13309/35720 Training loss: 0.9188 0.2239 sec/batch\n",
      "Epoch 8/20  Iteration 13310/35720 Training loss: 0.9188 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 13311/35720 Training loss: 0.9189 0.2224 sec/batch\n",
      "Epoch 8/20  Iteration 13312/35720 Training loss: 0.9190 0.2118 sec/batch\n",
      "Epoch 8/20  Iteration 13313/35720 Training loss: 0.9190 0.2310 sec/batch\n",
      "Epoch 8/20  Iteration 13314/35720 Training loss: 0.9190 0.2281 sec/batch\n",
      "Epoch 8/20  Iteration 13315/35720 Training loss: 0.9191 0.2240 sec/batch\n",
      "Epoch 8/20  Iteration 13316/35720 Training loss: 0.9191 0.2171 sec/batch\n",
      "Epoch 8/20  Iteration 13317/35720 Training loss: 0.9192 0.2267 sec/batch\n",
      "Epoch 8/20  Iteration 13318/35720 Training loss: 0.9192 0.2271 sec/batch\n",
      "Epoch 8/20  Iteration 13319/35720 Training loss: 0.9191 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 13320/35720 Training loss: 0.9192 0.2218 sec/batch\n",
      "Epoch 8/20  Iteration 13321/35720 Training loss: 0.9191 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 13322/35720 Training loss: 0.9191 0.2193 sec/batch\n",
      "Epoch 8/20  Iteration 13323/35720 Training loss: 0.9191 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13324/35720 Training loss: 0.9191 0.2297 sec/batch\n",
      "Epoch 8/20  Iteration 13325/35720 Training loss: 0.9190 0.2277 sec/batch\n",
      "Epoch 8/20  Iteration 13326/35720 Training loss: 0.9189 0.2232 sec/batch\n",
      "Epoch 8/20  Iteration 13327/35720 Training loss: 0.9188 0.2146 sec/batch\n",
      "Epoch 8/20  Iteration 13328/35720 Training loss: 0.9188 0.2139 sec/batch\n",
      "Epoch 8/20  Iteration 13329/35720 Training loss: 0.9187 0.2274 sec/batch\n",
      "Epoch 8/20  Iteration 13330/35720 Training loss: 0.9186 0.2146 sec/batch\n",
      "Epoch 8/20  Iteration 13331/35720 Training loss: 0.9186 0.2281 sec/batch\n",
      "Epoch 8/20  Iteration 13332/35720 Training loss: 0.9185 0.2128 sec/batch\n",
      "Epoch 8/20  Iteration 13333/35720 Training loss: 0.9184 0.2174 sec/batch\n",
      "Epoch 8/20  Iteration 13334/35720 Training loss: 0.9185 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 13335/35720 Training loss: 0.9186 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13336/35720 Training loss: 0.9186 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 13337/35720 Training loss: 0.9185 0.2224 sec/batch\n",
      "Epoch 8/20  Iteration 13338/35720 Training loss: 0.9185 0.2265 sec/batch\n",
      "Epoch 8/20  Iteration 13339/35720 Training loss: 0.9185 0.2084 sec/batch\n",
      "Epoch 8/20  Iteration 13340/35720 Training loss: 0.9184 0.2212 sec/batch\n",
      "Epoch 8/20  Iteration 13341/35720 Training loss: 0.9185 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 13342/35720 Training loss: 0.9184 0.2280 sec/batch\n",
      "Epoch 8/20  Iteration 13343/35720 Training loss: 0.9185 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 13344/35720 Training loss: 0.9185 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 13345/35720 Training loss: 0.9184 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 13346/35720 Training loss: 0.9185 0.2212 sec/batch\n",
      "Epoch 8/20  Iteration 13347/35720 Training loss: 0.9185 0.2172 sec/batch\n",
      "Epoch 8/20  Iteration 13348/35720 Training loss: 0.9185 0.2247 sec/batch\n",
      "Epoch 8/20  Iteration 13349/35720 Training loss: 0.9184 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 13350/35720 Training loss: 0.9184 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 13351/35720 Training loss: 0.9184 0.2179 sec/batch\n",
      "Epoch 8/20  Iteration 13352/35720 Training loss: 0.9184 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 13353/35720 Training loss: 0.9184 0.2295 sec/batch\n",
      "Epoch 8/20  Iteration 13354/35720 Training loss: 0.9183 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 13355/35720 Training loss: 0.9183 0.2061 sec/batch\n",
      "Epoch 8/20  Iteration 13356/35720 Training loss: 0.9183 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 13357/35720 Training loss: 0.9184 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 13358/35720 Training loss: 0.9183 0.2183 sec/batch\n",
      "Epoch 8/20  Iteration 13359/35720 Training loss: 0.9182 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 13360/35720 Training loss: 0.9183 0.2058 sec/batch\n",
      "Epoch 8/20  Iteration 13361/35720 Training loss: 0.9182 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 13362/35720 Training loss: 0.9181 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 13363/35720 Training loss: 0.9182 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 13364/35720 Training loss: 0.9181 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 13365/35720 Training loss: 0.9182 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 13366/35720 Training loss: 0.9181 0.2210 sec/batch\n",
      "Epoch 8/20  Iteration 13367/35720 Training loss: 0.9180 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 13368/35720 Training loss: 0.9181 0.2338 sec/batch\n",
      "Epoch 8/20  Iteration 13369/35720 Training loss: 0.9182 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 13370/35720 Training loss: 0.9181 0.2298 sec/batch\n",
      "Epoch 8/20  Iteration 13371/35720 Training loss: 0.9181 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13372/35720 Training loss: 0.9181 0.2210 sec/batch\n",
      "Epoch 8/20  Iteration 13373/35720 Training loss: 0.9180 0.2100 sec/batch\n",
      "Epoch 8/20  Iteration 13374/35720 Training loss: 0.9180 0.2242 sec/batch\n",
      "Epoch 8/20  Iteration 13375/35720 Training loss: 0.9179 0.2213 sec/batch\n",
      "Epoch 8/20  Iteration 13376/35720 Training loss: 0.9179 0.2236 sec/batch\n",
      "Epoch 8/20  Iteration 13377/35720 Training loss: 0.9178 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 13378/35720 Training loss: 0.9178 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 13379/35720 Training loss: 0.9178 0.2215 sec/batch\n",
      "Epoch 8/20  Iteration 13380/35720 Training loss: 0.9178 0.2251 sec/batch\n",
      "Epoch 8/20  Iteration 13381/35720 Training loss: 0.9178 0.2273 sec/batch\n",
      "Epoch 8/20  Iteration 13382/35720 Training loss: 0.9177 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 13383/35720 Training loss: 0.9177 0.2148 sec/batch\n",
      "Epoch 8/20  Iteration 13384/35720 Training loss: 0.9177 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 13385/35720 Training loss: 0.9177 0.2190 sec/batch\n",
      "Epoch 8/20  Iteration 13386/35720 Training loss: 0.9177 0.2127 sec/batch\n",
      "Epoch 8/20  Iteration 13387/35720 Training loss: 0.9177 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 13388/35720 Training loss: 0.9176 0.2066 sec/batch\n",
      "Epoch 8/20  Iteration 13389/35720 Training loss: 0.9176 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 13390/35720 Training loss: 0.9176 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 13391/35720 Training loss: 0.9175 0.2086 sec/batch\n",
      "Epoch 8/20  Iteration 13392/35720 Training loss: 0.9174 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 13393/35720 Training loss: 0.9173 0.2207 sec/batch\n",
      "Epoch 8/20  Iteration 13394/35720 Training loss: 0.9173 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 13395/35720 Training loss: 0.9172 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13396/35720 Training loss: 0.9171 0.2261 sec/batch\n",
      "Epoch 8/20  Iteration 13397/35720 Training loss: 0.9171 0.2123 sec/batch\n",
      "Epoch 8/20  Iteration 13398/35720 Training loss: 0.9171 0.2269 sec/batch\n",
      "Epoch 8/20  Iteration 13399/35720 Training loss: 0.9170 0.2263 sec/batch\n",
      "Epoch 8/20  Iteration 13400/35720 Training loss: 0.9168 0.2161 sec/batch\n",
      "Validation loss: 1.35717 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 13401/35720 Training loss: 0.9171 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 13402/35720 Training loss: 0.9171 0.2239 sec/batch\n",
      "Epoch 8/20  Iteration 13403/35720 Training loss: 0.9170 0.2066 sec/batch\n",
      "Epoch 8/20  Iteration 13404/35720 Training loss: 0.9170 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 13405/35720 Training loss: 0.9169 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 13406/35720 Training loss: 0.9168 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 13407/35720 Training loss: 0.9168 0.2215 sec/batch\n",
      "Epoch 8/20  Iteration 13408/35720 Training loss: 0.9168 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13409/35720 Training loss: 0.9168 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 13410/35720 Training loss: 0.9168 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 13411/35720 Training loss: 0.9168 0.2221 sec/batch\n",
      "Epoch 8/20  Iteration 13412/35720 Training loss: 0.9168 0.2253 sec/batch\n",
      "Epoch 8/20  Iteration 13413/35720 Training loss: 0.9168 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 13414/35720 Training loss: 0.9168 0.2237 sec/batch\n",
      "Epoch 8/20  Iteration 13415/35720 Training loss: 0.9168 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13416/35720 Training loss: 0.9168 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 13417/35720 Training loss: 0.9169 0.2272 sec/batch\n",
      "Epoch 8/20  Iteration 13418/35720 Training loss: 0.9169 0.2138 sec/batch\n",
      "Epoch 8/20  Iteration 13419/35720 Training loss: 0.9169 0.2284 sec/batch\n",
      "Epoch 8/20  Iteration 13420/35720 Training loss: 0.9169 0.2238 sec/batch\n",
      "Epoch 8/20  Iteration 13421/35720 Training loss: 0.9168 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 13422/35720 Training loss: 0.9168 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 13423/35720 Training loss: 0.9168 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 13424/35720 Training loss: 0.9167 0.2267 sec/batch\n",
      "Epoch 8/20  Iteration 13425/35720 Training loss: 0.9167 0.2227 sec/batch\n",
      "Epoch 8/20  Iteration 13426/35720 Training loss: 0.9167 0.2151 sec/batch\n",
      "Epoch 8/20  Iteration 13427/35720 Training loss: 0.9167 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 13428/35720 Training loss: 0.9167 0.2151 sec/batch\n",
      "Epoch 8/20  Iteration 13429/35720 Training loss: 0.9168 0.2143 sec/batch\n",
      "Epoch 8/20  Iteration 13430/35720 Training loss: 0.9167 0.2280 sec/batch\n",
      "Epoch 8/20  Iteration 13431/35720 Training loss: 0.9168 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 13432/35720 Training loss: 0.9168 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 13433/35720 Training loss: 0.9169 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 13434/35720 Training loss: 0.9169 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13435/35720 Training loss: 0.9169 0.2314 sec/batch\n",
      "Epoch 8/20  Iteration 13436/35720 Training loss: 0.9169 0.2227 sec/batch\n",
      "Epoch 8/20  Iteration 13437/35720 Training loss: 0.9167 0.2086 sec/batch\n",
      "Epoch 8/20  Iteration 13438/35720 Training loss: 0.9166 0.2143 sec/batch\n",
      "Epoch 8/20  Iteration 13439/35720 Training loss: 0.9165 0.2247 sec/batch\n",
      "Epoch 8/20  Iteration 13440/35720 Training loss: 0.9165 0.2398 sec/batch\n",
      "Epoch 8/20  Iteration 13441/35720 Training loss: 0.9164 0.2259 sec/batch\n",
      "Epoch 8/20  Iteration 13442/35720 Training loss: 0.9163 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 13443/35720 Training loss: 0.9163 0.2297 sec/batch\n",
      "Epoch 8/20  Iteration 13444/35720 Training loss: 0.9163 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 13445/35720 Training loss: 0.9162 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 13446/35720 Training loss: 0.9163 0.2071 sec/batch\n",
      "Epoch 8/20  Iteration 13447/35720 Training loss: 0.9162 0.2058 sec/batch\n",
      "Epoch 8/20  Iteration 13448/35720 Training loss: 0.9162 0.2133 sec/batch\n",
      "Epoch 8/20  Iteration 13449/35720 Training loss: 0.9162 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 13450/35720 Training loss: 0.9161 0.2350 sec/batch\n",
      "Epoch 8/20  Iteration 13451/35720 Training loss: 0.9162 0.2211 sec/batch\n",
      "Epoch 8/20  Iteration 13452/35720 Training loss: 0.9161 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13453/35720 Training loss: 0.9160 0.2057 sec/batch\n",
      "Epoch 8/20  Iteration 13454/35720 Training loss: 0.9159 0.2069 sec/batch\n",
      "Epoch 8/20  Iteration 13455/35720 Training loss: 0.9158 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 13456/35720 Training loss: 0.9159 0.2191 sec/batch\n",
      "Epoch 8/20  Iteration 13457/35720 Training loss: 0.9158 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13458/35720 Training loss: 0.9157 0.2266 sec/batch\n",
      "Epoch 8/20  Iteration 13459/35720 Training loss: 0.9157 0.2063 sec/batch\n",
      "Epoch 8/20  Iteration 13460/35720 Training loss: 0.9156 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 13461/35720 Training loss: 0.9156 0.2239 sec/batch\n",
      "Epoch 8/20  Iteration 13462/35720 Training loss: 0.9156 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 13463/35720 Training loss: 0.9155 0.2156 sec/batch\n",
      "Epoch 8/20  Iteration 13464/35720 Training loss: 0.9155 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13465/35720 Training loss: 0.9154 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13466/35720 Training loss: 0.9154 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 13467/35720 Training loss: 0.9154 0.2228 sec/batch\n",
      "Epoch 8/20  Iteration 13468/35720 Training loss: 0.9153 0.2066 sec/batch\n",
      "Epoch 8/20  Iteration 13469/35720 Training loss: 0.9153 0.2232 sec/batch\n",
      "Epoch 8/20  Iteration 13470/35720 Training loss: 0.9153 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 13471/35720 Training loss: 0.9154 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 13472/35720 Training loss: 0.9156 0.2203 sec/batch\n",
      "Epoch 8/20  Iteration 13473/35720 Training loss: 0.9155 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 13474/35720 Training loss: 0.9154 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 13475/35720 Training loss: 0.9155 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 13476/35720 Training loss: 0.9154 0.2068 sec/batch\n",
      "Epoch 8/20  Iteration 13477/35720 Training loss: 0.9153 0.2141 sec/batch\n",
      "Epoch 8/20  Iteration 13478/35720 Training loss: 0.9153 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 13479/35720 Training loss: 0.9153 0.2183 sec/batch\n",
      "Epoch 8/20  Iteration 13480/35720 Training loss: 0.9154 0.2156 sec/batch\n",
      "Epoch 8/20  Iteration 13481/35720 Training loss: 0.9153 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 13482/35720 Training loss: 0.9153 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13483/35720 Training loss: 0.9153 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 13484/35720 Training loss: 0.9151 0.2179 sec/batch\n",
      "Epoch 8/20  Iteration 13485/35720 Training loss: 0.9151 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 13486/35720 Training loss: 0.9150 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 13487/35720 Training loss: 0.9151 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 13488/35720 Training loss: 0.9150 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13489/35720 Training loss: 0.9150 0.2331 sec/batch\n",
      "Epoch 8/20  Iteration 13490/35720 Training loss: 0.9150 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 13491/35720 Training loss: 0.9149 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13492/35720 Training loss: 0.9149 0.2248 sec/batch\n",
      "Epoch 8/20  Iteration 13493/35720 Training loss: 0.9149 0.2128 sec/batch\n",
      "Epoch 8/20  Iteration 13494/35720 Training loss: 0.9149 0.2069 sec/batch\n",
      "Epoch 8/20  Iteration 13495/35720 Training loss: 0.9149 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 13496/35720 Training loss: 0.9149 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 13497/35720 Training loss: 0.9150 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 13498/35720 Training loss: 0.9150 0.2062 sec/batch\n",
      "Epoch 8/20  Iteration 13499/35720 Training loss: 0.9150 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 13500/35720 Training loss: 0.9150 0.2122 sec/batch\n",
      "Epoch 8/20  Iteration 13501/35720 Training loss: 0.9150 0.2170 sec/batch\n",
      "Epoch 8/20  Iteration 13502/35720 Training loss: 0.9150 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13503/35720 Training loss: 0.9149 0.2063 sec/batch\n",
      "Epoch 8/20  Iteration 13504/35720 Training loss: 0.9148 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 13505/35720 Training loss: 0.9147 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 13506/35720 Training loss: 0.9146 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 13507/35720 Training loss: 0.9145 0.2138 sec/batch\n",
      "Epoch 8/20  Iteration 13508/35720 Training loss: 0.9145 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 13509/35720 Training loss: 0.9144 0.2061 sec/batch\n",
      "Epoch 8/20  Iteration 13510/35720 Training loss: 0.9144 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 13511/35720 Training loss: 0.9143 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13512/35720 Training loss: 0.9142 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 13513/35720 Training loss: 0.9142 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 13514/35720 Training loss: 0.9142 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 13515/35720 Training loss: 0.9142 0.2267 sec/batch\n",
      "Epoch 8/20  Iteration 13516/35720 Training loss: 0.9142 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13517/35720 Training loss: 0.9141 0.2215 sec/batch\n",
      "Epoch 8/20  Iteration 13518/35720 Training loss: 0.9141 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 13519/35720 Training loss: 0.9141 0.2144 sec/batch\n",
      "Epoch 8/20  Iteration 13520/35720 Training loss: 0.9141 0.2062 sec/batch\n",
      "Epoch 8/20  Iteration 13521/35720 Training loss: 0.9140 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 13522/35720 Training loss: 0.9140 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 13523/35720 Training loss: 0.9139 0.2245 sec/batch\n",
      "Epoch 8/20  Iteration 13524/35720 Training loss: 0.9140 0.2265 sec/batch\n",
      "Epoch 8/20  Iteration 13525/35720 Training loss: 0.9141 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 13526/35720 Training loss: 0.9141 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13527/35720 Training loss: 0.9141 0.2062 sec/batch\n",
      "Epoch 8/20  Iteration 13528/35720 Training loss: 0.9141 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 13529/35720 Training loss: 0.9140 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 13530/35720 Training loss: 0.9140 0.2131 sec/batch\n",
      "Epoch 8/20  Iteration 13531/35720 Training loss: 0.9139 0.2281 sec/batch\n",
      "Epoch 8/20  Iteration 13532/35720 Training loss: 0.9139 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 13533/35720 Training loss: 0.9139 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 13534/35720 Training loss: 0.9139 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 13535/35720 Training loss: 0.9139 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 13536/35720 Training loss: 0.9139 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13537/35720 Training loss: 0.9139 0.2240 sec/batch\n",
      "Epoch 8/20  Iteration 13538/35720 Training loss: 0.9140 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 13539/35720 Training loss: 0.9140 0.2254 sec/batch\n",
      "Epoch 8/20  Iteration 13540/35720 Training loss: 0.9140 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 13541/35720 Training loss: 0.9140 0.2398 sec/batch\n",
      "Epoch 8/20  Iteration 13542/35720 Training loss: 0.9140 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 13543/35720 Training loss: 0.9140 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 13544/35720 Training loss: 0.9140 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 13545/35720 Training loss: 0.9139 0.2265 sec/batch\n",
      "Epoch 8/20  Iteration 13546/35720 Training loss: 0.9139 0.2235 sec/batch\n",
      "Epoch 8/20  Iteration 13547/35720 Training loss: 0.9140 0.2252 sec/batch\n",
      "Epoch 8/20  Iteration 13548/35720 Training loss: 0.9140 0.2252 sec/batch\n",
      "Epoch 8/20  Iteration 13549/35720 Training loss: 0.9139 0.2366 sec/batch\n",
      "Epoch 8/20  Iteration 13550/35720 Training loss: 0.9139 0.2329 sec/batch\n",
      "Epoch 8/20  Iteration 13551/35720 Training loss: 0.9139 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 13552/35720 Training loss: 0.9139 0.2271 sec/batch\n",
      "Epoch 8/20  Iteration 13553/35720 Training loss: 0.9139 0.2225 sec/batch\n",
      "Epoch 8/20  Iteration 13554/35720 Training loss: 0.9139 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 13555/35720 Training loss: 0.9140 0.2196 sec/batch\n",
      "Epoch 8/20  Iteration 13556/35720 Training loss: 0.9140 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 13557/35720 Training loss: 0.9140 0.2278 sec/batch\n",
      "Epoch 8/20  Iteration 13558/35720 Training loss: 0.9140 0.2242 sec/batch\n",
      "Epoch 8/20  Iteration 13559/35720 Training loss: 0.9140 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 13560/35720 Training loss: 0.9140 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 13561/35720 Training loss: 0.9140 0.2172 sec/batch\n",
      "Epoch 8/20  Iteration 13562/35720 Training loss: 0.9139 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13563/35720 Training loss: 0.9140 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 13564/35720 Training loss: 0.9141 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 13565/35720 Training loss: 0.9141 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 13566/35720 Training loss: 0.9141 0.2170 sec/batch\n",
      "Epoch 8/20  Iteration 13567/35720 Training loss: 0.9141 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 13568/35720 Training loss: 0.9141 0.2063 sec/batch\n",
      "Epoch 8/20  Iteration 13569/35720 Training loss: 0.9140 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 13570/35720 Training loss: 0.9141 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13571/35720 Training loss: 0.9141 0.2128 sec/batch\n",
      "Epoch 8/20  Iteration 13572/35720 Training loss: 0.9141 0.2236 sec/batch\n",
      "Epoch 8/20  Iteration 13573/35720 Training loss: 0.9141 0.2265 sec/batch\n",
      "Epoch 8/20  Iteration 13574/35720 Training loss: 0.9140 0.2307 sec/batch\n",
      "Epoch 8/20  Iteration 13575/35720 Training loss: 0.9140 0.2054 sec/batch\n",
      "Epoch 8/20  Iteration 13576/35720 Training loss: 0.9140 0.2119 sec/batch\n",
      "Epoch 8/20  Iteration 13577/35720 Training loss: 0.9140 0.2152 sec/batch\n",
      "Epoch 8/20  Iteration 13578/35720 Training loss: 0.9141 0.2213 sec/batch\n",
      "Epoch 8/20  Iteration 13579/35720 Training loss: 0.9140 0.2292 sec/batch\n",
      "Epoch 8/20  Iteration 13580/35720 Training loss: 0.9140 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 13581/35720 Training loss: 0.9140 0.2062 sec/batch\n",
      "Epoch 8/20  Iteration 13582/35720 Training loss: 0.9140 0.2122 sec/batch\n",
      "Epoch 8/20  Iteration 13583/35720 Training loss: 0.9140 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 13584/35720 Training loss: 0.9139 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 13585/35720 Training loss: 0.9140 0.2200 sec/batch\n",
      "Epoch 8/20  Iteration 13586/35720 Training loss: 0.9139 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 13587/35720 Training loss: 0.9140 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13588/35720 Training loss: 0.9140 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13589/35720 Training loss: 0.9139 0.2145 sec/batch\n",
      "Epoch 8/20  Iteration 13590/35720 Training loss: 0.9140 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 13591/35720 Training loss: 0.9140 0.2190 sec/batch\n",
      "Epoch 8/20  Iteration 13592/35720 Training loss: 0.9140 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 13593/35720 Training loss: 0.9140 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 13594/35720 Training loss: 0.9140 0.2568 sec/batch\n",
      "Epoch 8/20  Iteration 13595/35720 Training loss: 0.9140 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13596/35720 Training loss: 0.9140 0.2248 sec/batch\n",
      "Epoch 8/20  Iteration 13597/35720 Training loss: 0.9139 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 13598/35720 Training loss: 0.9139 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 13599/35720 Training loss: 0.9138 0.2084 sec/batch\n",
      "Epoch 8/20  Iteration 13600/35720 Training loss: 0.9139 0.2288 sec/batch\n",
      "Validation loss: 1.356 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 13601/35720 Training loss: 0.9141 0.2086 sec/batch\n",
      "Epoch 8/20  Iteration 13602/35720 Training loss: 0.9141 0.2136 sec/batch\n",
      "Epoch 8/20  Iteration 13603/35720 Training loss: 0.9142 0.2159 sec/batch\n",
      "Epoch 8/20  Iteration 13604/35720 Training loss: 0.9142 0.2177 sec/batch\n",
      "Epoch 8/20  Iteration 13605/35720 Training loss: 0.9142 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 13606/35720 Training loss: 0.9142 0.2145 sec/batch\n",
      "Epoch 8/20  Iteration 13607/35720 Training loss: 0.9142 0.2196 sec/batch\n",
      "Epoch 8/20  Iteration 13608/35720 Training loss: 0.9141 0.2071 sec/batch\n",
      "Epoch 8/20  Iteration 13609/35720 Training loss: 0.9142 0.2118 sec/batch\n",
      "Epoch 8/20  Iteration 13610/35720 Training loss: 0.9142 0.2283 sec/batch\n",
      "Epoch 8/20  Iteration 13611/35720 Training loss: 0.9142 0.2235 sec/batch\n",
      "Epoch 8/20  Iteration 13612/35720 Training loss: 0.9142 0.2284 sec/batch\n",
      "Epoch 8/20  Iteration 13613/35720 Training loss: 0.9142 0.2057 sec/batch\n",
      "Epoch 8/20  Iteration 13614/35720 Training loss: 0.9141 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13615/35720 Training loss: 0.9140 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13616/35720 Training loss: 0.9141 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 13617/35720 Training loss: 0.9141 0.2307 sec/batch\n",
      "Epoch 8/20  Iteration 13618/35720 Training loss: 0.9141 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 13619/35720 Training loss: 0.9140 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 13620/35720 Training loss: 0.9140 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13621/35720 Training loss: 0.9140 0.2197 sec/batch\n",
      "Epoch 8/20  Iteration 13622/35720 Training loss: 0.9140 0.2079 sec/batch\n",
      "Epoch 8/20  Iteration 13623/35720 Training loss: 0.9141 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 13624/35720 Training loss: 0.9141 0.2288 sec/batch\n",
      "Epoch 8/20  Iteration 13625/35720 Training loss: 0.9142 0.2122 sec/batch\n",
      "Epoch 8/20  Iteration 13626/35720 Training loss: 0.9143 0.2310 sec/batch\n",
      "Epoch 8/20  Iteration 13627/35720 Training loss: 0.9142 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 13628/35720 Training loss: 0.9142 0.2193 sec/batch\n",
      "Epoch 8/20  Iteration 13629/35720 Training loss: 0.9142 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 13630/35720 Training loss: 0.9142 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 13631/35720 Training loss: 0.9141 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13632/35720 Training loss: 0.9141 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 13633/35720 Training loss: 0.9141 0.2220 sec/batch\n",
      "Epoch 8/20  Iteration 13634/35720 Training loss: 0.9140 0.2272 sec/batch\n",
      "Epoch 8/20  Iteration 13635/35720 Training loss: 0.9140 0.2217 sec/batch\n",
      "Epoch 8/20  Iteration 13636/35720 Training loss: 0.9141 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 13637/35720 Training loss: 0.9140 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 13638/35720 Training loss: 0.9140 0.2208 sec/batch\n",
      "Epoch 8/20  Iteration 13639/35720 Training loss: 0.9139 0.2326 sec/batch\n",
      "Epoch 8/20  Iteration 13640/35720 Training loss: 0.9138 0.2211 sec/batch\n",
      "Epoch 8/20  Iteration 13641/35720 Training loss: 0.9138 0.2196 sec/batch\n",
      "Epoch 8/20  Iteration 13642/35720 Training loss: 0.9138 0.2323 sec/batch\n",
      "Epoch 8/20  Iteration 13643/35720 Training loss: 0.9138 0.2284 sec/batch\n",
      "Epoch 8/20  Iteration 13644/35720 Training loss: 0.9138 0.2357 sec/batch\n",
      "Epoch 8/20  Iteration 13645/35720 Training loss: 0.9137 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13646/35720 Training loss: 0.9137 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 13647/35720 Training loss: 0.9137 0.2123 sec/batch\n",
      "Epoch 8/20  Iteration 13648/35720 Training loss: 0.9136 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 13649/35720 Training loss: 0.9136 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 13650/35720 Training loss: 0.9135 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 13651/35720 Training loss: 0.9135 0.2073 sec/batch\n",
      "Epoch 8/20  Iteration 13652/35720 Training loss: 0.9134 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13653/35720 Training loss: 0.9135 0.2155 sec/batch\n",
      "Epoch 8/20  Iteration 13654/35720 Training loss: 0.9135 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 13655/35720 Training loss: 0.9135 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 13656/35720 Training loss: 0.9135 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 13657/35720 Training loss: 0.9134 0.2221 sec/batch\n",
      "Epoch 8/20  Iteration 13658/35720 Training loss: 0.9134 0.2191 sec/batch\n",
      "Epoch 8/20  Iteration 13659/35720 Training loss: 0.9134 0.2286 sec/batch\n",
      "Epoch 8/20  Iteration 13660/35720 Training loss: 0.9133 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 13661/35720 Training loss: 0.9133 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 13662/35720 Training loss: 0.9133 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13663/35720 Training loss: 0.9132 0.2062 sec/batch\n",
      "Epoch 8/20  Iteration 13664/35720 Training loss: 0.9132 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 13665/35720 Training loss: 0.9133 0.2253 sec/batch\n",
      "Epoch 8/20  Iteration 13666/35720 Training loss: 0.9133 0.2229 sec/batch\n",
      "Epoch 8/20  Iteration 13667/35720 Training loss: 0.9133 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13668/35720 Training loss: 0.9133 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 13669/35720 Training loss: 0.9132 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 13670/35720 Training loss: 0.9132 0.2195 sec/batch\n",
      "Epoch 8/20  Iteration 13671/35720 Training loss: 0.9133 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 13672/35720 Training loss: 0.9133 0.2374 sec/batch\n",
      "Epoch 8/20  Iteration 13673/35720 Training loss: 0.9133 0.2051 sec/batch\n",
      "Epoch 8/20  Iteration 13674/35720 Training loss: 0.9133 0.2127 sec/batch\n",
      "Epoch 8/20  Iteration 13675/35720 Training loss: 0.9133 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 13676/35720 Training loss: 0.9132 0.2193 sec/batch\n",
      "Epoch 8/20  Iteration 13677/35720 Training loss: 0.9132 0.2250 sec/batch\n",
      "Epoch 8/20  Iteration 13678/35720 Training loss: 0.9133 0.2077 sec/batch\n",
      "Epoch 8/20  Iteration 13679/35720 Training loss: 0.9133 0.2194 sec/batch\n",
      "Epoch 8/20  Iteration 13680/35720 Training loss: 0.9133 0.2086 sec/batch\n",
      "Epoch 8/20  Iteration 13681/35720 Training loss: 0.9133 0.2271 sec/batch\n",
      "Epoch 8/20  Iteration 13682/35720 Training loss: 0.9133 0.2107 sec/batch\n",
      "Epoch 8/20  Iteration 13683/35720 Training loss: 0.9133 0.2319 sec/batch\n",
      "Epoch 8/20  Iteration 13684/35720 Training loss: 0.9133 0.2243 sec/batch\n",
      "Epoch 8/20  Iteration 13685/35720 Training loss: 0.9133 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 13686/35720 Training loss: 0.9132 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 13687/35720 Training loss: 0.9131 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13688/35720 Training loss: 0.9131 0.2266 sec/batch\n",
      "Epoch 8/20  Iteration 13689/35720 Training loss: 0.9131 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 13690/35720 Training loss: 0.9131 0.2107 sec/batch\n",
      "Epoch 8/20  Iteration 13691/35720 Training loss: 0.9130 0.2323 sec/batch\n",
      "Epoch 8/20  Iteration 13692/35720 Training loss: 0.9130 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 13693/35720 Training loss: 0.9130 0.2131 sec/batch\n",
      "Epoch 8/20  Iteration 13694/35720 Training loss: 0.9130 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 13695/35720 Training loss: 0.9130 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 13696/35720 Training loss: 0.9130 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13697/35720 Training loss: 0.9130 0.2138 sec/batch\n",
      "Epoch 8/20  Iteration 13698/35720 Training loss: 0.9130 0.2309 sec/batch\n",
      "Epoch 8/20  Iteration 13699/35720 Training loss: 0.9131 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 13700/35720 Training loss: 0.9131 0.2263 sec/batch\n",
      "Epoch 8/20  Iteration 13701/35720 Training loss: 0.9131 0.2061 sec/batch\n",
      "Epoch 8/20  Iteration 13702/35720 Training loss: 0.9130 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 13703/35720 Training loss: 0.9130 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 13704/35720 Training loss: 0.9129 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 13705/35720 Training loss: 0.9129 0.2196 sec/batch\n",
      "Epoch 8/20  Iteration 13706/35720 Training loss: 0.9128 0.2066 sec/batch\n",
      "Epoch 8/20  Iteration 13707/35720 Training loss: 0.9127 0.2068 sec/batch\n",
      "Epoch 8/20  Iteration 13708/35720 Training loss: 0.9127 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13709/35720 Training loss: 0.9127 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 13710/35720 Training loss: 0.9127 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 13711/35720 Training loss: 0.9127 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 13712/35720 Training loss: 0.9127 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 13713/35720 Training loss: 0.9126 0.2074 sec/batch\n",
      "Epoch 8/20  Iteration 13714/35720 Training loss: 0.9126 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 13715/35720 Training loss: 0.9126 0.2357 sec/batch\n",
      "Epoch 8/20  Iteration 13716/35720 Training loss: 0.9126 0.2268 sec/batch\n",
      "Epoch 8/20  Iteration 13717/35720 Training loss: 0.9126 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 13718/35720 Training loss: 0.9126 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 13719/35720 Training loss: 0.9125 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 13720/35720 Training loss: 0.9125 0.2194 sec/batch\n",
      "Epoch 8/20  Iteration 13721/35720 Training loss: 0.9125 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 13722/35720 Training loss: 0.9124 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13723/35720 Training loss: 0.9124 0.2057 sec/batch\n",
      "Epoch 8/20  Iteration 13724/35720 Training loss: 0.9124 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 13725/35720 Training loss: 0.9124 0.2131 sec/batch\n",
      "Epoch 8/20  Iteration 13726/35720 Training loss: 0.9124 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 13727/35720 Training loss: 0.9123 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13728/35720 Training loss: 0.9124 0.2208 sec/batch\n",
      "Epoch 8/20  Iteration 13729/35720 Training loss: 0.9124 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13730/35720 Training loss: 0.9124 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13731/35720 Training loss: 0.9124 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 13732/35720 Training loss: 0.9124 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 13733/35720 Training loss: 0.9124 0.2255 sec/batch\n",
      "Epoch 8/20  Iteration 13734/35720 Training loss: 0.9124 0.2171 sec/batch\n",
      "Epoch 8/20  Iteration 13735/35720 Training loss: 0.9123 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13736/35720 Training loss: 0.9123 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 13737/35720 Training loss: 0.9123 0.2075 sec/batch\n",
      "Epoch 8/20  Iteration 13738/35720 Training loss: 0.9123 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13739/35720 Training loss: 0.9122 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13740/35720 Training loss: 0.9122 0.2257 sec/batch\n",
      "Epoch 8/20  Iteration 13741/35720 Training loss: 0.9121 0.2269 sec/batch\n",
      "Epoch 8/20  Iteration 13742/35720 Training loss: 0.9120 0.2289 sec/batch\n",
      "Epoch 8/20  Iteration 13743/35720 Training loss: 0.9120 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 13744/35720 Training loss: 0.9120 0.2342 sec/batch\n",
      "Epoch 8/20  Iteration 13745/35720 Training loss: 0.9119 0.2148 sec/batch\n",
      "Epoch 8/20  Iteration 13746/35720 Training loss: 0.9119 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 13747/35720 Training loss: 0.9119 0.2080 sec/batch\n",
      "Epoch 8/20  Iteration 13748/35720 Training loss: 0.9118 0.2133 sec/batch\n",
      "Epoch 8/20  Iteration 13749/35720 Training loss: 0.9118 0.2127 sec/batch\n",
      "Epoch 8/20  Iteration 13750/35720 Training loss: 0.9117 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 13751/35720 Training loss: 0.9117 0.2179 sec/batch\n",
      "Epoch 8/20  Iteration 13752/35720 Training loss: 0.9117 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 13753/35720 Training loss: 0.9116 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 13754/35720 Training loss: 0.9116 0.2056 sec/batch\n",
      "Epoch 8/20  Iteration 13755/35720 Training loss: 0.9116 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 13756/35720 Training loss: 0.9116 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13757/35720 Training loss: 0.9115 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 13758/35720 Training loss: 0.9115 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13759/35720 Training loss: 0.9114 0.2328 sec/batch\n",
      "Epoch 8/20  Iteration 13760/35720 Training loss: 0.9114 0.2241 sec/batch\n",
      "Epoch 8/20  Iteration 13761/35720 Training loss: 0.9113 0.2252 sec/batch\n",
      "Epoch 8/20  Iteration 13762/35720 Training loss: 0.9114 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 13763/35720 Training loss: 0.9113 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 13764/35720 Training loss: 0.9113 0.2364 sec/batch\n",
      "Epoch 8/20  Iteration 13765/35720 Training loss: 0.9112 0.2133 sec/batch\n",
      "Epoch 8/20  Iteration 13766/35720 Training loss: 0.9112 0.2291 sec/batch\n",
      "Epoch 8/20  Iteration 13767/35720 Training loss: 0.9111 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 13768/35720 Training loss: 0.9110 0.2191 sec/batch\n",
      "Epoch 8/20  Iteration 13769/35720 Training loss: 0.9110 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 13770/35720 Training loss: 0.9110 0.2320 sec/batch\n",
      "Epoch 8/20  Iteration 13771/35720 Training loss: 0.9110 0.2122 sec/batch\n",
      "Epoch 8/20  Iteration 13772/35720 Training loss: 0.9110 0.2177 sec/batch\n",
      "Epoch 8/20  Iteration 13773/35720 Training loss: 0.9109 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 13774/35720 Training loss: 0.9109 0.2138 sec/batch\n",
      "Epoch 8/20  Iteration 13775/35720 Training loss: 0.9108 0.2208 sec/batch\n",
      "Epoch 8/20  Iteration 13776/35720 Training loss: 0.9108 0.2279 sec/batch\n",
      "Epoch 8/20  Iteration 13777/35720 Training loss: 0.9108 0.2425 sec/batch\n",
      "Epoch 8/20  Iteration 13778/35720 Training loss: 0.9108 0.2107 sec/batch\n",
      "Epoch 8/20  Iteration 13779/35720 Training loss: 0.9107 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 13780/35720 Training loss: 0.9106 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 13781/35720 Training loss: 0.9106 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 13782/35720 Training loss: 0.9105 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 13783/35720 Training loss: 0.9105 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 13784/35720 Training loss: 0.9104 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 13785/35720 Training loss: 0.9104 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 13786/35720 Training loss: 0.9103 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 13787/35720 Training loss: 0.9103 0.2143 sec/batch\n",
      "Epoch 8/20  Iteration 13788/35720 Training loss: 0.9103 0.2234 sec/batch\n",
      "Epoch 8/20  Iteration 13789/35720 Training loss: 0.9102 0.2143 sec/batch\n",
      "Epoch 8/20  Iteration 13790/35720 Training loss: 0.9102 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13791/35720 Training loss: 0.9102 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13792/35720 Training loss: 0.9102 0.2261 sec/batch\n",
      "Epoch 8/20  Iteration 13793/35720 Training loss: 0.9101 0.2144 sec/batch\n",
      "Epoch 8/20  Iteration 13794/35720 Training loss: 0.9101 0.2219 sec/batch\n",
      "Epoch 8/20  Iteration 13795/35720 Training loss: 0.9101 0.2249 sec/batch\n",
      "Epoch 8/20  Iteration 13796/35720 Training loss: 0.9102 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 13797/35720 Training loss: 0.9102 0.2245 sec/batch\n",
      "Epoch 8/20  Iteration 13798/35720 Training loss: 0.9102 0.2188 sec/batch\n",
      "Epoch 8/20  Iteration 13799/35720 Training loss: 0.9102 0.2079 sec/batch\n",
      "Epoch 8/20  Iteration 13800/35720 Training loss: 0.9101 0.2303 sec/batch\n",
      "Validation loss: 1.36076 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 13801/35720 Training loss: 0.9103 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 13802/35720 Training loss: 0.9102 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 13803/35720 Training loss: 0.9102 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13804/35720 Training loss: 0.9102 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 13805/35720 Training loss: 0.9101 0.2108 sec/batch\n",
      "Epoch 8/20  Iteration 13806/35720 Training loss: 0.9101 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 13807/35720 Training loss: 0.9100 0.2085 sec/batch\n",
      "Epoch 8/20  Iteration 13808/35720 Training loss: 0.9100 0.2216 sec/batch\n",
      "Epoch 8/20  Iteration 13809/35720 Training loss: 0.9100 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 13810/35720 Training loss: 0.9099 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 13811/35720 Training loss: 0.9099 0.2062 sec/batch\n",
      "Epoch 8/20  Iteration 13812/35720 Training loss: 0.9098 0.2124 sec/batch\n",
      "Epoch 8/20  Iteration 13813/35720 Training loss: 0.9098 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 13814/35720 Training loss: 0.9098 0.2242 sec/batch\n",
      "Epoch 8/20  Iteration 13815/35720 Training loss: 0.9098 0.2330 sec/batch\n",
      "Epoch 8/20  Iteration 13816/35720 Training loss: 0.9098 0.2077 sec/batch\n",
      "Epoch 8/20  Iteration 13817/35720 Training loss: 0.9098 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13818/35720 Training loss: 0.9098 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 13819/35720 Training loss: 0.9098 0.2170 sec/batch\n",
      "Epoch 8/20  Iteration 13820/35720 Training loss: 0.9097 0.2101 sec/batch\n",
      "Epoch 8/20  Iteration 13821/35720 Training loss: 0.9097 0.2215 sec/batch\n",
      "Epoch 8/20  Iteration 13822/35720 Training loss: 0.9097 0.2248 sec/batch\n",
      "Epoch 8/20  Iteration 13823/35720 Training loss: 0.9097 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 13824/35720 Training loss: 0.9097 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 13825/35720 Training loss: 0.9097 0.2241 sec/batch\n",
      "Epoch 8/20  Iteration 13826/35720 Training loss: 0.9097 0.2653 sec/batch\n",
      "Epoch 8/20  Iteration 13827/35720 Training loss: 0.9097 0.2417 sec/batch\n",
      "Epoch 8/20  Iteration 13828/35720 Training loss: 0.9097 0.2146 sec/batch\n",
      "Epoch 8/20  Iteration 13829/35720 Training loss: 0.9097 0.2080 sec/batch\n",
      "Epoch 8/20  Iteration 13830/35720 Training loss: 0.9097 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 13831/35720 Training loss: 0.9097 0.2125 sec/batch\n",
      "Epoch 8/20  Iteration 13832/35720 Training loss: 0.9096 0.2306 sec/batch\n",
      "Epoch 8/20  Iteration 13833/35720 Training loss: 0.9096 0.2127 sec/batch\n",
      "Epoch 8/20  Iteration 13834/35720 Training loss: 0.9096 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 13835/35720 Training loss: 0.9096 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 13836/35720 Training loss: 0.9096 0.2096 sec/batch\n",
      "Epoch 8/20  Iteration 13837/35720 Training loss: 0.9096 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13838/35720 Training loss: 0.9096 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 13839/35720 Training loss: 0.9097 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 13840/35720 Training loss: 0.9097 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 13841/35720 Training loss: 0.9097 0.2265 sec/batch\n",
      "Epoch 8/20  Iteration 13842/35720 Training loss: 0.9097 0.2329 sec/batch\n",
      "Epoch 8/20  Iteration 13843/35720 Training loss: 0.9097 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 13844/35720 Training loss: 0.9097 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 13845/35720 Training loss: 0.9096 0.2218 sec/batch\n",
      "Epoch 8/20  Iteration 13846/35720 Training loss: 0.9096 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 13847/35720 Training loss: 0.9096 0.2128 sec/batch\n",
      "Epoch 8/20  Iteration 13848/35720 Training loss: 0.9096 0.2179 sec/batch\n",
      "Epoch 8/20  Iteration 13849/35720 Training loss: 0.9096 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 13850/35720 Training loss: 0.9097 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 13851/35720 Training loss: 0.9097 0.2082 sec/batch\n",
      "Epoch 8/20  Iteration 13852/35720 Training loss: 0.9097 0.2155 sec/batch\n",
      "Epoch 8/20  Iteration 13853/35720 Training loss: 0.9097 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 13854/35720 Training loss: 0.9098 0.2241 sec/batch\n",
      "Epoch 8/20  Iteration 13855/35720 Training loss: 0.9099 0.2129 sec/batch\n",
      "Epoch 8/20  Iteration 13856/35720 Training loss: 0.9098 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 13857/35720 Training loss: 0.9097 0.2280 sec/batch\n",
      "Epoch 8/20  Iteration 13858/35720 Training loss: 0.9097 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 13859/35720 Training loss: 0.9097 0.2365 sec/batch\n",
      "Epoch 8/20  Iteration 13860/35720 Training loss: 0.9097 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 13861/35720 Training loss: 0.9097 0.2070 sec/batch\n",
      "Epoch 8/20  Iteration 13862/35720 Training loss: 0.9096 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13863/35720 Training loss: 0.9096 0.2279 sec/batch\n",
      "Epoch 8/20  Iteration 13864/35720 Training loss: 0.9096 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 13865/35720 Training loss: 0.9096 0.2204 sec/batch\n",
      "Epoch 8/20  Iteration 13866/35720 Training loss: 0.9096 0.2271 sec/batch\n",
      "Epoch 8/20  Iteration 13867/35720 Training loss: 0.9096 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 13868/35720 Training loss: 0.9095 0.2212 sec/batch\n",
      "Epoch 8/20  Iteration 13869/35720 Training loss: 0.9095 0.2223 sec/batch\n",
      "Epoch 8/20  Iteration 13870/35720 Training loss: 0.9095 0.2221 sec/batch\n",
      "Epoch 8/20  Iteration 13871/35720 Training loss: 0.9096 0.2266 sec/batch\n",
      "Epoch 8/20  Iteration 13872/35720 Training loss: 0.9096 0.2077 sec/batch\n",
      "Epoch 8/20  Iteration 13873/35720 Training loss: 0.9096 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 13874/35720 Training loss: 0.9096 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 13875/35720 Training loss: 0.9096 0.2174 sec/batch\n",
      "Epoch 8/20  Iteration 13876/35720 Training loss: 0.9096 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13877/35720 Training loss: 0.9095 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13878/35720 Training loss: 0.9095 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 13879/35720 Training loss: 0.9095 0.2255 sec/batch\n",
      "Epoch 8/20  Iteration 13880/35720 Training loss: 0.9095 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 13881/35720 Training loss: 0.9095 0.2120 sec/batch\n",
      "Epoch 8/20  Iteration 13882/35720 Training loss: 0.9096 0.2215 sec/batch\n",
      "Epoch 8/20  Iteration 13883/35720 Training loss: 0.9095 0.2193 sec/batch\n",
      "Epoch 8/20  Iteration 13884/35720 Training loss: 0.9096 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 13885/35720 Training loss: 0.9095 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 13886/35720 Training loss: 0.9095 0.2144 sec/batch\n",
      "Epoch 8/20  Iteration 13887/35720 Training loss: 0.9095 0.2186 sec/batch\n",
      "Epoch 8/20  Iteration 13888/35720 Training loss: 0.9096 0.2211 sec/batch\n",
      "Epoch 8/20  Iteration 13889/35720 Training loss: 0.9095 0.2136 sec/batch\n",
      "Epoch 8/20  Iteration 13890/35720 Training loss: 0.9095 0.2193 sec/batch\n",
      "Epoch 8/20  Iteration 13891/35720 Training loss: 0.9095 0.2241 sec/batch\n",
      "Epoch 8/20  Iteration 13892/35720 Training loss: 0.9095 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 13893/35720 Training loss: 0.9094 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 13894/35720 Training loss: 0.9094 0.2074 sec/batch\n",
      "Epoch 8/20  Iteration 13895/35720 Training loss: 0.9093 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 13896/35720 Training loss: 0.9093 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 13897/35720 Training loss: 0.9092 0.2147 sec/batch\n",
      "Epoch 8/20  Iteration 13898/35720 Training loss: 0.9092 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 13899/35720 Training loss: 0.9092 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 13900/35720 Training loss: 0.9091 0.2254 sec/batch\n",
      "Epoch 8/20  Iteration 13901/35720 Training loss: 0.9090 0.2152 sec/batch\n",
      "Epoch 8/20  Iteration 13902/35720 Training loss: 0.9090 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 13903/35720 Training loss: 0.9090 0.2215 sec/batch\n",
      "Epoch 8/20  Iteration 13904/35720 Training loss: 0.9090 0.2267 sec/batch\n",
      "Epoch 8/20  Iteration 13905/35720 Training loss: 0.9090 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 13906/35720 Training loss: 0.9090 0.2092 sec/batch\n",
      "Epoch 8/20  Iteration 13907/35720 Training loss: 0.9089 0.2203 sec/batch\n",
      "Epoch 8/20  Iteration 13908/35720 Training loss: 0.9089 0.2205 sec/batch\n",
      "Epoch 8/20  Iteration 13909/35720 Training loss: 0.9089 0.2148 sec/batch\n",
      "Epoch 8/20  Iteration 13910/35720 Training loss: 0.9089 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 13911/35720 Training loss: 0.9089 0.2141 sec/batch\n",
      "Epoch 8/20  Iteration 13912/35720 Training loss: 0.9089 0.2239 sec/batch\n",
      "Epoch 8/20  Iteration 13913/35720 Training loss: 0.9089 0.2343 sec/batch\n",
      "Epoch 8/20  Iteration 13914/35720 Training loss: 0.9089 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13915/35720 Training loss: 0.9089 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 13916/35720 Training loss: 0.9089 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 13917/35720 Training loss: 0.9089 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13918/35720 Training loss: 0.9088 0.2167 sec/batch\n",
      "Epoch 8/20  Iteration 13919/35720 Training loss: 0.9088 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13920/35720 Training loss: 0.9088 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 13921/35720 Training loss: 0.9089 0.2218 sec/batch\n",
      "Epoch 8/20  Iteration 13922/35720 Training loss: 0.9089 0.2248 sec/batch\n",
      "Epoch 8/20  Iteration 13923/35720 Training loss: 0.9089 0.2307 sec/batch\n",
      "Epoch 8/20  Iteration 13924/35720 Training loss: 0.9088 0.2079 sec/batch\n",
      "Epoch 8/20  Iteration 13925/35720 Training loss: 0.9088 0.2146 sec/batch\n",
      "Epoch 8/20  Iteration 13926/35720 Training loss: 0.9088 0.2193 sec/batch\n",
      "Epoch 8/20  Iteration 13927/35720 Training loss: 0.9088 0.2066 sec/batch\n",
      "Epoch 8/20  Iteration 13928/35720 Training loss: 0.9088 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 13929/35720 Training loss: 0.9088 0.2300 sec/batch\n",
      "Epoch 8/20  Iteration 13930/35720 Training loss: 0.9089 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 13931/35720 Training loss: 0.9089 0.2248 sec/batch\n",
      "Epoch 8/20  Iteration 13932/35720 Training loss: 0.9088 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 13933/35720 Training loss: 0.9088 0.2134 sec/batch\n",
      "Epoch 8/20  Iteration 13934/35720 Training loss: 0.9088 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 13935/35720 Training loss: 0.9087 0.2316 sec/batch\n",
      "Epoch 8/20  Iteration 13936/35720 Training loss: 0.9087 0.2075 sec/batch\n",
      "Epoch 8/20  Iteration 13937/35720 Training loss: 0.9087 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13938/35720 Training loss: 0.9087 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 13939/35720 Training loss: 0.9086 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 13940/35720 Training loss: 0.9086 0.2202 sec/batch\n",
      "Epoch 8/20  Iteration 13941/35720 Training loss: 0.9086 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 13942/35720 Training loss: 0.9086 0.2270 sec/batch\n",
      "Epoch 8/20  Iteration 13943/35720 Training loss: 0.9087 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 13944/35720 Training loss: 0.9087 0.2062 sec/batch\n",
      "Epoch 8/20  Iteration 13945/35720 Training loss: 0.9087 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 13946/35720 Training loss: 0.9086 0.2197 sec/batch\n",
      "Epoch 8/20  Iteration 13947/35720 Training loss: 0.9086 0.2177 sec/batch\n",
      "Epoch 8/20  Iteration 13948/35720 Training loss: 0.9086 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 13949/35720 Training loss: 0.9086 0.2055 sec/batch\n",
      "Epoch 8/20  Iteration 13950/35720 Training loss: 0.9086 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 13951/35720 Training loss: 0.9086 0.2308 sec/batch\n",
      "Epoch 8/20  Iteration 13952/35720 Training loss: 0.9085 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 13953/35720 Training loss: 0.9084 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 13954/35720 Training loss: 0.9084 0.2071 sec/batch\n",
      "Epoch 8/20  Iteration 13955/35720 Training loss: 0.9084 0.2124 sec/batch\n",
      "Epoch 8/20  Iteration 13956/35720 Training loss: 0.9084 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 13957/35720 Training loss: 0.9084 0.2133 sec/batch\n",
      "Epoch 8/20  Iteration 13958/35720 Training loss: 0.9084 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 13959/35720 Training loss: 0.9084 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 13960/35720 Training loss: 0.9084 0.2142 sec/batch\n",
      "Epoch 8/20  Iteration 13961/35720 Training loss: 0.9084 0.2130 sec/batch\n",
      "Epoch 8/20  Iteration 13962/35720 Training loss: 0.9085 0.2130 sec/batch\n",
      "Epoch 8/20  Iteration 13963/35720 Training loss: 0.9085 0.2175 sec/batch\n",
      "Epoch 8/20  Iteration 13964/35720 Training loss: 0.9086 0.2221 sec/batch\n",
      "Epoch 8/20  Iteration 13965/35720 Training loss: 0.9086 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 13966/35720 Training loss: 0.9085 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 13967/35720 Training loss: 0.9085 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 13968/35720 Training loss: 0.9085 0.2313 sec/batch\n",
      "Epoch 8/20  Iteration 13969/35720 Training loss: 0.9085 0.2255 sec/batch\n",
      "Epoch 8/20  Iteration 13970/35720 Training loss: 0.9084 0.2255 sec/batch\n",
      "Epoch 8/20  Iteration 13971/35720 Training loss: 0.9084 0.2095 sec/batch\n",
      "Epoch 8/20  Iteration 13972/35720 Training loss: 0.9084 0.2175 sec/batch\n",
      "Epoch 8/20  Iteration 13973/35720 Training loss: 0.9084 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 13974/35720 Training loss: 0.9083 0.2199 sec/batch\n",
      "Epoch 8/20  Iteration 13975/35720 Training loss: 0.9083 0.2364 sec/batch\n",
      "Epoch 8/20  Iteration 13976/35720 Training loss: 0.9083 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 13977/35720 Training loss: 0.9082 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 13978/35720 Training loss: 0.9081 0.2216 sec/batch\n",
      "Epoch 8/20  Iteration 13979/35720 Training loss: 0.9080 0.2280 sec/batch\n",
      "Epoch 8/20  Iteration 13980/35720 Training loss: 0.9080 0.2083 sec/batch\n",
      "Epoch 8/20  Iteration 13981/35720 Training loss: 0.9079 0.2252 sec/batch\n",
      "Epoch 8/20  Iteration 13982/35720 Training loss: 0.9079 0.2241 sec/batch\n",
      "Epoch 8/20  Iteration 13983/35720 Training loss: 0.9079 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 13984/35720 Training loss: 0.9079 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 13985/35720 Training loss: 0.9078 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 13986/35720 Training loss: 0.9078 0.2175 sec/batch\n",
      "Epoch 8/20  Iteration 13987/35720 Training loss: 0.9078 0.2156 sec/batch\n",
      "Epoch 8/20  Iteration 13988/35720 Training loss: 0.9078 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 13989/35720 Training loss: 0.9078 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 13990/35720 Training loss: 0.9077 0.2192 sec/batch\n",
      "Epoch 8/20  Iteration 13991/35720 Training loss: 0.9077 0.2186 sec/batch\n",
      "Epoch 8/20  Iteration 13992/35720 Training loss: 0.9077 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 13993/35720 Training loss: 0.9077 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 13994/35720 Training loss: 0.9076 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 13995/35720 Training loss: 0.9076 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 13996/35720 Training loss: 0.9076 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 13997/35720 Training loss: 0.9076 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 13998/35720 Training loss: 0.9075 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 13999/35720 Training loss: 0.9075 0.2170 sec/batch\n",
      "Epoch 8/20  Iteration 14000/35720 Training loss: 0.9075 0.2086 sec/batch\n",
      "Validation loss: 1.36778 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 14001/35720 Training loss: 0.9076 0.2123 sec/batch\n",
      "Epoch 8/20  Iteration 14002/35720 Training loss: 0.9076 0.2261 sec/batch\n",
      "Epoch 8/20  Iteration 14003/35720 Training loss: 0.9075 0.2059 sec/batch\n",
      "Epoch 8/20  Iteration 14004/35720 Training loss: 0.9075 0.2108 sec/batch\n",
      "Epoch 8/20  Iteration 14005/35720 Training loss: 0.9075 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 14006/35720 Training loss: 0.9075 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 14007/35720 Training loss: 0.9075 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 14008/35720 Training loss: 0.9076 0.2221 sec/batch\n",
      "Epoch 8/20  Iteration 14009/35720 Training loss: 0.9075 0.2204 sec/batch\n",
      "Epoch 8/20  Iteration 14010/35720 Training loss: 0.9075 0.2063 sec/batch\n",
      "Epoch 8/20  Iteration 14011/35720 Training loss: 0.9075 0.2164 sec/batch\n",
      "Epoch 8/20  Iteration 14012/35720 Training loss: 0.9075 0.2182 sec/batch\n",
      "Epoch 8/20  Iteration 14013/35720 Training loss: 0.9075 0.2078 sec/batch\n",
      "Epoch 8/20  Iteration 14014/35720 Training loss: 0.9074 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 14015/35720 Training loss: 0.9074 0.2115 sec/batch\n",
      "Epoch 8/20  Iteration 14016/35720 Training loss: 0.9075 0.2145 sec/batch\n",
      "Epoch 8/20  Iteration 14017/35720 Training loss: 0.9075 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 14018/35720 Training loss: 0.9075 0.2302 sec/batch\n",
      "Epoch 8/20  Iteration 14019/35720 Training loss: 0.9075 0.2240 sec/batch\n",
      "Epoch 8/20  Iteration 14020/35720 Training loss: 0.9075 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 14021/35720 Training loss: 0.9075 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 14022/35720 Training loss: 0.9075 0.2253 sec/batch\n",
      "Epoch 8/20  Iteration 14023/35720 Training loss: 0.9075 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 14024/35720 Training loss: 0.9075 0.2183 sec/batch\n",
      "Epoch 8/20  Iteration 14025/35720 Training loss: 0.9075 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 14026/35720 Training loss: 0.9076 0.2125 sec/batch\n",
      "Epoch 8/20  Iteration 14027/35720 Training loss: 0.9076 0.2174 sec/batch\n",
      "Epoch 8/20  Iteration 14028/35720 Training loss: 0.9076 0.2181 sec/batch\n",
      "Epoch 8/20  Iteration 14029/35720 Training loss: 0.9076 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 14030/35720 Training loss: 0.9076 0.2232 sec/batch\n",
      "Epoch 8/20  Iteration 14031/35720 Training loss: 0.9077 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 14032/35720 Training loss: 0.9077 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 14033/35720 Training loss: 0.9077 0.2146 sec/batch\n",
      "Epoch 8/20  Iteration 14034/35720 Training loss: 0.9077 0.2251 sec/batch\n",
      "Epoch 8/20  Iteration 14035/35720 Training loss: 0.9077 0.2255 sec/batch\n",
      "Epoch 8/20  Iteration 14036/35720 Training loss: 0.9077 0.2339 sec/batch\n",
      "Epoch 8/20  Iteration 14037/35720 Training loss: 0.9077 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 14038/35720 Training loss: 0.9077 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 14039/35720 Training loss: 0.9076 0.2253 sec/batch\n",
      "Epoch 8/20  Iteration 14040/35720 Training loss: 0.9076 0.2189 sec/batch\n",
      "Epoch 8/20  Iteration 14041/35720 Training loss: 0.9076 0.2162 sec/batch\n",
      "Epoch 8/20  Iteration 14042/35720 Training loss: 0.9076 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 14043/35720 Training loss: 0.9075 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 14044/35720 Training loss: 0.9075 0.2098 sec/batch\n",
      "Epoch 8/20  Iteration 14045/35720 Training loss: 0.9074 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 14046/35720 Training loss: 0.9074 0.2144 sec/batch\n",
      "Epoch 8/20  Iteration 14047/35720 Training loss: 0.9074 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 14048/35720 Training loss: 0.9073 0.2137 sec/batch\n",
      "Epoch 8/20  Iteration 14049/35720 Training loss: 0.9074 0.2179 sec/batch\n",
      "Epoch 8/20  Iteration 14050/35720 Training loss: 0.9074 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 14051/35720 Training loss: 0.9074 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 14052/35720 Training loss: 0.9073 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 14053/35720 Training loss: 0.9073 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 14054/35720 Training loss: 0.9073 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 14055/35720 Training loss: 0.9073 0.2269 sec/batch\n",
      "Epoch 8/20  Iteration 14056/35720 Training loss: 0.9073 0.2198 sec/batch\n",
      "Epoch 8/20  Iteration 14057/35720 Training loss: 0.9072 0.2141 sec/batch\n",
      "Epoch 8/20  Iteration 14058/35720 Training loss: 0.9072 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 14059/35720 Training loss: 0.9071 0.2106 sec/batch\n",
      "Epoch 8/20  Iteration 14060/35720 Training loss: 0.9071 0.2322 sec/batch\n",
      "Epoch 8/20  Iteration 14061/35720 Training loss: 0.9070 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 14062/35720 Training loss: 0.9070 0.2127 sec/batch\n",
      "Epoch 8/20  Iteration 14063/35720 Training loss: 0.9069 0.2260 sec/batch\n",
      "Epoch 8/20  Iteration 14064/35720 Training loss: 0.9069 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 14065/35720 Training loss: 0.9069 0.2155 sec/batch\n",
      "Epoch 8/20  Iteration 14066/35720 Training loss: 0.9068 0.2174 sec/batch\n",
      "Epoch 8/20  Iteration 14067/35720 Training loss: 0.9068 0.2240 sec/batch\n",
      "Epoch 8/20  Iteration 14068/35720 Training loss: 0.9068 0.2157 sec/batch\n",
      "Epoch 8/20  Iteration 14069/35720 Training loss: 0.9068 0.2221 sec/batch\n",
      "Epoch 8/20  Iteration 14070/35720 Training loss: 0.9068 0.2354 sec/batch\n",
      "Epoch 8/20  Iteration 14071/35720 Training loss: 0.9068 0.2089 sec/batch\n",
      "Epoch 8/20  Iteration 14072/35720 Training loss: 0.9068 0.2319 sec/batch\n",
      "Epoch 8/20  Iteration 14073/35720 Training loss: 0.9067 0.2124 sec/batch\n",
      "Epoch 8/20  Iteration 14074/35720 Training loss: 0.9067 0.2161 sec/batch\n",
      "Epoch 8/20  Iteration 14075/35720 Training loss: 0.9066 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 14076/35720 Training loss: 0.9066 0.2068 sec/batch\n",
      "Epoch 8/20  Iteration 14077/35720 Training loss: 0.9066 0.2105 sec/batch\n",
      "Epoch 8/20  Iteration 14078/35720 Training loss: 0.9066 0.2171 sec/batch\n",
      "Epoch 8/20  Iteration 14079/35720 Training loss: 0.9065 0.2211 sec/batch\n",
      "Epoch 8/20  Iteration 14080/35720 Training loss: 0.9065 0.2166 sec/batch\n",
      "Epoch 8/20  Iteration 14081/35720 Training loss: 0.9065 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 14082/35720 Training loss: 0.9065 0.2172 sec/batch\n",
      "Epoch 8/20  Iteration 14083/35720 Training loss: 0.9065 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 14084/35720 Training loss: 0.9065 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 14085/35720 Training loss: 0.9065 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 14086/35720 Training loss: 0.9064 0.2080 sec/batch\n",
      "Epoch 8/20  Iteration 14087/35720 Training loss: 0.9064 0.2061 sec/batch\n",
      "Epoch 8/20  Iteration 14088/35720 Training loss: 0.9064 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 14089/35720 Training loss: 0.9063 0.2170 sec/batch\n",
      "Epoch 8/20  Iteration 14090/35720 Training loss: 0.9063 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 14091/35720 Training loss: 0.9062 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 14092/35720 Training loss: 0.9062 0.2168 sec/batch\n",
      "Epoch 8/20  Iteration 14093/35720 Training loss: 0.9062 0.2049 sec/batch\n",
      "Epoch 8/20  Iteration 14094/35720 Training loss: 0.9062 0.2179 sec/batch\n",
      "Epoch 8/20  Iteration 14095/35720 Training loss: 0.9062 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 14096/35720 Training loss: 0.9062 0.2206 sec/batch\n",
      "Epoch 8/20  Iteration 14097/35720 Training loss: 0.9061 0.2170 sec/batch\n",
      "Epoch 8/20  Iteration 14098/35720 Training loss: 0.9060 0.2066 sec/batch\n",
      "Epoch 8/20  Iteration 14099/35720 Training loss: 0.9060 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 14100/35720 Training loss: 0.9060 0.2178 sec/batch\n",
      "Epoch 8/20  Iteration 14101/35720 Training loss: 0.9060 0.2213 sec/batch\n",
      "Epoch 8/20  Iteration 14102/35720 Training loss: 0.9060 0.2244 sec/batch\n",
      "Epoch 8/20  Iteration 14103/35720 Training loss: 0.9059 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 14104/35720 Training loss: 0.9059 0.2173 sec/batch\n",
      "Epoch 8/20  Iteration 14105/35720 Training loss: 0.9059 0.2282 sec/batch\n",
      "Epoch 8/20  Iteration 14106/35720 Training loss: 0.9059 0.2155 sec/batch\n",
      "Epoch 8/20  Iteration 14107/35720 Training loss: 0.9059 0.2067 sec/batch\n",
      "Epoch 8/20  Iteration 14108/35720 Training loss: 0.9059 0.2262 sec/batch\n",
      "Epoch 8/20  Iteration 14109/35720 Training loss: 0.9058 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 14110/35720 Training loss: 0.9058 0.2257 sec/batch\n",
      "Epoch 8/20  Iteration 14111/35720 Training loss: 0.9058 0.2134 sec/batch\n",
      "Epoch 8/20  Iteration 14112/35720 Training loss: 0.9057 0.2083 sec/batch\n",
      "Epoch 8/20  Iteration 14113/35720 Training loss: 0.9057 0.2226 sec/batch\n",
      "Epoch 8/20  Iteration 14114/35720 Training loss: 0.9056 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 14115/35720 Training loss: 0.9056 0.2064 sec/batch\n",
      "Epoch 8/20  Iteration 14116/35720 Training loss: 0.9056 0.2130 sec/batch\n",
      "Epoch 8/20  Iteration 14117/35720 Training loss: 0.9056 0.2272 sec/batch\n",
      "Epoch 8/20  Iteration 14118/35720 Training loss: 0.9056 0.2260 sec/batch\n",
      "Epoch 8/20  Iteration 14119/35720 Training loss: 0.9056 0.2235 sec/batch\n",
      "Epoch 8/20  Iteration 14120/35720 Training loss: 0.9056 0.2194 sec/batch\n",
      "Epoch 8/20  Iteration 14121/35720 Training loss: 0.9055 0.2132 sec/batch\n",
      "Epoch 8/20  Iteration 14122/35720 Training loss: 0.9055 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 14123/35720 Training loss: 0.9055 0.2079 sec/batch\n",
      "Epoch 8/20  Iteration 14124/35720 Training loss: 0.9055 0.2151 sec/batch\n",
      "Epoch 8/20  Iteration 14125/35720 Training loss: 0.9055 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 14126/35720 Training loss: 0.9054 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 14127/35720 Training loss: 0.9055 0.2080 sec/batch\n",
      "Epoch 8/20  Iteration 14128/35720 Training loss: 0.9055 0.2271 sec/batch\n",
      "Epoch 8/20  Iteration 14129/35720 Training loss: 0.9055 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 14130/35720 Training loss: 0.9055 0.2149 sec/batch\n",
      "Epoch 8/20  Iteration 14131/35720 Training loss: 0.9054 0.2109 sec/batch\n",
      "Epoch 8/20  Iteration 14132/35720 Training loss: 0.9055 0.2187 sec/batch\n",
      "Epoch 8/20  Iteration 14133/35720 Training loss: 0.9055 0.2175 sec/batch\n",
      "Epoch 8/20  Iteration 14134/35720 Training loss: 0.9054 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 14135/35720 Training loss: 0.9054 0.2234 sec/batch\n",
      "Epoch 8/20  Iteration 14136/35720 Training loss: 0.9054 0.2220 sec/batch\n",
      "Epoch 8/20  Iteration 14137/35720 Training loss: 0.9054 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 14138/35720 Training loss: 0.9055 0.2119 sec/batch\n",
      "Epoch 8/20  Iteration 14139/35720 Training loss: 0.9054 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 14140/35720 Training loss: 0.9054 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 14141/35720 Training loss: 0.9054 0.2144 sec/batch\n",
      "Epoch 8/20  Iteration 14142/35720 Training loss: 0.9054 0.2203 sec/batch\n",
      "Epoch 8/20  Iteration 14143/35720 Training loss: 0.9054 0.2087 sec/batch\n",
      "Epoch 8/20  Iteration 14144/35720 Training loss: 0.9054 0.2217 sec/batch\n",
      "Epoch 8/20  Iteration 14145/35720 Training loss: 0.9053 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 14146/35720 Training loss: 0.9054 0.2151 sec/batch\n",
      "Epoch 8/20  Iteration 14147/35720 Training loss: 0.9054 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 14148/35720 Training loss: 0.9054 0.2111 sec/batch\n",
      "Epoch 8/20  Iteration 14149/35720 Training loss: 0.9054 0.2256 sec/batch\n",
      "Epoch 8/20  Iteration 14150/35720 Training loss: 0.9054 0.2269 sec/batch\n",
      "Epoch 8/20  Iteration 14151/35720 Training loss: 0.9054 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 14152/35720 Training loss: 0.9054 0.2302 sec/batch\n",
      "Epoch 8/20  Iteration 14153/35720 Training loss: 0.9055 0.2209 sec/batch\n",
      "Epoch 8/20  Iteration 14154/35720 Training loss: 0.9054 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 14155/35720 Training loss: 0.9055 0.2313 sec/batch\n",
      "Epoch 8/20  Iteration 14156/35720 Training loss: 0.9055 0.2106 sec/batch\n",
      "Epoch 8/20  Iteration 14157/35720 Training loss: 0.9055 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 14158/35720 Training loss: 0.9055 0.2068 sec/batch\n",
      "Epoch 8/20  Iteration 14159/35720 Training loss: 0.9055 0.2229 sec/batch\n",
      "Epoch 8/20  Iteration 14160/35720 Training loss: 0.9055 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 14161/35720 Training loss: 0.9055 0.2239 sec/batch\n",
      "Epoch 8/20  Iteration 14162/35720 Training loss: 0.9055 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 14163/35720 Training loss: 0.9055 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 14164/35720 Training loss: 0.9055 0.2336 sec/batch\n",
      "Epoch 8/20  Iteration 14165/35720 Training loss: 0.9055 0.2086 sec/batch\n",
      "Epoch 8/20  Iteration 14166/35720 Training loss: 0.9055 0.2136 sec/batch\n",
      "Epoch 8/20  Iteration 14167/35720 Training loss: 0.9055 0.2217 sec/batch\n",
      "Epoch 8/20  Iteration 14168/35720 Training loss: 0.9055 0.2180 sec/batch\n",
      "Epoch 8/20  Iteration 14169/35720 Training loss: 0.9055 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 14170/35720 Training loss: 0.9056 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 14171/35720 Training loss: 0.9056 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 14172/35720 Training loss: 0.9056 0.2171 sec/batch\n",
      "Epoch 8/20  Iteration 14173/35720 Training loss: 0.9056 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 14174/35720 Training loss: 0.9056 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 14175/35720 Training loss: 0.9057 0.2213 sec/batch\n",
      "Epoch 8/20  Iteration 14176/35720 Training loss: 0.9057 0.2169 sec/batch\n",
      "Epoch 8/20  Iteration 14177/35720 Training loss: 0.9057 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 14178/35720 Training loss: 0.9057 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 14179/35720 Training loss: 0.9056 0.2238 sec/batch\n",
      "Epoch 8/20  Iteration 14180/35720 Training loss: 0.9056 0.2184 sec/batch\n",
      "Epoch 8/20  Iteration 14181/35720 Training loss: 0.9056 0.2165 sec/batch\n",
      "Epoch 8/20  Iteration 14182/35720 Training loss: 0.9056 0.2104 sec/batch\n",
      "Epoch 8/20  Iteration 14183/35720 Training loss: 0.9056 0.2293 sec/batch\n",
      "Epoch 8/20  Iteration 14184/35720 Training loss: 0.9055 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 14185/35720 Training loss: 0.9055 0.2344 sec/batch\n",
      "Epoch 8/20  Iteration 14186/35720 Training loss: 0.9055 0.2078 sec/batch\n",
      "Epoch 8/20  Iteration 14187/35720 Training loss: 0.9055 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 14188/35720 Training loss: 0.9056 0.2194 sec/batch\n",
      "Epoch 8/20  Iteration 14189/35720 Training loss: 0.9056 0.2100 sec/batch\n",
      "Epoch 8/20  Iteration 14190/35720 Training loss: 0.9055 0.2146 sec/batch\n",
      "Epoch 8/20  Iteration 14191/35720 Training loss: 0.9055 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 14192/35720 Training loss: 0.9055 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 14193/35720 Training loss: 0.9055 0.2099 sec/batch\n",
      "Epoch 8/20  Iteration 14194/35720 Training loss: 0.9054 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 14195/35720 Training loss: 0.9054 0.2183 sec/batch\n",
      "Epoch 8/20  Iteration 14196/35720 Training loss: 0.9054 0.2369 sec/batch\n",
      "Epoch 8/20  Iteration 14197/35720 Training loss: 0.9054 0.2260 sec/batch\n",
      "Epoch 8/20  Iteration 14198/35720 Training loss: 0.9053 0.2102 sec/batch\n",
      "Epoch 8/20  Iteration 14199/35720 Training loss: 0.9053 0.2090 sec/batch\n",
      "Epoch 8/20  Iteration 14200/35720 Training loss: 0.9053 0.2231 sec/batch\n",
      "Validation loss: 1.36925 Saving checkpoint!\n",
      "Epoch 8/20  Iteration 14201/35720 Training loss: 0.9055 0.2100 sec/batch\n",
      "Epoch 8/20  Iteration 14202/35720 Training loss: 0.9055 0.2069 sec/batch\n",
      "Epoch 8/20  Iteration 14203/35720 Training loss: 0.9055 0.2130 sec/batch\n",
      "Epoch 8/20  Iteration 14204/35720 Training loss: 0.9054 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 14205/35720 Training loss: 0.9054 0.2213 sec/batch\n",
      "Epoch 8/20  Iteration 14206/35720 Training loss: 0.9054 0.2286 sec/batch\n",
      "Epoch 8/20  Iteration 14207/35720 Training loss: 0.9054 0.2282 sec/batch\n",
      "Epoch 8/20  Iteration 14208/35720 Training loss: 0.9053 0.2063 sec/batch\n",
      "Epoch 8/20  Iteration 14209/35720 Training loss: 0.9053 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 14210/35720 Training loss: 0.9053 0.2185 sec/batch\n",
      "Epoch 8/20  Iteration 14211/35720 Training loss: 0.9053 0.2241 sec/batch\n",
      "Epoch 8/20  Iteration 14212/35720 Training loss: 0.9054 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 14213/35720 Training loss: 0.9054 0.2080 sec/batch\n",
      "Epoch 8/20  Iteration 14214/35720 Training loss: 0.9054 0.2100 sec/batch\n",
      "Epoch 8/20  Iteration 14215/35720 Training loss: 0.9054 0.2150 sec/batch\n",
      "Epoch 8/20  Iteration 14216/35720 Training loss: 0.9054 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 14217/35720 Training loss: 0.9054 0.2075 sec/batch\n",
      "Epoch 8/20  Iteration 14218/35720 Training loss: 0.9054 0.2153 sec/batch\n",
      "Epoch 8/20  Iteration 14219/35720 Training loss: 0.9054 0.2237 sec/batch\n",
      "Epoch 8/20  Iteration 14220/35720 Training loss: 0.9054 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 14221/35720 Training loss: 0.9054 0.2367 sec/batch\n",
      "Epoch 8/20  Iteration 14222/35720 Training loss: 0.9054 0.2200 sec/batch\n",
      "Epoch 8/20  Iteration 14223/35720 Training loss: 0.9054 0.2277 sec/batch\n",
      "Epoch 8/20  Iteration 14224/35720 Training loss: 0.9054 0.2261 sec/batch\n",
      "Epoch 8/20  Iteration 14225/35720 Training loss: 0.9054 0.2093 sec/batch\n",
      "Epoch 8/20  Iteration 14226/35720 Training loss: 0.9054 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 14227/35720 Training loss: 0.9053 0.2075 sec/batch\n",
      "Epoch 8/20  Iteration 14228/35720 Training loss: 0.9053 0.2262 sec/batch\n",
      "Epoch 8/20  Iteration 14229/35720 Training loss: 0.9053 0.2315 sec/batch\n",
      "Epoch 8/20  Iteration 14230/35720 Training loss: 0.9053 0.2072 sec/batch\n",
      "Epoch 8/20  Iteration 14231/35720 Training loss: 0.9053 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 14232/35720 Training loss: 0.9053 0.2135 sec/batch\n",
      "Epoch 8/20  Iteration 14233/35720 Training loss: 0.9053 0.2245 sec/batch\n",
      "Epoch 8/20  Iteration 14234/35720 Training loss: 0.9053 0.2176 sec/batch\n",
      "Epoch 8/20  Iteration 14235/35720 Training loss: 0.9053 0.2117 sec/batch\n",
      "Epoch 8/20  Iteration 14236/35720 Training loss: 0.9053 0.2172 sec/batch\n",
      "Epoch 8/20  Iteration 14237/35720 Training loss: 0.9053 0.2094 sec/batch\n",
      "Epoch 8/20  Iteration 14238/35720 Training loss: 0.9053 0.2329 sec/batch\n",
      "Epoch 8/20  Iteration 14239/35720 Training loss: 0.9053 0.2154 sec/batch\n",
      "Epoch 8/20  Iteration 14240/35720 Training loss: 0.9053 0.2239 sec/batch\n",
      "Epoch 8/20  Iteration 14241/35720 Training loss: 0.9053 0.2282 sec/batch\n",
      "Epoch 8/20  Iteration 14242/35720 Training loss: 0.9053 0.2264 sec/batch\n",
      "Epoch 8/20  Iteration 14243/35720 Training loss: 0.9053 0.2322 sec/batch\n",
      "Epoch 8/20  Iteration 14244/35720 Training loss: 0.9053 0.2188 sec/batch\n",
      "Epoch 8/20  Iteration 14245/35720 Training loss: 0.9053 0.2279 sec/batch\n",
      "Epoch 8/20  Iteration 14246/35720 Training loss: 0.9053 0.2121 sec/batch\n",
      "Epoch 8/20  Iteration 14247/35720 Training loss: 0.9053 0.2065 sec/batch\n",
      "Epoch 8/20  Iteration 14248/35720 Training loss: 0.9054 0.2113 sec/batch\n",
      "Epoch 8/20  Iteration 14249/35720 Training loss: 0.9054 0.2079 sec/batch\n",
      "Epoch 8/20  Iteration 14250/35720 Training loss: 0.9054 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 14251/35720 Training loss: 0.9054 0.2214 sec/batch\n",
      "Epoch 8/20  Iteration 14252/35720 Training loss: 0.9053 0.2116 sec/batch\n",
      "Epoch 8/20  Iteration 14253/35720 Training loss: 0.9053 0.2097 sec/batch\n",
      "Epoch 8/20  Iteration 14254/35720 Training loss: 0.9053 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 14255/35720 Training loss: 0.9053 0.2353 sec/batch\n",
      "Epoch 8/20  Iteration 14256/35720 Training loss: 0.9053 0.2136 sec/batch\n",
      "Epoch 8/20  Iteration 14257/35720 Training loss: 0.9053 0.2072 sec/batch\n",
      "Epoch 8/20  Iteration 14258/35720 Training loss: 0.9053 0.2103 sec/batch\n",
      "Epoch 8/20  Iteration 14259/35720 Training loss: 0.9053 0.2276 sec/batch\n",
      "Epoch 8/20  Iteration 14260/35720 Training loss: 0.9053 0.2163 sec/batch\n",
      "Epoch 8/20  Iteration 14261/35720 Training loss: 0.9052 0.2248 sec/batch\n",
      "Epoch 8/20  Iteration 14262/35720 Training loss: 0.9052 0.2126 sec/batch\n",
      "Epoch 8/20  Iteration 14263/35720 Training loss: 0.9052 0.2142 sec/batch\n",
      "Epoch 8/20  Iteration 14264/35720 Training loss: 0.9052 0.2160 sec/batch\n",
      "Epoch 8/20  Iteration 14265/35720 Training loss: 0.9052 0.2177 sec/batch\n",
      "Epoch 8/20  Iteration 14266/35720 Training loss: 0.9052 0.2110 sec/batch\n",
      "Epoch 8/20  Iteration 14267/35720 Training loss: 0.9051 0.2114 sec/batch\n",
      "Epoch 8/20  Iteration 14268/35720 Training loss: 0.9051 0.2195 sec/batch\n",
      "Epoch 8/20  Iteration 14269/35720 Training loss: 0.9051 0.2158 sec/batch\n",
      "Epoch 8/20  Iteration 14270/35720 Training loss: 0.9051 0.2175 sec/batch\n",
      "Epoch 8/20  Iteration 14271/35720 Training loss: 0.9051 0.2356 sec/batch\n",
      "Epoch 8/20  Iteration 14272/35720 Training loss: 0.9051 0.2210 sec/batch\n",
      "Epoch 8/20  Iteration 14273/35720 Training loss: 0.9051 0.2183 sec/batch\n",
      "Epoch 8/20  Iteration 14274/35720 Training loss: 0.9051 0.2060 sec/batch\n",
      "Epoch 8/20  Iteration 14275/35720 Training loss: 0.9051 0.2091 sec/batch\n",
      "Epoch 8/20  Iteration 14276/35720 Training loss: 0.9050 0.2208 sec/batch\n",
      "Epoch 8/20  Iteration 14277/35720 Training loss: 0.9050 0.2201 sec/batch\n",
      "Epoch 8/20  Iteration 14278/35720 Training loss: 0.9050 0.2140 sec/batch\n",
      "Epoch 8/20  Iteration 14279/35720 Training loss: 0.9050 0.2061 sec/batch\n",
      "Epoch 8/20  Iteration 14280/35720 Training loss: 0.9050 0.2083 sec/batch\n",
      "Epoch 8/20  Iteration 14281/35720 Training loss: 0.9049 0.2088 sec/batch\n",
      "Epoch 8/20  Iteration 14282/35720 Training loss: 0.9049 0.2288 sec/batch\n",
      "Epoch 8/20  Iteration 14283/35720 Training loss: 0.9048 0.2145 sec/batch\n",
      "Epoch 8/20  Iteration 14284/35720 Training loss: 0.9048 0.2119 sec/batch\n",
      "Epoch 8/20  Iteration 14285/35720 Training loss: 0.9048 0.2112 sec/batch\n",
      "Epoch 8/20  Iteration 14286/35720 Training loss: 0.9047 0.2274 sec/batch\n",
      "Epoch 8/20  Iteration 14287/35720 Training loss: 0.9047 0.2305 sec/batch\n",
      "Epoch 8/20  Iteration 14288/35720 Training loss: 0.9047 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 14289/35720 Training loss: 0.9310 0.2242 sec/batch\n",
      "Epoch 9/20  Iteration 14290/35720 Training loss: 0.9176 0.2255 sec/batch\n",
      "Epoch 9/20  Iteration 14291/35720 Training loss: 0.9147 0.2077 sec/batch\n",
      "Epoch 9/20  Iteration 14292/35720 Training loss: 0.9089 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 14293/35720 Training loss: 0.9142 0.2313 sec/batch\n",
      "Epoch 9/20  Iteration 14294/35720 Training loss: 0.8998 0.2113 sec/batch\n",
      "Epoch 9/20  Iteration 14295/35720 Training loss: 0.9000 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 14296/35720 Training loss: 0.8899 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 14297/35720 Training loss: 0.8855 0.2070 sec/batch\n",
      "Epoch 9/20  Iteration 14298/35720 Training loss: 0.8876 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 14299/35720 Training loss: 0.8870 0.2262 sec/batch\n",
      "Epoch 9/20  Iteration 14300/35720 Training loss: 0.8838 0.2395 sec/batch\n",
      "Epoch 9/20  Iteration 14301/35720 Training loss: 0.8855 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 14302/35720 Training loss: 0.8910 0.2123 sec/batch\n",
      "Epoch 9/20  Iteration 14303/35720 Training loss: 0.8937 0.2128 sec/batch\n",
      "Epoch 9/20  Iteration 14304/35720 Training loss: 0.8921 0.2281 sec/batch\n",
      "Epoch 9/20  Iteration 14305/35720 Training loss: 0.8924 0.2279 sec/batch\n",
      "Epoch 9/20  Iteration 14306/35720 Training loss: 0.8902 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 14307/35720 Training loss: 0.8878 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 14308/35720 Training loss: 0.8888 0.2191 sec/batch\n",
      "Epoch 9/20  Iteration 14309/35720 Training loss: 0.8910 0.2193 sec/batch\n",
      "Epoch 9/20  Iteration 14310/35720 Training loss: 0.8886 0.2243 sec/batch\n",
      "Epoch 9/20  Iteration 14311/35720 Training loss: 0.8879 0.2244 sec/batch\n",
      "Epoch 9/20  Iteration 14312/35720 Training loss: 0.8894 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 14313/35720 Training loss: 0.8903 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 14314/35720 Training loss: 0.8894 0.2131 sec/batch\n",
      "Epoch 9/20  Iteration 14315/35720 Training loss: 0.8923 0.2187 sec/batch\n",
      "Epoch 9/20  Iteration 14316/35720 Training loss: 0.8931 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 14317/35720 Training loss: 0.8920 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 14318/35720 Training loss: 0.8921 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 14319/35720 Training loss: 0.8957 0.2113 sec/batch\n",
      "Epoch 9/20  Iteration 14320/35720 Training loss: 0.8941 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 14321/35720 Training loss: 0.8959 0.2258 sec/batch\n",
      "Epoch 9/20  Iteration 14322/35720 Training loss: 0.8984 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 14323/35720 Training loss: 0.9015 0.2080 sec/batch\n",
      "Epoch 9/20  Iteration 14324/35720 Training loss: 0.9021 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 14325/35720 Training loss: 0.9020 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 14326/35720 Training loss: 0.9017 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 14327/35720 Training loss: 0.9001 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 14328/35720 Training loss: 0.9014 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 14329/35720 Training loss: 0.9011 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 14330/35720 Training loss: 0.8995 0.2074 sec/batch\n",
      "Epoch 9/20  Iteration 14331/35720 Training loss: 0.8975 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 14332/35720 Training loss: 0.8966 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 14333/35720 Training loss: 0.8962 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 14334/35720 Training loss: 0.8954 0.2272 sec/batch\n",
      "Epoch 9/20  Iteration 14335/35720 Training loss: 0.8955 0.2186 sec/batch\n",
      "Epoch 9/20  Iteration 14336/35720 Training loss: 0.8951 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 14337/35720 Training loss: 0.8949 0.2280 sec/batch\n",
      "Epoch 9/20  Iteration 14338/35720 Training loss: 0.8936 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 14339/35720 Training loss: 0.8945 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 14340/35720 Training loss: 0.8946 0.2262 sec/batch\n",
      "Epoch 9/20  Iteration 14341/35720 Training loss: 0.8943 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 14342/35720 Training loss: 0.8925 0.2101 sec/batch\n",
      "Epoch 9/20  Iteration 14343/35720 Training loss: 0.8916 0.2134 sec/batch\n",
      "Epoch 9/20  Iteration 14344/35720 Training loss: 0.8912 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 14345/35720 Training loss: 0.8915 0.2233 sec/batch\n",
      "Epoch 9/20  Iteration 14346/35720 Training loss: 0.8911 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 14347/35720 Training loss: 0.8902 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14348/35720 Training loss: 0.8889 0.2290 sec/batch\n",
      "Epoch 9/20  Iteration 14349/35720 Training loss: 0.8879 0.2071 sec/batch\n",
      "Epoch 9/20  Iteration 14350/35720 Training loss: 0.8858 0.2211 sec/batch\n",
      "Epoch 9/20  Iteration 14351/35720 Training loss: 0.8864 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 14352/35720 Training loss: 0.8864 0.2067 sec/batch\n",
      "Epoch 9/20  Iteration 14353/35720 Training loss: 0.8870 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 14354/35720 Training loss: 0.8870 0.2171 sec/batch\n",
      "Epoch 9/20  Iteration 14355/35720 Training loss: 0.8867 0.2197 sec/batch\n",
      "Epoch 9/20  Iteration 14356/35720 Training loss: 0.8857 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 14357/35720 Training loss: 0.8864 0.2072 sec/batch\n",
      "Epoch 9/20  Iteration 14358/35720 Training loss: 0.8861 0.2068 sec/batch\n",
      "Epoch 9/20  Iteration 14359/35720 Training loss: 0.8867 0.2086 sec/batch\n",
      "Epoch 9/20  Iteration 14360/35720 Training loss: 0.8873 0.2185 sec/batch\n",
      "Epoch 9/20  Iteration 14361/35720 Training loss: 0.8875 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 14362/35720 Training loss: 0.8877 0.2146 sec/batch\n",
      "Epoch 9/20  Iteration 14363/35720 Training loss: 0.8866 0.2111 sec/batch\n",
      "Epoch 9/20  Iteration 14364/35720 Training loss: 0.8864 0.2151 sec/batch\n",
      "Epoch 9/20  Iteration 14365/35720 Training loss: 0.8854 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 14366/35720 Training loss: 0.8863 0.2083 sec/batch\n",
      "Epoch 9/20  Iteration 14367/35720 Training loss: 0.8861 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 14368/35720 Training loss: 0.8875 0.2186 sec/batch\n",
      "Epoch 9/20  Iteration 14369/35720 Training loss: 0.8878 0.2228 sec/batch\n",
      "Epoch 9/20  Iteration 14370/35720 Training loss: 0.8877 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 14371/35720 Training loss: 0.8875 0.2216 sec/batch\n",
      "Epoch 9/20  Iteration 14372/35720 Training loss: 0.8877 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 14373/35720 Training loss: 0.8875 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 14374/35720 Training loss: 0.8875 0.2283 sec/batch\n",
      "Epoch 9/20  Iteration 14375/35720 Training loss: 0.8873 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14376/35720 Training loss: 0.8870 0.2231 sec/batch\n",
      "Epoch 9/20  Iteration 14377/35720 Training loss: 0.8860 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 14378/35720 Training loss: 0.8849 0.2186 sec/batch\n",
      "Epoch 9/20  Iteration 14379/35720 Training loss: 0.8850 0.2139 sec/batch\n",
      "Epoch 9/20  Iteration 14380/35720 Training loss: 0.8847 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 14381/35720 Training loss: 0.8845 0.2074 sec/batch\n",
      "Epoch 9/20  Iteration 14382/35720 Training loss: 0.8846 0.2186 sec/batch\n",
      "Epoch 9/20  Iteration 14383/35720 Training loss: 0.8844 0.2323 sec/batch\n",
      "Epoch 9/20  Iteration 14384/35720 Training loss: 0.8837 0.2270 sec/batch\n",
      "Epoch 9/20  Iteration 14385/35720 Training loss: 0.8841 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14386/35720 Training loss: 0.8841 0.2086 sec/batch\n",
      "Epoch 9/20  Iteration 14387/35720 Training loss: 0.8841 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 14388/35720 Training loss: 0.8838 0.2186 sec/batch\n",
      "Epoch 9/20  Iteration 14389/35720 Training loss: 0.8839 0.2228 sec/batch\n",
      "Epoch 9/20  Iteration 14390/35720 Training loss: 0.8840 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 14391/35720 Training loss: 0.8839 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 14392/35720 Training loss: 0.8836 0.2162 sec/batch\n",
      "Epoch 9/20  Iteration 14393/35720 Training loss: 0.8835 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 14394/35720 Training loss: 0.8830 0.2149 sec/batch\n",
      "Epoch 9/20  Iteration 14395/35720 Training loss: 0.8830 0.2200 sec/batch\n",
      "Epoch 9/20  Iteration 14396/35720 Training loss: 0.8833 0.2264 sec/batch\n",
      "Epoch 9/20  Iteration 14397/35720 Training loss: 0.8837 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 14398/35720 Training loss: 0.8837 0.2232 sec/batch\n",
      "Epoch 9/20  Iteration 14399/35720 Training loss: 0.8842 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 14400/35720 Training loss: 0.8849 0.2150 sec/batch\n",
      "Validation loss: 1.36635 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 14401/35720 Training loss: 0.8876 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14402/35720 Training loss: 0.8880 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 14403/35720 Training loss: 0.8877 0.2083 sec/batch\n",
      "Epoch 9/20  Iteration 14404/35720 Training loss: 0.8878 0.2134 sec/batch\n",
      "Epoch 9/20  Iteration 14405/35720 Training loss: 0.8877 0.2279 sec/batch\n",
      "Epoch 9/20  Iteration 14406/35720 Training loss: 0.8880 0.2156 sec/batch\n",
      "Epoch 9/20  Iteration 14407/35720 Training loss: 0.8881 0.2270 sec/batch\n",
      "Epoch 9/20  Iteration 14408/35720 Training loss: 0.8889 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 14409/35720 Training loss: 0.8896 0.2367 sec/batch\n",
      "Epoch 9/20  Iteration 14410/35720 Training loss: 0.8895 0.2326 sec/batch\n",
      "Epoch 9/20  Iteration 14411/35720 Training loss: 0.8898 0.2272 sec/batch\n",
      "Epoch 9/20  Iteration 14412/35720 Training loss: 0.8904 0.2070 sec/batch\n",
      "Epoch 9/20  Iteration 14413/35720 Training loss: 0.8900 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 14414/35720 Training loss: 0.8899 0.2249 sec/batch\n",
      "Epoch 9/20  Iteration 14415/35720 Training loss: 0.8900 0.2291 sec/batch\n",
      "Epoch 9/20  Iteration 14416/35720 Training loss: 0.8900 0.2245 sec/batch\n",
      "Epoch 9/20  Iteration 14417/35720 Training loss: 0.8896 0.2204 sec/batch\n",
      "Epoch 9/20  Iteration 14418/35720 Training loss: 0.8896 0.2251 sec/batch\n",
      "Epoch 9/20  Iteration 14419/35720 Training loss: 0.8893 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14420/35720 Training loss: 0.8890 0.2279 sec/batch\n",
      "Epoch 9/20  Iteration 14421/35720 Training loss: 0.8890 0.2156 sec/batch\n",
      "Epoch 9/20  Iteration 14422/35720 Training loss: 0.8891 0.2132 sec/batch\n",
      "Epoch 9/20  Iteration 14423/35720 Training loss: 0.8890 0.2140 sec/batch\n",
      "Epoch 9/20  Iteration 14424/35720 Training loss: 0.8889 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 14425/35720 Training loss: 0.8895 0.2246 sec/batch\n",
      "Epoch 9/20  Iteration 14426/35720 Training loss: 0.8898 0.2277 sec/batch\n",
      "Epoch 9/20  Iteration 14427/35720 Training loss: 0.8899 0.2182 sec/batch\n",
      "Epoch 9/20  Iteration 14428/35720 Training loss: 0.8900 0.2100 sec/batch\n",
      "Epoch 9/20  Iteration 14429/35720 Training loss: 0.8898 0.2083 sec/batch\n",
      "Epoch 9/20  Iteration 14430/35720 Training loss: 0.8893 0.2224 sec/batch\n",
      "Epoch 9/20  Iteration 14431/35720 Training loss: 0.8888 0.2297 sec/batch\n",
      "Epoch 9/20  Iteration 14432/35720 Training loss: 0.8882 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 14433/35720 Training loss: 0.8883 0.2258 sec/batch\n",
      "Epoch 9/20  Iteration 14434/35720 Training loss: 0.8887 0.2203 sec/batch\n",
      "Epoch 9/20  Iteration 14435/35720 Training loss: 0.8883 0.2333 sec/batch\n",
      "Epoch 9/20  Iteration 14436/35720 Training loss: 0.8881 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 14437/35720 Training loss: 0.8882 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14438/35720 Training loss: 0.8875 0.2283 sec/batch\n",
      "Epoch 9/20  Iteration 14439/35720 Training loss: 0.8874 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 14440/35720 Training loss: 0.8875 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14441/35720 Training loss: 0.8873 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 14442/35720 Training loss: 0.8877 0.2304 sec/batch\n",
      "Epoch 9/20  Iteration 14443/35720 Training loss: 0.8879 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 14444/35720 Training loss: 0.8882 0.2228 sec/batch\n",
      "Epoch 9/20  Iteration 14445/35720 Training loss: 0.8882 0.2113 sec/batch\n",
      "Epoch 9/20  Iteration 14446/35720 Training loss: 0.8885 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 14447/35720 Training loss: 0.8882 0.2329 sec/batch\n",
      "Epoch 9/20  Iteration 14448/35720 Training loss: 0.8884 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 14449/35720 Training loss: 0.8881 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 14450/35720 Training loss: 0.8882 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 14451/35720 Training loss: 0.8883 0.2110 sec/batch\n",
      "Epoch 9/20  Iteration 14452/35720 Training loss: 0.8885 0.2150 sec/batch\n",
      "Epoch 9/20  Iteration 14453/35720 Training loss: 0.8887 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 14454/35720 Training loss: 0.8890 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 14455/35720 Training loss: 0.8891 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 14456/35720 Training loss: 0.8894 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 14457/35720 Training loss: 0.8899 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 14458/35720 Training loss: 0.8902 0.2599 sec/batch\n",
      "Epoch 9/20  Iteration 14459/35720 Training loss: 0.8907 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 14460/35720 Training loss: 0.8911 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 14461/35720 Training loss: 0.8913 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 14462/35720 Training loss: 0.8919 0.2221 sec/batch\n",
      "Epoch 9/20  Iteration 14463/35720 Training loss: 0.8920 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 14464/35720 Training loss: 0.8920 0.2180 sec/batch\n",
      "Epoch 9/20  Iteration 14465/35720 Training loss: 0.8921 0.2282 sec/batch\n",
      "Epoch 9/20  Iteration 14466/35720 Training loss: 0.8919 0.2319 sec/batch\n",
      "Epoch 9/20  Iteration 14467/35720 Training loss: 0.8918 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 14468/35720 Training loss: 0.8916 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 14469/35720 Training loss: 0.8917 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 14470/35720 Training loss: 0.8919 0.2220 sec/batch\n",
      "Epoch 9/20  Iteration 14471/35720 Training loss: 0.8919 0.2334 sec/batch\n",
      "Epoch 9/20  Iteration 14472/35720 Training loss: 0.8920 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 14473/35720 Training loss: 0.8918 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 14474/35720 Training loss: 0.8918 0.2074 sec/batch\n",
      "Epoch 9/20  Iteration 14475/35720 Training loss: 0.8916 0.2244 sec/batch\n",
      "Epoch 9/20  Iteration 14476/35720 Training loss: 0.8917 0.2328 sec/batch\n",
      "Epoch 9/20  Iteration 14477/35720 Training loss: 0.8918 0.2259 sec/batch\n",
      "Epoch 9/20  Iteration 14478/35720 Training loss: 0.8919 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 14479/35720 Training loss: 0.8920 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 14480/35720 Training loss: 0.8925 0.2197 sec/batch\n",
      "Epoch 9/20  Iteration 14481/35720 Training loss: 0.8927 0.2284 sec/batch\n",
      "Epoch 9/20  Iteration 14482/35720 Training loss: 0.8929 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 14483/35720 Training loss: 0.8929 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 14484/35720 Training loss: 0.8931 0.2143 sec/batch\n",
      "Epoch 9/20  Iteration 14485/35720 Training loss: 0.8927 0.2243 sec/batch\n",
      "Epoch 9/20  Iteration 14486/35720 Training loss: 0.8927 0.2314 sec/batch\n",
      "Epoch 9/20  Iteration 14487/35720 Training loss: 0.8928 0.2095 sec/batch\n",
      "Epoch 9/20  Iteration 14488/35720 Training loss: 0.8930 0.2318 sec/batch\n",
      "Epoch 9/20  Iteration 14489/35720 Training loss: 0.8929 0.2313 sec/batch\n",
      "Epoch 9/20  Iteration 14490/35720 Training loss: 0.8928 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 14491/35720 Training loss: 0.8928 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 14492/35720 Training loss: 0.8928 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 14493/35720 Training loss: 0.8928 0.2343 sec/batch\n",
      "Epoch 9/20  Iteration 14494/35720 Training loss: 0.8927 0.2219 sec/batch\n",
      "Epoch 9/20  Iteration 14495/35720 Training loss: 0.8931 0.2224 sec/batch\n",
      "Epoch 9/20  Iteration 14496/35720 Training loss: 0.8934 0.2251 sec/batch\n",
      "Epoch 9/20  Iteration 14497/35720 Training loss: 0.8938 0.2139 sec/batch\n",
      "Epoch 9/20  Iteration 14498/35720 Training loss: 0.8939 0.2079 sec/batch\n",
      "Epoch 9/20  Iteration 14499/35720 Training loss: 0.8942 0.2210 sec/batch\n",
      "Epoch 9/20  Iteration 14500/35720 Training loss: 0.8941 0.2061 sec/batch\n",
      "Epoch 9/20  Iteration 14501/35720 Training loss: 0.8940 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 14502/35720 Training loss: 0.8940 0.2236 sec/batch\n",
      "Epoch 9/20  Iteration 14503/35720 Training loss: 0.8941 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 14504/35720 Training loss: 0.8940 0.2258 sec/batch\n",
      "Epoch 9/20  Iteration 14505/35720 Training loss: 0.8938 0.2101 sec/batch\n",
      "Epoch 9/20  Iteration 14506/35720 Training loss: 0.8937 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 14507/35720 Training loss: 0.8937 0.2143 sec/batch\n",
      "Epoch 9/20  Iteration 14508/35720 Training loss: 0.8938 0.2248 sec/batch\n",
      "Epoch 9/20  Iteration 14509/35720 Training loss: 0.8938 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 14510/35720 Training loss: 0.8937 0.2444 sec/batch\n",
      "Epoch 9/20  Iteration 14511/35720 Training loss: 0.8941 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 14512/35720 Training loss: 0.8944 0.2139 sec/batch\n",
      "Epoch 9/20  Iteration 14513/35720 Training loss: 0.8945 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 14514/35720 Training loss: 0.8943 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 14515/35720 Training loss: 0.8942 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 14516/35720 Training loss: 0.8941 0.2070 sec/batch\n",
      "Epoch 9/20  Iteration 14517/35720 Training loss: 0.8938 0.2241 sec/batch\n",
      "Epoch 9/20  Iteration 14518/35720 Training loss: 0.8939 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 14519/35720 Training loss: 0.8943 0.2224 sec/batch\n",
      "Epoch 9/20  Iteration 14520/35720 Training loss: 0.8942 0.2266 sec/batch\n",
      "Epoch 9/20  Iteration 14521/35720 Training loss: 0.8943 0.2070 sec/batch\n",
      "Epoch 9/20  Iteration 14522/35720 Training loss: 0.8944 0.2121 sec/batch\n",
      "Epoch 9/20  Iteration 14523/35720 Training loss: 0.8944 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 14524/35720 Training loss: 0.8944 0.2074 sec/batch\n",
      "Epoch 9/20  Iteration 14525/35720 Training loss: 0.8945 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 14526/35720 Training loss: 0.8943 0.2139 sec/batch\n",
      "Epoch 9/20  Iteration 14527/35720 Training loss: 0.8941 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 14528/35720 Training loss: 0.8942 0.2071 sec/batch\n",
      "Epoch 9/20  Iteration 14529/35720 Training loss: 0.8939 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 14530/35720 Training loss: 0.8939 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 14531/35720 Training loss: 0.8938 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 14532/35720 Training loss: 0.8939 0.2177 sec/batch\n",
      "Epoch 9/20  Iteration 14533/35720 Training loss: 0.8937 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 14534/35720 Training loss: 0.8938 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 14535/35720 Training loss: 0.8938 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 14536/35720 Training loss: 0.8937 0.2335 sec/batch\n",
      "Epoch 9/20  Iteration 14537/35720 Training loss: 0.8936 0.2226 sec/batch\n",
      "Epoch 9/20  Iteration 14538/35720 Training loss: 0.8933 0.2248 sec/batch\n",
      "Epoch 9/20  Iteration 14539/35720 Training loss: 0.8934 0.2056 sec/batch\n",
      "Epoch 9/20  Iteration 14540/35720 Training loss: 0.8932 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 14541/35720 Training loss: 0.8930 0.2139 sec/batch\n",
      "Epoch 9/20  Iteration 14542/35720 Training loss: 0.8932 0.2223 sec/batch\n",
      "Epoch 9/20  Iteration 14543/35720 Training loss: 0.8935 0.2244 sec/batch\n",
      "Epoch 9/20  Iteration 14544/35720 Training loss: 0.8935 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 14545/35720 Training loss: 0.8936 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 14546/35720 Training loss: 0.8936 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 14547/35720 Training loss: 0.8939 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 14548/35720 Training loss: 0.8938 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 14549/35720 Training loss: 0.8938 0.2277 sec/batch\n",
      "Epoch 9/20  Iteration 14550/35720 Training loss: 0.8937 0.2267 sec/batch\n",
      "Epoch 9/20  Iteration 14551/35720 Training loss: 0.8936 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 14552/35720 Training loss: 0.8937 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 14553/35720 Training loss: 0.8937 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 14554/35720 Training loss: 0.8939 0.2255 sec/batch\n",
      "Epoch 9/20  Iteration 14555/35720 Training loss: 0.8937 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 14556/35720 Training loss: 0.8937 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 14557/35720 Training loss: 0.8936 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 14558/35720 Training loss: 0.8930 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 14559/35720 Training loss: 0.8928 0.2098 sec/batch\n",
      "Epoch 9/20  Iteration 14560/35720 Training loss: 0.8928 0.2222 sec/batch\n",
      "Epoch 9/20  Iteration 14561/35720 Training loss: 0.8929 0.2154 sec/batch\n",
      "Epoch 9/20  Iteration 14562/35720 Training loss: 0.8930 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 14563/35720 Training loss: 0.8928 0.2249 sec/batch\n",
      "Epoch 9/20  Iteration 14564/35720 Training loss: 0.8926 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 14565/35720 Training loss: 0.8923 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 14566/35720 Training loss: 0.8921 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 14567/35720 Training loss: 0.8920 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 14568/35720 Training loss: 0.8920 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 14569/35720 Training loss: 0.8919 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 14570/35720 Training loss: 0.8916 0.2371 sec/batch\n",
      "Epoch 9/20  Iteration 14571/35720 Training loss: 0.8913 0.2262 sec/batch\n",
      "Epoch 9/20  Iteration 14572/35720 Training loss: 0.8911 0.2051 sec/batch\n",
      "Epoch 9/20  Iteration 14573/35720 Training loss: 0.8914 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 14574/35720 Training loss: 0.8913 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 14575/35720 Training loss: 0.8910 0.2290 sec/batch\n",
      "Epoch 9/20  Iteration 14576/35720 Training loss: 0.8910 0.2316 sec/batch\n",
      "Epoch 9/20  Iteration 14577/35720 Training loss: 0.8910 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 14578/35720 Training loss: 0.8910 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 14579/35720 Training loss: 0.8910 0.2083 sec/batch\n",
      "Epoch 9/20  Iteration 14580/35720 Training loss: 0.8909 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 14581/35720 Training loss: 0.8909 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 14582/35720 Training loss: 0.8909 0.2228 sec/batch\n",
      "Epoch 9/20  Iteration 14583/35720 Training loss: 0.8912 0.2237 sec/batch\n",
      "Epoch 9/20  Iteration 14584/35720 Training loss: 0.8912 0.2140 sec/batch\n",
      "Epoch 9/20  Iteration 14585/35720 Training loss: 0.8912 0.2146 sec/batch\n",
      "Epoch 9/20  Iteration 14586/35720 Training loss: 0.8914 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 14587/35720 Training loss: 0.8913 0.2096 sec/batch\n",
      "Epoch 9/20  Iteration 14588/35720 Training loss: 0.8913 0.2264 sec/batch\n",
      "Epoch 9/20  Iteration 14589/35720 Training loss: 0.8913 0.2056 sec/batch\n",
      "Epoch 9/20  Iteration 14590/35720 Training loss: 0.8911 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 14591/35720 Training loss: 0.8912 0.2313 sec/batch\n",
      "Epoch 9/20  Iteration 14592/35720 Training loss: 0.8912 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 14593/35720 Training loss: 0.8911 0.2225 sec/batch\n",
      "Epoch 9/20  Iteration 14594/35720 Training loss: 0.8910 0.2066 sec/batch\n",
      "Epoch 9/20  Iteration 14595/35720 Training loss: 0.8909 0.2066 sec/batch\n",
      "Epoch 9/20  Iteration 14596/35720 Training loss: 0.8910 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 14597/35720 Training loss: 0.8908 0.2219 sec/batch\n",
      "Epoch 9/20  Iteration 14598/35720 Training loss: 0.8907 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 14599/35720 Training loss: 0.8905 0.2157 sec/batch\n",
      "Epoch 9/20  Iteration 14600/35720 Training loss: 0.8905 0.2114 sec/batch\n",
      "Validation loss: 1.36932 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 14601/35720 Training loss: 0.8910 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14602/35720 Training loss: 0.8909 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 14603/35720 Training loss: 0.8909 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 14604/35720 Training loss: 0.8909 0.2216 sec/batch\n",
      "Epoch 9/20  Iteration 14605/35720 Training loss: 0.8908 0.2059 sec/batch\n",
      "Epoch 9/20  Iteration 14606/35720 Training loss: 0.8909 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 14607/35720 Training loss: 0.8909 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 14608/35720 Training loss: 0.8909 0.2132 sec/batch\n",
      "Epoch 9/20  Iteration 14609/35720 Training loss: 0.8910 0.2296 sec/batch\n",
      "Epoch 9/20  Iteration 14610/35720 Training loss: 0.8909 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 14611/35720 Training loss: 0.8910 0.2146 sec/batch\n",
      "Epoch 9/20  Iteration 14612/35720 Training loss: 0.8911 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 14613/35720 Training loss: 0.8909 0.2254 sec/batch\n",
      "Epoch 9/20  Iteration 14614/35720 Training loss: 0.8910 0.2297 sec/batch\n",
      "Epoch 9/20  Iteration 14615/35720 Training loss: 0.8911 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 14616/35720 Training loss: 0.8911 0.2071 sec/batch\n",
      "Epoch 9/20  Iteration 14617/35720 Training loss: 0.8911 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 14618/35720 Training loss: 0.8911 0.2282 sec/batch\n",
      "Epoch 9/20  Iteration 14619/35720 Training loss: 0.8912 0.2278 sec/batch\n",
      "Epoch 9/20  Iteration 14620/35720 Training loss: 0.8913 0.2251 sec/batch\n",
      "Epoch 9/20  Iteration 14621/35720 Training loss: 0.8913 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 14622/35720 Training loss: 0.8913 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 14623/35720 Training loss: 0.8912 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 14624/35720 Training loss: 0.8910 0.2267 sec/batch\n",
      "Epoch 9/20  Iteration 14625/35720 Training loss: 0.8910 0.2287 sec/batch\n",
      "Epoch 9/20  Iteration 14626/35720 Training loss: 0.8908 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 14627/35720 Training loss: 0.8908 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 14628/35720 Training loss: 0.8909 0.2220 sec/batch\n",
      "Epoch 9/20  Iteration 14629/35720 Training loss: 0.8907 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 14630/35720 Training loss: 0.8906 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 14631/35720 Training loss: 0.8907 0.2162 sec/batch\n",
      "Epoch 9/20  Iteration 14632/35720 Training loss: 0.8907 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 14633/35720 Training loss: 0.8904 0.2287 sec/batch\n",
      "Epoch 9/20  Iteration 14634/35720 Training loss: 0.8905 0.2180 sec/batch\n",
      "Epoch 9/20  Iteration 14635/35720 Training loss: 0.8906 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 14636/35720 Training loss: 0.8907 0.2130 sec/batch\n",
      "Epoch 9/20  Iteration 14637/35720 Training loss: 0.8908 0.2607 sec/batch\n",
      "Epoch 9/20  Iteration 14638/35720 Training loss: 0.8908 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 14639/35720 Training loss: 0.8909 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 14640/35720 Training loss: 0.8908 0.2227 sec/batch\n",
      "Epoch 9/20  Iteration 14641/35720 Training loss: 0.8909 0.2100 sec/batch\n",
      "Epoch 9/20  Iteration 14642/35720 Training loss: 0.8909 0.2266 sec/batch\n",
      "Epoch 9/20  Iteration 14643/35720 Training loss: 0.8910 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 14644/35720 Training loss: 0.8910 0.2071 sec/batch\n",
      "Epoch 9/20  Iteration 14645/35720 Training loss: 0.8910 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 14646/35720 Training loss: 0.8910 0.2254 sec/batch\n",
      "Epoch 9/20  Iteration 14647/35720 Training loss: 0.8909 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 14648/35720 Training loss: 0.8908 0.2208 sec/batch\n",
      "Epoch 9/20  Iteration 14649/35720 Training loss: 0.8907 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 14650/35720 Training loss: 0.8908 0.2130 sec/batch\n",
      "Epoch 9/20  Iteration 14651/35720 Training loss: 0.8908 0.2249 sec/batch\n",
      "Epoch 9/20  Iteration 14652/35720 Training loss: 0.8908 0.2301 sec/batch\n",
      "Epoch 9/20  Iteration 14653/35720 Training loss: 0.8907 0.2272 sec/batch\n",
      "Epoch 9/20  Iteration 14654/35720 Training loss: 0.8906 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 14655/35720 Training loss: 0.8906 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 14656/35720 Training loss: 0.8905 0.2189 sec/batch\n",
      "Epoch 9/20  Iteration 14657/35720 Training loss: 0.8903 0.2295 sec/batch\n",
      "Epoch 9/20  Iteration 14658/35720 Training loss: 0.8903 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 14659/35720 Training loss: 0.8901 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14660/35720 Training loss: 0.8901 0.2100 sec/batch\n",
      "Epoch 9/20  Iteration 14661/35720 Training loss: 0.8900 0.2305 sec/batch\n",
      "Epoch 9/20  Iteration 14662/35720 Training loss: 0.8900 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 14663/35720 Training loss: 0.8900 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14664/35720 Training loss: 0.8900 0.2280 sec/batch\n",
      "Epoch 9/20  Iteration 14665/35720 Training loss: 0.8901 0.2149 sec/batch\n",
      "Epoch 9/20  Iteration 14666/35720 Training loss: 0.8900 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 14667/35720 Training loss: 0.8898 0.2154 sec/batch\n",
      "Epoch 9/20  Iteration 14668/35720 Training loss: 0.8897 0.2179 sec/batch\n",
      "Epoch 9/20  Iteration 14669/35720 Training loss: 0.8895 0.2275 sec/batch\n",
      "Epoch 9/20  Iteration 14670/35720 Training loss: 0.8895 0.2285 sec/batch\n",
      "Epoch 9/20  Iteration 14671/35720 Training loss: 0.8894 0.2053 sec/batch\n",
      "Epoch 9/20  Iteration 14672/35720 Training loss: 0.8894 0.2174 sec/batch\n",
      "Epoch 9/20  Iteration 14673/35720 Training loss: 0.8894 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 14674/35720 Training loss: 0.8894 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 14675/35720 Training loss: 0.8894 0.2142 sec/batch\n",
      "Epoch 9/20  Iteration 14676/35720 Training loss: 0.8895 0.2154 sec/batch\n",
      "Epoch 9/20  Iteration 14677/35720 Training loss: 0.8895 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 14678/35720 Training loss: 0.8895 0.2193 sec/batch\n",
      "Epoch 9/20  Iteration 14679/35720 Training loss: 0.8894 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 14680/35720 Training loss: 0.8892 0.2269 sec/batch\n",
      "Epoch 9/20  Iteration 14681/35720 Training loss: 0.8892 0.2330 sec/batch\n",
      "Epoch 9/20  Iteration 14682/35720 Training loss: 0.8890 0.2239 sec/batch\n",
      "Epoch 9/20  Iteration 14683/35720 Training loss: 0.8890 0.2131 sec/batch\n",
      "Epoch 9/20  Iteration 14684/35720 Training loss: 0.8889 0.2213 sec/batch\n",
      "Epoch 9/20  Iteration 14685/35720 Training loss: 0.8888 0.2302 sec/batch\n",
      "Epoch 9/20  Iteration 14686/35720 Training loss: 0.8886 0.2261 sec/batch\n",
      "Epoch 9/20  Iteration 14687/35720 Training loss: 0.8885 0.2086 sec/batch\n",
      "Epoch 9/20  Iteration 14688/35720 Training loss: 0.8884 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 14689/35720 Training loss: 0.8883 0.2257 sec/batch\n",
      "Epoch 9/20  Iteration 14690/35720 Training loss: 0.8884 0.2280 sec/batch\n",
      "Epoch 9/20  Iteration 14691/35720 Training loss: 0.8885 0.2241 sec/batch\n",
      "Epoch 9/20  Iteration 14692/35720 Training loss: 0.8884 0.2224 sec/batch\n",
      "Epoch 9/20  Iteration 14693/35720 Training loss: 0.8883 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 14694/35720 Training loss: 0.8882 0.2160 sec/batch\n",
      "Epoch 9/20  Iteration 14695/35720 Training loss: 0.8881 0.2233 sec/batch\n",
      "Epoch 9/20  Iteration 14696/35720 Training loss: 0.8881 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 14697/35720 Training loss: 0.8879 0.2269 sec/batch\n",
      "Epoch 9/20  Iteration 14698/35720 Training loss: 0.8878 0.2162 sec/batch\n",
      "Epoch 9/20  Iteration 14699/35720 Training loss: 0.8878 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 14700/35720 Training loss: 0.8877 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 14701/35720 Training loss: 0.8874 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 14702/35720 Training loss: 0.8874 0.2098 sec/batch\n",
      "Epoch 9/20  Iteration 14703/35720 Training loss: 0.8873 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 14704/35720 Training loss: 0.8872 0.2203 sec/batch\n",
      "Epoch 9/20  Iteration 14705/35720 Training loss: 0.8872 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 14706/35720 Training loss: 0.8870 0.2083 sec/batch\n",
      "Epoch 9/20  Iteration 14707/35720 Training loss: 0.8868 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 14708/35720 Training loss: 0.8868 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 14709/35720 Training loss: 0.8869 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 14710/35720 Training loss: 0.8868 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 14711/35720 Training loss: 0.8870 0.2100 sec/batch\n",
      "Epoch 9/20  Iteration 14712/35720 Training loss: 0.8870 0.2101 sec/batch\n",
      "Epoch 9/20  Iteration 14713/35720 Training loss: 0.8870 0.2838 sec/batch\n",
      "Epoch 9/20  Iteration 14714/35720 Training loss: 0.8868 0.2891 sec/batch\n",
      "Epoch 9/20  Iteration 14715/35720 Training loss: 0.8867 0.2261 sec/batch\n",
      "Epoch 9/20  Iteration 14716/35720 Training loss: 0.8868 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 14717/35720 Training loss: 0.8868 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 14718/35720 Training loss: 0.8869 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 14719/35720 Training loss: 0.8870 0.2162 sec/batch\n",
      "Epoch 9/20  Iteration 14720/35720 Training loss: 0.8870 0.2060 sec/batch\n",
      "Epoch 9/20  Iteration 14721/35720 Training loss: 0.8871 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 14722/35720 Training loss: 0.8872 0.2196 sec/batch\n",
      "Epoch 9/20  Iteration 14723/35720 Training loss: 0.8873 0.2185 sec/batch\n",
      "Epoch 9/20  Iteration 14724/35720 Training loss: 0.8871 0.2157 sec/batch\n",
      "Epoch 9/20  Iteration 14725/35720 Training loss: 0.8873 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 14726/35720 Training loss: 0.8874 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 14727/35720 Training loss: 0.8875 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 14728/35720 Training loss: 0.8875 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 14729/35720 Training loss: 0.8876 0.2128 sec/batch\n",
      "Epoch 9/20  Iteration 14730/35720 Training loss: 0.8878 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 14731/35720 Training loss: 0.8878 0.2059 sec/batch\n",
      "Epoch 9/20  Iteration 14732/35720 Training loss: 0.8878 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 14733/35720 Training loss: 0.8877 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 14734/35720 Training loss: 0.8879 0.2156 sec/batch\n",
      "Epoch 9/20  Iteration 14735/35720 Training loss: 0.8882 0.2240 sec/batch\n",
      "Epoch 9/20  Iteration 14736/35720 Training loss: 0.8884 0.2219 sec/batch\n",
      "Epoch 9/20  Iteration 14737/35720 Training loss: 0.8885 0.2214 sec/batch\n",
      "Epoch 9/20  Iteration 14738/35720 Training loss: 0.8885 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 14739/35720 Training loss: 0.8883 0.2350 sec/batch\n",
      "Epoch 9/20  Iteration 14740/35720 Training loss: 0.8882 0.2098 sec/batch\n",
      "Epoch 9/20  Iteration 14741/35720 Training loss: 0.8882 0.2193 sec/batch\n",
      "Epoch 9/20  Iteration 14742/35720 Training loss: 0.8883 0.2061 sec/batch\n",
      "Epoch 9/20  Iteration 14743/35720 Training loss: 0.8885 0.2121 sec/batch\n",
      "Epoch 9/20  Iteration 14744/35720 Training loss: 0.8886 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 14745/35720 Training loss: 0.8888 0.2209 sec/batch\n",
      "Epoch 9/20  Iteration 14746/35720 Training loss: 0.8890 0.2274 sec/batch\n",
      "Epoch 9/20  Iteration 14747/35720 Training loss: 0.8889 0.2060 sec/batch\n",
      "Epoch 9/20  Iteration 14748/35720 Training loss: 0.8888 0.2189 sec/batch\n",
      "Epoch 9/20  Iteration 14749/35720 Training loss: 0.8888 0.2322 sec/batch\n",
      "Epoch 9/20  Iteration 14750/35720 Training loss: 0.8889 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 14751/35720 Training loss: 0.8890 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 14752/35720 Training loss: 0.8889 0.2289 sec/batch\n",
      "Epoch 9/20  Iteration 14753/35720 Training loss: 0.8889 0.2269 sec/batch\n",
      "Epoch 9/20  Iteration 14754/35720 Training loss: 0.8889 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14755/35720 Training loss: 0.8889 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 14756/35720 Training loss: 0.8888 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 14757/35720 Training loss: 0.8887 0.2193 sec/batch\n",
      "Epoch 9/20  Iteration 14758/35720 Training loss: 0.8886 0.2241 sec/batch\n",
      "Epoch 9/20  Iteration 14759/35720 Training loss: 0.8885 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 14760/35720 Training loss: 0.8884 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 14761/35720 Training loss: 0.8884 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 14762/35720 Training loss: 0.8882 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 14763/35720 Training loss: 0.8883 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 14764/35720 Training loss: 0.8882 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 14765/35720 Training loss: 0.8883 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 14766/35720 Training loss: 0.8882 0.2182 sec/batch\n",
      "Epoch 9/20  Iteration 14767/35720 Training loss: 0.8883 0.2222 sec/batch\n",
      "Epoch 9/20  Iteration 14768/35720 Training loss: 0.8882 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 14769/35720 Training loss: 0.8880 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 14770/35720 Training loss: 0.8879 0.2176 sec/batch\n",
      "Epoch 9/20  Iteration 14771/35720 Training loss: 0.8880 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 14772/35720 Training loss: 0.8879 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 14773/35720 Training loss: 0.8880 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 14774/35720 Training loss: 0.8878 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 14775/35720 Training loss: 0.8878 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 14776/35720 Training loss: 0.8877 0.2070 sec/batch\n",
      "Epoch 9/20  Iteration 14777/35720 Training loss: 0.8877 0.2138 sec/batch\n",
      "Epoch 9/20  Iteration 14778/35720 Training loss: 0.8876 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 14779/35720 Training loss: 0.8875 0.2203 sec/batch\n",
      "Epoch 9/20  Iteration 14780/35720 Training loss: 0.8876 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 14781/35720 Training loss: 0.8875 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14782/35720 Training loss: 0.8875 0.2134 sec/batch\n",
      "Epoch 9/20  Iteration 14783/35720 Training loss: 0.8874 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 14784/35720 Training loss: 0.8873 0.2332 sec/batch\n",
      "Epoch 9/20  Iteration 14785/35720 Training loss: 0.8873 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 14786/35720 Training loss: 0.8873 0.2053 sec/batch\n",
      "Epoch 9/20  Iteration 14787/35720 Training loss: 0.8873 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 14788/35720 Training loss: 0.8873 0.2225 sec/batch\n",
      "Epoch 9/20  Iteration 14789/35720 Training loss: 0.8872 0.2208 sec/batch\n",
      "Epoch 9/20  Iteration 14790/35720 Training loss: 0.8871 0.2214 sec/batch\n",
      "Epoch 9/20  Iteration 14791/35720 Training loss: 0.8869 0.2254 sec/batch\n",
      "Epoch 9/20  Iteration 14792/35720 Training loss: 0.8868 0.2132 sec/batch\n",
      "Epoch 9/20  Iteration 14793/35720 Training loss: 0.8869 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14794/35720 Training loss: 0.8869 0.2258 sec/batch\n",
      "Epoch 9/20  Iteration 14795/35720 Training loss: 0.8868 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 14796/35720 Training loss: 0.8868 0.2241 sec/batch\n",
      "Epoch 9/20  Iteration 14797/35720 Training loss: 0.8869 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 14798/35720 Training loss: 0.8868 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 14799/35720 Training loss: 0.8866 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 14800/35720 Training loss: 0.8866 0.2163 sec/batch\n",
      "Validation loss: 1.36239 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 14801/35720 Training loss: 0.8872 0.2086 sec/batch\n",
      "Epoch 9/20  Iteration 14802/35720 Training loss: 0.8872 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 14803/35720 Training loss: 0.8873 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 14804/35720 Training loss: 0.8873 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14805/35720 Training loss: 0.8873 0.2110 sec/batch\n",
      "Epoch 9/20  Iteration 14806/35720 Training loss: 0.8873 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 14807/35720 Training loss: 0.8872 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 14808/35720 Training loss: 0.8873 0.2143 sec/batch\n",
      "Epoch 9/20  Iteration 14809/35720 Training loss: 0.8871 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 14810/35720 Training loss: 0.8871 0.2184 sec/batch\n",
      "Epoch 9/20  Iteration 14811/35720 Training loss: 0.8870 0.2197 sec/batch\n",
      "Epoch 9/20  Iteration 14812/35720 Training loss: 0.8869 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 14813/35720 Training loss: 0.8869 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 14814/35720 Training loss: 0.8869 0.2113 sec/batch\n",
      "Epoch 9/20  Iteration 14815/35720 Training loss: 0.8869 0.2151 sec/batch\n",
      "Epoch 9/20  Iteration 14816/35720 Training loss: 0.8868 0.2179 sec/batch\n",
      "Epoch 9/20  Iteration 14817/35720 Training loss: 0.8868 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 14818/35720 Training loss: 0.8867 0.2138 sec/batch\n",
      "Epoch 9/20  Iteration 14819/35720 Training loss: 0.8867 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 14820/35720 Training loss: 0.8866 0.2174 sec/batch\n",
      "Epoch 9/20  Iteration 14821/35720 Training loss: 0.8866 0.2076 sec/batch\n",
      "Epoch 9/20  Iteration 14822/35720 Training loss: 0.8867 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 14823/35720 Training loss: 0.8865 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 14824/35720 Training loss: 0.8865 0.2231 sec/batch\n",
      "Epoch 9/20  Iteration 14825/35720 Training loss: 0.8864 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 14826/35720 Training loss: 0.8863 0.2243 sec/batch\n",
      "Epoch 9/20  Iteration 14827/35720 Training loss: 0.8863 0.2214 sec/batch\n",
      "Epoch 9/20  Iteration 14828/35720 Training loss: 0.8862 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 14829/35720 Training loss: 0.8862 0.2213 sec/batch\n",
      "Epoch 9/20  Iteration 14830/35720 Training loss: 0.8861 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 14831/35720 Training loss: 0.8860 0.2246 sec/batch\n",
      "Epoch 9/20  Iteration 14832/35720 Training loss: 0.8859 0.2124 sec/batch\n",
      "Epoch 9/20  Iteration 14833/35720 Training loss: 0.8860 0.2302 sec/batch\n",
      "Epoch 9/20  Iteration 14834/35720 Training loss: 0.8861 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 14835/35720 Training loss: 0.8860 0.2067 sec/batch\n",
      "Epoch 9/20  Iteration 14836/35720 Training loss: 0.8861 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 14837/35720 Training loss: 0.8860 0.2095 sec/batch\n",
      "Epoch 9/20  Iteration 14838/35720 Training loss: 0.8861 0.2281 sec/batch\n",
      "Epoch 9/20  Iteration 14839/35720 Training loss: 0.8861 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 14840/35720 Training loss: 0.8860 0.2218 sec/batch\n",
      "Epoch 9/20  Iteration 14841/35720 Training loss: 0.8860 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 14842/35720 Training loss: 0.8859 0.2066 sec/batch\n",
      "Epoch 9/20  Iteration 14843/35720 Training loss: 0.8859 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14844/35720 Training loss: 0.8859 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 14845/35720 Training loss: 0.8859 0.2122 sec/batch\n",
      "Epoch 9/20  Iteration 14846/35720 Training loss: 0.8861 0.2213 sec/batch\n",
      "Epoch 9/20  Iteration 14847/35720 Training loss: 0.8860 0.2201 sec/batch\n",
      "Epoch 9/20  Iteration 14848/35720 Training loss: 0.8860 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14849/35720 Training loss: 0.8860 0.2184 sec/batch\n",
      "Epoch 9/20  Iteration 14850/35720 Training loss: 0.8859 0.2221 sec/batch\n",
      "Epoch 9/20  Iteration 14851/35720 Training loss: 0.8859 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 14852/35720 Training loss: 0.8859 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 14853/35720 Training loss: 0.8857 0.2187 sec/batch\n",
      "Epoch 9/20  Iteration 14854/35720 Training loss: 0.8856 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 14855/35720 Training loss: 0.8856 0.2271 sec/batch\n",
      "Epoch 9/20  Iteration 14856/35720 Training loss: 0.8855 0.2187 sec/batch\n",
      "Epoch 9/20  Iteration 14857/35720 Training loss: 0.8854 0.2089 sec/batch\n",
      "Epoch 9/20  Iteration 14858/35720 Training loss: 0.8854 0.2198 sec/batch\n",
      "Epoch 9/20  Iteration 14859/35720 Training loss: 0.8854 0.2189 sec/batch\n",
      "Epoch 9/20  Iteration 14860/35720 Training loss: 0.8854 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 14861/35720 Training loss: 0.8855 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 14862/35720 Training loss: 0.8856 0.2208 sec/batch\n",
      "Epoch 9/20  Iteration 14863/35720 Training loss: 0.8856 0.2157 sec/batch\n",
      "Epoch 9/20  Iteration 14864/35720 Training loss: 0.8856 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 14865/35720 Training loss: 0.8856 0.2112 sec/batch\n",
      "Epoch 9/20  Iteration 14866/35720 Training loss: 0.8856 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 14867/35720 Training loss: 0.8856 0.2176 sec/batch\n",
      "Epoch 9/20  Iteration 14868/35720 Training loss: 0.8855 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 14869/35720 Training loss: 0.8856 0.2129 sec/batch\n",
      "Epoch 9/20  Iteration 14870/35720 Training loss: 0.8856 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 14871/35720 Training loss: 0.8856 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 14872/35720 Training loss: 0.8855 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 14873/35720 Training loss: 0.8854 0.2333 sec/batch\n",
      "Epoch 9/20  Iteration 14874/35720 Training loss: 0.8852 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 14875/35720 Training loss: 0.8852 0.2160 sec/batch\n",
      "Epoch 9/20  Iteration 14876/35720 Training loss: 0.8851 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 14877/35720 Training loss: 0.8849 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 14878/35720 Training loss: 0.8849 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 14879/35720 Training loss: 0.8849 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 14880/35720 Training loss: 0.8849 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 14881/35720 Training loss: 0.8848 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 14882/35720 Training loss: 0.8849 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 14883/35720 Training loss: 0.8848 0.2199 sec/batch\n",
      "Epoch 9/20  Iteration 14884/35720 Training loss: 0.8847 0.2193 sec/batch\n",
      "Epoch 9/20  Iteration 14885/35720 Training loss: 0.8846 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 14886/35720 Training loss: 0.8848 0.2059 sec/batch\n",
      "Epoch 9/20  Iteration 14887/35720 Training loss: 0.8846 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 14888/35720 Training loss: 0.8844 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 14889/35720 Training loss: 0.8844 0.2229 sec/batch\n",
      "Epoch 9/20  Iteration 14890/35720 Training loss: 0.8842 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 14891/35720 Training loss: 0.8842 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 14892/35720 Training loss: 0.8842 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 14893/35720 Training loss: 0.8842 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 14894/35720 Training loss: 0.8842 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 14895/35720 Training loss: 0.8842 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 14896/35720 Training loss: 0.8841 0.2206 sec/batch\n",
      "Epoch 9/20  Iteration 14897/35720 Training loss: 0.8843 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 14898/35720 Training loss: 0.8841 0.2281 sec/batch\n",
      "Epoch 9/20  Iteration 14899/35720 Training loss: 0.8841 0.2187 sec/batch\n",
      "Epoch 9/20  Iteration 14900/35720 Training loss: 0.8840 0.2200 sec/batch\n",
      "Epoch 9/20  Iteration 14901/35720 Training loss: 0.8840 0.2211 sec/batch\n",
      "Epoch 9/20  Iteration 14902/35720 Training loss: 0.8839 0.2205 sec/batch\n",
      "Epoch 9/20  Iteration 14903/35720 Training loss: 0.8840 0.2187 sec/batch\n",
      "Epoch 9/20  Iteration 14904/35720 Training loss: 0.8839 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14905/35720 Training loss: 0.8839 0.2185 sec/batch\n",
      "Epoch 9/20  Iteration 14906/35720 Training loss: 0.8838 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 14907/35720 Training loss: 0.8837 0.2236 sec/batch\n",
      "Epoch 9/20  Iteration 14908/35720 Training loss: 0.8837 0.2271 sec/batch\n",
      "Epoch 9/20  Iteration 14909/35720 Training loss: 0.8836 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 14910/35720 Training loss: 0.8837 0.2157 sec/batch\n",
      "Epoch 9/20  Iteration 14911/35720 Training loss: 0.8836 0.2096 sec/batch\n",
      "Epoch 9/20  Iteration 14912/35720 Training loss: 0.8835 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 14913/35720 Training loss: 0.8834 0.2201 sec/batch\n",
      "Epoch 9/20  Iteration 14914/35720 Training loss: 0.8834 0.2138 sec/batch\n",
      "Epoch 9/20  Iteration 14915/35720 Training loss: 0.8834 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 14916/35720 Training loss: 0.8833 0.2215 sec/batch\n",
      "Epoch 9/20  Iteration 14917/35720 Training loss: 0.8833 0.2251 sec/batch\n",
      "Epoch 9/20  Iteration 14918/35720 Training loss: 0.8832 0.2199 sec/batch\n",
      "Epoch 9/20  Iteration 14919/35720 Training loss: 0.8833 0.2059 sec/batch\n",
      "Epoch 9/20  Iteration 14920/35720 Training loss: 0.8832 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 14921/35720 Training loss: 0.8832 0.2222 sec/batch\n",
      "Epoch 9/20  Iteration 14922/35720 Training loss: 0.8832 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 14923/35720 Training loss: 0.8831 0.2123 sec/batch\n",
      "Epoch 9/20  Iteration 14924/35720 Training loss: 0.8832 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 14925/35720 Training loss: 0.8831 0.2101 sec/batch\n",
      "Epoch 9/20  Iteration 14926/35720 Training loss: 0.8830 0.2251 sec/batch\n",
      "Epoch 9/20  Iteration 14927/35720 Training loss: 0.8830 0.2150 sec/batch\n",
      "Epoch 9/20  Iteration 14928/35720 Training loss: 0.8831 0.2086 sec/batch\n",
      "Epoch 9/20  Iteration 14929/35720 Training loss: 0.8832 0.2200 sec/batch\n",
      "Epoch 9/20  Iteration 14930/35720 Training loss: 0.8832 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 14931/35720 Training loss: 0.8832 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 14932/35720 Training loss: 0.8831 0.2208 sec/batch\n",
      "Epoch 9/20  Iteration 14933/35720 Training loss: 0.8831 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 14934/35720 Training loss: 0.8831 0.2239 sec/batch\n",
      "Epoch 9/20  Iteration 14935/35720 Training loss: 0.8831 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 14936/35720 Training loss: 0.8831 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 14937/35720 Training loss: 0.8831 0.2210 sec/batch\n",
      "Epoch 9/20  Iteration 14938/35720 Training loss: 0.8830 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 14939/35720 Training loss: 0.8829 0.2184 sec/batch\n",
      "Epoch 9/20  Iteration 14940/35720 Training loss: 0.8829 0.2512 sec/batch\n",
      "Epoch 9/20  Iteration 14941/35720 Training loss: 0.8829 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 14942/35720 Training loss: 0.8830 0.2053 sec/batch\n",
      "Epoch 9/20  Iteration 14943/35720 Training loss: 0.8830 0.2176 sec/batch\n",
      "Epoch 9/20  Iteration 14944/35720 Training loss: 0.8830 0.2275 sec/batch\n",
      "Epoch 9/20  Iteration 14945/35720 Training loss: 0.8831 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 14946/35720 Training loss: 0.8832 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 14947/35720 Training loss: 0.8832 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 14948/35720 Training loss: 0.8832 0.2096 sec/batch\n",
      "Epoch 9/20  Iteration 14949/35720 Training loss: 0.8832 0.2149 sec/batch\n",
      "Epoch 9/20  Iteration 14950/35720 Training loss: 0.8833 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 14951/35720 Training loss: 0.8833 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 14952/35720 Training loss: 0.8833 0.2161 sec/batch\n",
      "Epoch 9/20  Iteration 14953/35720 Training loss: 0.8833 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 14954/35720 Training loss: 0.8834 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 14955/35720 Training loss: 0.8835 0.2162 sec/batch\n",
      "Epoch 9/20  Iteration 14956/35720 Training loss: 0.8834 0.2201 sec/batch\n",
      "Epoch 9/20  Iteration 14957/35720 Training loss: 0.8833 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 14958/35720 Training loss: 0.8833 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 14959/35720 Training loss: 0.8831 0.2055 sec/batch\n",
      "Epoch 9/20  Iteration 14960/35720 Training loss: 0.8831 0.2216 sec/batch\n",
      "Epoch 9/20  Iteration 14961/35720 Training loss: 0.8832 0.2245 sec/batch\n",
      "Epoch 9/20  Iteration 14962/35720 Training loss: 0.8831 0.2282 sec/batch\n",
      "Epoch 9/20  Iteration 14963/35720 Training loss: 0.8831 0.2095 sec/batch\n",
      "Epoch 9/20  Iteration 14964/35720 Training loss: 0.8830 0.2072 sec/batch\n",
      "Epoch 9/20  Iteration 14965/35720 Training loss: 0.8829 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 14966/35720 Training loss: 0.8829 0.2230 sec/batch\n",
      "Epoch 9/20  Iteration 14967/35720 Training loss: 0.8830 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 14968/35720 Training loss: 0.8829 0.2204 sec/batch\n",
      "Epoch 9/20  Iteration 14969/35720 Training loss: 0.8829 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 14970/35720 Training loss: 0.8829 0.2055 sec/batch\n",
      "Epoch 9/20  Iteration 14971/35720 Training loss: 0.8828 0.2204 sec/batch\n",
      "Epoch 9/20  Iteration 14972/35720 Training loss: 0.8828 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 14973/35720 Training loss: 0.8828 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 14974/35720 Training loss: 0.8828 0.2162 sec/batch\n",
      "Epoch 9/20  Iteration 14975/35720 Training loss: 0.8828 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 14976/35720 Training loss: 0.8828 0.2110 sec/batch\n",
      "Epoch 9/20  Iteration 14977/35720 Training loss: 0.8827 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 14978/35720 Training loss: 0.8827 0.2267 sec/batch\n",
      "Epoch 9/20  Iteration 14979/35720 Training loss: 0.8827 0.2209 sec/batch\n",
      "Epoch 9/20  Iteration 14980/35720 Training loss: 0.8828 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 14981/35720 Training loss: 0.8830 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 14982/35720 Training loss: 0.8829 0.2253 sec/batch\n",
      "Epoch 9/20  Iteration 14983/35720 Training loss: 0.8830 0.2380 sec/batch\n",
      "Epoch 9/20  Iteration 14984/35720 Training loss: 0.8830 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 14985/35720 Training loss: 0.8830 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 14986/35720 Training loss: 0.8830 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 14987/35720 Training loss: 0.8830 0.2201 sec/batch\n",
      "Epoch 9/20  Iteration 14988/35720 Training loss: 0.8830 0.2214 sec/batch\n",
      "Epoch 9/20  Iteration 14989/35720 Training loss: 0.8829 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 14990/35720 Training loss: 0.8830 0.2343 sec/batch\n",
      "Epoch 9/20  Iteration 14991/35720 Training loss: 0.8829 0.2060 sec/batch\n",
      "Epoch 9/20  Iteration 14992/35720 Training loss: 0.8829 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 14993/35720 Training loss: 0.8829 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 14994/35720 Training loss: 0.8830 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 14995/35720 Training loss: 0.8831 0.2072 sec/batch\n",
      "Epoch 9/20  Iteration 14996/35720 Training loss: 0.8831 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 14997/35720 Training loss: 0.8832 0.2157 sec/batch\n",
      "Epoch 9/20  Iteration 14998/35720 Training loss: 0.8832 0.2095 sec/batch\n",
      "Epoch 9/20  Iteration 14999/35720 Training loss: 0.8833 0.2183 sec/batch\n",
      "Epoch 9/20  Iteration 15000/35720 Training loss: 0.8833 0.2094 sec/batch\n",
      "Validation loss: 1.36046 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 15001/35720 Training loss: 0.8836 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15002/35720 Training loss: 0.8836 0.2068 sec/batch\n",
      "Epoch 9/20  Iteration 15003/35720 Training loss: 0.8837 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 15004/35720 Training loss: 0.8837 0.2204 sec/batch\n",
      "Epoch 9/20  Iteration 15005/35720 Training loss: 0.8838 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 15006/35720 Training loss: 0.8839 0.2076 sec/batch\n",
      "Epoch 9/20  Iteration 15007/35720 Training loss: 0.8838 0.2123 sec/batch\n",
      "Epoch 9/20  Iteration 15008/35720 Training loss: 0.8838 0.2138 sec/batch\n",
      "Epoch 9/20  Iteration 15009/35720 Training loss: 0.8839 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 15010/35720 Training loss: 0.8839 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 15011/35720 Training loss: 0.8840 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 15012/35720 Training loss: 0.8839 0.2211 sec/batch\n",
      "Epoch 9/20  Iteration 15013/35720 Training loss: 0.8839 0.2350 sec/batch\n",
      "Epoch 9/20  Iteration 15014/35720 Training loss: 0.8839 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 15015/35720 Training loss: 0.8841 0.2223 sec/batch\n",
      "Epoch 9/20  Iteration 15016/35720 Training loss: 0.8841 0.2101 sec/batch\n",
      "Epoch 9/20  Iteration 15017/35720 Training loss: 0.8841 0.2156 sec/batch\n",
      "Epoch 9/20  Iteration 15018/35720 Training loss: 0.8841 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 15019/35720 Training loss: 0.8841 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 15020/35720 Training loss: 0.8840 0.2150 sec/batch\n",
      "Epoch 9/20  Iteration 15021/35720 Training loss: 0.8840 0.2241 sec/batch\n",
      "Epoch 9/20  Iteration 15022/35720 Training loss: 0.8840 0.2245 sec/batch\n",
      "Epoch 9/20  Iteration 15023/35720 Training loss: 0.8839 0.2196 sec/batch\n",
      "Epoch 9/20  Iteration 15024/35720 Training loss: 0.8840 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15025/35720 Training loss: 0.8840 0.2136 sec/batch\n",
      "Epoch 9/20  Iteration 15026/35720 Training loss: 0.8840 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 15027/35720 Training loss: 0.8840 0.2187 sec/batch\n",
      "Epoch 9/20  Iteration 15028/35720 Training loss: 0.8841 0.2258 sec/batch\n",
      "Epoch 9/20  Iteration 15029/35720 Training loss: 0.8840 0.2177 sec/batch\n",
      "Epoch 9/20  Iteration 15030/35720 Training loss: 0.8841 0.2193 sec/batch\n",
      "Epoch 9/20  Iteration 15031/35720 Training loss: 0.8840 0.2086 sec/batch\n",
      "Epoch 9/20  Iteration 15032/35720 Training loss: 0.8840 0.2337 sec/batch\n",
      "Epoch 9/20  Iteration 15033/35720 Training loss: 0.8840 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 15034/35720 Training loss: 0.8841 0.2177 sec/batch\n",
      "Epoch 9/20  Iteration 15035/35720 Training loss: 0.8840 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15036/35720 Training loss: 0.8840 0.2055 sec/batch\n",
      "Epoch 9/20  Iteration 15037/35720 Training loss: 0.8839 0.2083 sec/batch\n",
      "Epoch 9/20  Iteration 15038/35720 Training loss: 0.8839 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 15039/35720 Training loss: 0.8839 0.2136 sec/batch\n",
      "Epoch 9/20  Iteration 15040/35720 Training loss: 0.8840 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 15041/35720 Training loss: 0.8839 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 15042/35720 Training loss: 0.8839 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15043/35720 Training loss: 0.8839 0.2242 sec/batch\n",
      "Epoch 9/20  Iteration 15044/35720 Training loss: 0.8838 0.2142 sec/batch\n",
      "Epoch 9/20  Iteration 15045/35720 Training loss: 0.8838 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 15046/35720 Training loss: 0.8839 0.2129 sec/batch\n",
      "Epoch 9/20  Iteration 15047/35720 Training loss: 0.8839 0.2162 sec/batch\n",
      "Epoch 9/20  Iteration 15048/35720 Training loss: 0.8839 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15049/35720 Training loss: 0.8840 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 15050/35720 Training loss: 0.8840 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15051/35720 Training loss: 0.8840 0.2161 sec/batch\n",
      "Epoch 9/20  Iteration 15052/35720 Training loss: 0.8839 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 15053/35720 Training loss: 0.8839 0.2277 sec/batch\n",
      "Epoch 9/20  Iteration 15054/35720 Training loss: 0.8840 0.2206 sec/batch\n",
      "Epoch 9/20  Iteration 15055/35720 Training loss: 0.8840 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15056/35720 Training loss: 0.8840 0.2320 sec/batch\n",
      "Epoch 9/20  Iteration 15057/35720 Training loss: 0.8840 0.2203 sec/batch\n",
      "Epoch 9/20  Iteration 15058/35720 Training loss: 0.8842 0.2132 sec/batch\n",
      "Epoch 9/20  Iteration 15059/35720 Training loss: 0.8842 0.2151 sec/batch\n",
      "Epoch 9/20  Iteration 15060/35720 Training loss: 0.8843 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 15061/35720 Training loss: 0.8843 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 15062/35720 Training loss: 0.8842 0.2267 sec/batch\n",
      "Epoch 9/20  Iteration 15063/35720 Training loss: 0.8841 0.2055 sec/batch\n",
      "Epoch 9/20  Iteration 15064/35720 Training loss: 0.8841 0.2123 sec/batch\n",
      "Epoch 9/20  Iteration 15065/35720 Training loss: 0.8841 0.2240 sec/batch\n",
      "Epoch 9/20  Iteration 15066/35720 Training loss: 0.8841 0.2293 sec/batch\n",
      "Epoch 9/20  Iteration 15067/35720 Training loss: 0.8841 0.2275 sec/batch\n",
      "Epoch 9/20  Iteration 15068/35720 Training loss: 0.8841 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15069/35720 Training loss: 0.8841 0.2176 sec/batch\n",
      "Epoch 9/20  Iteration 15070/35720 Training loss: 0.8842 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 15071/35720 Training loss: 0.8842 0.2271 sec/batch\n",
      "Epoch 9/20  Iteration 15072/35720 Training loss: 0.8842 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 15073/35720 Training loss: 0.8842 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 15074/35720 Training loss: 0.8841 0.2130 sec/batch\n",
      "Epoch 9/20  Iteration 15075/35720 Training loss: 0.8842 0.2298 sec/batch\n",
      "Epoch 9/20  Iteration 15076/35720 Training loss: 0.8842 0.2271 sec/batch\n",
      "Epoch 9/20  Iteration 15077/35720 Training loss: 0.8842 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 15078/35720 Training loss: 0.8842 0.2347 sec/batch\n",
      "Epoch 9/20  Iteration 15079/35720 Training loss: 0.8843 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 15080/35720 Training loss: 0.8843 0.2057 sec/batch\n",
      "Epoch 9/20  Iteration 15081/35720 Training loss: 0.8843 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 15082/35720 Training loss: 0.8842 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 15083/35720 Training loss: 0.8843 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 15084/35720 Training loss: 0.8843 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 15085/35720 Training loss: 0.8842 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 15086/35720 Training loss: 0.8841 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15087/35720 Training loss: 0.8841 0.2151 sec/batch\n",
      "Epoch 9/20  Iteration 15088/35720 Training loss: 0.8841 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 15089/35720 Training loss: 0.8841 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 15090/35720 Training loss: 0.8842 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 15091/35720 Training loss: 0.8842 0.2068 sec/batch\n",
      "Epoch 9/20  Iteration 15092/35720 Training loss: 0.8843 0.2079 sec/batch\n",
      "Epoch 9/20  Iteration 15093/35720 Training loss: 0.8844 0.2157 sec/batch\n",
      "Epoch 9/20  Iteration 15094/35720 Training loss: 0.8844 0.2364 sec/batch\n",
      "Epoch 9/20  Iteration 15095/35720 Training loss: 0.8844 0.2233 sec/batch\n",
      "Epoch 9/20  Iteration 15096/35720 Training loss: 0.8844 0.2183 sec/batch\n",
      "Epoch 9/20  Iteration 15097/35720 Training loss: 0.8845 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 15098/35720 Training loss: 0.8845 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 15099/35720 Training loss: 0.8846 0.2256 sec/batch\n",
      "Epoch 9/20  Iteration 15100/35720 Training loss: 0.8846 0.2282 sec/batch\n",
      "Epoch 9/20  Iteration 15101/35720 Training loss: 0.8846 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 15102/35720 Training loss: 0.8847 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 15103/35720 Training loss: 0.8847 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 15104/35720 Training loss: 0.8847 0.2283 sec/batch\n",
      "Epoch 9/20  Iteration 15105/35720 Training loss: 0.8847 0.2264 sec/batch\n",
      "Epoch 9/20  Iteration 15106/35720 Training loss: 0.8847 0.2211 sec/batch\n",
      "Epoch 9/20  Iteration 15107/35720 Training loss: 0.8847 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15108/35720 Training loss: 0.8847 0.2290 sec/batch\n",
      "Epoch 9/20  Iteration 15109/35720 Training loss: 0.8847 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 15110/35720 Training loss: 0.8846 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 15111/35720 Training loss: 0.8845 0.2123 sec/batch\n",
      "Epoch 9/20  Iteration 15112/35720 Training loss: 0.8845 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 15113/35720 Training loss: 0.8843 0.2079 sec/batch\n",
      "Epoch 9/20  Iteration 15114/35720 Training loss: 0.8843 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15115/35720 Training loss: 0.8843 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 15116/35720 Training loss: 0.8842 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 15117/35720 Training loss: 0.8842 0.2278 sec/batch\n",
      "Epoch 9/20  Iteration 15118/35720 Training loss: 0.8841 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 15119/35720 Training loss: 0.8840 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 15120/35720 Training loss: 0.8841 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15121/35720 Training loss: 0.8842 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 15122/35720 Training loss: 0.8842 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 15123/35720 Training loss: 0.8842 0.2201 sec/batch\n",
      "Epoch 9/20  Iteration 15124/35720 Training loss: 0.8841 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15125/35720 Training loss: 0.8841 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 15126/35720 Training loss: 0.8841 0.2280 sec/batch\n",
      "Epoch 9/20  Iteration 15127/35720 Training loss: 0.8841 0.2357 sec/batch\n",
      "Epoch 9/20  Iteration 15128/35720 Training loss: 0.8841 0.2239 sec/batch\n",
      "Epoch 9/20  Iteration 15129/35720 Training loss: 0.8842 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 15130/35720 Training loss: 0.8841 0.2289 sec/batch\n",
      "Epoch 9/20  Iteration 15131/35720 Training loss: 0.8841 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 15132/35720 Training loss: 0.8841 0.2216 sec/batch\n",
      "Epoch 9/20  Iteration 15133/35720 Training loss: 0.8841 0.2151 sec/batch\n",
      "Epoch 9/20  Iteration 15134/35720 Training loss: 0.8841 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15135/35720 Training loss: 0.8841 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 15136/35720 Training loss: 0.8841 0.2276 sec/batch\n",
      "Epoch 9/20  Iteration 15137/35720 Training loss: 0.8840 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 15138/35720 Training loss: 0.8841 0.2098 sec/batch\n",
      "Epoch 9/20  Iteration 15139/35720 Training loss: 0.8841 0.2240 sec/batch\n",
      "Epoch 9/20  Iteration 15140/35720 Training loss: 0.8840 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15141/35720 Training loss: 0.8840 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15142/35720 Training loss: 0.8840 0.2357 sec/batch\n",
      "Epoch 9/20  Iteration 15143/35720 Training loss: 0.8841 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15144/35720 Training loss: 0.8840 0.2238 sec/batch\n",
      "Epoch 9/20  Iteration 15145/35720 Training loss: 0.8839 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15146/35720 Training loss: 0.8840 0.2055 sec/batch\n",
      "Epoch 9/20  Iteration 15147/35720 Training loss: 0.8838 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 15148/35720 Training loss: 0.8838 0.2208 sec/batch\n",
      "Epoch 9/20  Iteration 15149/35720 Training loss: 0.8838 0.2150 sec/batch\n",
      "Epoch 9/20  Iteration 15150/35720 Training loss: 0.8838 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15151/35720 Training loss: 0.8838 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15152/35720 Training loss: 0.8837 0.2197 sec/batch\n",
      "Epoch 9/20  Iteration 15153/35720 Training loss: 0.8837 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15154/35720 Training loss: 0.8838 0.2366 sec/batch\n",
      "Epoch 9/20  Iteration 15155/35720 Training loss: 0.8838 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15156/35720 Training loss: 0.8838 0.2128 sec/batch\n",
      "Epoch 9/20  Iteration 15157/35720 Training loss: 0.8838 0.3181 sec/batch\n",
      "Epoch 9/20  Iteration 15158/35720 Training loss: 0.8838 0.2311 sec/batch\n",
      "Epoch 9/20  Iteration 15159/35720 Training loss: 0.8837 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 15160/35720 Training loss: 0.8837 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 15161/35720 Training loss: 0.8836 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15162/35720 Training loss: 0.8836 0.2070 sec/batch\n",
      "Epoch 9/20  Iteration 15163/35720 Training loss: 0.8836 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15164/35720 Training loss: 0.8835 0.2424 sec/batch\n",
      "Epoch 9/20  Iteration 15165/35720 Training loss: 0.8835 0.2124 sec/batch\n",
      "Epoch 9/20  Iteration 15166/35720 Training loss: 0.8835 0.2217 sec/batch\n",
      "Epoch 9/20  Iteration 15167/35720 Training loss: 0.8835 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15168/35720 Training loss: 0.8834 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 15169/35720 Training loss: 0.8834 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15170/35720 Training loss: 0.8834 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15171/35720 Training loss: 0.8834 0.2237 sec/batch\n",
      "Epoch 9/20  Iteration 15172/35720 Training loss: 0.8834 0.2293 sec/batch\n",
      "Epoch 9/20  Iteration 15173/35720 Training loss: 0.8834 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15174/35720 Training loss: 0.8833 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15175/35720 Training loss: 0.8833 0.2303 sec/batch\n",
      "Epoch 9/20  Iteration 15176/35720 Training loss: 0.8833 0.2375 sec/batch\n",
      "Epoch 9/20  Iteration 15177/35720 Training loss: 0.8833 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 15178/35720 Training loss: 0.8832 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15179/35720 Training loss: 0.8831 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15180/35720 Training loss: 0.8830 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 15181/35720 Training loss: 0.8830 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 15182/35720 Training loss: 0.8829 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 15183/35720 Training loss: 0.8829 0.2245 sec/batch\n",
      "Epoch 9/20  Iteration 15184/35720 Training loss: 0.8829 0.2086 sec/batch\n",
      "Epoch 9/20  Iteration 15185/35720 Training loss: 0.8828 0.2199 sec/batch\n",
      "Epoch 9/20  Iteration 15186/35720 Training loss: 0.8826 0.2177 sec/batch\n",
      "Epoch 9/20  Iteration 15187/35720 Training loss: 0.8826 0.2217 sec/batch\n",
      "Epoch 9/20  Iteration 15188/35720 Training loss: 0.8825 0.2292 sec/batch\n",
      "Epoch 9/20  Iteration 15189/35720 Training loss: 0.8825 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15190/35720 Training loss: 0.8824 0.2143 sec/batch\n",
      "Epoch 9/20  Iteration 15191/35720 Training loss: 0.8823 0.2124 sec/batch\n",
      "Epoch 9/20  Iteration 15192/35720 Training loss: 0.8823 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 15193/35720 Training loss: 0.8822 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 15194/35720 Training loss: 0.8822 0.2149 sec/batch\n",
      "Epoch 9/20  Iteration 15195/35720 Training loss: 0.8822 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 15196/35720 Training loss: 0.8822 0.2079 sec/batch\n",
      "Epoch 9/20  Iteration 15197/35720 Training loss: 0.8822 0.2287 sec/batch\n",
      "Epoch 9/20  Iteration 15198/35720 Training loss: 0.8822 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15199/35720 Training loss: 0.8822 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 15200/35720 Training loss: 0.8822 0.2125 sec/batch\n",
      "Validation loss: 1.38173 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 15201/35720 Training loss: 0.8825 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15202/35720 Training loss: 0.8825 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 15203/35720 Training loss: 0.8826 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 15204/35720 Training loss: 0.8827 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 15205/35720 Training loss: 0.8827 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 15206/35720 Training loss: 0.8826 0.2267 sec/batch\n",
      "Epoch 9/20  Iteration 15207/35720 Training loss: 0.8826 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 15208/35720 Training loss: 0.8826 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 15209/35720 Training loss: 0.8826 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15210/35720 Training loss: 0.8825 0.2122 sec/batch\n",
      "Epoch 9/20  Iteration 15211/35720 Training loss: 0.8825 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 15212/35720 Training loss: 0.8825 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 15213/35720 Training loss: 0.8825 0.2209 sec/batch\n",
      "Epoch 9/20  Iteration 15214/35720 Training loss: 0.8825 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15215/35720 Training loss: 0.8826 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 15216/35720 Training loss: 0.8825 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 15217/35720 Training loss: 0.8826 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 15218/35720 Training loss: 0.8826 0.2191 sec/batch\n",
      "Epoch 9/20  Iteration 15219/35720 Training loss: 0.8827 0.2123 sec/batch\n",
      "Epoch 9/20  Iteration 15220/35720 Training loss: 0.8827 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 15221/35720 Training loss: 0.8827 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 15222/35720 Training loss: 0.8827 0.2067 sec/batch\n",
      "Epoch 9/20  Iteration 15223/35720 Training loss: 0.8825 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15224/35720 Training loss: 0.8824 0.2189 sec/batch\n",
      "Epoch 9/20  Iteration 15225/35720 Training loss: 0.8823 0.2318 sec/batch\n",
      "Epoch 9/20  Iteration 15226/35720 Training loss: 0.8823 0.2193 sec/batch\n",
      "Epoch 9/20  Iteration 15227/35720 Training loss: 0.8822 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 15228/35720 Training loss: 0.8821 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 15229/35720 Training loss: 0.8821 0.2160 sec/batch\n",
      "Epoch 9/20  Iteration 15230/35720 Training loss: 0.8821 0.2281 sec/batch\n",
      "Epoch 9/20  Iteration 15231/35720 Training loss: 0.8820 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15232/35720 Training loss: 0.8821 0.2369 sec/batch\n",
      "Epoch 9/20  Iteration 15233/35720 Training loss: 0.8820 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15234/35720 Training loss: 0.8820 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 15235/35720 Training loss: 0.8820 0.2182 sec/batch\n",
      "Epoch 9/20  Iteration 15236/35720 Training loss: 0.8819 0.2242 sec/batch\n",
      "Epoch 9/20  Iteration 15237/35720 Training loss: 0.8820 0.2259 sec/batch\n",
      "Epoch 9/20  Iteration 15238/35720 Training loss: 0.8819 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 15239/35720 Training loss: 0.8818 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 15240/35720 Training loss: 0.8817 0.2257 sec/batch\n",
      "Epoch 9/20  Iteration 15241/35720 Training loss: 0.8817 0.2322 sec/batch\n",
      "Epoch 9/20  Iteration 15242/35720 Training loss: 0.8816 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 15243/35720 Training loss: 0.8816 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 15244/35720 Training loss: 0.8815 0.2279 sec/batch\n",
      "Epoch 9/20  Iteration 15245/35720 Training loss: 0.8815 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15246/35720 Training loss: 0.8814 0.2183 sec/batch\n",
      "Epoch 9/20  Iteration 15247/35720 Training loss: 0.8814 0.2121 sec/batch\n",
      "Epoch 9/20  Iteration 15248/35720 Training loss: 0.8813 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15249/35720 Training loss: 0.8813 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 15250/35720 Training loss: 0.8812 0.2066 sec/batch\n",
      "Epoch 9/20  Iteration 15251/35720 Training loss: 0.8812 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 15252/35720 Training loss: 0.8812 0.2262 sec/batch\n",
      "Epoch 9/20  Iteration 15253/35720 Training loss: 0.8811 0.2111 sec/batch\n",
      "Epoch 9/20  Iteration 15254/35720 Training loss: 0.8811 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 15255/35720 Training loss: 0.8811 0.2051 sec/batch\n",
      "Epoch 9/20  Iteration 15256/35720 Training loss: 0.8810 0.2072 sec/batch\n",
      "Epoch 9/20  Iteration 15257/35720 Training loss: 0.8812 0.2111 sec/batch\n",
      "Epoch 9/20  Iteration 15258/35720 Training loss: 0.8813 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15259/35720 Training loss: 0.8812 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 15260/35720 Training loss: 0.8812 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 15261/35720 Training loss: 0.8812 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15262/35720 Training loss: 0.8811 0.2074 sec/batch\n",
      "Epoch 9/20  Iteration 15263/35720 Training loss: 0.8810 0.2326 sec/batch\n",
      "Epoch 9/20  Iteration 15264/35720 Training loss: 0.8810 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 15265/35720 Training loss: 0.8811 0.2221 sec/batch\n",
      "Epoch 9/20  Iteration 15266/35720 Training loss: 0.8811 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 15267/35720 Training loss: 0.8810 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15268/35720 Training loss: 0.8810 0.2083 sec/batch\n",
      "Epoch 9/20  Iteration 15269/35720 Training loss: 0.8810 0.2284 sec/batch\n",
      "Epoch 9/20  Iteration 15270/35720 Training loss: 0.8809 0.2122 sec/batch\n",
      "Epoch 9/20  Iteration 15271/35720 Training loss: 0.8808 0.2240 sec/batch\n",
      "Epoch 9/20  Iteration 15272/35720 Training loss: 0.8808 0.2257 sec/batch\n",
      "Epoch 9/20  Iteration 15273/35720 Training loss: 0.8808 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 15274/35720 Training loss: 0.8808 0.2262 sec/batch\n",
      "Epoch 9/20  Iteration 15275/35720 Training loss: 0.8808 0.2171 sec/batch\n",
      "Epoch 9/20  Iteration 15276/35720 Training loss: 0.8807 0.2306 sec/batch\n",
      "Epoch 9/20  Iteration 15277/35720 Training loss: 0.8807 0.2100 sec/batch\n",
      "Epoch 9/20  Iteration 15278/35720 Training loss: 0.8807 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15279/35720 Training loss: 0.8806 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 15280/35720 Training loss: 0.8807 0.2139 sec/batch\n",
      "Epoch 9/20  Iteration 15281/35720 Training loss: 0.8807 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15282/35720 Training loss: 0.8806 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 15283/35720 Training loss: 0.8807 0.2068 sec/batch\n",
      "Epoch 9/20  Iteration 15284/35720 Training loss: 0.8807 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 15285/35720 Training loss: 0.8807 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15286/35720 Training loss: 0.8808 0.2211 sec/batch\n",
      "Epoch 9/20  Iteration 15287/35720 Training loss: 0.8808 0.2235 sec/batch\n",
      "Epoch 9/20  Iteration 15288/35720 Training loss: 0.8807 0.2180 sec/batch\n",
      "Epoch 9/20  Iteration 15289/35720 Training loss: 0.8806 0.2072 sec/batch\n",
      "Epoch 9/20  Iteration 15290/35720 Training loss: 0.8805 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 15291/35720 Training loss: 0.8804 0.2252 sec/batch\n",
      "Epoch 9/20  Iteration 15292/35720 Training loss: 0.8804 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 15293/35720 Training loss: 0.8803 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 15294/35720 Training loss: 0.8802 0.2235 sec/batch\n",
      "Epoch 9/20  Iteration 15295/35720 Training loss: 0.8802 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 15296/35720 Training loss: 0.8802 0.2142 sec/batch\n",
      "Epoch 9/20  Iteration 15297/35720 Training loss: 0.8801 0.2142 sec/batch\n",
      "Epoch 9/20  Iteration 15298/35720 Training loss: 0.8800 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 15299/35720 Training loss: 0.8800 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15300/35720 Training loss: 0.8800 0.2295 sec/batch\n",
      "Epoch 9/20  Iteration 15301/35720 Training loss: 0.8799 0.2095 sec/batch\n",
      "Epoch 9/20  Iteration 15302/35720 Training loss: 0.8799 0.2240 sec/batch\n",
      "Epoch 9/20  Iteration 15303/35720 Training loss: 0.8799 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15304/35720 Training loss: 0.8799 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 15305/35720 Training loss: 0.8798 0.2225 sec/batch\n",
      "Epoch 9/20  Iteration 15306/35720 Training loss: 0.8798 0.2052 sec/batch\n",
      "Epoch 9/20  Iteration 15307/35720 Training loss: 0.8797 0.2089 sec/batch\n",
      "Epoch 9/20  Iteration 15308/35720 Training loss: 0.8797 0.2209 sec/batch\n",
      "Epoch 9/20  Iteration 15309/35720 Training loss: 0.8797 0.2320 sec/batch\n",
      "Epoch 9/20  Iteration 15310/35720 Training loss: 0.8797 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 15311/35720 Training loss: 0.8798 0.2061 sec/batch\n",
      "Epoch 9/20  Iteration 15312/35720 Training loss: 0.8798 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15313/35720 Training loss: 0.8799 0.2198 sec/batch\n",
      "Epoch 9/20  Iteration 15314/35720 Training loss: 0.8798 0.2250 sec/batch\n",
      "Epoch 9/20  Iteration 15315/35720 Training loss: 0.8798 0.2374 sec/batch\n",
      "Epoch 9/20  Iteration 15316/35720 Training loss: 0.8798 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 15317/35720 Training loss: 0.8797 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 15318/35720 Training loss: 0.8797 0.2110 sec/batch\n",
      "Epoch 9/20  Iteration 15319/35720 Training loss: 0.8797 0.2208 sec/batch\n",
      "Epoch 9/20  Iteration 15320/35720 Training loss: 0.8797 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 15321/35720 Training loss: 0.8797 0.2210 sec/batch\n",
      "Epoch 9/20  Iteration 15322/35720 Training loss: 0.8797 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15323/35720 Training loss: 0.8797 0.2182 sec/batch\n",
      "Epoch 9/20  Iteration 15324/35720 Training loss: 0.8797 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 15325/35720 Training loss: 0.8798 0.2157 sec/batch\n",
      "Epoch 9/20  Iteration 15326/35720 Training loss: 0.8798 0.2193 sec/batch\n",
      "Epoch 9/20  Iteration 15327/35720 Training loss: 0.8797 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15328/35720 Training loss: 0.8798 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 15329/35720 Training loss: 0.8798 0.2189 sec/batch\n",
      "Epoch 9/20  Iteration 15330/35720 Training loss: 0.8798 0.2226 sec/batch\n",
      "Epoch 9/20  Iteration 15331/35720 Training loss: 0.8797 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15332/35720 Training loss: 0.8797 0.2292 sec/batch\n",
      "Epoch 9/20  Iteration 15333/35720 Training loss: 0.8797 0.2218 sec/batch\n",
      "Epoch 9/20  Iteration 15334/35720 Training loss: 0.8798 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15335/35720 Training loss: 0.8797 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15336/35720 Training loss: 0.8797 0.2307 sec/batch\n",
      "Epoch 9/20  Iteration 15337/35720 Training loss: 0.8797 0.2210 sec/batch\n",
      "Epoch 9/20  Iteration 15338/35720 Training loss: 0.8797 0.2301 sec/batch\n",
      "Epoch 9/20  Iteration 15339/35720 Training loss: 0.8797 0.2070 sec/batch\n",
      "Epoch 9/20  Iteration 15340/35720 Training loss: 0.8797 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15341/35720 Training loss: 0.8798 0.2300 sec/batch\n",
      "Epoch 9/20  Iteration 15342/35720 Training loss: 0.8798 0.2201 sec/batch\n",
      "Epoch 9/20  Iteration 15343/35720 Training loss: 0.8799 0.2284 sec/batch\n",
      "Epoch 9/20  Iteration 15344/35720 Training loss: 0.8799 0.2213 sec/batch\n",
      "Epoch 9/20  Iteration 15345/35720 Training loss: 0.8799 0.2080 sec/batch\n",
      "Epoch 9/20  Iteration 15346/35720 Training loss: 0.8799 0.2272 sec/batch\n",
      "Epoch 9/20  Iteration 15347/35720 Training loss: 0.8799 0.2131 sec/batch\n",
      "Epoch 9/20  Iteration 15348/35720 Training loss: 0.8798 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 15349/35720 Training loss: 0.8799 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15350/35720 Training loss: 0.8799 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 15351/35720 Training loss: 0.8800 0.2132 sec/batch\n",
      "Epoch 9/20  Iteration 15352/35720 Training loss: 0.8800 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 15353/35720 Training loss: 0.8800 0.2239 sec/batch\n",
      "Epoch 9/20  Iteration 15354/35720 Training loss: 0.8799 0.2303 sec/batch\n",
      "Epoch 9/20  Iteration 15355/35720 Training loss: 0.8799 0.2156 sec/batch\n",
      "Epoch 9/20  Iteration 15356/35720 Training loss: 0.8799 0.2074 sec/batch\n",
      "Epoch 9/20  Iteration 15357/35720 Training loss: 0.8799 0.2199 sec/batch\n",
      "Epoch 9/20  Iteration 15358/35720 Training loss: 0.8800 0.2289 sec/batch\n",
      "Epoch 9/20  Iteration 15359/35720 Training loss: 0.8800 0.2286 sec/batch\n",
      "Epoch 9/20  Iteration 15360/35720 Training loss: 0.8799 0.2140 sec/batch\n",
      "Epoch 9/20  Iteration 15361/35720 Training loss: 0.8799 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 15362/35720 Training loss: 0.8799 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 15363/35720 Training loss: 0.8799 0.2287 sec/batch\n",
      "Epoch 9/20  Iteration 15364/35720 Training loss: 0.8800 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15365/35720 Training loss: 0.8800 0.2138 sec/batch\n",
      "Epoch 9/20  Iteration 15366/35720 Training loss: 0.8799 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 15367/35720 Training loss: 0.8799 0.2227 sec/batch\n",
      "Epoch 9/20  Iteration 15368/35720 Training loss: 0.8799 0.2128 sec/batch\n",
      "Epoch 9/20  Iteration 15369/35720 Training loss: 0.8799 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 15370/35720 Training loss: 0.8799 0.2249 sec/batch\n",
      "Epoch 9/20  Iteration 15371/35720 Training loss: 0.8799 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15372/35720 Training loss: 0.8799 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15373/35720 Training loss: 0.8799 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 15374/35720 Training loss: 0.8799 0.2213 sec/batch\n",
      "Epoch 9/20  Iteration 15375/35720 Training loss: 0.8799 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15376/35720 Training loss: 0.8799 0.2233 sec/batch\n",
      "Epoch 9/20  Iteration 15377/35720 Training loss: 0.8799 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 15378/35720 Training loss: 0.8799 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 15379/35720 Training loss: 0.8799 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 15380/35720 Training loss: 0.8799 0.2223 sec/batch\n",
      "Epoch 9/20  Iteration 15381/35720 Training loss: 0.8799 0.2481 sec/batch\n",
      "Epoch 9/20  Iteration 15382/35720 Training loss: 0.8799 0.2138 sec/batch\n",
      "Epoch 9/20  Iteration 15383/35720 Training loss: 0.8799 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15384/35720 Training loss: 0.8799 0.2111 sec/batch\n",
      "Epoch 9/20  Iteration 15385/35720 Training loss: 0.8798 0.2185 sec/batch\n",
      "Epoch 9/20  Iteration 15386/35720 Training loss: 0.8798 0.2186 sec/batch\n",
      "Epoch 9/20  Iteration 15387/35720 Training loss: 0.8799 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 15388/35720 Training loss: 0.8799 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15389/35720 Training loss: 0.8799 0.2072 sec/batch\n",
      "Epoch 9/20  Iteration 15390/35720 Training loss: 0.8799 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 15391/35720 Training loss: 0.8800 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15392/35720 Training loss: 0.8800 0.2225 sec/batch\n",
      "Epoch 9/20  Iteration 15393/35720 Training loss: 0.8800 0.2196 sec/batch\n",
      "Epoch 9/20  Iteration 15394/35720 Training loss: 0.8799 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 15395/35720 Training loss: 0.8799 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15396/35720 Training loss: 0.8799 0.2282 sec/batch\n",
      "Epoch 9/20  Iteration 15397/35720 Training loss: 0.8799 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 15398/35720 Training loss: 0.8800 0.2171 sec/batch\n",
      "Epoch 9/20  Iteration 15399/35720 Training loss: 0.8799 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 15400/35720 Training loss: 0.8799 0.2172 sec/batch\n",
      "Validation loss: 1.39185 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 15401/35720 Training loss: 0.8800 0.2101 sec/batch\n",
      "Epoch 9/20  Iteration 15402/35720 Training loss: 0.8801 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15403/35720 Training loss: 0.8801 0.2294 sec/batch\n",
      "Epoch 9/20  Iteration 15404/35720 Training loss: 0.8801 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15405/35720 Training loss: 0.8801 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 15406/35720 Training loss: 0.8800 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15407/35720 Training loss: 0.8800 0.2146 sec/batch\n",
      "Epoch 9/20  Iteration 15408/35720 Training loss: 0.8800 0.2183 sec/batch\n",
      "Epoch 9/20  Iteration 15409/35720 Training loss: 0.8801 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15410/35720 Training loss: 0.8801 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15411/35720 Training loss: 0.8802 0.2307 sec/batch\n",
      "Epoch 9/20  Iteration 15412/35720 Training loss: 0.8802 0.2256 sec/batch\n",
      "Epoch 9/20  Iteration 15413/35720 Training loss: 0.8802 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15414/35720 Training loss: 0.8802 0.2387 sec/batch\n",
      "Epoch 9/20  Iteration 15415/35720 Training loss: 0.8802 0.2182 sec/batch\n",
      "Epoch 9/20  Iteration 15416/35720 Training loss: 0.8802 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 15417/35720 Training loss: 0.8802 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15418/35720 Training loss: 0.8802 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 15419/35720 Training loss: 0.8802 0.2198 sec/batch\n",
      "Epoch 9/20  Iteration 15420/35720 Training loss: 0.8801 0.2248 sec/batch\n",
      "Epoch 9/20  Iteration 15421/35720 Training loss: 0.8801 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 15422/35720 Training loss: 0.8802 0.2122 sec/batch\n",
      "Epoch 9/20  Iteration 15423/35720 Training loss: 0.8801 0.2457 sec/batch\n",
      "Epoch 9/20  Iteration 15424/35720 Training loss: 0.8801 0.2200 sec/batch\n",
      "Epoch 9/20  Iteration 15425/35720 Training loss: 0.8800 0.2161 sec/batch\n",
      "Epoch 9/20  Iteration 15426/35720 Training loss: 0.8799 0.2100 sec/batch\n",
      "Epoch 9/20  Iteration 15427/35720 Training loss: 0.8799 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15428/35720 Training loss: 0.8799 0.2327 sec/batch\n",
      "Epoch 9/20  Iteration 15429/35720 Training loss: 0.8799 0.2284 sec/batch\n",
      "Epoch 9/20  Iteration 15430/35720 Training loss: 0.8799 0.2279 sec/batch\n",
      "Epoch 9/20  Iteration 15431/35720 Training loss: 0.8799 0.2139 sec/batch\n",
      "Epoch 9/20  Iteration 15432/35720 Training loss: 0.8799 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15433/35720 Training loss: 0.8798 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 15434/35720 Training loss: 0.8797 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15435/35720 Training loss: 0.8797 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15436/35720 Training loss: 0.8797 0.2261 sec/batch\n",
      "Epoch 9/20  Iteration 15437/35720 Training loss: 0.8797 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15438/35720 Training loss: 0.8796 0.2066 sec/batch\n",
      "Epoch 9/20  Iteration 15439/35720 Training loss: 0.8797 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 15440/35720 Training loss: 0.8797 0.2219 sec/batch\n",
      "Epoch 9/20  Iteration 15441/35720 Training loss: 0.8797 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 15442/35720 Training loss: 0.8797 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15443/35720 Training loss: 0.8797 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 15444/35720 Training loss: 0.8796 0.2132 sec/batch\n",
      "Epoch 9/20  Iteration 15445/35720 Training loss: 0.8796 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 15446/35720 Training loss: 0.8796 0.2161 sec/batch\n",
      "Epoch 9/20  Iteration 15447/35720 Training loss: 0.8796 0.2212 sec/batch\n",
      "Epoch 9/20  Iteration 15448/35720 Training loss: 0.8796 0.2259 sec/batch\n",
      "Epoch 9/20  Iteration 15449/35720 Training loss: 0.8795 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15450/35720 Training loss: 0.8795 0.2144 sec/batch\n",
      "Epoch 9/20  Iteration 15451/35720 Training loss: 0.8796 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15452/35720 Training loss: 0.8797 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 15453/35720 Training loss: 0.8796 0.2209 sec/batch\n",
      "Epoch 9/20  Iteration 15454/35720 Training loss: 0.8796 0.2053 sec/batch\n",
      "Epoch 9/20  Iteration 15455/35720 Training loss: 0.8795 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 15456/35720 Training loss: 0.8796 0.2218 sec/batch\n",
      "Epoch 9/20  Iteration 15457/35720 Training loss: 0.8796 0.2237 sec/batch\n",
      "Epoch 9/20  Iteration 15458/35720 Training loss: 0.8796 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 15459/35720 Training loss: 0.8796 0.2156 sec/batch\n",
      "Epoch 9/20  Iteration 15460/35720 Training loss: 0.8797 0.2174 sec/batch\n",
      "Epoch 9/20  Iteration 15461/35720 Training loss: 0.8796 0.2140 sec/batch\n",
      "Epoch 9/20  Iteration 15462/35720 Training loss: 0.8796 0.2199 sec/batch\n",
      "Epoch 9/20  Iteration 15463/35720 Training loss: 0.8796 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15464/35720 Training loss: 0.8797 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 15465/35720 Training loss: 0.8797 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15466/35720 Training loss: 0.8797 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15467/35720 Training loss: 0.8797 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15468/35720 Training loss: 0.8796 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 15469/35720 Training loss: 0.8797 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 15470/35720 Training loss: 0.8797 0.2076 sec/batch\n",
      "Epoch 9/20  Iteration 15471/35720 Training loss: 0.8796 0.2214 sec/batch\n",
      "Epoch 9/20  Iteration 15472/35720 Training loss: 0.8795 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15473/35720 Training loss: 0.8795 0.2253 sec/batch\n",
      "Epoch 9/20  Iteration 15474/35720 Training loss: 0.8794 0.2340 sec/batch\n",
      "Epoch 9/20  Iteration 15475/35720 Training loss: 0.8794 0.2261 sec/batch\n",
      "Epoch 9/20  Iteration 15476/35720 Training loss: 0.8794 0.2098 sec/batch\n",
      "Epoch 9/20  Iteration 15477/35720 Training loss: 0.8794 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 15478/35720 Training loss: 0.8794 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 15479/35720 Training loss: 0.8794 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 15480/35720 Training loss: 0.8794 0.2160 sec/batch\n",
      "Epoch 9/20  Iteration 15481/35720 Training loss: 0.8794 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 15482/35720 Training loss: 0.8794 0.2187 sec/batch\n",
      "Epoch 9/20  Iteration 15483/35720 Training loss: 0.8794 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 15484/35720 Training loss: 0.8794 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 15485/35720 Training loss: 0.8794 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 15486/35720 Training loss: 0.8794 0.2223 sec/batch\n",
      "Epoch 9/20  Iteration 15487/35720 Training loss: 0.8794 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 15488/35720 Training loss: 0.8794 0.2070 sec/batch\n",
      "Epoch 9/20  Iteration 15489/35720 Training loss: 0.8794 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15490/35720 Training loss: 0.8793 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 15491/35720 Training loss: 0.8792 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 15492/35720 Training loss: 0.8792 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 15493/35720 Training loss: 0.8791 0.2130 sec/batch\n",
      "Epoch 9/20  Iteration 15494/35720 Training loss: 0.8791 0.2142 sec/batch\n",
      "Epoch 9/20  Iteration 15495/35720 Training loss: 0.8791 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 15496/35720 Training loss: 0.8791 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 15497/35720 Training loss: 0.8791 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15498/35720 Training loss: 0.8791 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 15499/35720 Training loss: 0.8790 0.2067 sec/batch\n",
      "Epoch 9/20  Iteration 15500/35720 Training loss: 0.8790 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15501/35720 Training loss: 0.8790 0.2236 sec/batch\n",
      "Epoch 9/20  Iteration 15502/35720 Training loss: 0.8790 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 15503/35720 Training loss: 0.8790 0.2160 sec/batch\n",
      "Epoch 9/20  Iteration 15504/35720 Training loss: 0.8790 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15505/35720 Training loss: 0.8790 0.2187 sec/batch\n",
      "Epoch 9/20  Iteration 15506/35720 Training loss: 0.8789 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15507/35720 Training loss: 0.8789 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 15508/35720 Training loss: 0.8788 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15509/35720 Training loss: 0.8789 0.2232 sec/batch\n",
      "Epoch 9/20  Iteration 15510/35720 Training loss: 0.8789 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 15511/35720 Training loss: 0.8789 0.2205 sec/batch\n",
      "Epoch 9/20  Iteration 15512/35720 Training loss: 0.8788 0.2203 sec/batch\n",
      "Epoch 9/20  Iteration 15513/35720 Training loss: 0.8788 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 15514/35720 Training loss: 0.8788 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 15515/35720 Training loss: 0.8788 0.2110 sec/batch\n",
      "Epoch 9/20  Iteration 15516/35720 Training loss: 0.8788 0.2184 sec/batch\n",
      "Epoch 9/20  Iteration 15517/35720 Training loss: 0.8789 0.2136 sec/batch\n",
      "Epoch 9/20  Iteration 15518/35720 Training loss: 0.8789 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 15519/35720 Training loss: 0.8789 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 15520/35720 Training loss: 0.8788 0.2131 sec/batch\n",
      "Epoch 9/20  Iteration 15521/35720 Training loss: 0.8788 0.2226 sec/batch\n",
      "Epoch 9/20  Iteration 15522/35720 Training loss: 0.8788 0.2131 sec/batch\n",
      "Epoch 9/20  Iteration 15523/35720 Training loss: 0.8788 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 15524/35720 Training loss: 0.8788 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 15525/35720 Training loss: 0.8787 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 15526/35720 Training loss: 0.8787 0.2216 sec/batch\n",
      "Epoch 9/20  Iteration 15527/35720 Training loss: 0.8786 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 15528/35720 Training loss: 0.8785 0.2089 sec/batch\n",
      "Epoch 9/20  Iteration 15529/35720 Training loss: 0.8785 0.2196 sec/batch\n",
      "Epoch 9/20  Iteration 15530/35720 Training loss: 0.8785 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15531/35720 Training loss: 0.8785 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 15532/35720 Training loss: 0.8784 0.2080 sec/batch\n",
      "Epoch 9/20  Iteration 15533/35720 Training loss: 0.8783 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15534/35720 Training loss: 0.8783 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15535/35720 Training loss: 0.8783 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 15536/35720 Training loss: 0.8782 0.2254 sec/batch\n",
      "Epoch 9/20  Iteration 15537/35720 Training loss: 0.8782 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 15538/35720 Training loss: 0.8782 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15539/35720 Training loss: 0.8781 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15540/35720 Training loss: 0.8781 0.2222 sec/batch\n",
      "Epoch 9/20  Iteration 15541/35720 Training loss: 0.8781 0.2264 sec/batch\n",
      "Epoch 9/20  Iteration 15542/35720 Training loss: 0.8781 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15543/35720 Training loss: 0.8781 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 15544/35720 Training loss: 0.8780 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 15545/35720 Training loss: 0.8780 0.2225 sec/batch\n",
      "Epoch 9/20  Iteration 15546/35720 Training loss: 0.8779 0.2354 sec/batch\n",
      "Epoch 9/20  Iteration 15547/35720 Training loss: 0.8779 0.2278 sec/batch\n",
      "Epoch 9/20  Iteration 15548/35720 Training loss: 0.8779 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15549/35720 Training loss: 0.8779 0.2160 sec/batch\n",
      "Epoch 9/20  Iteration 15550/35720 Training loss: 0.8779 0.2129 sec/batch\n",
      "Epoch 9/20  Iteration 15551/35720 Training loss: 0.8778 0.2266 sec/batch\n",
      "Epoch 9/20  Iteration 15552/35720 Training loss: 0.8777 0.2112 sec/batch\n",
      "Epoch 9/20  Iteration 15553/35720 Training loss: 0.8777 0.2259 sec/batch\n",
      "Epoch 9/20  Iteration 15554/35720 Training loss: 0.8776 0.2203 sec/batch\n",
      "Epoch 9/20  Iteration 15555/35720 Training loss: 0.8776 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15556/35720 Training loss: 0.8775 0.2176 sec/batch\n",
      "Epoch 9/20  Iteration 15557/35720 Training loss: 0.8775 0.2140 sec/batch\n",
      "Epoch 9/20  Iteration 15558/35720 Training loss: 0.8775 0.2240 sec/batch\n",
      "Epoch 9/20  Iteration 15559/35720 Training loss: 0.8775 0.2157 sec/batch\n",
      "Epoch 9/20  Iteration 15560/35720 Training loss: 0.8774 0.2060 sec/batch\n",
      "Epoch 9/20  Iteration 15561/35720 Training loss: 0.8774 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15562/35720 Training loss: 0.8774 0.2279 sec/batch\n",
      "Epoch 9/20  Iteration 15563/35720 Training loss: 0.8773 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 15564/35720 Training loss: 0.8773 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 15565/35720 Training loss: 0.8772 0.2072 sec/batch\n",
      "Epoch 9/20  Iteration 15566/35720 Training loss: 0.8772 0.2160 sec/batch\n",
      "Epoch 9/20  Iteration 15567/35720 Training loss: 0.8771 0.2288 sec/batch\n",
      "Epoch 9/20  Iteration 15568/35720 Training loss: 0.8771 0.2278 sec/batch\n",
      "Epoch 9/20  Iteration 15569/35720 Training loss: 0.8771 0.2237 sec/batch\n",
      "Epoch 9/20  Iteration 15570/35720 Training loss: 0.8770 0.2059 sec/batch\n",
      "Epoch 9/20  Iteration 15571/35720 Training loss: 0.8770 0.2191 sec/batch\n",
      "Epoch 9/20  Iteration 15572/35720 Training loss: 0.8769 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15573/35720 Training loss: 0.8769 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15574/35720 Training loss: 0.8768 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15575/35720 Training loss: 0.8768 0.2242 sec/batch\n",
      "Epoch 9/20  Iteration 15576/35720 Training loss: 0.8768 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 15577/35720 Training loss: 0.8767 0.2333 sec/batch\n",
      "Epoch 9/20  Iteration 15578/35720 Training loss: 0.8767 0.2210 sec/batch\n",
      "Epoch 9/20  Iteration 15579/35720 Training loss: 0.8767 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15580/35720 Training loss: 0.8767 0.2124 sec/batch\n",
      "Epoch 9/20  Iteration 15581/35720 Training loss: 0.8767 0.2111 sec/batch\n",
      "Epoch 9/20  Iteration 15582/35720 Training loss: 0.8767 0.2203 sec/batch\n",
      "Epoch 9/20  Iteration 15583/35720 Training loss: 0.8767 0.2130 sec/batch\n",
      "Epoch 9/20  Iteration 15584/35720 Training loss: 0.8767 0.2269 sec/batch\n",
      "Epoch 9/20  Iteration 15585/35720 Training loss: 0.8767 0.2174 sec/batch\n",
      "Epoch 9/20  Iteration 15586/35720 Training loss: 0.8767 0.2140 sec/batch\n",
      "Epoch 9/20  Iteration 15587/35720 Training loss: 0.8767 0.2074 sec/batch\n",
      "Epoch 9/20  Iteration 15588/35720 Training loss: 0.8766 0.2077 sec/batch\n",
      "Epoch 9/20  Iteration 15589/35720 Training loss: 0.8766 0.2056 sec/batch\n",
      "Epoch 9/20  Iteration 15590/35720 Training loss: 0.8765 0.2134 sec/batch\n",
      "Epoch 9/20  Iteration 15591/35720 Training loss: 0.8765 0.2230 sec/batch\n",
      "Epoch 9/20  Iteration 15592/35720 Training loss: 0.8765 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 15593/35720 Training loss: 0.8764 0.2067 sec/batch\n",
      "Epoch 9/20  Iteration 15594/35720 Training loss: 0.8764 0.2134 sec/batch\n",
      "Epoch 9/20  Iteration 15595/35720 Training loss: 0.8764 0.2274 sec/batch\n",
      "Epoch 9/20  Iteration 15596/35720 Training loss: 0.8763 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 15597/35720 Training loss: 0.8763 0.2209 sec/batch\n",
      "Epoch 9/20  Iteration 15598/35720 Training loss: 0.8762 0.2212 sec/batch\n",
      "Epoch 9/20  Iteration 15599/35720 Training loss: 0.8762 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15600/35720 Training loss: 0.8762 0.2188 sec/batch\n",
      "Validation loss: 1.3803 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 15601/35720 Training loss: 0.8764 0.2122 sec/batch\n",
      "Epoch 9/20  Iteration 15602/35720 Training loss: 0.8764 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15603/35720 Training loss: 0.8764 0.2132 sec/batch\n",
      "Epoch 9/20  Iteration 15604/35720 Training loss: 0.8764 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 15605/35720 Training loss: 0.8763 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15606/35720 Training loss: 0.8763 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15607/35720 Training loss: 0.8763 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15608/35720 Training loss: 0.8763 0.2149 sec/batch\n",
      "Epoch 9/20  Iteration 15609/35720 Training loss: 0.8763 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 15610/35720 Training loss: 0.8763 0.2129 sec/batch\n",
      "Epoch 9/20  Iteration 15611/35720 Training loss: 0.8763 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 15612/35720 Training loss: 0.8763 0.2235 sec/batch\n",
      "Epoch 9/20  Iteration 15613/35720 Training loss: 0.8763 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 15614/35720 Training loss: 0.8763 0.2064 sec/batch\n",
      "Epoch 9/20  Iteration 15615/35720 Training loss: 0.8763 0.2112 sec/batch\n",
      "Epoch 9/20  Iteration 15616/35720 Training loss: 0.8763 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15617/35720 Training loss: 0.8763 0.2227 sec/batch\n",
      "Epoch 9/20  Iteration 15618/35720 Training loss: 0.8762 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 15619/35720 Training loss: 0.8762 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 15620/35720 Training loss: 0.8762 0.2232 sec/batch\n",
      "Epoch 9/20  Iteration 15621/35720 Training loss: 0.8762 0.2112 sec/batch\n",
      "Epoch 9/20  Iteration 15622/35720 Training loss: 0.8762 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 15623/35720 Training loss: 0.8762 0.2083 sec/batch\n",
      "Epoch 9/20  Iteration 15624/35720 Training loss: 0.8763 0.2271 sec/batch\n",
      "Epoch 9/20  Iteration 15625/35720 Training loss: 0.8763 0.2110 sec/batch\n",
      "Epoch 9/20  Iteration 15626/35720 Training loss: 0.8763 0.2225 sec/batch\n",
      "Epoch 9/20  Iteration 15627/35720 Training loss: 0.8763 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 15628/35720 Training loss: 0.8763 0.2123 sec/batch\n",
      "Epoch 9/20  Iteration 15629/35720 Training loss: 0.8763 0.2257 sec/batch\n",
      "Epoch 9/20  Iteration 15630/35720 Training loss: 0.8763 0.2289 sec/batch\n",
      "Epoch 9/20  Iteration 15631/35720 Training loss: 0.8762 0.2089 sec/batch\n",
      "Epoch 9/20  Iteration 15632/35720 Training loss: 0.8762 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 15633/35720 Training loss: 0.8762 0.2343 sec/batch\n",
      "Epoch 9/20  Iteration 15634/35720 Training loss: 0.8762 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 15635/35720 Training loss: 0.8762 0.2256 sec/batch\n",
      "Epoch 9/20  Iteration 15636/35720 Training loss: 0.8762 0.2177 sec/batch\n",
      "Epoch 9/20  Iteration 15637/35720 Training loss: 0.8762 0.2112 sec/batch\n",
      "Epoch 9/20  Iteration 15638/35720 Training loss: 0.8763 0.2339 sec/batch\n",
      "Epoch 9/20  Iteration 15639/35720 Training loss: 0.8763 0.2174 sec/batch\n",
      "Epoch 9/20  Iteration 15640/35720 Training loss: 0.8763 0.2265 sec/batch\n",
      "Epoch 9/20  Iteration 15641/35720 Training loss: 0.8764 0.2130 sec/batch\n",
      "Epoch 9/20  Iteration 15642/35720 Training loss: 0.8763 0.2199 sec/batch\n",
      "Epoch 9/20  Iteration 15643/35720 Training loss: 0.8763 0.2191 sec/batch\n",
      "Epoch 9/20  Iteration 15644/35720 Training loss: 0.8763 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15645/35720 Training loss: 0.8762 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 15646/35720 Training loss: 0.8762 0.2220 sec/batch\n",
      "Epoch 9/20  Iteration 15647/35720 Training loss: 0.8762 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15648/35720 Training loss: 0.8762 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15649/35720 Training loss: 0.8762 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 15650/35720 Training loss: 0.8761 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15651/35720 Training loss: 0.8761 0.2305 sec/batch\n",
      "Epoch 9/20  Iteration 15652/35720 Training loss: 0.8761 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15653/35720 Training loss: 0.8761 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 15654/35720 Training loss: 0.8761 0.2097 sec/batch\n",
      "Epoch 9/20  Iteration 15655/35720 Training loss: 0.8761 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 15656/35720 Training loss: 0.8761 0.2100 sec/batch\n",
      "Epoch 9/20  Iteration 15657/35720 Training loss: 0.8761 0.2229 sec/batch\n",
      "Epoch 9/20  Iteration 15658/35720 Training loss: 0.8762 0.2296 sec/batch\n",
      "Epoch 9/20  Iteration 15659/35720 Training loss: 0.8762 0.2146 sec/batch\n",
      "Epoch 9/20  Iteration 15660/35720 Training loss: 0.8762 0.2234 sec/batch\n",
      "Epoch 9/20  Iteration 15661/35720 Training loss: 0.8761 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 15662/35720 Training loss: 0.8761 0.2204 sec/batch\n",
      "Epoch 9/20  Iteration 15663/35720 Training loss: 0.8761 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 15664/35720 Training loss: 0.8761 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15665/35720 Training loss: 0.8761 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 15666/35720 Training loss: 0.8761 0.2239 sec/batch\n",
      "Epoch 9/20  Iteration 15667/35720 Training loss: 0.8761 0.2368 sec/batch\n",
      "Epoch 9/20  Iteration 15668/35720 Training loss: 0.8761 0.2260 sec/batch\n",
      "Epoch 9/20  Iteration 15669/35720 Training loss: 0.8761 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 15670/35720 Training loss: 0.8761 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 15671/35720 Training loss: 0.8761 0.2154 sec/batch\n",
      "Epoch 9/20  Iteration 15672/35720 Training loss: 0.8761 0.2189 sec/batch\n",
      "Epoch 9/20  Iteration 15673/35720 Training loss: 0.8761 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15674/35720 Training loss: 0.8761 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 15675/35720 Training loss: 0.8761 0.2239 sec/batch\n",
      "Epoch 9/20  Iteration 15676/35720 Training loss: 0.8761 0.2095 sec/batch\n",
      "Epoch 9/20  Iteration 15677/35720 Training loss: 0.8761 0.2123 sec/batch\n",
      "Epoch 9/20  Iteration 15678/35720 Training loss: 0.8761 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 15679/35720 Training loss: 0.8760 0.2268 sec/batch\n",
      "Epoch 9/20  Iteration 15680/35720 Training loss: 0.8760 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15681/35720 Training loss: 0.8759 0.2156 sec/batch\n",
      "Epoch 9/20  Iteration 15682/35720 Training loss: 0.8759 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 15683/35720 Training loss: 0.8758 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15684/35720 Training loss: 0.8758 0.2191 sec/batch\n",
      "Epoch 9/20  Iteration 15685/35720 Training loss: 0.8758 0.2176 sec/batch\n",
      "Epoch 9/20  Iteration 15686/35720 Training loss: 0.8757 0.2081 sec/batch\n",
      "Epoch 9/20  Iteration 15687/35720 Training loss: 0.8757 0.2089 sec/batch\n",
      "Epoch 9/20  Iteration 15688/35720 Training loss: 0.8756 0.2275 sec/batch\n",
      "Epoch 9/20  Iteration 15689/35720 Training loss: 0.8756 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15690/35720 Training loss: 0.8756 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 15691/35720 Training loss: 0.8756 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 15692/35720 Training loss: 0.8756 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 15693/35720 Training loss: 0.8756 0.2094 sec/batch\n",
      "Epoch 9/20  Iteration 15694/35720 Training loss: 0.8755 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15695/35720 Training loss: 0.8755 0.2178 sec/batch\n",
      "Epoch 9/20  Iteration 15696/35720 Training loss: 0.8755 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 15697/35720 Training loss: 0.8756 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 15698/35720 Training loss: 0.8755 0.2089 sec/batch\n",
      "Epoch 9/20  Iteration 15699/35720 Training loss: 0.8755 0.2219 sec/batch\n",
      "Epoch 9/20  Iteration 15700/35720 Training loss: 0.8755 0.2267 sec/batch\n",
      "Epoch 9/20  Iteration 15701/35720 Training loss: 0.8755 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 15702/35720 Training loss: 0.8755 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15703/35720 Training loss: 0.8755 0.2068 sec/batch\n",
      "Epoch 9/20  Iteration 15704/35720 Training loss: 0.8755 0.2136 sec/batch\n",
      "Epoch 9/20  Iteration 15705/35720 Training loss: 0.8754 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 15706/35720 Training loss: 0.8754 0.2273 sec/batch\n",
      "Epoch 9/20  Iteration 15707/35720 Training loss: 0.8755 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 15708/35720 Training loss: 0.8755 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 15709/35720 Training loss: 0.8755 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 15710/35720 Training loss: 0.8754 0.2361 sec/batch\n",
      "Epoch 9/20  Iteration 15711/35720 Training loss: 0.8754 0.2080 sec/batch\n",
      "Epoch 9/20  Iteration 15712/35720 Training loss: 0.8754 0.2293 sec/batch\n",
      "Epoch 9/20  Iteration 15713/35720 Training loss: 0.8754 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 15714/35720 Training loss: 0.8754 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 15715/35720 Training loss: 0.8754 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 15716/35720 Training loss: 0.8755 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 15717/35720 Training loss: 0.8755 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 15718/35720 Training loss: 0.8754 0.2160 sec/batch\n",
      "Epoch 9/20  Iteration 15719/35720 Training loss: 0.8754 0.2067 sec/batch\n",
      "Epoch 9/20  Iteration 15720/35720 Training loss: 0.8754 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 15721/35720 Training loss: 0.8753 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 15722/35720 Training loss: 0.8753 0.2134 sec/batch\n",
      "Epoch 9/20  Iteration 15723/35720 Training loss: 0.8753 0.2243 sec/batch\n",
      "Epoch 9/20  Iteration 15724/35720 Training loss: 0.8753 0.2224 sec/batch\n",
      "Epoch 9/20  Iteration 15725/35720 Training loss: 0.8752 0.2057 sec/batch\n",
      "Epoch 9/20  Iteration 15726/35720 Training loss: 0.8753 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 15727/35720 Training loss: 0.8753 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 15728/35720 Training loss: 0.8753 0.2271 sec/batch\n",
      "Epoch 9/20  Iteration 15729/35720 Training loss: 0.8753 0.2381 sec/batch\n",
      "Epoch 9/20  Iteration 15730/35720 Training loss: 0.8753 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15731/35720 Training loss: 0.8753 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 15732/35720 Training loss: 0.8753 0.2143 sec/batch\n",
      "Epoch 9/20  Iteration 15733/35720 Training loss: 0.8752 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15734/35720 Training loss: 0.8753 0.2263 sec/batch\n",
      "Epoch 9/20  Iteration 15735/35720 Training loss: 0.8752 0.2245 sec/batch\n",
      "Epoch 9/20  Iteration 15736/35720 Training loss: 0.8752 0.2111 sec/batch\n",
      "Epoch 9/20  Iteration 15737/35720 Training loss: 0.8752 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15738/35720 Training loss: 0.8751 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 15739/35720 Training loss: 0.8751 0.2176 sec/batch\n",
      "Epoch 9/20  Iteration 15740/35720 Training loss: 0.8751 0.2179 sec/batch\n",
      "Epoch 9/20  Iteration 15741/35720 Training loss: 0.8751 0.2151 sec/batch\n",
      "Epoch 9/20  Iteration 15742/35720 Training loss: 0.8751 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 15743/35720 Training loss: 0.8751 0.2200 sec/batch\n",
      "Epoch 9/20  Iteration 15744/35720 Training loss: 0.8751 0.2259 sec/batch\n",
      "Epoch 9/20  Iteration 15745/35720 Training loss: 0.8751 0.2117 sec/batch\n",
      "Epoch 9/20  Iteration 15746/35720 Training loss: 0.8751 0.2113 sec/batch\n",
      "Epoch 9/20  Iteration 15747/35720 Training loss: 0.8751 0.2088 sec/batch\n",
      "Epoch 9/20  Iteration 15748/35720 Training loss: 0.8752 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15749/35720 Training loss: 0.8752 0.2199 sec/batch\n",
      "Epoch 9/20  Iteration 15750/35720 Training loss: 0.8752 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 15751/35720 Training loss: 0.8752 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 15752/35720 Training loss: 0.8752 0.2191 sec/batch\n",
      "Epoch 9/20  Iteration 15753/35720 Training loss: 0.8751 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15754/35720 Training loss: 0.8751 0.2129 sec/batch\n",
      "Epoch 9/20  Iteration 15755/35720 Training loss: 0.8751 0.2151 sec/batch\n",
      "Epoch 9/20  Iteration 15756/35720 Training loss: 0.8751 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 15757/35720 Training loss: 0.8751 0.2236 sec/batch\n",
      "Epoch 9/20  Iteration 15758/35720 Training loss: 0.8751 0.2213 sec/batch\n",
      "Epoch 9/20  Iteration 15759/35720 Training loss: 0.8751 0.2114 sec/batch\n",
      "Epoch 9/20  Iteration 15760/35720 Training loss: 0.8750 0.2239 sec/batch\n",
      "Epoch 9/20  Iteration 15761/35720 Training loss: 0.8750 0.2131 sec/batch\n",
      "Epoch 9/20  Iteration 15762/35720 Training loss: 0.8750 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 15763/35720 Training loss: 0.8749 0.2173 sec/batch\n",
      "Epoch 9/20  Iteration 15764/35720 Training loss: 0.8748 0.2059 sec/batch\n",
      "Epoch 9/20  Iteration 15765/35720 Training loss: 0.8747 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15766/35720 Training loss: 0.8747 0.2289 sec/batch\n",
      "Epoch 9/20  Iteration 15767/35720 Training loss: 0.8747 0.2250 sec/batch\n",
      "Epoch 9/20  Iteration 15768/35720 Training loss: 0.8747 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15769/35720 Training loss: 0.8747 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 15770/35720 Training loss: 0.8746 0.2075 sec/batch\n",
      "Epoch 9/20  Iteration 15771/35720 Training loss: 0.8746 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15772/35720 Training loss: 0.8745 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 15773/35720 Training loss: 0.8745 0.2235 sec/batch\n",
      "Epoch 9/20  Iteration 15774/35720 Training loss: 0.8745 0.2228 sec/batch\n",
      "Epoch 9/20  Iteration 15775/35720 Training loss: 0.8745 0.2057 sec/batch\n",
      "Epoch 9/20  Iteration 15776/35720 Training loss: 0.8745 0.2110 sec/batch\n",
      "Epoch 9/20  Iteration 15777/35720 Training loss: 0.8744 0.2289 sec/batch\n",
      "Epoch 9/20  Iteration 15778/35720 Training loss: 0.8744 0.2328 sec/batch\n",
      "Epoch 9/20  Iteration 15779/35720 Training loss: 0.8744 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 15780/35720 Training loss: 0.8743 0.2096 sec/batch\n",
      "Epoch 9/20  Iteration 15781/35720 Training loss: 0.8743 0.2107 sec/batch\n",
      "Epoch 9/20  Iteration 15782/35720 Training loss: 0.8743 0.2342 sec/batch\n",
      "Epoch 9/20  Iteration 15783/35720 Training loss: 0.8743 0.2348 sec/batch\n",
      "Epoch 9/20  Iteration 15784/35720 Training loss: 0.8743 0.2258 sec/batch\n",
      "Epoch 9/20  Iteration 15785/35720 Training loss: 0.8742 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15786/35720 Training loss: 0.8742 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15787/35720 Training loss: 0.8742 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 15788/35720 Training loss: 0.8742 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 15789/35720 Training loss: 0.8741 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 15790/35720 Training loss: 0.8741 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 15791/35720 Training loss: 0.8741 0.2136 sec/batch\n",
      "Epoch 9/20  Iteration 15792/35720 Training loss: 0.8741 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15793/35720 Training loss: 0.8741 0.2143 sec/batch\n",
      "Epoch 9/20  Iteration 15794/35720 Training loss: 0.8742 0.2161 sec/batch\n",
      "Epoch 9/20  Iteration 15795/35720 Training loss: 0.8741 0.2198 sec/batch\n",
      "Epoch 9/20  Iteration 15796/35720 Training loss: 0.8741 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15797/35720 Training loss: 0.8741 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15798/35720 Training loss: 0.8741 0.2182 sec/batch\n",
      "Epoch 9/20  Iteration 15799/35720 Training loss: 0.8741 0.2121 sec/batch\n",
      "Epoch 9/20  Iteration 15800/35720 Training loss: 0.8741 0.2094 sec/batch\n",
      "Validation loss: 1.40232 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 15801/35720 Training loss: 0.8742 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 15802/35720 Training loss: 0.8743 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 15803/35720 Training loss: 0.8743 0.2270 sec/batch\n",
      "Epoch 9/20  Iteration 15804/35720 Training loss: 0.8743 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 15805/35720 Training loss: 0.8743 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 15806/35720 Training loss: 0.8743 0.2272 sec/batch\n",
      "Epoch 9/20  Iteration 15807/35720 Training loss: 0.8743 0.2166 sec/batch\n",
      "Epoch 9/20  Iteration 15808/35720 Training loss: 0.8743 0.2127 sec/batch\n",
      "Epoch 9/20  Iteration 15809/35720 Training loss: 0.8743 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 15810/35720 Training loss: 0.8743 0.2213 sec/batch\n",
      "Epoch 9/20  Iteration 15811/35720 Training loss: 0.8744 0.2129 sec/batch\n",
      "Epoch 9/20  Iteration 15812/35720 Training loss: 0.8744 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 15813/35720 Training loss: 0.8744 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15814/35720 Training loss: 0.8744 0.2182 sec/batch\n",
      "Epoch 9/20  Iteration 15815/35720 Training loss: 0.8744 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 15816/35720 Training loss: 0.8744 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15817/35720 Training loss: 0.8745 0.2171 sec/batch\n",
      "Epoch 9/20  Iteration 15818/35720 Training loss: 0.8745 0.2061 sec/batch\n",
      "Epoch 9/20  Iteration 15819/35720 Training loss: 0.8745 0.2102 sec/batch\n",
      "Epoch 9/20  Iteration 15820/35720 Training loss: 0.8746 0.2297 sec/batch\n",
      "Epoch 9/20  Iteration 15821/35720 Training loss: 0.8746 0.2293 sec/batch\n",
      "Epoch 9/20  Iteration 15822/35720 Training loss: 0.8746 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 15823/35720 Training loss: 0.8746 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15824/35720 Training loss: 0.8746 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 15825/35720 Training loss: 0.8745 0.2080 sec/batch\n",
      "Epoch 9/20  Iteration 15826/35720 Training loss: 0.8745 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 15827/35720 Training loss: 0.8745 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 15828/35720 Training loss: 0.8744 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 15829/35720 Training loss: 0.8744 0.2140 sec/batch\n",
      "Epoch 9/20  Iteration 15830/35720 Training loss: 0.8744 0.2124 sec/batch\n",
      "Epoch 9/20  Iteration 15831/35720 Training loss: 0.8743 0.2149 sec/batch\n",
      "Epoch 9/20  Iteration 15832/35720 Training loss: 0.8743 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 15833/35720 Training loss: 0.8743 0.2256 sec/batch\n",
      "Epoch 9/20  Iteration 15834/35720 Training loss: 0.8742 0.2161 sec/batch\n",
      "Epoch 9/20  Iteration 15835/35720 Training loss: 0.8742 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 15836/35720 Training loss: 0.8743 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 15837/35720 Training loss: 0.8743 0.2257 sec/batch\n",
      "Epoch 9/20  Iteration 15838/35720 Training loss: 0.8742 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 15839/35720 Training loss: 0.8742 0.2279 sec/batch\n",
      "Epoch 9/20  Iteration 15840/35720 Training loss: 0.8742 0.2249 sec/batch\n",
      "Epoch 9/20  Iteration 15841/35720 Training loss: 0.8742 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 15842/35720 Training loss: 0.8742 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 15843/35720 Training loss: 0.8741 0.2262 sec/batch\n",
      "Epoch 9/20  Iteration 15844/35720 Training loss: 0.8741 0.2283 sec/batch\n",
      "Epoch 9/20  Iteration 15845/35720 Training loss: 0.8741 0.2074 sec/batch\n",
      "Epoch 9/20  Iteration 15846/35720 Training loss: 0.8740 0.2066 sec/batch\n",
      "Epoch 9/20  Iteration 15847/35720 Training loss: 0.8740 0.2122 sec/batch\n",
      "Epoch 9/20  Iteration 15848/35720 Training loss: 0.8739 0.2199 sec/batch\n",
      "Epoch 9/20  Iteration 15849/35720 Training loss: 0.8739 0.2277 sec/batch\n",
      "Epoch 9/20  Iteration 15850/35720 Training loss: 0.8738 0.2267 sec/batch\n",
      "Epoch 9/20  Iteration 15851/35720 Training loss: 0.8738 0.2111 sec/batch\n",
      "Epoch 9/20  Iteration 15852/35720 Training loss: 0.8738 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15853/35720 Training loss: 0.8738 0.2243 sec/batch\n",
      "Epoch 9/20  Iteration 15854/35720 Training loss: 0.8738 0.2101 sec/batch\n",
      "Epoch 9/20  Iteration 15855/35720 Training loss: 0.8737 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 15856/35720 Training loss: 0.8738 0.2145 sec/batch\n",
      "Epoch 9/20  Iteration 15857/35720 Training loss: 0.8737 0.2109 sec/batch\n",
      "Epoch 9/20  Iteration 15858/35720 Training loss: 0.8737 0.2244 sec/batch\n",
      "Epoch 9/20  Iteration 15859/35720 Training loss: 0.8737 0.2300 sec/batch\n",
      "Epoch 9/20  Iteration 15860/35720 Training loss: 0.8736 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 15861/35720 Training loss: 0.8736 0.2179 sec/batch\n",
      "Epoch 9/20  Iteration 15862/35720 Training loss: 0.8736 0.2106 sec/batch\n",
      "Epoch 9/20  Iteration 15863/35720 Training loss: 0.8736 0.2060 sec/batch\n",
      "Epoch 9/20  Iteration 15864/35720 Training loss: 0.8735 0.2181 sec/batch\n",
      "Epoch 9/20  Iteration 15865/35720 Training loss: 0.8735 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 15866/35720 Training loss: 0.8735 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 15867/35720 Training loss: 0.8735 0.2272 sec/batch\n",
      "Epoch 9/20  Iteration 15868/35720 Training loss: 0.8734 0.2263 sec/batch\n",
      "Epoch 9/20  Iteration 15869/35720 Training loss: 0.8735 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 15870/35720 Training loss: 0.8734 0.2191 sec/batch\n",
      "Epoch 9/20  Iteration 15871/35720 Training loss: 0.8734 0.2095 sec/batch\n",
      "Epoch 9/20  Iteration 15872/35720 Training loss: 0.8733 0.2165 sec/batch\n",
      "Epoch 9/20  Iteration 15873/35720 Training loss: 0.8733 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 15874/35720 Training loss: 0.8733 0.2058 sec/batch\n",
      "Epoch 9/20  Iteration 15875/35720 Training loss: 0.8733 0.2116 sec/batch\n",
      "Epoch 9/20  Iteration 15876/35720 Training loss: 0.8732 0.2280 sec/batch\n",
      "Epoch 9/20  Iteration 15877/35720 Training loss: 0.8732 0.2238 sec/batch\n",
      "Epoch 9/20  Iteration 15878/35720 Training loss: 0.8732 0.2264 sec/batch\n",
      "Epoch 9/20  Iteration 15879/35720 Training loss: 0.8731 0.2069 sec/batch\n",
      "Epoch 9/20  Iteration 15880/35720 Training loss: 0.8731 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15881/35720 Training loss: 0.8732 0.2253 sec/batch\n",
      "Epoch 9/20  Iteration 15882/35720 Training loss: 0.8731 0.2200 sec/batch\n",
      "Epoch 9/20  Iteration 15883/35720 Training loss: 0.8731 0.2197 sec/batch\n",
      "Epoch 9/20  Iteration 15884/35720 Training loss: 0.8730 0.2067 sec/batch\n",
      "Epoch 9/20  Iteration 15885/35720 Training loss: 0.8730 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 15886/35720 Training loss: 0.8729 0.2186 sec/batch\n",
      "Epoch 9/20  Iteration 15887/35720 Training loss: 0.8729 0.2225 sec/batch\n",
      "Epoch 9/20  Iteration 15888/35720 Training loss: 0.8729 0.2280 sec/batch\n",
      "Epoch 9/20  Iteration 15889/35720 Training loss: 0.8729 0.2258 sec/batch\n",
      "Epoch 9/20  Iteration 15890/35720 Training loss: 0.8729 0.2072 sec/batch\n",
      "Epoch 9/20  Iteration 15891/35720 Training loss: 0.8729 0.2208 sec/batch\n",
      "Epoch 9/20  Iteration 15892/35720 Training loss: 0.8729 0.2206 sec/batch\n",
      "Epoch 9/20  Iteration 15893/35720 Training loss: 0.8728 0.2080 sec/batch\n",
      "Epoch 9/20  Iteration 15894/35720 Training loss: 0.8728 0.2323 sec/batch\n",
      "Epoch 9/20  Iteration 15895/35720 Training loss: 0.8728 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 15896/35720 Training loss: 0.8728 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 15897/35720 Training loss: 0.8727 0.2085 sec/batch\n",
      "Epoch 9/20  Iteration 15898/35720 Training loss: 0.8726 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 15899/35720 Training loss: 0.8726 0.2089 sec/batch\n",
      "Epoch 9/20  Iteration 15900/35720 Training loss: 0.8726 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15901/35720 Training loss: 0.8725 0.2113 sec/batch\n",
      "Epoch 9/20  Iteration 15902/35720 Training loss: 0.8725 0.2311 sec/batch\n",
      "Epoch 9/20  Iteration 15903/35720 Training loss: 0.8725 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15904/35720 Training loss: 0.8726 0.2133 sec/batch\n",
      "Epoch 9/20  Iteration 15905/35720 Training loss: 0.8725 0.2071 sec/batch\n",
      "Epoch 9/20  Iteration 15906/35720 Training loss: 0.8725 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 15907/35720 Training loss: 0.8725 0.2156 sec/batch\n",
      "Epoch 9/20  Iteration 15908/35720 Training loss: 0.8725 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 15909/35720 Training loss: 0.8724 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15910/35720 Training loss: 0.8724 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15911/35720 Training loss: 0.8724 0.2218 sec/batch\n",
      "Epoch 9/20  Iteration 15912/35720 Training loss: 0.8724 0.2346 sec/batch\n",
      "Epoch 9/20  Iteration 15913/35720 Training loss: 0.8724 0.2066 sec/batch\n",
      "Epoch 9/20  Iteration 15914/35720 Training loss: 0.8724 0.2141 sec/batch\n",
      "Epoch 9/20  Iteration 15915/35720 Training loss: 0.8724 0.2136 sec/batch\n",
      "Epoch 9/20  Iteration 15916/35720 Training loss: 0.8724 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15917/35720 Training loss: 0.8723 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 15918/35720 Training loss: 0.8723 0.2286 sec/batch\n",
      "Epoch 9/20  Iteration 15919/35720 Training loss: 0.8723 0.2148 sec/batch\n",
      "Epoch 9/20  Iteration 15920/35720 Training loss: 0.8723 0.2186 sec/batch\n",
      "Epoch 9/20  Iteration 15921/35720 Training loss: 0.8723 0.2113 sec/batch\n",
      "Epoch 9/20  Iteration 15922/35720 Training loss: 0.8723 0.2391 sec/batch\n",
      "Epoch 9/20  Iteration 15923/35720 Training loss: 0.8723 0.2286 sec/batch\n",
      "Epoch 9/20  Iteration 15924/35720 Training loss: 0.8723 0.2063 sec/batch\n",
      "Epoch 9/20  Iteration 15925/35720 Training loss: 0.8723 0.2137 sec/batch\n",
      "Epoch 9/20  Iteration 15926/35720 Training loss: 0.8722 0.2128 sec/batch\n",
      "Epoch 9/20  Iteration 15927/35720 Training loss: 0.8723 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 15928/35720 Training loss: 0.8722 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 15929/35720 Training loss: 0.8722 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15930/35720 Training loss: 0.8722 0.2134 sec/batch\n",
      "Epoch 9/20  Iteration 15931/35720 Training loss: 0.8722 0.2260 sec/batch\n",
      "Epoch 9/20  Iteration 15932/35720 Training loss: 0.8722 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 15933/35720 Training loss: 0.8722 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 15934/35720 Training loss: 0.8722 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 15935/35720 Training loss: 0.8722 0.2068 sec/batch\n",
      "Epoch 9/20  Iteration 15936/35720 Training loss: 0.8723 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 15937/35720 Training loss: 0.8723 0.2189 sec/batch\n",
      "Epoch 9/20  Iteration 15938/35720 Training loss: 0.8723 0.2180 sec/batch\n",
      "Epoch 9/20  Iteration 15939/35720 Training loss: 0.8723 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 15940/35720 Training loss: 0.8723 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15941/35720 Training loss: 0.8723 0.2388 sec/batch\n",
      "Epoch 9/20  Iteration 15942/35720 Training loss: 0.8723 0.2180 sec/batch\n",
      "Epoch 9/20  Iteration 15943/35720 Training loss: 0.8723 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 15944/35720 Training loss: 0.8723 0.2226 sec/batch\n",
      "Epoch 9/20  Iteration 15945/35720 Training loss: 0.8723 0.2068 sec/batch\n",
      "Epoch 9/20  Iteration 15946/35720 Training loss: 0.8723 0.2174 sec/batch\n",
      "Epoch 9/20  Iteration 15947/35720 Training loss: 0.8723 0.2105 sec/batch\n",
      "Epoch 9/20  Iteration 15948/35720 Training loss: 0.8723 0.2247 sec/batch\n",
      "Epoch 9/20  Iteration 15949/35720 Training loss: 0.8723 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 15950/35720 Training loss: 0.8723 0.2142 sec/batch\n",
      "Epoch 9/20  Iteration 15951/35720 Training loss: 0.8723 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 15952/35720 Training loss: 0.8723 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15953/35720 Training loss: 0.8723 0.2082 sec/batch\n",
      "Epoch 9/20  Iteration 15954/35720 Training loss: 0.8723 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15955/35720 Training loss: 0.8723 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 15956/35720 Training loss: 0.8724 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 15957/35720 Training loss: 0.8724 0.2179 sec/batch\n",
      "Epoch 9/20  Iteration 15958/35720 Training loss: 0.8724 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 15959/35720 Training loss: 0.8724 0.2296 sec/batch\n",
      "Epoch 9/20  Iteration 15960/35720 Training loss: 0.8724 0.2099 sec/batch\n",
      "Epoch 9/20  Iteration 15961/35720 Training loss: 0.8725 0.2240 sec/batch\n",
      "Epoch 9/20  Iteration 15962/35720 Training loss: 0.8725 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 15963/35720 Training loss: 0.8725 0.2080 sec/batch\n",
      "Epoch 9/20  Iteration 15964/35720 Training loss: 0.8725 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15965/35720 Training loss: 0.8725 0.2260 sec/batch\n",
      "Epoch 9/20  Iteration 15966/35720 Training loss: 0.8724 0.2128 sec/batch\n",
      "Epoch 9/20  Iteration 15967/35720 Training loss: 0.8724 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 15968/35720 Training loss: 0.8724 0.2269 sec/batch\n",
      "Epoch 9/20  Iteration 15969/35720 Training loss: 0.8723 0.2139 sec/batch\n",
      "Epoch 9/20  Iteration 15970/35720 Training loss: 0.8723 0.2213 sec/batch\n",
      "Epoch 9/20  Iteration 15971/35720 Training loss: 0.8723 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 15972/35720 Training loss: 0.8723 0.2189 sec/batch\n",
      "Epoch 9/20  Iteration 15973/35720 Training loss: 0.8723 0.2161 sec/batch\n",
      "Epoch 9/20  Iteration 15974/35720 Training loss: 0.8724 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 15975/35720 Training loss: 0.8723 0.2122 sec/batch\n",
      "Epoch 9/20  Iteration 15976/35720 Training loss: 0.8723 0.2188 sec/batch\n",
      "Epoch 9/20  Iteration 15977/35720 Training loss: 0.8723 0.2241 sec/batch\n",
      "Epoch 9/20  Iteration 15978/35720 Training loss: 0.8723 0.2282 sec/batch\n",
      "Epoch 9/20  Iteration 15979/35720 Training loss: 0.8722 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 15980/35720 Training loss: 0.8722 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 15981/35720 Training loss: 0.8722 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 15982/35720 Training loss: 0.8722 0.2348 sec/batch\n",
      "Epoch 9/20  Iteration 15983/35720 Training loss: 0.8721 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 15984/35720 Training loss: 0.8721 0.2066 sec/batch\n",
      "Epoch 9/20  Iteration 15985/35720 Training loss: 0.8721 0.2158 sec/batch\n",
      "Epoch 9/20  Iteration 15986/35720 Training loss: 0.8721 0.2093 sec/batch\n",
      "Epoch 9/20  Iteration 15987/35720 Training loss: 0.8721 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 15988/35720 Training loss: 0.8721 0.2124 sec/batch\n",
      "Epoch 9/20  Iteration 15989/35720 Training loss: 0.8721 0.2226 sec/batch\n",
      "Epoch 9/20  Iteration 15990/35720 Training loss: 0.8721 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 15991/35720 Training loss: 0.8720 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 15992/35720 Training loss: 0.8720 0.2087 sec/batch\n",
      "Epoch 9/20  Iteration 15993/35720 Training loss: 0.8720 0.2205 sec/batch\n",
      "Epoch 9/20  Iteration 15994/35720 Training loss: 0.8720 0.2291 sec/batch\n",
      "Epoch 9/20  Iteration 15995/35720 Training loss: 0.8719 0.2067 sec/batch\n",
      "Epoch 9/20  Iteration 15996/35720 Training loss: 0.8719 0.2084 sec/batch\n",
      "Epoch 9/20  Iteration 15997/35720 Training loss: 0.8719 0.2091 sec/batch\n",
      "Epoch 9/20  Iteration 15998/35720 Training loss: 0.8720 0.2220 sec/batch\n",
      "Epoch 9/20  Iteration 15999/35720 Training loss: 0.8720 0.2112 sec/batch\n",
      "Epoch 9/20  Iteration 16000/35720 Training loss: 0.8720 0.2161 sec/batch\n",
      "Validation loss: 1.39888 Saving checkpoint!\n",
      "Epoch 9/20  Iteration 16001/35720 Training loss: 0.8722 0.2126 sec/batch\n",
      "Epoch 9/20  Iteration 16002/35720 Training loss: 0.8722 0.2155 sec/batch\n",
      "Epoch 9/20  Iteration 16003/35720 Training loss: 0.8722 0.2179 sec/batch\n",
      "Epoch 9/20  Iteration 16004/35720 Training loss: 0.8722 0.2163 sec/batch\n",
      "Epoch 9/20  Iteration 16005/35720 Training loss: 0.8722 0.2103 sec/batch\n",
      "Epoch 9/20  Iteration 16006/35720 Training loss: 0.8722 0.2135 sec/batch\n",
      "Epoch 9/20  Iteration 16007/35720 Training loss: 0.8722 0.2276 sec/batch\n",
      "Epoch 9/20  Iteration 16008/35720 Training loss: 0.8722 0.2338 sec/batch\n",
      "Epoch 9/20  Iteration 16009/35720 Training loss: 0.8722 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 16010/35720 Training loss: 0.8722 0.2192 sec/batch\n",
      "Epoch 9/20  Iteration 16011/35720 Training loss: 0.8722 0.2221 sec/batch\n",
      "Epoch 9/20  Iteration 16012/35720 Training loss: 0.8722 0.2125 sec/batch\n",
      "Epoch 9/20  Iteration 16013/35720 Training loss: 0.8721 0.2170 sec/batch\n",
      "Epoch 9/20  Iteration 16014/35720 Training loss: 0.8721 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 16015/35720 Training loss: 0.8721 0.2172 sec/batch\n",
      "Epoch 9/20  Iteration 16016/35720 Training loss: 0.8721 0.2142 sec/batch\n",
      "Epoch 9/20  Iteration 16017/35720 Training loss: 0.8721 0.2078 sec/batch\n",
      "Epoch 9/20  Iteration 16018/35720 Training loss: 0.8721 0.2267 sec/batch\n",
      "Epoch 9/20  Iteration 16019/35720 Training loss: 0.8721 0.2195 sec/batch\n",
      "Epoch 9/20  Iteration 16020/35720 Training loss: 0.8721 0.2275 sec/batch\n",
      "Epoch 9/20  Iteration 16021/35720 Training loss: 0.8721 0.2169 sec/batch\n",
      "Epoch 9/20  Iteration 16022/35720 Training loss: 0.8721 0.2056 sec/batch\n",
      "Epoch 9/20  Iteration 16023/35720 Training loss: 0.8721 0.2118 sec/batch\n",
      "Epoch 9/20  Iteration 16024/35720 Training loss: 0.8721 0.2197 sec/batch\n",
      "Epoch 9/20  Iteration 16025/35720 Training loss: 0.8722 0.2303 sec/batch\n",
      "Epoch 9/20  Iteration 16026/35720 Training loss: 0.8721 0.2386 sec/batch\n",
      "Epoch 9/20  Iteration 16027/35720 Training loss: 0.8721 0.2062 sec/batch\n",
      "Epoch 9/20  Iteration 16028/35720 Training loss: 0.8721 0.2149 sec/batch\n",
      "Epoch 9/20  Iteration 16029/35720 Training loss: 0.8721 0.2363 sec/batch\n",
      "Epoch 9/20  Iteration 16030/35720 Training loss: 0.8721 0.2279 sec/batch\n",
      "Epoch 9/20  Iteration 16031/35720 Training loss: 0.8721 0.2153 sec/batch\n",
      "Epoch 9/20  Iteration 16032/35720 Training loss: 0.8721 0.2177 sec/batch\n",
      "Epoch 9/20  Iteration 16033/35720 Training loss: 0.8721 0.2196 sec/batch\n",
      "Epoch 9/20  Iteration 16034/35720 Training loss: 0.8722 0.2122 sec/batch\n",
      "Epoch 9/20  Iteration 16035/35720 Training loss: 0.8722 0.2143 sec/batch\n",
      "Epoch 9/20  Iteration 16036/35720 Training loss: 0.8722 0.2147 sec/batch\n",
      "Epoch 9/20  Iteration 16037/35720 Training loss: 0.8722 0.2202 sec/batch\n",
      "Epoch 9/20  Iteration 16038/35720 Training loss: 0.8722 0.2190 sec/batch\n",
      "Epoch 9/20  Iteration 16039/35720 Training loss: 0.8721 0.2113 sec/batch\n",
      "Epoch 9/20  Iteration 16040/35720 Training loss: 0.8721 0.2098 sec/batch\n",
      "Epoch 9/20  Iteration 16041/35720 Training loss: 0.8721 0.2271 sec/batch\n",
      "Epoch 9/20  Iteration 16042/35720 Training loss: 0.8721 0.2207 sec/batch\n",
      "Epoch 9/20  Iteration 16043/35720 Training loss: 0.8721 0.2330 sec/batch\n",
      "Epoch 9/20  Iteration 16044/35720 Training loss: 0.8721 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 16045/35720 Training loss: 0.8721 0.2115 sec/batch\n",
      "Epoch 9/20  Iteration 16046/35720 Training loss: 0.8721 0.2194 sec/batch\n",
      "Epoch 9/20  Iteration 16047/35720 Training loss: 0.8721 0.2095 sec/batch\n",
      "Epoch 9/20  Iteration 16048/35720 Training loss: 0.8721 0.2201 sec/batch\n",
      "Epoch 9/20  Iteration 16049/35720 Training loss: 0.8721 0.2104 sec/batch\n",
      "Epoch 9/20  Iteration 16050/35720 Training loss: 0.8720 0.2090 sec/batch\n",
      "Epoch 9/20  Iteration 16051/35720 Training loss: 0.8720 0.2238 sec/batch\n",
      "Epoch 9/20  Iteration 16052/35720 Training loss: 0.8720 0.2203 sec/batch\n",
      "Epoch 9/20  Iteration 16053/35720 Training loss: 0.8720 0.2168 sec/batch\n",
      "Epoch 9/20  Iteration 16054/35720 Training loss: 0.8720 0.2159 sec/batch\n",
      "Epoch 9/20  Iteration 16055/35720 Training loss: 0.8720 0.2065 sec/batch\n",
      "Epoch 9/20  Iteration 16056/35720 Training loss: 0.8720 0.2092 sec/batch\n",
      "Epoch 9/20  Iteration 16057/35720 Training loss: 0.8720 0.2201 sec/batch\n",
      "Epoch 9/20  Iteration 16058/35720 Training loss: 0.8720 0.2205 sec/batch\n",
      "Epoch 9/20  Iteration 16059/35720 Training loss: 0.8720 0.2152 sec/batch\n",
      "Epoch 9/20  Iteration 16060/35720 Training loss: 0.8720 0.2060 sec/batch\n",
      "Epoch 9/20  Iteration 16061/35720 Training loss: 0.8719 0.2120 sec/batch\n",
      "Epoch 9/20  Iteration 16062/35720 Training loss: 0.8719 0.2164 sec/batch\n",
      "Epoch 9/20  Iteration 16063/35720 Training loss: 0.8719 0.2214 sec/batch\n",
      "Epoch 9/20  Iteration 16064/35720 Training loss: 0.8719 0.2272 sec/batch\n",
      "Epoch 9/20  Iteration 16065/35720 Training loss: 0.8719 0.2271 sec/batch\n",
      "Epoch 9/20  Iteration 16066/35720 Training loss: 0.8719 0.2119 sec/batch\n",
      "Epoch 9/20  Iteration 16067/35720 Training loss: 0.8718 0.2167 sec/batch\n",
      "Epoch 9/20  Iteration 16068/35720 Training loss: 0.8718 0.2215 sec/batch\n",
      "Epoch 9/20  Iteration 16069/35720 Training loss: 0.8717 0.2380 sec/batch\n",
      "Epoch 9/20  Iteration 16070/35720 Training loss: 0.8717 0.2370 sec/batch\n",
      "Epoch 9/20  Iteration 16071/35720 Training loss: 0.8717 0.2108 sec/batch\n",
      "Epoch 9/20  Iteration 16072/35720 Training loss: 0.8717 0.2111 sec/batch\n",
      "Epoch 9/20  Iteration 16073/35720 Training loss: 0.8716 0.2180 sec/batch\n",
      "Epoch 9/20  Iteration 16074/35720 Training loss: 0.8716 0.2301 sec/batch\n",
      "Epoch 10/20  Iteration 16075/35720 Training loss: 0.9027 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 16076/35720 Training loss: 0.8887 0.2122 sec/batch\n",
      "Epoch 10/20  Iteration 16077/35720 Training loss: 0.8851 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 16078/35720 Training loss: 0.8814 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 16079/35720 Training loss: 0.8872 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 16080/35720 Training loss: 0.8743 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16081/35720 Training loss: 0.8731 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 16082/35720 Training loss: 0.8632 0.2079 sec/batch\n",
      "Epoch 10/20  Iteration 16083/35720 Training loss: 0.8603 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16084/35720 Training loss: 0.8609 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 16085/35720 Training loss: 0.8604 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 16086/35720 Training loss: 0.8573 0.2239 sec/batch\n",
      "Epoch 10/20  Iteration 16087/35720 Training loss: 0.8594 0.2159 sec/batch\n",
      "Epoch 10/20  Iteration 16088/35720 Training loss: 0.8652 0.2224 sec/batch\n",
      "Epoch 10/20  Iteration 16089/35720 Training loss: 0.8659 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16090/35720 Training loss: 0.8651 0.2261 sec/batch\n",
      "Epoch 10/20  Iteration 16091/35720 Training loss: 0.8647 0.2113 sec/batch\n",
      "Epoch 10/20  Iteration 16092/35720 Training loss: 0.8622 0.2157 sec/batch\n",
      "Epoch 10/20  Iteration 16093/35720 Training loss: 0.8607 0.2280 sec/batch\n",
      "Epoch 10/20  Iteration 16094/35720 Training loss: 0.8617 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 16095/35720 Training loss: 0.8619 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 16096/35720 Training loss: 0.8595 0.2269 sec/batch\n",
      "Epoch 10/20  Iteration 16097/35720 Training loss: 0.8582 0.2105 sec/batch\n",
      "Epoch 10/20  Iteration 16098/35720 Training loss: 0.8590 0.2180 sec/batch\n",
      "Epoch 10/20  Iteration 16099/35720 Training loss: 0.8597 0.2126 sec/batch\n",
      "Epoch 10/20  Iteration 16100/35720 Training loss: 0.8586 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 16101/35720 Training loss: 0.8606 0.2181 sec/batch\n",
      "Epoch 10/20  Iteration 16102/35720 Training loss: 0.8614 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 16103/35720 Training loss: 0.8603 0.2105 sec/batch\n",
      "Epoch 10/20  Iteration 16104/35720 Training loss: 0.8603 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 16105/35720 Training loss: 0.8634 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 16106/35720 Training loss: 0.8622 0.2130 sec/batch\n",
      "Epoch 10/20  Iteration 16107/35720 Training loss: 0.8639 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 16108/35720 Training loss: 0.8659 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 16109/35720 Training loss: 0.8687 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 16110/35720 Training loss: 0.8689 0.2140 sec/batch\n",
      "Epoch 10/20  Iteration 16111/35720 Training loss: 0.8692 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 16112/35720 Training loss: 0.8686 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 16113/35720 Training loss: 0.8667 0.2127 sec/batch\n",
      "Epoch 10/20  Iteration 16114/35720 Training loss: 0.8679 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 16115/35720 Training loss: 0.8676 0.2193 sec/batch\n",
      "Epoch 10/20  Iteration 16116/35720 Training loss: 0.8663 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 16117/35720 Training loss: 0.8643 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 16118/35720 Training loss: 0.8639 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 16119/35720 Training loss: 0.8638 0.2077 sec/batch\n",
      "Epoch 10/20  Iteration 16120/35720 Training loss: 0.8630 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 16121/35720 Training loss: 0.8625 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 16122/35720 Training loss: 0.8624 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 16123/35720 Training loss: 0.8622 0.2123 sec/batch\n",
      "Epoch 10/20  Iteration 16124/35720 Training loss: 0.8612 0.2202 sec/batch\n",
      "Epoch 10/20  Iteration 16125/35720 Training loss: 0.8619 0.2198 sec/batch\n",
      "Epoch 10/20  Iteration 16126/35720 Training loss: 0.8621 0.2123 sec/batch\n",
      "Epoch 10/20  Iteration 16127/35720 Training loss: 0.8622 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 16128/35720 Training loss: 0.8605 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 16129/35720 Training loss: 0.8599 0.2285 sec/batch\n",
      "Epoch 10/20  Iteration 16130/35720 Training loss: 0.8591 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 16131/35720 Training loss: 0.8594 0.2295 sec/batch\n",
      "Epoch 10/20  Iteration 16132/35720 Training loss: 0.8591 0.2076 sec/batch\n",
      "Epoch 10/20  Iteration 16133/35720 Training loss: 0.8583 0.2072 sec/batch\n",
      "Epoch 10/20  Iteration 16134/35720 Training loss: 0.8569 0.2142 sec/batch\n",
      "Epoch 10/20  Iteration 16135/35720 Training loss: 0.8559 0.2244 sec/batch\n",
      "Epoch 10/20  Iteration 16136/35720 Training loss: 0.8543 0.2214 sec/batch\n",
      "Epoch 10/20  Iteration 16137/35720 Training loss: 0.8550 0.2259 sec/batch\n",
      "Epoch 10/20  Iteration 16138/35720 Training loss: 0.8548 0.2069 sec/batch\n",
      "Epoch 10/20  Iteration 16139/35720 Training loss: 0.8554 0.2062 sec/batch\n",
      "Epoch 10/20  Iteration 16140/35720 Training loss: 0.8552 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 16141/35720 Training loss: 0.8546 0.2215 sec/batch\n",
      "Epoch 10/20  Iteration 16142/35720 Training loss: 0.8536 0.2238 sec/batch\n",
      "Epoch 10/20  Iteration 16143/35720 Training loss: 0.8544 0.2234 sec/batch\n",
      "Epoch 10/20  Iteration 16144/35720 Training loss: 0.8540 0.2184 sec/batch\n",
      "Epoch 10/20  Iteration 16145/35720 Training loss: 0.8544 0.2274 sec/batch\n",
      "Epoch 10/20  Iteration 16146/35720 Training loss: 0.8549 0.2215 sec/batch\n",
      "Epoch 10/20  Iteration 16147/35720 Training loss: 0.8553 0.2126 sec/batch\n",
      "Epoch 10/20  Iteration 16148/35720 Training loss: 0.8553 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16149/35720 Training loss: 0.8545 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16150/35720 Training loss: 0.8544 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 16151/35720 Training loss: 0.8535 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 16152/35720 Training loss: 0.8541 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 16153/35720 Training loss: 0.8540 0.2212 sec/batch\n",
      "Epoch 10/20  Iteration 16154/35720 Training loss: 0.8553 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16155/35720 Training loss: 0.8557 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16156/35720 Training loss: 0.8555 0.2263 sec/batch\n",
      "Epoch 10/20  Iteration 16157/35720 Training loss: 0.8554 0.2184 sec/batch\n",
      "Epoch 10/20  Iteration 16158/35720 Training loss: 0.8554 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 16159/35720 Training loss: 0.8553 0.2295 sec/batch\n",
      "Epoch 10/20  Iteration 16160/35720 Training loss: 0.8554 0.2127 sec/batch\n",
      "Epoch 10/20  Iteration 16161/35720 Training loss: 0.8553 0.2227 sec/batch\n",
      "Epoch 10/20  Iteration 16162/35720 Training loss: 0.8552 0.2196 sec/batch\n",
      "Epoch 10/20  Iteration 16163/35720 Training loss: 0.8543 0.2118 sec/batch\n",
      "Epoch 10/20  Iteration 16164/35720 Training loss: 0.8533 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 16165/35720 Training loss: 0.8536 0.2140 sec/batch\n",
      "Epoch 10/20  Iteration 16166/35720 Training loss: 0.8533 0.2263 sec/batch\n",
      "Epoch 10/20  Iteration 16167/35720 Training loss: 0.8533 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 16168/35720 Training loss: 0.8535 0.2207 sec/batch\n",
      "Epoch 10/20  Iteration 16169/35720 Training loss: 0.8533 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16170/35720 Training loss: 0.8526 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 16171/35720 Training loss: 0.8529 0.2277 sec/batch\n",
      "Epoch 10/20  Iteration 16172/35720 Training loss: 0.8527 0.2071 sec/batch\n",
      "Epoch 10/20  Iteration 16173/35720 Training loss: 0.8525 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 16174/35720 Training loss: 0.8524 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 16175/35720 Training loss: 0.8522 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 16176/35720 Training loss: 0.8523 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 16177/35720 Training loss: 0.8521 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 16178/35720 Training loss: 0.8518 0.2125 sec/batch\n",
      "Epoch 10/20  Iteration 16179/35720 Training loss: 0.8515 0.2178 sec/batch\n",
      "Epoch 10/20  Iteration 16180/35720 Training loss: 0.8511 0.2233 sec/batch\n",
      "Epoch 10/20  Iteration 16181/35720 Training loss: 0.8512 0.2218 sec/batch\n",
      "Epoch 10/20  Iteration 16182/35720 Training loss: 0.8514 0.2058 sec/batch\n",
      "Epoch 10/20  Iteration 16183/35720 Training loss: 0.8517 0.2081 sec/batch\n",
      "Epoch 10/20  Iteration 16184/35720 Training loss: 0.8517 0.2136 sec/batch\n",
      "Epoch 10/20  Iteration 16185/35720 Training loss: 0.8519 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 16186/35720 Training loss: 0.8526 0.2293 sec/batch\n",
      "Epoch 10/20  Iteration 16187/35720 Training loss: 0.8528 0.2228 sec/batch\n",
      "Epoch 10/20  Iteration 16188/35720 Training loss: 0.8531 0.2122 sec/batch\n",
      "Epoch 10/20  Iteration 16189/35720 Training loss: 0.8528 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16190/35720 Training loss: 0.8529 0.2206 sec/batch\n",
      "Epoch 10/20  Iteration 16191/35720 Training loss: 0.8529 0.2100 sec/batch\n",
      "Epoch 10/20  Iteration 16192/35720 Training loss: 0.8535 0.2049 sec/batch\n",
      "Epoch 10/20  Iteration 16193/35720 Training loss: 0.8536 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 16194/35720 Training loss: 0.8543 0.2110 sec/batch\n",
      "Epoch 10/20  Iteration 16195/35720 Training loss: 0.8549 0.2303 sec/batch\n",
      "Epoch 10/20  Iteration 16196/35720 Training loss: 0.8547 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 16197/35720 Training loss: 0.8549 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 16198/35720 Training loss: 0.8555 0.2211 sec/batch\n",
      "Epoch 10/20  Iteration 16199/35720 Training loss: 0.8551 0.2052 sec/batch\n",
      "Epoch 10/20  Iteration 16200/35720 Training loss: 0.8549 0.2109 sec/batch\n",
      "Validation loss: 1.38497 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 16201/35720 Training loss: 0.8580 0.2081 sec/batch\n",
      "Epoch 10/20  Iteration 16202/35720 Training loss: 0.8584 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 16203/35720 Training loss: 0.8581 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 16204/35720 Training loss: 0.8581 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16205/35720 Training loss: 0.8578 0.2083 sec/batch\n",
      "Epoch 10/20  Iteration 16206/35720 Training loss: 0.8575 0.2113 sec/batch\n",
      "Epoch 10/20  Iteration 16207/35720 Training loss: 0.8575 0.2215 sec/batch\n",
      "Epoch 10/20  Iteration 16208/35720 Training loss: 0.8575 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 16209/35720 Training loss: 0.8575 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 16210/35720 Training loss: 0.8575 0.2124 sec/batch\n",
      "Epoch 10/20  Iteration 16211/35720 Training loss: 0.8581 0.2594 sec/batch\n",
      "Epoch 10/20  Iteration 16212/35720 Training loss: 0.8583 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 16213/35720 Training loss: 0.8583 0.2453 sec/batch\n",
      "Epoch 10/20  Iteration 16214/35720 Training loss: 0.8585 0.2258 sec/batch\n",
      "Epoch 10/20  Iteration 16215/35720 Training loss: 0.8581 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 16216/35720 Training loss: 0.8578 0.2166 sec/batch\n",
      "Epoch 10/20  Iteration 16217/35720 Training loss: 0.8573 0.2154 sec/batch\n",
      "Epoch 10/20  Iteration 16218/35720 Training loss: 0.8568 0.2294 sec/batch\n",
      "Epoch 10/20  Iteration 16219/35720 Training loss: 0.8569 0.2230 sec/batch\n",
      "Epoch 10/20  Iteration 16220/35720 Training loss: 0.8572 0.2083 sec/batch\n",
      "Epoch 10/20  Iteration 16221/35720 Training loss: 0.8569 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 16222/35720 Training loss: 0.8567 0.2355 sec/batch\n",
      "Epoch 10/20  Iteration 16223/35720 Training loss: 0.8568 0.2266 sec/batch\n",
      "Epoch 10/20  Iteration 16224/35720 Training loss: 0.8561 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 16225/35720 Training loss: 0.8559 0.2290 sec/batch\n",
      "Epoch 10/20  Iteration 16226/35720 Training loss: 0.8559 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 16227/35720 Training loss: 0.8559 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 16228/35720 Training loss: 0.8562 0.2256 sec/batch\n",
      "Epoch 10/20  Iteration 16229/35720 Training loss: 0.8564 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 16230/35720 Training loss: 0.8566 0.2194 sec/batch\n",
      "Epoch 10/20  Iteration 16231/35720 Training loss: 0.8568 0.2327 sec/batch\n",
      "Epoch 10/20  Iteration 16232/35720 Training loss: 0.8570 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16233/35720 Training loss: 0.8566 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16234/35720 Training loss: 0.8567 0.2253 sec/batch\n",
      "Epoch 10/20  Iteration 16235/35720 Training loss: 0.8564 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 16236/35720 Training loss: 0.8564 0.2148 sec/batch\n",
      "Epoch 10/20  Iteration 16237/35720 Training loss: 0.8565 0.2215 sec/batch\n",
      "Epoch 10/20  Iteration 16238/35720 Training loss: 0.8567 0.2082 sec/batch\n",
      "Epoch 10/20  Iteration 16239/35720 Training loss: 0.8569 0.2215 sec/batch\n",
      "Epoch 10/20  Iteration 16240/35720 Training loss: 0.8570 0.2130 sec/batch\n",
      "Epoch 10/20  Iteration 16241/35720 Training loss: 0.8571 0.2401 sec/batch\n",
      "Epoch 10/20  Iteration 16242/35720 Training loss: 0.8574 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 16243/35720 Training loss: 0.8578 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16244/35720 Training loss: 0.8579 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16245/35720 Training loss: 0.8585 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 16246/35720 Training loss: 0.8588 0.2229 sec/batch\n",
      "Epoch 10/20  Iteration 16247/35720 Training loss: 0.8591 0.2225 sec/batch\n",
      "Epoch 10/20  Iteration 16248/35720 Training loss: 0.8597 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16249/35720 Training loss: 0.8599 0.2159 sec/batch\n",
      "Epoch 10/20  Iteration 16250/35720 Training loss: 0.8599 0.2251 sec/batch\n",
      "Epoch 10/20  Iteration 16251/35720 Training loss: 0.8601 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 16252/35720 Training loss: 0.8600 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 16253/35720 Training loss: 0.8598 0.2205 sec/batch\n",
      "Epoch 10/20  Iteration 16254/35720 Training loss: 0.8596 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16255/35720 Training loss: 0.8598 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 16256/35720 Training loss: 0.8599 0.2260 sec/batch\n",
      "Epoch 10/20  Iteration 16257/35720 Training loss: 0.8599 0.2193 sec/batch\n",
      "Epoch 10/20  Iteration 16258/35720 Training loss: 0.8600 0.2278 sec/batch\n",
      "Epoch 10/20  Iteration 16259/35720 Training loss: 0.8599 0.2075 sec/batch\n",
      "Epoch 10/20  Iteration 16260/35720 Training loss: 0.8599 0.2170 sec/batch\n",
      "Epoch 10/20  Iteration 16261/35720 Training loss: 0.8597 0.2384 sec/batch\n",
      "Epoch 10/20  Iteration 16262/35720 Training loss: 0.8598 0.2244 sec/batch\n",
      "Epoch 10/20  Iteration 16263/35720 Training loss: 0.8600 0.2276 sec/batch\n",
      "Epoch 10/20  Iteration 16264/35720 Training loss: 0.8600 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 16265/35720 Training loss: 0.8602 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16266/35720 Training loss: 0.8606 0.2147 sec/batch\n",
      "Epoch 10/20  Iteration 16267/35720 Training loss: 0.8609 0.2271 sec/batch\n",
      "Epoch 10/20  Iteration 16268/35720 Training loss: 0.8611 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 16269/35720 Training loss: 0.8611 0.2075 sec/batch\n",
      "Epoch 10/20  Iteration 16270/35720 Training loss: 0.8614 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 16271/35720 Training loss: 0.8609 0.2272 sec/batch\n",
      "Epoch 10/20  Iteration 16272/35720 Training loss: 0.8609 0.2218 sec/batch\n",
      "Epoch 10/20  Iteration 16273/35720 Training loss: 0.8610 0.2130 sec/batch\n",
      "Epoch 10/20  Iteration 16274/35720 Training loss: 0.8612 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 16275/35720 Training loss: 0.8612 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 16276/35720 Training loss: 0.8612 0.2372 sec/batch\n",
      "Epoch 10/20  Iteration 16277/35720 Training loss: 0.8612 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 16278/35720 Training loss: 0.8611 0.2243 sec/batch\n",
      "Epoch 10/20  Iteration 16279/35720 Training loss: 0.8612 0.2226 sec/batch\n",
      "Epoch 10/20  Iteration 16280/35720 Training loss: 0.8611 0.2189 sec/batch\n",
      "Epoch 10/20  Iteration 16281/35720 Training loss: 0.8616 0.2184 sec/batch\n",
      "Epoch 10/20  Iteration 16282/35720 Training loss: 0.8620 0.2084 sec/batch\n",
      "Epoch 10/20  Iteration 16283/35720 Training loss: 0.8625 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16284/35720 Training loss: 0.8625 0.2110 sec/batch\n",
      "Epoch 10/20  Iteration 16285/35720 Training loss: 0.8627 0.2241 sec/batch\n",
      "Epoch 10/20  Iteration 16286/35720 Training loss: 0.8627 0.2128 sec/batch\n",
      "Epoch 10/20  Iteration 16287/35720 Training loss: 0.8625 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16288/35720 Training loss: 0.8625 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 16289/35720 Training loss: 0.8625 0.2197 sec/batch\n",
      "Epoch 10/20  Iteration 16290/35720 Training loss: 0.8624 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 16291/35720 Training loss: 0.8623 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16292/35720 Training loss: 0.8622 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 16293/35720 Training loss: 0.8623 0.2172 sec/batch\n",
      "Epoch 10/20  Iteration 16294/35720 Training loss: 0.8625 0.2194 sec/batch\n",
      "Epoch 10/20  Iteration 16295/35720 Training loss: 0.8624 0.2225 sec/batch\n",
      "Epoch 10/20  Iteration 16296/35720 Training loss: 0.8625 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 16297/35720 Training loss: 0.8628 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 16298/35720 Training loss: 0.8630 0.2180 sec/batch\n",
      "Epoch 10/20  Iteration 16299/35720 Training loss: 0.8631 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 16300/35720 Training loss: 0.8630 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 16301/35720 Training loss: 0.8628 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 16302/35720 Training loss: 0.8626 0.2245 sec/batch\n",
      "Epoch 10/20  Iteration 16303/35720 Training loss: 0.8624 0.2160 sec/batch\n",
      "Epoch 10/20  Iteration 16304/35720 Training loss: 0.8625 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16305/35720 Training loss: 0.8628 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 16306/35720 Training loss: 0.8627 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 16307/35720 Training loss: 0.8628 0.2193 sec/batch\n",
      "Epoch 10/20  Iteration 16308/35720 Training loss: 0.8629 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 16309/35720 Training loss: 0.8629 0.2081 sec/batch\n",
      "Epoch 10/20  Iteration 16310/35720 Training loss: 0.8629 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 16311/35720 Training loss: 0.8631 0.2349 sec/batch\n",
      "Epoch 10/20  Iteration 16312/35720 Training loss: 0.8629 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 16313/35720 Training loss: 0.8628 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 16314/35720 Training loss: 0.8628 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 16315/35720 Training loss: 0.8626 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 16316/35720 Training loss: 0.8625 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16317/35720 Training loss: 0.8624 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 16318/35720 Training loss: 0.8624 0.2167 sec/batch\n",
      "Epoch 10/20  Iteration 16319/35720 Training loss: 0.8622 0.2213 sec/batch\n",
      "Epoch 10/20  Iteration 16320/35720 Training loss: 0.8622 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 16321/35720 Training loss: 0.8622 0.2245 sec/batch\n",
      "Epoch 10/20  Iteration 16322/35720 Training loss: 0.8621 0.2166 sec/batch\n",
      "Epoch 10/20  Iteration 16323/35720 Training loss: 0.8620 0.2201 sec/batch\n",
      "Epoch 10/20  Iteration 16324/35720 Training loss: 0.8617 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 16325/35720 Training loss: 0.8618 0.2058 sec/batch\n",
      "Epoch 10/20  Iteration 16326/35720 Training loss: 0.8617 0.2071 sec/batch\n",
      "Epoch 10/20  Iteration 16327/35720 Training loss: 0.8614 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16328/35720 Training loss: 0.8616 0.2177 sec/batch\n",
      "Epoch 10/20  Iteration 16329/35720 Training loss: 0.8619 0.2185 sec/batch\n",
      "Epoch 10/20  Iteration 16330/35720 Training loss: 0.8619 0.2197 sec/batch\n",
      "Epoch 10/20  Iteration 16331/35720 Training loss: 0.8620 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 16332/35720 Training loss: 0.8619 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 16333/35720 Training loss: 0.8622 0.2219 sec/batch\n",
      "Epoch 10/20  Iteration 16334/35720 Training loss: 0.8621 0.2197 sec/batch\n",
      "Epoch 10/20  Iteration 16335/35720 Training loss: 0.8621 0.2178 sec/batch\n",
      "Epoch 10/20  Iteration 16336/35720 Training loss: 0.8620 0.2071 sec/batch\n",
      "Epoch 10/20  Iteration 16337/35720 Training loss: 0.8620 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 16338/35720 Training loss: 0.8621 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 16339/35720 Training loss: 0.8621 0.2198 sec/batch\n",
      "Epoch 10/20  Iteration 16340/35720 Training loss: 0.8622 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 16341/35720 Training loss: 0.8621 0.2257 sec/batch\n",
      "Epoch 10/20  Iteration 16342/35720 Training loss: 0.8621 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 16343/35720 Training loss: 0.8620 0.2275 sec/batch\n",
      "Epoch 10/20  Iteration 16344/35720 Training loss: 0.8615 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16345/35720 Training loss: 0.8612 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 16346/35720 Training loss: 0.8612 0.2204 sec/batch\n",
      "Epoch 10/20  Iteration 16347/35720 Training loss: 0.8613 0.2199 sec/batch\n",
      "Epoch 10/20  Iteration 16348/35720 Training loss: 0.8613 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 16349/35720 Training loss: 0.8612 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 16350/35720 Training loss: 0.8610 0.2139 sec/batch\n",
      "Epoch 10/20  Iteration 16351/35720 Training loss: 0.8607 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 16352/35720 Training loss: 0.8605 0.2167 sec/batch\n",
      "Epoch 10/20  Iteration 16353/35720 Training loss: 0.8604 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 16354/35720 Training loss: 0.8604 0.2072 sec/batch\n",
      "Epoch 10/20  Iteration 16355/35720 Training loss: 0.8603 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 16356/35720 Training loss: 0.8601 0.2194 sec/batch\n",
      "Epoch 10/20  Iteration 16357/35720 Training loss: 0.8598 0.2339 sec/batch\n",
      "Epoch 10/20  Iteration 16358/35720 Training loss: 0.8596 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16359/35720 Training loss: 0.8598 0.2128 sec/batch\n",
      "Epoch 10/20  Iteration 16360/35720 Training loss: 0.8598 0.2171 sec/batch\n",
      "Epoch 10/20  Iteration 16361/35720 Training loss: 0.8594 0.2242 sec/batch\n",
      "Epoch 10/20  Iteration 16362/35720 Training loss: 0.8594 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 16363/35720 Training loss: 0.8594 0.2182 sec/batch\n",
      "Epoch 10/20  Iteration 16364/35720 Training loss: 0.8594 0.2273 sec/batch\n",
      "Epoch 10/20  Iteration 16365/35720 Training loss: 0.8593 0.2100 sec/batch\n",
      "Epoch 10/20  Iteration 16366/35720 Training loss: 0.8593 0.2287 sec/batch\n",
      "Epoch 10/20  Iteration 16367/35720 Training loss: 0.8592 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 16368/35720 Training loss: 0.8592 0.2142 sec/batch\n",
      "Epoch 10/20  Iteration 16369/35720 Training loss: 0.8595 0.2175 sec/batch\n",
      "Epoch 10/20  Iteration 16370/35720 Training loss: 0.8595 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 16371/35720 Training loss: 0.8595 0.2079 sec/batch\n",
      "Epoch 10/20  Iteration 16372/35720 Training loss: 0.8596 0.2317 sec/batch\n",
      "Epoch 10/20  Iteration 16373/35720 Training loss: 0.8595 0.2176 sec/batch\n",
      "Epoch 10/20  Iteration 16374/35720 Training loss: 0.8596 0.2346 sec/batch\n",
      "Epoch 10/20  Iteration 16375/35720 Training loss: 0.8596 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 16376/35720 Training loss: 0.8594 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 16377/35720 Training loss: 0.8595 0.2082 sec/batch\n",
      "Epoch 10/20  Iteration 16378/35720 Training loss: 0.8595 0.2265 sec/batch\n",
      "Epoch 10/20  Iteration 16379/35720 Training loss: 0.8594 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 16380/35720 Training loss: 0.8592 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 16381/35720 Training loss: 0.8592 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16382/35720 Training loss: 0.8593 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 16383/35720 Training loss: 0.8591 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 16384/35720 Training loss: 0.8590 0.2100 sec/batch\n",
      "Epoch 10/20  Iteration 16385/35720 Training loss: 0.8589 0.2295 sec/batch\n",
      "Epoch 10/20  Iteration 16386/35720 Training loss: 0.8588 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 16387/35720 Training loss: 0.8587 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 16388/35720 Training loss: 0.8587 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 16389/35720 Training loss: 0.8586 0.2183 sec/batch\n",
      "Epoch 10/20  Iteration 16390/35720 Training loss: 0.8585 0.2278 sec/batch\n",
      "Epoch 10/20  Iteration 16391/35720 Training loss: 0.8584 0.2251 sec/batch\n",
      "Epoch 10/20  Iteration 16392/35720 Training loss: 0.8585 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16393/35720 Training loss: 0.8586 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16394/35720 Training loss: 0.8586 0.2180 sec/batch\n",
      "Epoch 10/20  Iteration 16395/35720 Training loss: 0.8587 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 16396/35720 Training loss: 0.8586 0.2253 sec/batch\n",
      "Epoch 10/20  Iteration 16397/35720 Training loss: 0.8587 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16398/35720 Training loss: 0.8588 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 16399/35720 Training loss: 0.8586 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 16400/35720 Training loss: 0.8588 0.2188 sec/batch\n",
      "Validation loss: 1.40369 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 16401/35720 Training loss: 0.8597 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 16402/35720 Training loss: 0.8598 0.2075 sec/batch\n",
      "Epoch 10/20  Iteration 16403/35720 Training loss: 0.8598 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 16404/35720 Training loss: 0.8599 0.2144 sec/batch\n",
      "Epoch 10/20  Iteration 16405/35720 Training loss: 0.8599 0.2197 sec/batch\n",
      "Epoch 10/20  Iteration 16406/35720 Training loss: 0.8600 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 16407/35720 Training loss: 0.8600 0.2230 sec/batch\n",
      "Epoch 10/20  Iteration 16408/35720 Training loss: 0.8601 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 16409/35720 Training loss: 0.8600 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 16410/35720 Training loss: 0.8598 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 16411/35720 Training loss: 0.8598 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 16412/35720 Training loss: 0.8597 0.2261 sec/batch\n",
      "Epoch 10/20  Iteration 16413/35720 Training loss: 0.8597 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16414/35720 Training loss: 0.8598 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 16415/35720 Training loss: 0.8597 0.2159 sec/batch\n",
      "Epoch 10/20  Iteration 16416/35720 Training loss: 0.8596 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 16417/35720 Training loss: 0.8597 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16418/35720 Training loss: 0.8597 0.2169 sec/batch\n",
      "Epoch 10/20  Iteration 16419/35720 Training loss: 0.8594 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 16420/35720 Training loss: 0.8595 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 16421/35720 Training loss: 0.8596 0.2222 sec/batch\n",
      "Epoch 10/20  Iteration 16422/35720 Training loss: 0.8597 0.2312 sec/batch\n",
      "Epoch 10/20  Iteration 16423/35720 Training loss: 0.8598 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 16424/35720 Training loss: 0.8598 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 16425/35720 Training loss: 0.8599 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 16426/35720 Training loss: 0.8598 0.2231 sec/batch\n",
      "Epoch 10/20  Iteration 16427/35720 Training loss: 0.8599 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 16428/35720 Training loss: 0.8599 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 16429/35720 Training loss: 0.8600 0.2120 sec/batch\n",
      "Epoch 10/20  Iteration 16430/35720 Training loss: 0.8599 0.2118 sec/batch\n",
      "Epoch 10/20  Iteration 16431/35720 Training loss: 0.8600 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 16432/35720 Training loss: 0.8600 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 16433/35720 Training loss: 0.8599 0.2095 sec/batch\n",
      "Epoch 10/20  Iteration 16434/35720 Training loss: 0.8599 0.2223 sec/batch\n",
      "Epoch 10/20  Iteration 16435/35720 Training loss: 0.8598 0.2268 sec/batch\n",
      "Epoch 10/20  Iteration 16436/35720 Training loss: 0.8599 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 16437/35720 Training loss: 0.8599 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 16438/35720 Training loss: 0.8599 0.2307 sec/batch\n",
      "Epoch 10/20  Iteration 16439/35720 Training loss: 0.8598 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 16440/35720 Training loss: 0.8597 0.2149 sec/batch\n",
      "Epoch 10/20  Iteration 16441/35720 Training loss: 0.8597 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 16442/35720 Training loss: 0.8596 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 16443/35720 Training loss: 0.8595 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 16444/35720 Training loss: 0.8594 0.2166 sec/batch\n",
      "Epoch 10/20  Iteration 16445/35720 Training loss: 0.8593 0.2183 sec/batch\n",
      "Epoch 10/20  Iteration 16446/35720 Training loss: 0.8592 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 16447/35720 Training loss: 0.8592 0.2270 sec/batch\n",
      "Epoch 10/20  Iteration 16448/35720 Training loss: 0.8592 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 16449/35720 Training loss: 0.8592 0.2312 sec/batch\n",
      "Epoch 10/20  Iteration 16450/35720 Training loss: 0.8593 0.2231 sec/batch\n",
      "Epoch 10/20  Iteration 16451/35720 Training loss: 0.8593 0.2207 sec/batch\n",
      "Epoch 10/20  Iteration 16452/35720 Training loss: 0.8593 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 16453/35720 Training loss: 0.8591 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 16454/35720 Training loss: 0.8589 0.2270 sec/batch\n",
      "Epoch 10/20  Iteration 16455/35720 Training loss: 0.8588 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 16456/35720 Training loss: 0.8587 0.2126 sec/batch\n",
      "Epoch 10/20  Iteration 16457/35720 Training loss: 0.8587 0.2154 sec/batch\n",
      "Epoch 10/20  Iteration 16458/35720 Training loss: 0.8588 0.2074 sec/batch\n",
      "Epoch 10/20  Iteration 16459/35720 Training loss: 0.8588 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 16460/35720 Training loss: 0.8588 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 16461/35720 Training loss: 0.8588 0.2120 sec/batch\n",
      "Epoch 10/20  Iteration 16462/35720 Training loss: 0.8589 0.2253 sec/batch\n",
      "Epoch 10/20  Iteration 16463/35720 Training loss: 0.8589 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16464/35720 Training loss: 0.8589 0.2218 sec/batch\n",
      "Epoch 10/20  Iteration 16465/35720 Training loss: 0.8588 0.2142 sec/batch\n",
      "Epoch 10/20  Iteration 16466/35720 Training loss: 0.8586 0.2224 sec/batch\n",
      "Epoch 10/20  Iteration 16467/35720 Training loss: 0.8587 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 16468/35720 Training loss: 0.8585 0.2180 sec/batch\n",
      "Epoch 10/20  Iteration 16469/35720 Training loss: 0.8585 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16470/35720 Training loss: 0.8584 0.2206 sec/batch\n",
      "Epoch 10/20  Iteration 16471/35720 Training loss: 0.8583 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 16472/35720 Training loss: 0.8581 0.2178 sec/batch\n",
      "Epoch 10/20  Iteration 16473/35720 Training loss: 0.8580 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 16474/35720 Training loss: 0.8579 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16475/35720 Training loss: 0.8579 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 16476/35720 Training loss: 0.8580 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 16477/35720 Training loss: 0.8580 0.2201 sec/batch\n",
      "Epoch 10/20  Iteration 16478/35720 Training loss: 0.8579 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 16479/35720 Training loss: 0.8578 0.2235 sec/batch\n",
      "Epoch 10/20  Iteration 16480/35720 Training loss: 0.8576 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 16481/35720 Training loss: 0.8576 0.2105 sec/batch\n",
      "Epoch 10/20  Iteration 16482/35720 Training loss: 0.8576 0.2271 sec/batch\n",
      "Epoch 10/20  Iteration 16483/35720 Training loss: 0.8575 0.2267 sec/batch\n",
      "Epoch 10/20  Iteration 16484/35720 Training loss: 0.8572 0.2276 sec/batch\n",
      "Epoch 10/20  Iteration 16485/35720 Training loss: 0.8573 0.2113 sec/batch\n",
      "Epoch 10/20  Iteration 16486/35720 Training loss: 0.8571 0.2221 sec/batch\n",
      "Epoch 10/20  Iteration 16487/35720 Training loss: 0.8569 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 16488/35720 Training loss: 0.8569 0.2254 sec/batch\n",
      "Epoch 10/20  Iteration 16489/35720 Training loss: 0.8568 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 16490/35720 Training loss: 0.8567 0.2212 sec/batch\n",
      "Epoch 10/20  Iteration 16491/35720 Training loss: 0.8567 0.2140 sec/batch\n",
      "Epoch 10/20  Iteration 16492/35720 Training loss: 0.8565 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 16493/35720 Training loss: 0.8563 0.2286 sec/batch\n",
      "Epoch 10/20  Iteration 16494/35720 Training loss: 0.8563 0.2127 sec/batch\n",
      "Epoch 10/20  Iteration 16495/35720 Training loss: 0.8563 0.2265 sec/batch\n",
      "Epoch 10/20  Iteration 16496/35720 Training loss: 0.8563 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 16497/35720 Training loss: 0.8564 0.2262 sec/batch\n",
      "Epoch 10/20  Iteration 16498/35720 Training loss: 0.8564 0.2235 sec/batch\n",
      "Epoch 10/20  Iteration 16499/35720 Training loss: 0.8564 0.2216 sec/batch\n",
      "Epoch 10/20  Iteration 16500/35720 Training loss: 0.8562 0.2253 sec/batch\n",
      "Epoch 10/20  Iteration 16501/35720 Training loss: 0.8560 0.2074 sec/batch\n",
      "Epoch 10/20  Iteration 16502/35720 Training loss: 0.8561 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 16503/35720 Training loss: 0.8561 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 16504/35720 Training loss: 0.8561 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 16505/35720 Training loss: 0.8563 0.2076 sec/batch\n",
      "Epoch 10/20  Iteration 16506/35720 Training loss: 0.8562 0.2312 sec/batch\n",
      "Epoch 10/20  Iteration 16507/35720 Training loss: 0.8563 0.2190 sec/batch\n",
      "Epoch 10/20  Iteration 16508/35720 Training loss: 0.8564 0.2073 sec/batch\n",
      "Epoch 10/20  Iteration 16509/35720 Training loss: 0.8565 0.2078 sec/batch\n",
      "Epoch 10/20  Iteration 16510/35720 Training loss: 0.8564 0.2232 sec/batch\n",
      "Epoch 10/20  Iteration 16511/35720 Training loss: 0.8566 0.2084 sec/batch\n",
      "Epoch 10/20  Iteration 16512/35720 Training loss: 0.8567 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 16513/35720 Training loss: 0.8568 0.2294 sec/batch\n",
      "Epoch 10/20  Iteration 16514/35720 Training loss: 0.8568 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 16515/35720 Training loss: 0.8569 0.2157 sec/batch\n",
      "Epoch 10/20  Iteration 16516/35720 Training loss: 0.8571 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 16517/35720 Training loss: 0.8570 0.2202 sec/batch\n",
      "Epoch 10/20  Iteration 16518/35720 Training loss: 0.8571 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16519/35720 Training loss: 0.8571 0.2140 sec/batch\n",
      "Epoch 10/20  Iteration 16520/35720 Training loss: 0.8572 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 16521/35720 Training loss: 0.8574 0.2191 sec/batch\n",
      "Epoch 10/20  Iteration 16522/35720 Training loss: 0.8576 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 16523/35720 Training loss: 0.8577 0.2185 sec/batch\n",
      "Epoch 10/20  Iteration 16524/35720 Training loss: 0.8576 0.2236 sec/batch\n",
      "Epoch 10/20  Iteration 16525/35720 Training loss: 0.8575 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 16526/35720 Training loss: 0.8575 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 16527/35720 Training loss: 0.8574 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 16528/35720 Training loss: 0.8576 0.2219 sec/batch\n",
      "Epoch 10/20  Iteration 16529/35720 Training loss: 0.8577 0.2278 sec/batch\n",
      "Epoch 10/20  Iteration 16530/35720 Training loss: 0.8579 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16531/35720 Training loss: 0.8581 0.2135 sec/batch\n",
      "Epoch 10/20  Iteration 16532/35720 Training loss: 0.8582 0.2264 sec/batch\n",
      "Epoch 10/20  Iteration 16533/35720 Training loss: 0.8581 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 16534/35720 Training loss: 0.8580 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 16535/35720 Training loss: 0.8580 0.2175 sec/batch\n",
      "Epoch 10/20  Iteration 16536/35720 Training loss: 0.8580 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 16537/35720 Training loss: 0.8581 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 16538/35720 Training loss: 0.8581 0.2136 sec/batch\n",
      "Epoch 10/20  Iteration 16539/35720 Training loss: 0.8581 0.2185 sec/batch\n",
      "Epoch 10/20  Iteration 16540/35720 Training loss: 0.8581 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 16541/35720 Training loss: 0.8581 0.2191 sec/batch\n",
      "Epoch 10/20  Iteration 16542/35720 Training loss: 0.8581 0.2244 sec/batch\n",
      "Epoch 10/20  Iteration 16543/35720 Training loss: 0.8580 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 16544/35720 Training loss: 0.8578 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 16545/35720 Training loss: 0.8578 0.2176 sec/batch\n",
      "Epoch 10/20  Iteration 16546/35720 Training loss: 0.8577 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 16547/35720 Training loss: 0.8577 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 16548/35720 Training loss: 0.8575 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16549/35720 Training loss: 0.8576 0.2186 sec/batch\n",
      "Epoch 10/20  Iteration 16550/35720 Training loss: 0.8575 0.2191 sec/batch\n",
      "Epoch 10/20  Iteration 16551/35720 Training loss: 0.8575 0.2160 sec/batch\n",
      "Epoch 10/20  Iteration 16552/35720 Training loss: 0.8575 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 16553/35720 Training loss: 0.8575 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 16554/35720 Training loss: 0.8574 0.2308 sec/batch\n",
      "Epoch 10/20  Iteration 16555/35720 Training loss: 0.8573 0.2181 sec/batch\n",
      "Epoch 10/20  Iteration 16556/35720 Training loss: 0.8571 0.2171 sec/batch\n",
      "Epoch 10/20  Iteration 16557/35720 Training loss: 0.8572 0.2055 sec/batch\n",
      "Epoch 10/20  Iteration 16558/35720 Training loss: 0.8572 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 16559/35720 Training loss: 0.8572 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 16560/35720 Training loss: 0.8570 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 16561/35720 Training loss: 0.8569 0.2265 sec/batch\n",
      "Epoch 10/20  Iteration 16562/35720 Training loss: 0.8568 0.2255 sec/batch\n",
      "Epoch 10/20  Iteration 16563/35720 Training loss: 0.8568 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 16564/35720 Training loss: 0.8567 0.2205 sec/batch\n",
      "Epoch 10/20  Iteration 16565/35720 Training loss: 0.8567 0.2229 sec/batch\n",
      "Epoch 10/20  Iteration 16566/35720 Training loss: 0.8568 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 16567/35720 Training loss: 0.8568 0.2176 sec/batch\n",
      "Epoch 10/20  Iteration 16568/35720 Training loss: 0.8567 0.2057 sec/batch\n",
      "Epoch 10/20  Iteration 16569/35720 Training loss: 0.8567 0.2123 sec/batch\n",
      "Epoch 10/20  Iteration 16570/35720 Training loss: 0.8566 0.2291 sec/batch\n",
      "Epoch 10/20  Iteration 16571/35720 Training loss: 0.8567 0.2285 sec/batch\n",
      "Epoch 10/20  Iteration 16572/35720 Training loss: 0.8567 0.2257 sec/batch\n",
      "Epoch 10/20  Iteration 16573/35720 Training loss: 0.8566 0.2294 sec/batch\n",
      "Epoch 10/20  Iteration 16574/35720 Training loss: 0.8567 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 16575/35720 Training loss: 0.8566 0.2256 sec/batch\n",
      "Epoch 10/20  Iteration 16576/35720 Training loss: 0.8564 0.2239 sec/batch\n",
      "Epoch 10/20  Iteration 16577/35720 Training loss: 0.8562 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 16578/35720 Training loss: 0.8562 0.2270 sec/batch\n",
      "Epoch 10/20  Iteration 16579/35720 Training loss: 0.8562 0.2082 sec/batch\n",
      "Epoch 10/20  Iteration 16580/35720 Training loss: 0.8562 0.2177 sec/batch\n",
      "Epoch 10/20  Iteration 16581/35720 Training loss: 0.8562 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16582/35720 Training loss: 0.8561 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 16583/35720 Training loss: 0.8562 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 16584/35720 Training loss: 0.8561 0.2183 sec/batch\n",
      "Epoch 10/20  Iteration 16585/35720 Training loss: 0.8559 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 16586/35720 Training loss: 0.8559 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 16587/35720 Training loss: 0.8560 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 16588/35720 Training loss: 0.8560 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16589/35720 Training loss: 0.8561 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 16590/35720 Training loss: 0.8561 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 16591/35720 Training loss: 0.8561 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 16592/35720 Training loss: 0.8561 0.2147 sec/batch\n",
      "Epoch 10/20  Iteration 16593/35720 Training loss: 0.8560 0.2242 sec/batch\n",
      "Epoch 10/20  Iteration 16594/35720 Training loss: 0.8561 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 16595/35720 Training loss: 0.8560 0.2339 sec/batch\n",
      "Epoch 10/20  Iteration 16596/35720 Training loss: 0.8560 0.2191 sec/batch\n",
      "Epoch 10/20  Iteration 16597/35720 Training loss: 0.8560 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 16598/35720 Training loss: 0.8558 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16599/35720 Training loss: 0.8559 0.2148 sec/batch\n",
      "Epoch 10/20  Iteration 16600/35720 Training loss: 0.8558 0.2170 sec/batch\n",
      "Validation loss: 1.40045 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 16601/35720 Training loss: 0.8565 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 16602/35720 Training loss: 0.8565 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 16603/35720 Training loss: 0.8565 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 16604/35720 Training loss: 0.8564 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16605/35720 Training loss: 0.8564 0.2326 sec/batch\n",
      "Epoch 10/20  Iteration 16606/35720 Training loss: 0.8563 0.2170 sec/batch\n",
      "Epoch 10/20  Iteration 16607/35720 Training loss: 0.8563 0.2198 sec/batch\n",
      "Epoch 10/20  Iteration 16608/35720 Training loss: 0.8564 0.2170 sec/batch\n",
      "Epoch 10/20  Iteration 16609/35720 Training loss: 0.8562 0.2218 sec/batch\n",
      "Epoch 10/20  Iteration 16610/35720 Training loss: 0.8561 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 16611/35720 Training loss: 0.8561 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 16612/35720 Training loss: 0.8560 0.2166 sec/batch\n",
      "Epoch 10/20  Iteration 16613/35720 Training loss: 0.8560 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 16614/35720 Training loss: 0.8559 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 16615/35720 Training loss: 0.8559 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 16616/35720 Training loss: 0.8558 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 16617/35720 Training loss: 0.8557 0.2100 sec/batch\n",
      "Epoch 10/20  Iteration 16618/35720 Training loss: 0.8556 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16619/35720 Training loss: 0.8557 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 16620/35720 Training loss: 0.8558 0.2244 sec/batch\n",
      "Epoch 10/20  Iteration 16621/35720 Training loss: 0.8558 0.2082 sec/batch\n",
      "Epoch 10/20  Iteration 16622/35720 Training loss: 0.8558 0.2139 sec/batch\n",
      "Epoch 10/20  Iteration 16623/35720 Training loss: 0.8558 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 16624/35720 Training loss: 0.8558 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 16625/35720 Training loss: 0.8558 0.2258 sec/batch\n",
      "Epoch 10/20  Iteration 16626/35720 Training loss: 0.8557 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 16627/35720 Training loss: 0.8556 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 16628/35720 Training loss: 0.8556 0.2135 sec/batch\n",
      "Epoch 10/20  Iteration 16629/35720 Training loss: 0.8556 0.2249 sec/batch\n",
      "Epoch 10/20  Iteration 16630/35720 Training loss: 0.8557 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 16631/35720 Training loss: 0.8556 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 16632/35720 Training loss: 0.8558 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 16633/35720 Training loss: 0.8558 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 16634/35720 Training loss: 0.8557 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 16635/35720 Training loss: 0.8558 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 16636/35720 Training loss: 0.8557 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 16637/35720 Training loss: 0.8556 0.2175 sec/batch\n",
      "Epoch 10/20  Iteration 16638/35720 Training loss: 0.8556 0.2250 sec/batch\n",
      "Epoch 10/20  Iteration 16639/35720 Training loss: 0.8555 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 16640/35720 Training loss: 0.8554 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 16641/35720 Training loss: 0.8554 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16642/35720 Training loss: 0.8553 0.2181 sec/batch\n",
      "Epoch 10/20  Iteration 16643/35720 Training loss: 0.8552 0.2130 sec/batch\n",
      "Epoch 10/20  Iteration 16644/35720 Training loss: 0.8552 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 16645/35720 Training loss: 0.8551 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 16646/35720 Training loss: 0.8552 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16647/35720 Training loss: 0.8552 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 16648/35720 Training loss: 0.8553 0.2225 sec/batch\n",
      "Epoch 10/20  Iteration 16649/35720 Training loss: 0.8554 0.2199 sec/batch\n",
      "Epoch 10/20  Iteration 16650/35720 Training loss: 0.8554 0.2233 sec/batch\n",
      "Epoch 10/20  Iteration 16651/35720 Training loss: 0.8554 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16652/35720 Training loss: 0.8554 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 16653/35720 Training loss: 0.8554 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 16654/35720 Training loss: 0.8553 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 16655/35720 Training loss: 0.8554 0.2169 sec/batch\n",
      "Epoch 10/20  Iteration 16656/35720 Training loss: 0.8554 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 16657/35720 Training loss: 0.8554 0.2135 sec/batch\n",
      "Epoch 10/20  Iteration 16658/35720 Training loss: 0.8554 0.2203 sec/batch\n",
      "Epoch 10/20  Iteration 16659/35720 Training loss: 0.8553 0.2459 sec/batch\n",
      "Epoch 10/20  Iteration 16660/35720 Training loss: 0.8551 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 16661/35720 Training loss: 0.8550 0.2268 sec/batch\n",
      "Epoch 10/20  Iteration 16662/35720 Training loss: 0.8549 0.2241 sec/batch\n",
      "Epoch 10/20  Iteration 16663/35720 Training loss: 0.8548 0.2084 sec/batch\n",
      "Epoch 10/20  Iteration 16664/35720 Training loss: 0.8547 0.2275 sec/batch\n",
      "Epoch 10/20  Iteration 16665/35720 Training loss: 0.8548 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 16666/35720 Training loss: 0.8547 0.2238 sec/batch\n",
      "Epoch 10/20  Iteration 16667/35720 Training loss: 0.8547 0.2191 sec/batch\n",
      "Epoch 10/20  Iteration 16668/35720 Training loss: 0.8547 0.2053 sec/batch\n",
      "Epoch 10/20  Iteration 16669/35720 Training loss: 0.8546 0.2166 sec/batch\n",
      "Epoch 10/20  Iteration 16670/35720 Training loss: 0.8546 0.2228 sec/batch\n",
      "Epoch 10/20  Iteration 16671/35720 Training loss: 0.8545 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 16672/35720 Training loss: 0.8546 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 16673/35720 Training loss: 0.8545 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 16674/35720 Training loss: 0.8543 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 16675/35720 Training loss: 0.8542 0.2335 sec/batch\n",
      "Epoch 10/20  Iteration 16676/35720 Training loss: 0.8541 0.2268 sec/batch\n",
      "Epoch 10/20  Iteration 16677/35720 Training loss: 0.8541 0.2268 sec/batch\n",
      "Epoch 10/20  Iteration 16678/35720 Training loss: 0.8541 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 16679/35720 Training loss: 0.8541 0.2118 sec/batch\n",
      "Epoch 10/20  Iteration 16680/35720 Training loss: 0.8541 0.2144 sec/batch\n",
      "Epoch 10/20  Iteration 16681/35720 Training loss: 0.8541 0.2199 sec/batch\n",
      "Epoch 10/20  Iteration 16682/35720 Training loss: 0.8541 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 16683/35720 Training loss: 0.8542 0.2159 sec/batch\n",
      "Epoch 10/20  Iteration 16684/35720 Training loss: 0.8541 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16685/35720 Training loss: 0.8541 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16686/35720 Training loss: 0.8540 0.2256 sec/batch\n",
      "Epoch 10/20  Iteration 16687/35720 Training loss: 0.8539 0.2185 sec/batch\n",
      "Epoch 10/20  Iteration 16688/35720 Training loss: 0.8539 0.2128 sec/batch\n",
      "Epoch 10/20  Iteration 16689/35720 Training loss: 0.8539 0.2246 sec/batch\n",
      "Epoch 10/20  Iteration 16690/35720 Training loss: 0.8538 0.2222 sec/batch\n",
      "Epoch 10/20  Iteration 16691/35720 Training loss: 0.8538 0.2122 sec/batch\n",
      "Epoch 10/20  Iteration 16692/35720 Training loss: 0.8537 0.2253 sec/batch\n",
      "Epoch 10/20  Iteration 16693/35720 Training loss: 0.8537 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 16694/35720 Training loss: 0.8536 0.2202 sec/batch\n",
      "Epoch 10/20  Iteration 16695/35720 Training loss: 0.8536 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 16696/35720 Training loss: 0.8537 0.2113 sec/batch\n",
      "Epoch 10/20  Iteration 16697/35720 Training loss: 0.8536 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 16698/35720 Training loss: 0.8535 0.2260 sec/batch\n",
      "Epoch 10/20  Iteration 16699/35720 Training loss: 0.8534 0.2186 sec/batch\n",
      "Epoch 10/20  Iteration 16700/35720 Training loss: 0.8534 0.2124 sec/batch\n",
      "Epoch 10/20  Iteration 16701/35720 Training loss: 0.8535 0.2238 sec/batch\n",
      "Epoch 10/20  Iteration 16702/35720 Training loss: 0.8533 0.2400 sec/batch\n",
      "Epoch 10/20  Iteration 16703/35720 Training loss: 0.8533 0.2248 sec/batch\n",
      "Epoch 10/20  Iteration 16704/35720 Training loss: 0.8533 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 16705/35720 Training loss: 0.8534 0.2309 sec/batch\n",
      "Epoch 10/20  Iteration 16706/35720 Training loss: 0.8532 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 16707/35720 Training loss: 0.8532 0.2074 sec/batch\n",
      "Epoch 10/20  Iteration 16708/35720 Training loss: 0.8532 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 16709/35720 Training loss: 0.8532 0.2273 sec/batch\n",
      "Epoch 10/20  Iteration 16710/35720 Training loss: 0.8532 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 16711/35720 Training loss: 0.8531 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 16712/35720 Training loss: 0.8530 0.2299 sec/batch\n",
      "Epoch 10/20  Iteration 16713/35720 Training loss: 0.8530 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 16714/35720 Training loss: 0.8531 0.2142 sec/batch\n",
      "Epoch 10/20  Iteration 16715/35720 Training loss: 0.8531 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16716/35720 Training loss: 0.8532 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 16717/35720 Training loss: 0.8532 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16718/35720 Training loss: 0.8531 0.2241 sec/batch\n",
      "Epoch 10/20  Iteration 16719/35720 Training loss: 0.8531 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 16720/35720 Training loss: 0.8531 0.2214 sec/batch\n",
      "Epoch 10/20  Iteration 16721/35720 Training loss: 0.8531 0.2266 sec/batch\n",
      "Epoch 10/20  Iteration 16722/35720 Training loss: 0.8530 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 16723/35720 Training loss: 0.8530 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 16724/35720 Training loss: 0.8529 0.2232 sec/batch\n",
      "Epoch 10/20  Iteration 16725/35720 Training loss: 0.8529 0.2175 sec/batch\n",
      "Epoch 10/20  Iteration 16726/35720 Training loss: 0.8528 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 16727/35720 Training loss: 0.8529 0.2071 sec/batch\n",
      "Epoch 10/20  Iteration 16728/35720 Training loss: 0.8530 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 16729/35720 Training loss: 0.8530 0.2192 sec/batch\n",
      "Epoch 10/20  Iteration 16730/35720 Training loss: 0.8530 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 16731/35720 Training loss: 0.8531 0.2332 sec/batch\n",
      "Epoch 10/20  Iteration 16732/35720 Training loss: 0.8532 0.2312 sec/batch\n",
      "Epoch 10/20  Iteration 16733/35720 Training loss: 0.8532 0.2234 sec/batch\n",
      "Epoch 10/20  Iteration 16734/35720 Training loss: 0.8532 0.2059 sec/batch\n",
      "Epoch 10/20  Iteration 16735/35720 Training loss: 0.8532 0.2227 sec/batch\n",
      "Epoch 10/20  Iteration 16736/35720 Training loss: 0.8533 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 16737/35720 Training loss: 0.8533 0.2080 sec/batch\n",
      "Epoch 10/20  Iteration 16738/35720 Training loss: 0.8533 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 16739/35720 Training loss: 0.8533 0.2113 sec/batch\n",
      "Epoch 10/20  Iteration 16740/35720 Training loss: 0.8533 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16741/35720 Training loss: 0.8535 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 16742/35720 Training loss: 0.8534 0.2199 sec/batch\n",
      "Epoch 10/20  Iteration 16743/35720 Training loss: 0.8533 0.2206 sec/batch\n",
      "Epoch 10/20  Iteration 16744/35720 Training loss: 0.8533 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 16745/35720 Training loss: 0.8531 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 16746/35720 Training loss: 0.8531 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16747/35720 Training loss: 0.8532 0.2095 sec/batch\n",
      "Epoch 10/20  Iteration 16748/35720 Training loss: 0.8530 0.2159 sec/batch\n",
      "Epoch 10/20  Iteration 16749/35720 Training loss: 0.8530 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 16750/35720 Training loss: 0.8530 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 16751/35720 Training loss: 0.8529 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 16752/35720 Training loss: 0.8529 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 16753/35720 Training loss: 0.8529 0.2113 sec/batch\n",
      "Epoch 10/20  Iteration 16754/35720 Training loss: 0.8528 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16755/35720 Training loss: 0.8528 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 16756/35720 Training loss: 0.8528 0.2124 sec/batch\n",
      "Epoch 10/20  Iteration 16757/35720 Training loss: 0.8528 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 16758/35720 Training loss: 0.8527 0.2095 sec/batch\n",
      "Epoch 10/20  Iteration 16759/35720 Training loss: 0.8528 0.2213 sec/batch\n",
      "Epoch 10/20  Iteration 16760/35720 Training loss: 0.8528 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 16761/35720 Training loss: 0.8527 0.2192 sec/batch\n",
      "Epoch 10/20  Iteration 16762/35720 Training loss: 0.8527 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 16763/35720 Training loss: 0.8527 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 16764/35720 Training loss: 0.8526 0.2262 sec/batch\n",
      "Epoch 10/20  Iteration 16765/35720 Training loss: 0.8527 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 16766/35720 Training loss: 0.8527 0.2183 sec/batch\n",
      "Epoch 10/20  Iteration 16767/35720 Training loss: 0.8529 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 16768/35720 Training loss: 0.8528 0.2059 sec/batch\n",
      "Epoch 10/20  Iteration 16769/35720 Training loss: 0.8528 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16770/35720 Training loss: 0.8528 0.2248 sec/batch\n",
      "Epoch 10/20  Iteration 16771/35720 Training loss: 0.8529 0.2216 sec/batch\n",
      "Epoch 10/20  Iteration 16772/35720 Training loss: 0.8529 0.2275 sec/batch\n",
      "Epoch 10/20  Iteration 16773/35720 Training loss: 0.8528 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16774/35720 Training loss: 0.8528 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 16775/35720 Training loss: 0.8527 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 16776/35720 Training loss: 0.8528 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 16777/35720 Training loss: 0.8528 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 16778/35720 Training loss: 0.8528 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 16779/35720 Training loss: 0.8528 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 16780/35720 Training loss: 0.8529 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 16781/35720 Training loss: 0.8530 0.2338 sec/batch\n",
      "Epoch 10/20  Iteration 16782/35720 Training loss: 0.8530 0.2135 sec/batch\n",
      "Epoch 10/20  Iteration 16783/35720 Training loss: 0.8531 0.2139 sec/batch\n",
      "Epoch 10/20  Iteration 16784/35720 Training loss: 0.8531 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 16785/35720 Training loss: 0.8532 0.2192 sec/batch\n",
      "Epoch 10/20  Iteration 16786/35720 Training loss: 0.8532 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 16787/35720 Training loss: 0.8533 0.2207 sec/batch\n",
      "Epoch 10/20  Iteration 16788/35720 Training loss: 0.8533 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 16789/35720 Training loss: 0.8534 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 16790/35720 Training loss: 0.8534 0.2110 sec/batch\n",
      "Epoch 10/20  Iteration 16791/35720 Training loss: 0.8536 0.2216 sec/batch\n",
      "Epoch 10/20  Iteration 16792/35720 Training loss: 0.8536 0.2298 sec/batch\n",
      "Epoch 10/20  Iteration 16793/35720 Training loss: 0.8535 0.2139 sec/batch\n",
      "Epoch 10/20  Iteration 16794/35720 Training loss: 0.8536 0.2307 sec/batch\n",
      "Epoch 10/20  Iteration 16795/35720 Training loss: 0.8536 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 16796/35720 Training loss: 0.8537 0.2048 sec/batch\n",
      "Epoch 10/20  Iteration 16797/35720 Training loss: 0.8538 0.2201 sec/batch\n",
      "Epoch 10/20  Iteration 16798/35720 Training loss: 0.8537 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 16799/35720 Training loss: 0.8538 0.2221 sec/batch\n",
      "Epoch 10/20  Iteration 16800/35720 Training loss: 0.8538 0.2071 sec/batch\n",
      "Validation loss: 1.38751 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 16801/35720 Training loss: 0.8542 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16802/35720 Training loss: 0.8543 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 16803/35720 Training loss: 0.8543 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 16804/35720 Training loss: 0.8543 0.2055 sec/batch\n",
      "Epoch 10/20  Iteration 16805/35720 Training loss: 0.8543 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 16806/35720 Training loss: 0.8542 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 16807/35720 Training loss: 0.8543 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 16808/35720 Training loss: 0.8542 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 16809/35720 Training loss: 0.8542 0.2083 sec/batch\n",
      "Epoch 10/20  Iteration 16810/35720 Training loss: 0.8542 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 16811/35720 Training loss: 0.8543 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 16812/35720 Training loss: 0.8542 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 16813/35720 Training loss: 0.8543 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16814/35720 Training loss: 0.8543 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 16815/35720 Training loss: 0.8543 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 16816/35720 Training loss: 0.8543 0.2177 sec/batch\n",
      "Epoch 10/20  Iteration 16817/35720 Training loss: 0.8543 0.2211 sec/batch\n",
      "Epoch 10/20  Iteration 16818/35720 Training loss: 0.8543 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 16819/35720 Training loss: 0.8543 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 16820/35720 Training loss: 0.8544 0.2120 sec/batch\n",
      "Epoch 10/20  Iteration 16821/35720 Training loss: 0.8543 0.2300 sec/batch\n",
      "Epoch 10/20  Iteration 16822/35720 Training loss: 0.8543 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16823/35720 Training loss: 0.8542 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16824/35720 Training loss: 0.8542 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16825/35720 Training loss: 0.8542 0.2227 sec/batch\n",
      "Epoch 10/20  Iteration 16826/35720 Training loss: 0.8543 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 16827/35720 Training loss: 0.8542 0.2192 sec/batch\n",
      "Epoch 10/20  Iteration 16828/35720 Training loss: 0.8542 0.2201 sec/batch\n",
      "Epoch 10/20  Iteration 16829/35720 Training loss: 0.8541 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 16830/35720 Training loss: 0.8541 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 16831/35720 Training loss: 0.8540 0.2314 sec/batch\n",
      "Epoch 10/20  Iteration 16832/35720 Training loss: 0.8541 0.2149 sec/batch\n",
      "Epoch 10/20  Iteration 16833/35720 Training loss: 0.8542 0.2167 sec/batch\n",
      "Epoch 10/20  Iteration 16834/35720 Training loss: 0.8542 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 16835/35720 Training loss: 0.8542 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 16836/35720 Training loss: 0.8543 0.2284 sec/batch\n",
      "Epoch 10/20  Iteration 16837/35720 Training loss: 0.8542 0.2220 sec/batch\n",
      "Epoch 10/20  Iteration 16838/35720 Training loss: 0.8541 0.2206 sec/batch\n",
      "Epoch 10/20  Iteration 16839/35720 Training loss: 0.8542 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 16840/35720 Training loss: 0.8542 0.2171 sec/batch\n",
      "Epoch 10/20  Iteration 16841/35720 Training loss: 0.8542 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 16842/35720 Training loss: 0.8542 0.2256 sec/batch\n",
      "Epoch 10/20  Iteration 16843/35720 Training loss: 0.8542 0.2178 sec/batch\n",
      "Epoch 10/20  Iteration 16844/35720 Training loss: 0.8544 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 16845/35720 Training loss: 0.8544 0.2084 sec/batch\n",
      "Epoch 10/20  Iteration 16846/35720 Training loss: 0.8546 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16847/35720 Training loss: 0.8545 0.2284 sec/batch\n",
      "Epoch 10/20  Iteration 16848/35720 Training loss: 0.8544 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 16849/35720 Training loss: 0.8543 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 16850/35720 Training loss: 0.8543 0.2078 sec/batch\n",
      "Epoch 10/20  Iteration 16851/35720 Training loss: 0.8543 0.2050 sec/batch\n",
      "Epoch 10/20  Iteration 16852/35720 Training loss: 0.8543 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 16853/35720 Training loss: 0.8543 0.2202 sec/batch\n",
      "Epoch 10/20  Iteration 16854/35720 Training loss: 0.8543 0.2126 sec/batch\n",
      "Epoch 10/20  Iteration 16855/35720 Training loss: 0.8543 0.2261 sec/batch\n",
      "Epoch 10/20  Iteration 16856/35720 Training loss: 0.8543 0.2265 sec/batch\n",
      "Epoch 10/20  Iteration 16857/35720 Training loss: 0.8543 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 16858/35720 Training loss: 0.8543 0.2363 sec/batch\n",
      "Epoch 10/20  Iteration 16859/35720 Training loss: 0.8542 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16860/35720 Training loss: 0.8542 0.2203 sec/batch\n",
      "Epoch 10/20  Iteration 16861/35720 Training loss: 0.8542 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 16862/35720 Training loss: 0.8542 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 16863/35720 Training loss: 0.8543 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 16864/35720 Training loss: 0.8542 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 16865/35720 Training loss: 0.8543 0.2193 sec/batch\n",
      "Epoch 10/20  Iteration 16866/35720 Training loss: 0.8543 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 16867/35720 Training loss: 0.8543 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 16868/35720 Training loss: 0.8542 0.2176 sec/batch\n",
      "Epoch 10/20  Iteration 16869/35720 Training loss: 0.8543 0.2185 sec/batch\n",
      "Epoch 10/20  Iteration 16870/35720 Training loss: 0.8543 0.2185 sec/batch\n",
      "Epoch 10/20  Iteration 16871/35720 Training loss: 0.8543 0.2157 sec/batch\n",
      "Epoch 10/20  Iteration 16872/35720 Training loss: 0.8542 0.2059 sec/batch\n",
      "Epoch 10/20  Iteration 16873/35720 Training loss: 0.8542 0.2148 sec/batch\n",
      "Epoch 10/20  Iteration 16874/35720 Training loss: 0.8542 0.2193 sec/batch\n",
      "Epoch 10/20  Iteration 16875/35720 Training loss: 0.8542 0.2149 sec/batch\n",
      "Epoch 10/20  Iteration 16876/35720 Training loss: 0.8543 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 16877/35720 Training loss: 0.8543 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 16878/35720 Training loss: 0.8544 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 16879/35720 Training loss: 0.8544 0.2170 sec/batch\n",
      "Epoch 10/20  Iteration 16880/35720 Training loss: 0.8544 0.2610 sec/batch\n",
      "Epoch 10/20  Iteration 16881/35720 Training loss: 0.8544 0.2214 sec/batch\n",
      "Epoch 10/20  Iteration 16882/35720 Training loss: 0.8544 0.2182 sec/batch\n",
      "Epoch 10/20  Iteration 16883/35720 Training loss: 0.8545 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 16884/35720 Training loss: 0.8545 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 16885/35720 Training loss: 0.8545 0.2272 sec/batch\n",
      "Epoch 10/20  Iteration 16886/35720 Training loss: 0.8545 0.2234 sec/batch\n",
      "Epoch 10/20  Iteration 16887/35720 Training loss: 0.8546 0.2203 sec/batch\n",
      "Epoch 10/20  Iteration 16888/35720 Training loss: 0.8546 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 16889/35720 Training loss: 0.8547 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 16890/35720 Training loss: 0.8546 0.2407 sec/batch\n",
      "Epoch 10/20  Iteration 16891/35720 Training loss: 0.8546 0.2373 sec/batch\n",
      "Epoch 10/20  Iteration 16892/35720 Training loss: 0.8547 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 16893/35720 Training loss: 0.8546 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 16894/35720 Training loss: 0.8546 0.2231 sec/batch\n",
      "Epoch 10/20  Iteration 16895/35720 Training loss: 0.8546 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 16896/35720 Training loss: 0.8546 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16897/35720 Training loss: 0.8545 0.2272 sec/batch\n",
      "Epoch 10/20  Iteration 16898/35720 Training loss: 0.8544 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 16899/35720 Training loss: 0.8543 0.2230 sec/batch\n",
      "Epoch 10/20  Iteration 16900/35720 Training loss: 0.8543 0.2308 sec/batch\n",
      "Epoch 10/20  Iteration 16901/35720 Training loss: 0.8542 0.2128 sec/batch\n",
      "Epoch 10/20  Iteration 16902/35720 Training loss: 0.8541 0.2149 sec/batch\n",
      "Epoch 10/20  Iteration 16903/35720 Training loss: 0.8541 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 16904/35720 Training loss: 0.8541 0.2271 sec/batch\n",
      "Epoch 10/20  Iteration 16905/35720 Training loss: 0.8540 0.2181 sec/batch\n",
      "Epoch 10/20  Iteration 16906/35720 Training loss: 0.8540 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 16907/35720 Training loss: 0.8541 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 16908/35720 Training loss: 0.8541 0.2223 sec/batch\n",
      "Epoch 10/20  Iteration 16909/35720 Training loss: 0.8541 0.2305 sec/batch\n",
      "Epoch 10/20  Iteration 16910/35720 Training loss: 0.8540 0.2058 sec/batch\n",
      "Epoch 10/20  Iteration 16911/35720 Training loss: 0.8540 0.2054 sec/batch\n",
      "Epoch 10/20  Iteration 16912/35720 Training loss: 0.8540 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 16913/35720 Training loss: 0.8540 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 16914/35720 Training loss: 0.8540 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 16915/35720 Training loss: 0.8541 0.2302 sec/batch\n",
      "Epoch 10/20  Iteration 16916/35720 Training loss: 0.8540 0.2254 sec/batch\n",
      "Epoch 10/20  Iteration 16917/35720 Training loss: 0.8540 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 16918/35720 Training loss: 0.8540 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 16919/35720 Training loss: 0.8540 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 16920/35720 Training loss: 0.8540 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16921/35720 Training loss: 0.8539 0.2228 sec/batch\n",
      "Epoch 10/20  Iteration 16922/35720 Training loss: 0.8540 0.2248 sec/batch\n",
      "Epoch 10/20  Iteration 16923/35720 Training loss: 0.8539 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 16924/35720 Training loss: 0.8540 0.2123 sec/batch\n",
      "Epoch 10/20  Iteration 16925/35720 Training loss: 0.8539 0.2157 sec/batch\n",
      "Epoch 10/20  Iteration 16926/35720 Training loss: 0.8539 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 16927/35720 Training loss: 0.8539 0.2185 sec/batch\n",
      "Epoch 10/20  Iteration 16928/35720 Training loss: 0.8539 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 16929/35720 Training loss: 0.8540 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 16930/35720 Training loss: 0.8539 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 16931/35720 Training loss: 0.8538 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 16932/35720 Training loss: 0.8539 0.2139 sec/batch\n",
      "Epoch 10/20  Iteration 16933/35720 Training loss: 0.8538 0.2056 sec/batch\n",
      "Epoch 10/20  Iteration 16934/35720 Training loss: 0.8538 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 16935/35720 Training loss: 0.8538 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 16936/35720 Training loss: 0.8537 0.2183 sec/batch\n",
      "Epoch 10/20  Iteration 16937/35720 Training loss: 0.8538 0.2186 sec/batch\n",
      "Epoch 10/20  Iteration 16938/35720 Training loss: 0.8537 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 16939/35720 Training loss: 0.8536 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 16940/35720 Training loss: 0.8537 0.2124 sec/batch\n",
      "Epoch 10/20  Iteration 16941/35720 Training loss: 0.8537 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 16942/35720 Training loss: 0.8537 0.2281 sec/batch\n",
      "Epoch 10/20  Iteration 16943/35720 Training loss: 0.8537 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 16944/35720 Training loss: 0.8537 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 16945/35720 Training loss: 0.8536 0.2058 sec/batch\n",
      "Epoch 10/20  Iteration 16946/35720 Training loss: 0.8536 0.2100 sec/batch\n",
      "Epoch 10/20  Iteration 16947/35720 Training loss: 0.8535 0.2209 sec/batch\n",
      "Epoch 10/20  Iteration 16948/35720 Training loss: 0.8535 0.2190 sec/batch\n",
      "Epoch 10/20  Iteration 16949/35720 Training loss: 0.8535 0.2196 sec/batch\n",
      "Epoch 10/20  Iteration 16950/35720 Training loss: 0.8534 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 16951/35720 Training loss: 0.8534 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 16952/35720 Training loss: 0.8534 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 16953/35720 Training loss: 0.8534 0.2269 sec/batch\n",
      "Epoch 10/20  Iteration 16954/35720 Training loss: 0.8533 0.2235 sec/batch\n",
      "Epoch 10/20  Iteration 16955/35720 Training loss: 0.8533 0.2236 sec/batch\n",
      "Epoch 10/20  Iteration 16956/35720 Training loss: 0.8533 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 16957/35720 Training loss: 0.8533 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 16958/35720 Training loss: 0.8533 0.2186 sec/batch\n",
      "Epoch 10/20  Iteration 16959/35720 Training loss: 0.8533 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 16960/35720 Training loss: 0.8532 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 16961/35720 Training loss: 0.8532 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 16962/35720 Training loss: 0.8532 0.2110 sec/batch\n",
      "Epoch 10/20  Iteration 16963/35720 Training loss: 0.8531 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 16964/35720 Training loss: 0.8530 0.2287 sec/batch\n",
      "Epoch 10/20  Iteration 16965/35720 Training loss: 0.8529 0.2081 sec/batch\n",
      "Epoch 10/20  Iteration 16966/35720 Training loss: 0.8529 0.2167 sec/batch\n",
      "Epoch 10/20  Iteration 16967/35720 Training loss: 0.8528 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 16968/35720 Training loss: 0.8527 0.2113 sec/batch\n",
      "Epoch 10/20  Iteration 16969/35720 Training loss: 0.8527 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 16970/35720 Training loss: 0.8527 0.2282 sec/batch\n",
      "Epoch 10/20  Iteration 16971/35720 Training loss: 0.8526 0.2215 sec/batch\n",
      "Epoch 10/20  Iteration 16972/35720 Training loss: 0.8525 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 16973/35720 Training loss: 0.8524 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 16974/35720 Training loss: 0.8524 0.2290 sec/batch\n",
      "Epoch 10/20  Iteration 16975/35720 Training loss: 0.8523 0.2193 sec/batch\n",
      "Epoch 10/20  Iteration 16976/35720 Training loss: 0.8522 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 16977/35720 Training loss: 0.8521 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 16978/35720 Training loss: 0.8521 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 16979/35720 Training loss: 0.8520 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 16980/35720 Training loss: 0.8520 0.2237 sec/batch\n",
      "Epoch 10/20  Iteration 16981/35720 Training loss: 0.8520 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 16982/35720 Training loss: 0.8520 0.2355 sec/batch\n",
      "Epoch 10/20  Iteration 16983/35720 Training loss: 0.8520 0.2100 sec/batch\n",
      "Epoch 10/20  Iteration 16984/35720 Training loss: 0.8520 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 16985/35720 Training loss: 0.8520 0.2122 sec/batch\n",
      "Epoch 10/20  Iteration 16986/35720 Training loss: 0.8520 0.2264 sec/batch\n",
      "Epoch 10/20  Iteration 16987/35720 Training loss: 0.8520 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 16988/35720 Training loss: 0.8521 0.2249 sec/batch\n",
      "Epoch 10/20  Iteration 16989/35720 Training loss: 0.8521 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 16990/35720 Training loss: 0.8522 0.2076 sec/batch\n",
      "Epoch 10/20  Iteration 16991/35720 Training loss: 0.8522 0.2184 sec/batch\n",
      "Epoch 10/20  Iteration 16992/35720 Training loss: 0.8522 0.2083 sec/batch\n",
      "Epoch 10/20  Iteration 16993/35720 Training loss: 0.8521 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 16994/35720 Training loss: 0.8522 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 16995/35720 Training loss: 0.8521 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 16996/35720 Training loss: 0.8521 0.2235 sec/batch\n",
      "Epoch 10/20  Iteration 16997/35720 Training loss: 0.8521 0.2180 sec/batch\n",
      "Epoch 10/20  Iteration 16998/35720 Training loss: 0.8521 0.2176 sec/batch\n",
      "Epoch 10/20  Iteration 16999/35720 Training loss: 0.8521 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 17000/35720 Training loss: 0.8521 0.2194 sec/batch\n",
      "Validation loss: 1.41074 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 17001/35720 Training loss: 0.8524 0.2266 sec/batch\n",
      "Epoch 10/20  Iteration 17002/35720 Training loss: 0.8523 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 17003/35720 Training loss: 0.8525 0.2195 sec/batch\n",
      "Epoch 10/20  Iteration 17004/35720 Training loss: 0.8525 0.2059 sec/batch\n",
      "Epoch 10/20  Iteration 17005/35720 Training loss: 0.8525 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 17006/35720 Training loss: 0.8525 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 17007/35720 Training loss: 0.8525 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 17008/35720 Training loss: 0.8525 0.2254 sec/batch\n",
      "Epoch 10/20  Iteration 17009/35720 Training loss: 0.8524 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 17010/35720 Training loss: 0.8522 0.2056 sec/batch\n",
      "Epoch 10/20  Iteration 17011/35720 Training loss: 0.8522 0.2149 sec/batch\n",
      "Epoch 10/20  Iteration 17012/35720 Training loss: 0.8522 0.2135 sec/batch\n",
      "Epoch 10/20  Iteration 17013/35720 Training loss: 0.8521 0.2128 sec/batch\n",
      "Epoch 10/20  Iteration 17014/35720 Training loss: 0.8520 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 17015/35720 Training loss: 0.8519 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 17016/35720 Training loss: 0.8519 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 17017/35720 Training loss: 0.8519 0.2135 sec/batch\n",
      "Epoch 10/20  Iteration 17018/35720 Training loss: 0.8519 0.2239 sec/batch\n",
      "Epoch 10/20  Iteration 17019/35720 Training loss: 0.8518 0.2299 sec/batch\n",
      "Epoch 10/20  Iteration 17020/35720 Training loss: 0.8518 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17021/35720 Training loss: 0.8518 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 17022/35720 Training loss: 0.8518 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 17023/35720 Training loss: 0.8518 0.2139 sec/batch\n",
      "Epoch 10/20  Iteration 17024/35720 Training loss: 0.8517 0.2139 sec/batch\n",
      "Epoch 10/20  Iteration 17025/35720 Training loss: 0.8517 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 17026/35720 Training loss: 0.8516 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 17027/35720 Training loss: 0.8515 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 17028/35720 Training loss: 0.8515 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 17029/35720 Training loss: 0.8514 0.2166 sec/batch\n",
      "Epoch 10/20  Iteration 17030/35720 Training loss: 0.8514 0.2167 sec/batch\n",
      "Epoch 10/20  Iteration 17031/35720 Training loss: 0.8513 0.2124 sec/batch\n",
      "Epoch 10/20  Iteration 17032/35720 Training loss: 0.8513 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 17033/35720 Training loss: 0.8512 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17034/35720 Training loss: 0.8512 0.2276 sec/batch\n",
      "Epoch 10/20  Iteration 17035/35720 Training loss: 0.8511 0.2213 sec/batch\n",
      "Epoch 10/20  Iteration 17036/35720 Training loss: 0.8510 0.2080 sec/batch\n",
      "Epoch 10/20  Iteration 17037/35720 Training loss: 0.8510 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 17038/35720 Training loss: 0.8510 0.2062 sec/batch\n",
      "Epoch 10/20  Iteration 17039/35720 Training loss: 0.8509 0.2169 sec/batch\n",
      "Epoch 10/20  Iteration 17040/35720 Training loss: 0.8509 0.2142 sec/batch\n",
      "Epoch 10/20  Iteration 17041/35720 Training loss: 0.8509 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 17042/35720 Training loss: 0.8508 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 17043/35720 Training loss: 0.8509 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 17044/35720 Training loss: 0.8511 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 17045/35720 Training loss: 0.8510 0.2258 sec/batch\n",
      "Epoch 10/20  Iteration 17046/35720 Training loss: 0.8509 0.2199 sec/batch\n",
      "Epoch 10/20  Iteration 17047/35720 Training loss: 0.8509 0.2194 sec/batch\n",
      "Epoch 10/20  Iteration 17048/35720 Training loss: 0.8508 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 17049/35720 Training loss: 0.8507 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 17050/35720 Training loss: 0.8507 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 17051/35720 Training loss: 0.8507 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 17052/35720 Training loss: 0.8508 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 17053/35720 Training loss: 0.8507 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 17054/35720 Training loss: 0.8507 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 17055/35720 Training loss: 0.8507 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 17056/35720 Training loss: 0.8506 0.2132 sec/batch\n",
      "Epoch 10/20  Iteration 17057/35720 Training loss: 0.8505 0.2453 sec/batch\n",
      "Epoch 10/20  Iteration 17058/35720 Training loss: 0.8505 0.2095 sec/batch\n",
      "Epoch 10/20  Iteration 17059/35720 Training loss: 0.8505 0.2181 sec/batch\n",
      "Epoch 10/20  Iteration 17060/35720 Training loss: 0.8505 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 17061/35720 Training loss: 0.8505 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 17062/35720 Training loss: 0.8504 0.2248 sec/batch\n",
      "Epoch 10/20  Iteration 17063/35720 Training loss: 0.8504 0.2209 sec/batch\n",
      "Epoch 10/20  Iteration 17064/35720 Training loss: 0.8504 0.2197 sec/batch\n",
      "Epoch 10/20  Iteration 17065/35720 Training loss: 0.8503 0.2296 sec/batch\n",
      "Epoch 10/20  Iteration 17066/35720 Training loss: 0.8503 0.2075 sec/batch\n",
      "Epoch 10/20  Iteration 17067/35720 Training loss: 0.8503 0.2078 sec/batch\n",
      "Epoch 10/20  Iteration 17068/35720 Training loss: 0.8503 0.2287 sec/batch\n",
      "Epoch 10/20  Iteration 17069/35720 Training loss: 0.8504 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 17070/35720 Training loss: 0.8504 0.2232 sec/batch\n",
      "Epoch 10/20  Iteration 17071/35720 Training loss: 0.8504 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 17072/35720 Training loss: 0.8505 0.2252 sec/batch\n",
      "Epoch 10/20  Iteration 17073/35720 Training loss: 0.8504 0.2084 sec/batch\n",
      "Epoch 10/20  Iteration 17074/35720 Training loss: 0.8504 0.2261 sec/batch\n",
      "Epoch 10/20  Iteration 17075/35720 Training loss: 0.8503 0.2124 sec/batch\n",
      "Epoch 10/20  Iteration 17076/35720 Training loss: 0.8502 0.2183 sec/batch\n",
      "Epoch 10/20  Iteration 17077/35720 Training loss: 0.8501 0.2260 sec/batch\n",
      "Epoch 10/20  Iteration 17078/35720 Training loss: 0.8501 0.2077 sec/batch\n",
      "Epoch 10/20  Iteration 17079/35720 Training loss: 0.8500 0.2127 sec/batch\n",
      "Epoch 10/20  Iteration 17080/35720 Training loss: 0.8499 0.2191 sec/batch\n",
      "Epoch 10/20  Iteration 17081/35720 Training loss: 0.8499 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17082/35720 Training loss: 0.8499 0.2267 sec/batch\n",
      "Epoch 10/20  Iteration 17083/35720 Training loss: 0.8498 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 17084/35720 Training loss: 0.8497 0.2125 sec/batch\n",
      "Epoch 10/20  Iteration 17085/35720 Training loss: 0.8497 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 17086/35720 Training loss: 0.8497 0.2100 sec/batch\n",
      "Epoch 10/20  Iteration 17087/35720 Training loss: 0.8496 0.2224 sec/batch\n",
      "Epoch 10/20  Iteration 17088/35720 Training loss: 0.8496 0.2189 sec/batch\n",
      "Epoch 10/20  Iteration 17089/35720 Training loss: 0.8496 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 17090/35720 Training loss: 0.8496 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17091/35720 Training loss: 0.8496 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 17092/35720 Training loss: 0.8495 0.2122 sec/batch\n",
      "Epoch 10/20  Iteration 17093/35720 Training loss: 0.8495 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 17094/35720 Training loss: 0.8494 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 17095/35720 Training loss: 0.8494 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17096/35720 Training loss: 0.8495 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 17097/35720 Training loss: 0.8496 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17098/35720 Training loss: 0.8495 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 17099/35720 Training loss: 0.8496 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 17100/35720 Training loss: 0.8495 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 17101/35720 Training loss: 0.8495 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 17102/35720 Training loss: 0.8495 0.2267 sec/batch\n",
      "Epoch 10/20  Iteration 17103/35720 Training loss: 0.8494 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 17104/35720 Training loss: 0.8494 0.2237 sec/batch\n",
      "Epoch 10/20  Iteration 17105/35720 Training loss: 0.8494 0.2469 sec/batch\n",
      "Epoch 10/20  Iteration 17106/35720 Training loss: 0.8494 0.2189 sec/batch\n",
      "Epoch 10/20  Iteration 17107/35720 Training loss: 0.8494 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 17108/35720 Training loss: 0.8494 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 17109/35720 Training loss: 0.8494 0.2205 sec/batch\n",
      "Epoch 10/20  Iteration 17110/35720 Training loss: 0.8494 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17111/35720 Training loss: 0.8495 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17112/35720 Training loss: 0.8494 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 17113/35720 Training loss: 0.8494 0.2254 sec/batch\n",
      "Epoch 10/20  Iteration 17114/35720 Training loss: 0.8494 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 17115/35720 Training loss: 0.8495 0.2125 sec/batch\n",
      "Epoch 10/20  Iteration 17116/35720 Training loss: 0.8494 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 17117/35720 Training loss: 0.8494 0.2175 sec/batch\n",
      "Epoch 10/20  Iteration 17118/35720 Training loss: 0.8494 0.2145 sec/batch\n",
      "Epoch 10/20  Iteration 17119/35720 Training loss: 0.8494 0.2127 sec/batch\n",
      "Epoch 10/20  Iteration 17120/35720 Training loss: 0.8494 0.2207 sec/batch\n",
      "Epoch 10/20  Iteration 17121/35720 Training loss: 0.8494 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17122/35720 Training loss: 0.8494 0.2073 sec/batch\n",
      "Epoch 10/20  Iteration 17123/35720 Training loss: 0.8494 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17124/35720 Training loss: 0.8494 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 17125/35720 Training loss: 0.8493 0.2132 sec/batch\n",
      "Epoch 10/20  Iteration 17126/35720 Training loss: 0.8494 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 17127/35720 Training loss: 0.8495 0.2172 sec/batch\n",
      "Epoch 10/20  Iteration 17128/35720 Training loss: 0.8495 0.2060 sec/batch\n",
      "Epoch 10/20  Iteration 17129/35720 Training loss: 0.8495 0.2182 sec/batch\n",
      "Epoch 10/20  Iteration 17130/35720 Training loss: 0.8495 0.2254 sec/batch\n",
      "Epoch 10/20  Iteration 17131/35720 Training loss: 0.8495 0.2207 sec/batch\n",
      "Epoch 10/20  Iteration 17132/35720 Training loss: 0.8496 0.2250 sec/batch\n",
      "Epoch 10/20  Iteration 17133/35720 Training loss: 0.8496 0.2058 sec/batch\n",
      "Epoch 10/20  Iteration 17134/35720 Training loss: 0.8495 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17135/35720 Training loss: 0.8495 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 17136/35720 Training loss: 0.8495 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 17137/35720 Training loss: 0.8496 0.2191 sec/batch\n",
      "Epoch 10/20  Iteration 17138/35720 Training loss: 0.8496 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 17139/35720 Training loss: 0.8496 0.2062 sec/batch\n",
      "Epoch 10/20  Iteration 17140/35720 Training loss: 0.8495 0.2160 sec/batch\n",
      "Epoch 10/20  Iteration 17141/35720 Training loss: 0.8495 0.2250 sec/batch\n",
      "Epoch 10/20  Iteration 17142/35720 Training loss: 0.8495 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 17143/35720 Training loss: 0.8495 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17144/35720 Training loss: 0.8495 0.2235 sec/batch\n",
      "Epoch 10/20  Iteration 17145/35720 Training loss: 0.8495 0.2084 sec/batch\n",
      "Epoch 10/20  Iteration 17146/35720 Training loss: 0.8495 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 17147/35720 Training loss: 0.8495 0.2140 sec/batch\n",
      "Epoch 10/20  Iteration 17148/35720 Training loss: 0.8495 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17149/35720 Training loss: 0.8496 0.2169 sec/batch\n",
      "Epoch 10/20  Iteration 17150/35720 Training loss: 0.8496 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 17151/35720 Training loss: 0.8496 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 17152/35720 Training loss: 0.8495 0.2194 sec/batch\n",
      "Epoch 10/20  Iteration 17153/35720 Training loss: 0.8495 0.2136 sec/batch\n",
      "Epoch 10/20  Iteration 17154/35720 Training loss: 0.8495 0.2334 sec/batch\n",
      "Epoch 10/20  Iteration 17155/35720 Training loss: 0.8495 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 17156/35720 Training loss: 0.8494 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 17157/35720 Training loss: 0.8495 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 17158/35720 Training loss: 0.8494 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 17159/35720 Training loss: 0.8494 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 17160/35720 Training loss: 0.8494 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 17161/35720 Training loss: 0.8494 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 17162/35720 Training loss: 0.8495 0.2110 sec/batch\n",
      "Epoch 10/20  Iteration 17163/35720 Training loss: 0.8495 0.2345 sec/batch\n",
      "Epoch 10/20  Iteration 17164/35720 Training loss: 0.8495 0.2135 sec/batch\n",
      "Epoch 10/20  Iteration 17165/35720 Training loss: 0.8495 0.2268 sec/batch\n",
      "Epoch 10/20  Iteration 17166/35720 Training loss: 0.8495 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17167/35720 Training loss: 0.8495 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 17168/35720 Training loss: 0.8495 0.2078 sec/batch\n",
      "Epoch 10/20  Iteration 17169/35720 Training loss: 0.8495 0.2211 sec/batch\n",
      "Epoch 10/20  Iteration 17170/35720 Training loss: 0.8494 0.2198 sec/batch\n",
      "Epoch 10/20  Iteration 17171/35720 Training loss: 0.8494 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 17172/35720 Training loss: 0.8494 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 17173/35720 Training loss: 0.8494 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 17174/35720 Training loss: 0.8494 0.2176 sec/batch\n",
      "Epoch 10/20  Iteration 17175/35720 Training loss: 0.8495 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 17176/35720 Training loss: 0.8494 0.2255 sec/batch\n",
      "Epoch 10/20  Iteration 17177/35720 Training loss: 0.8495 0.2228 sec/batch\n",
      "Epoch 10/20  Iteration 17178/35720 Training loss: 0.8495 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 17179/35720 Training loss: 0.8495 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17180/35720 Training loss: 0.8494 0.2243 sec/batch\n",
      "Epoch 10/20  Iteration 17181/35720 Training loss: 0.8494 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 17182/35720 Training loss: 0.8495 0.2285 sec/batch\n",
      "Epoch 10/20  Iteration 17183/35720 Training loss: 0.8494 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 17184/35720 Training loss: 0.8495 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 17185/35720 Training loss: 0.8494 0.2159 sec/batch\n",
      "Epoch 10/20  Iteration 17186/35720 Training loss: 0.8494 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 17187/35720 Training loss: 0.8493 0.2220 sec/batch\n",
      "Epoch 10/20  Iteration 17188/35720 Training loss: 0.8493 0.2261 sec/batch\n",
      "Epoch 10/20  Iteration 17189/35720 Training loss: 0.8493 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17190/35720 Training loss: 0.8493 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 17191/35720 Training loss: 0.8493 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 17192/35720 Training loss: 0.8492 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17193/35720 Training loss: 0.8492 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 17194/35720 Training loss: 0.8493 0.2056 sec/batch\n",
      "Epoch 10/20  Iteration 17195/35720 Training loss: 0.8493 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17196/35720 Training loss: 0.8493 0.2316 sec/batch\n",
      "Epoch 10/20  Iteration 17197/35720 Training loss: 0.8494 0.2199 sec/batch\n",
      "Epoch 10/20  Iteration 17198/35720 Training loss: 0.8494 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 17199/35720 Training loss: 0.8494 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17200/35720 Training loss: 0.8494 0.2098 sec/batch\n",
      "Validation loss: 1.4023 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 17201/35720 Training loss: 0.8497 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 17202/35720 Training loss: 0.8497 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 17203/35720 Training loss: 0.8496 0.2339 sec/batch\n",
      "Epoch 10/20  Iteration 17204/35720 Training loss: 0.8497 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 17205/35720 Training loss: 0.8496 0.2059 sec/batch\n",
      "Epoch 10/20  Iteration 17206/35720 Training loss: 0.8496 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 17207/35720 Training loss: 0.8495 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17208/35720 Training loss: 0.8496 0.2181 sec/batch\n",
      "Epoch 10/20  Iteration 17209/35720 Training loss: 0.8495 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 17210/35720 Training loss: 0.8495 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 17211/35720 Training loss: 0.8494 0.2291 sec/batch\n",
      "Epoch 10/20  Iteration 17212/35720 Training loss: 0.8494 0.2073 sec/batch\n",
      "Epoch 10/20  Iteration 17213/35720 Training loss: 0.8493 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 17214/35720 Training loss: 0.8493 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 17215/35720 Training loss: 0.8493 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17216/35720 Training loss: 0.8493 0.2069 sec/batch\n",
      "Epoch 10/20  Iteration 17217/35720 Training loss: 0.8493 0.2079 sec/batch\n",
      "Epoch 10/20  Iteration 17218/35720 Training loss: 0.8493 0.2389 sec/batch\n",
      "Epoch 10/20  Iteration 17219/35720 Training loss: 0.8493 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 17220/35720 Training loss: 0.8492 0.2225 sec/batch\n",
      "Epoch 10/20  Iteration 17221/35720 Training loss: 0.8492 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 17222/35720 Training loss: 0.8491 0.2071 sec/batch\n",
      "Epoch 10/20  Iteration 17223/35720 Training loss: 0.8491 0.2348 sec/batch\n",
      "Epoch 10/20  Iteration 17224/35720 Training loss: 0.8490 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 17225/35720 Training loss: 0.8491 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 17226/35720 Training loss: 0.8491 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 17227/35720 Training loss: 0.8491 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 17228/35720 Training loss: 0.8491 0.2198 sec/batch\n",
      "Epoch 10/20  Iteration 17229/35720 Training loss: 0.8491 0.2176 sec/batch\n",
      "Epoch 10/20  Iteration 17230/35720 Training loss: 0.8491 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 17231/35720 Training loss: 0.8491 0.2221 sec/batch\n",
      "Epoch 10/20  Iteration 17232/35720 Training loss: 0.8490 0.2059 sec/batch\n",
      "Epoch 10/20  Iteration 17233/35720 Training loss: 0.8490 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 17234/35720 Training loss: 0.8490 0.2223 sec/batch\n",
      "Epoch 10/20  Iteration 17235/35720 Training loss: 0.8489 0.2171 sec/batch\n",
      "Epoch 10/20  Iteration 17236/35720 Training loss: 0.8489 0.2227 sec/batch\n",
      "Epoch 10/20  Iteration 17237/35720 Training loss: 0.8490 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 17238/35720 Training loss: 0.8490 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17239/35720 Training loss: 0.8490 0.2083 sec/batch\n",
      "Epoch 10/20  Iteration 17240/35720 Training loss: 0.8490 0.2311 sec/batch\n",
      "Epoch 10/20  Iteration 17241/35720 Training loss: 0.8489 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 17242/35720 Training loss: 0.8489 0.2291 sec/batch\n",
      "Epoch 10/20  Iteration 17243/35720 Training loss: 0.8490 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17244/35720 Training loss: 0.8490 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 17245/35720 Training loss: 0.8490 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 17246/35720 Training loss: 0.8490 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 17247/35720 Training loss: 0.8490 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17248/35720 Training loss: 0.8490 0.2126 sec/batch\n",
      "Epoch 10/20  Iteration 17249/35720 Training loss: 0.8490 0.2105 sec/batch\n",
      "Epoch 10/20  Iteration 17250/35720 Training loss: 0.8490 0.2131 sec/batch\n",
      "Epoch 10/20  Iteration 17251/35720 Training loss: 0.8491 0.2198 sec/batch\n",
      "Epoch 10/20  Iteration 17252/35720 Training loss: 0.8491 0.2378 sec/batch\n",
      "Epoch 10/20  Iteration 17253/35720 Training loss: 0.8491 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 17254/35720 Training loss: 0.8490 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 17255/35720 Training loss: 0.8491 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17256/35720 Training loss: 0.8491 0.2197 sec/batch\n",
      "Epoch 10/20  Iteration 17257/35720 Training loss: 0.8490 0.2219 sec/batch\n",
      "Epoch 10/20  Iteration 17258/35720 Training loss: 0.8489 0.2180 sec/batch\n",
      "Epoch 10/20  Iteration 17259/35720 Training loss: 0.8489 0.2232 sec/batch\n",
      "Epoch 10/20  Iteration 17260/35720 Training loss: 0.8488 0.2062 sec/batch\n",
      "Epoch 10/20  Iteration 17261/35720 Training loss: 0.8489 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 17262/35720 Training loss: 0.8489 0.2224 sec/batch\n",
      "Epoch 10/20  Iteration 17263/35720 Training loss: 0.8488 0.2260 sec/batch\n",
      "Epoch 10/20  Iteration 17264/35720 Training loss: 0.8488 0.2126 sec/batch\n",
      "Epoch 10/20  Iteration 17265/35720 Training loss: 0.8489 0.2201 sec/batch\n",
      "Epoch 10/20  Iteration 17266/35720 Training loss: 0.8489 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 17267/35720 Training loss: 0.8489 0.2149 sec/batch\n",
      "Epoch 10/20  Iteration 17268/35720 Training loss: 0.8488 0.2175 sec/batch\n",
      "Epoch 10/20  Iteration 17269/35720 Training loss: 0.8489 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 17270/35720 Training loss: 0.8489 0.2275 sec/batch\n",
      "Epoch 10/20  Iteration 17271/35720 Training loss: 0.8489 0.2140 sec/batch\n",
      "Epoch 10/20  Iteration 17272/35720 Training loss: 0.8489 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 17273/35720 Training loss: 0.8489 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 17274/35720 Training loss: 0.8489 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 17275/35720 Training loss: 0.8488 0.2265 sec/batch\n",
      "Epoch 10/20  Iteration 17276/35720 Training loss: 0.8487 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 17277/35720 Training loss: 0.8487 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 17278/35720 Training loss: 0.8486 0.2224 sec/batch\n",
      "Epoch 10/20  Iteration 17279/35720 Training loss: 0.8486 0.2275 sec/batch\n",
      "Epoch 10/20  Iteration 17280/35720 Training loss: 0.8486 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 17281/35720 Training loss: 0.8486 0.2258 sec/batch\n",
      "Epoch 10/20  Iteration 17282/35720 Training loss: 0.8486 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17283/35720 Training loss: 0.8486 0.2260 sec/batch\n",
      "Epoch 10/20  Iteration 17284/35720 Training loss: 0.8486 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 17285/35720 Training loss: 0.8485 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17286/35720 Training loss: 0.8485 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 17287/35720 Training loss: 0.8485 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 17288/35720 Training loss: 0.8485 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17289/35720 Training loss: 0.8485 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 17290/35720 Training loss: 0.8485 0.2126 sec/batch\n",
      "Epoch 10/20  Iteration 17291/35720 Training loss: 0.8485 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 17292/35720 Training loss: 0.8484 0.2169 sec/batch\n",
      "Epoch 10/20  Iteration 17293/35720 Training loss: 0.8484 0.2140 sec/batch\n",
      "Epoch 10/20  Iteration 17294/35720 Training loss: 0.8483 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 17295/35720 Training loss: 0.8484 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 17296/35720 Training loss: 0.8484 0.2062 sec/batch\n",
      "Epoch 10/20  Iteration 17297/35720 Training loss: 0.8484 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17298/35720 Training loss: 0.8483 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 17299/35720 Training loss: 0.8483 0.2180 sec/batch\n",
      "Epoch 10/20  Iteration 17300/35720 Training loss: 0.8483 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 17301/35720 Training loss: 0.8483 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 17302/35720 Training loss: 0.8483 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 17303/35720 Training loss: 0.8484 0.2209 sec/batch\n",
      "Epoch 10/20  Iteration 17304/35720 Training loss: 0.8484 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 17305/35720 Training loss: 0.8484 0.2182 sec/batch\n",
      "Epoch 10/20  Iteration 17306/35720 Training loss: 0.8484 0.2265 sec/batch\n",
      "Epoch 10/20  Iteration 17307/35720 Training loss: 0.8484 0.2237 sec/batch\n",
      "Epoch 10/20  Iteration 17308/35720 Training loss: 0.8484 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 17309/35720 Training loss: 0.8483 0.2160 sec/batch\n",
      "Epoch 10/20  Iteration 17310/35720 Training loss: 0.8483 0.2213 sec/batch\n",
      "Epoch 10/20  Iteration 17311/35720 Training loss: 0.8483 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17312/35720 Training loss: 0.8482 0.2330 sec/batch\n",
      "Epoch 10/20  Iteration 17313/35720 Training loss: 0.8481 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 17314/35720 Training loss: 0.8481 0.2298 sec/batch\n",
      "Epoch 10/20  Iteration 17315/35720 Training loss: 0.8480 0.2185 sec/batch\n",
      "Epoch 10/20  Iteration 17316/35720 Training loss: 0.8480 0.2062 sec/batch\n",
      "Epoch 10/20  Iteration 17317/35720 Training loss: 0.8480 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 17318/35720 Training loss: 0.8480 0.2264 sec/batch\n",
      "Epoch 10/20  Iteration 17319/35720 Training loss: 0.8480 0.2241 sec/batch\n",
      "Epoch 10/20  Iteration 17320/35720 Training loss: 0.8479 0.2195 sec/batch\n",
      "Epoch 10/20  Iteration 17321/35720 Training loss: 0.8479 0.2125 sec/batch\n",
      "Epoch 10/20  Iteration 17322/35720 Training loss: 0.8479 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 17323/35720 Training loss: 0.8478 0.2177 sec/batch\n",
      "Epoch 10/20  Iteration 17324/35720 Training loss: 0.8478 0.2241 sec/batch\n",
      "Epoch 10/20  Iteration 17325/35720 Training loss: 0.8478 0.2175 sec/batch\n",
      "Epoch 10/20  Iteration 17326/35720 Training loss: 0.8477 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 17327/35720 Training loss: 0.8477 0.2105 sec/batch\n",
      "Epoch 10/20  Iteration 17328/35720 Training loss: 0.8477 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17329/35720 Training loss: 0.8477 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 17330/35720 Training loss: 0.8477 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 17331/35720 Training loss: 0.8476 0.2196 sec/batch\n",
      "Epoch 10/20  Iteration 17332/35720 Training loss: 0.8476 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 17333/35720 Training loss: 0.8476 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 17334/35720 Training loss: 0.8476 0.2349 sec/batch\n",
      "Epoch 10/20  Iteration 17335/35720 Training loss: 0.8476 0.2218 sec/batch\n",
      "Epoch 10/20  Iteration 17336/35720 Training loss: 0.8475 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 17337/35720 Training loss: 0.8475 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 17338/35720 Training loss: 0.8474 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 17339/35720 Training loss: 0.8474 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 17340/35720 Training loss: 0.8473 0.2251 sec/batch\n",
      "Epoch 10/20  Iteration 17341/35720 Training loss: 0.8473 0.2112 sec/batch\n",
      "Epoch 10/20  Iteration 17342/35720 Training loss: 0.8473 0.2280 sec/batch\n",
      "Epoch 10/20  Iteration 17343/35720 Training loss: 0.8472 0.2242 sec/batch\n",
      "Epoch 10/20  Iteration 17344/35720 Training loss: 0.8472 0.2132 sec/batch\n",
      "Epoch 10/20  Iteration 17345/35720 Training loss: 0.8472 0.2193 sec/batch\n",
      "Epoch 10/20  Iteration 17346/35720 Training loss: 0.8472 0.2120 sec/batch\n",
      "Epoch 10/20  Iteration 17347/35720 Training loss: 0.8471 0.2419 sec/batch\n",
      "Epoch 10/20  Iteration 17348/35720 Training loss: 0.8471 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 17349/35720 Training loss: 0.8471 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 17350/35720 Training loss: 0.8471 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 17351/35720 Training loss: 0.8470 0.2157 sec/batch\n",
      "Epoch 10/20  Iteration 17352/35720 Training loss: 0.8469 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 17353/35720 Training loss: 0.8469 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 17354/35720 Training loss: 0.8468 0.2195 sec/batch\n",
      "Epoch 10/20  Iteration 17355/35720 Training loss: 0.8468 0.2079 sec/batch\n",
      "Epoch 10/20  Iteration 17356/35720 Training loss: 0.8468 0.2059 sec/batch\n",
      "Epoch 10/20  Iteration 17357/35720 Training loss: 0.8467 0.2144 sec/batch\n",
      "Epoch 10/20  Iteration 17358/35720 Training loss: 0.8467 0.2320 sec/batch\n",
      "Epoch 10/20  Iteration 17359/35720 Training loss: 0.8467 0.2149 sec/batch\n",
      "Epoch 10/20  Iteration 17360/35720 Training loss: 0.8466 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 17361/35720 Training loss: 0.8466 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 17362/35720 Training loss: 0.8466 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 17363/35720 Training loss: 0.8465 0.2169 sec/batch\n",
      "Epoch 10/20  Iteration 17364/35720 Training loss: 0.8466 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17365/35720 Training loss: 0.8465 0.2233 sec/batch\n",
      "Epoch 10/20  Iteration 17366/35720 Training loss: 0.8464 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 17367/35720 Training loss: 0.8465 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 17368/35720 Training loss: 0.8465 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17369/35720 Training loss: 0.8465 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17370/35720 Training loss: 0.8465 0.2307 sec/batch\n",
      "Epoch 10/20  Iteration 17371/35720 Training loss: 0.8465 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 17372/35720 Training loss: 0.8465 0.2084 sec/batch\n",
      "Epoch 10/20  Iteration 17373/35720 Training loss: 0.8464 0.2120 sec/batch\n",
      "Epoch 10/20  Iteration 17374/35720 Training loss: 0.8464 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 17375/35720 Training loss: 0.8463 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 17376/35720 Training loss: 0.8463 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 17377/35720 Training loss: 0.8463 0.2149 sec/batch\n",
      "Epoch 10/20  Iteration 17378/35720 Training loss: 0.8462 0.2136 sec/batch\n",
      "Epoch 10/20  Iteration 17379/35720 Training loss: 0.8462 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17380/35720 Training loss: 0.8462 0.2125 sec/batch\n",
      "Epoch 10/20  Iteration 17381/35720 Training loss: 0.8462 0.2315 sec/batch\n",
      "Epoch 10/20  Iteration 17382/35720 Training loss: 0.8462 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 17383/35720 Training loss: 0.8461 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17384/35720 Training loss: 0.8461 0.2071 sec/batch\n",
      "Epoch 10/20  Iteration 17385/35720 Training loss: 0.8460 0.2144 sec/batch\n",
      "Epoch 10/20  Iteration 17386/35720 Training loss: 0.8460 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17387/35720 Training loss: 0.8461 0.2162 sec/batch\n",
      "Epoch 10/20  Iteration 17388/35720 Training loss: 0.8460 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 17389/35720 Training loss: 0.8460 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 17390/35720 Training loss: 0.8460 0.2172 sec/batch\n",
      "Epoch 10/20  Iteration 17391/35720 Training loss: 0.8459 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 17392/35720 Training loss: 0.8459 0.2195 sec/batch\n",
      "Epoch 10/20  Iteration 17393/35720 Training loss: 0.8459 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 17394/35720 Training loss: 0.8459 0.2060 sec/batch\n",
      "Epoch 10/20  Iteration 17395/35720 Training loss: 0.8459 0.2237 sec/batch\n",
      "Epoch 10/20  Iteration 17396/35720 Training loss: 0.8459 0.2242 sec/batch\n",
      "Epoch 10/20  Iteration 17397/35720 Training loss: 0.8459 0.2182 sec/batch\n",
      "Epoch 10/20  Iteration 17398/35720 Training loss: 0.8459 0.2301 sec/batch\n",
      "Epoch 10/20  Iteration 17399/35720 Training loss: 0.8459 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 17400/35720 Training loss: 0.8459 0.2118 sec/batch\n",
      "Validation loss: 1.4073 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 17401/35720 Training loss: 0.8462 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 17402/35720 Training loss: 0.8462 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 17403/35720 Training loss: 0.8462 0.2119 sec/batch\n",
      "Epoch 10/20  Iteration 17404/35720 Training loss: 0.8462 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 17405/35720 Training loss: 0.8462 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 17406/35720 Training loss: 0.8462 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 17407/35720 Training loss: 0.8462 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17408/35720 Training loss: 0.8462 0.2177 sec/batch\n",
      "Epoch 10/20  Iteration 17409/35720 Training loss: 0.8462 0.2218 sec/batch\n",
      "Epoch 10/20  Iteration 17410/35720 Training loss: 0.8462 0.2097 sec/batch\n",
      "Epoch 10/20  Iteration 17411/35720 Training loss: 0.8463 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 17412/35720 Training loss: 0.8463 0.2262 sec/batch\n",
      "Epoch 10/20  Iteration 17413/35720 Training loss: 0.8462 0.2147 sec/batch\n",
      "Epoch 10/20  Iteration 17414/35720 Training loss: 0.8462 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 17415/35720 Training loss: 0.8462 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17416/35720 Training loss: 0.8462 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 17417/35720 Training loss: 0.8462 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 17418/35720 Training loss: 0.8462 0.2248 sec/batch\n",
      "Epoch 10/20  Iteration 17419/35720 Training loss: 0.8461 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 17420/35720 Training loss: 0.8462 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 17421/35720 Training loss: 0.8461 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 17422/35720 Training loss: 0.8462 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 17423/35720 Training loss: 0.8462 0.2273 sec/batch\n",
      "Epoch 10/20  Iteration 17424/35720 Training loss: 0.8462 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 17425/35720 Training loss: 0.8462 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17426/35720 Training loss: 0.8463 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 17427/35720 Training loss: 0.8463 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 17428/35720 Training loss: 0.8463 0.2126 sec/batch\n",
      "Epoch 10/20  Iteration 17429/35720 Training loss: 0.8462 0.2261 sec/batch\n",
      "Epoch 10/20  Iteration 17430/35720 Training loss: 0.8462 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 17431/35720 Training loss: 0.8462 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 17432/35720 Training loss: 0.8462 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 17433/35720 Training loss: 0.8461 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 17434/35720 Training loss: 0.8461 0.2170 sec/batch\n",
      "Epoch 10/20  Iteration 17435/35720 Training loss: 0.8461 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 17436/35720 Training loss: 0.8461 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 17437/35720 Training loss: 0.8461 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17438/35720 Training loss: 0.8461 0.2113 sec/batch\n",
      "Epoch 10/20  Iteration 17439/35720 Training loss: 0.8460 0.2212 sec/batch\n",
      "Epoch 10/20  Iteration 17440/35720 Training loss: 0.8460 0.2183 sec/batch\n",
      "Epoch 10/20  Iteration 17441/35720 Training loss: 0.8460 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 17442/35720 Training loss: 0.8460 0.2074 sec/batch\n",
      "Epoch 10/20  Iteration 17443/35720 Training loss: 0.8460 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 17444/35720 Training loss: 0.8461 0.2120 sec/batch\n",
      "Epoch 10/20  Iteration 17445/35720 Training loss: 0.8460 0.2274 sec/batch\n",
      "Epoch 10/20  Iteration 17446/35720 Training loss: 0.8461 0.2313 sec/batch\n",
      "Epoch 10/20  Iteration 17447/35720 Training loss: 0.8460 0.2251 sec/batch\n",
      "Epoch 10/20  Iteration 17448/35720 Training loss: 0.8460 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 17449/35720 Training loss: 0.8460 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17450/35720 Training loss: 0.8460 0.2169 sec/batch\n",
      "Epoch 10/20  Iteration 17451/35720 Training loss: 0.8460 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 17452/35720 Training loss: 0.8460 0.2325 sec/batch\n",
      "Epoch 10/20  Iteration 17453/35720 Training loss: 0.8460 0.2123 sec/batch\n",
      "Epoch 10/20  Iteration 17454/35720 Training loss: 0.8460 0.2116 sec/batch\n",
      "Epoch 10/20  Iteration 17455/35720 Training loss: 0.8460 0.2254 sec/batch\n",
      "Epoch 10/20  Iteration 17456/35720 Training loss: 0.8461 0.2144 sec/batch\n",
      "Epoch 10/20  Iteration 17457/35720 Training loss: 0.8460 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 17458/35720 Training loss: 0.8460 0.2160 sec/batch\n",
      "Epoch 10/20  Iteration 17459/35720 Training loss: 0.8460 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17460/35720 Training loss: 0.8461 0.2268 sec/batch\n",
      "Epoch 10/20  Iteration 17461/35720 Training loss: 0.8460 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 17462/35720 Training loss: 0.8460 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 17463/35720 Training loss: 0.8460 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 17464/35720 Training loss: 0.8460 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 17465/35720 Training loss: 0.8460 0.2187 sec/batch\n",
      "Epoch 10/20  Iteration 17466/35720 Training loss: 0.8459 0.2125 sec/batch\n",
      "Epoch 10/20  Iteration 17467/35720 Training loss: 0.8459 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 17468/35720 Training loss: 0.8458 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 17469/35720 Training loss: 0.8458 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 17470/35720 Training loss: 0.8458 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 17471/35720 Training loss: 0.8457 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 17472/35720 Training loss: 0.8457 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 17473/35720 Training loss: 0.8457 0.2186 sec/batch\n",
      "Epoch 10/20  Iteration 17474/35720 Training loss: 0.8456 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 17475/35720 Training loss: 0.8456 0.2241 sec/batch\n",
      "Epoch 10/20  Iteration 17476/35720 Training loss: 0.8456 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 17477/35720 Training loss: 0.8456 0.2105 sec/batch\n",
      "Epoch 10/20  Iteration 17478/35720 Training loss: 0.8456 0.2219 sec/batch\n",
      "Epoch 10/20  Iteration 17479/35720 Training loss: 0.8456 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 17480/35720 Training loss: 0.8455 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 17481/35720 Training loss: 0.8455 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 17482/35720 Training loss: 0.8456 0.2156 sec/batch\n",
      "Epoch 10/20  Iteration 17483/35720 Training loss: 0.8456 0.2136 sec/batch\n",
      "Epoch 10/20  Iteration 17484/35720 Training loss: 0.8456 0.2265 sec/batch\n",
      "Epoch 10/20  Iteration 17485/35720 Training loss: 0.8456 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 17486/35720 Training loss: 0.8456 0.2284 sec/batch\n",
      "Epoch 10/20  Iteration 17487/35720 Training loss: 0.8455 0.2306 sec/batch\n",
      "Epoch 10/20  Iteration 17488/35720 Training loss: 0.8455 0.2206 sec/batch\n",
      "Epoch 10/20  Iteration 17489/35720 Training loss: 0.8455 0.2229 sec/batch\n",
      "Epoch 10/20  Iteration 17490/35720 Training loss: 0.8455 0.2287 sec/batch\n",
      "Epoch 10/20  Iteration 17491/35720 Training loss: 0.8455 0.2243 sec/batch\n",
      "Epoch 10/20  Iteration 17492/35720 Training loss: 0.8455 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 17493/35720 Training loss: 0.8455 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 17494/35720 Training loss: 0.8455 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 17495/35720 Training loss: 0.8455 0.2227 sec/batch\n",
      "Epoch 10/20  Iteration 17496/35720 Training loss: 0.8455 0.2271 sec/batch\n",
      "Epoch 10/20  Iteration 17497/35720 Training loss: 0.8455 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 17498/35720 Training loss: 0.8455 0.2315 sec/batch\n",
      "Epoch 10/20  Iteration 17499/35720 Training loss: 0.8455 0.2180 sec/batch\n",
      "Epoch 10/20  Iteration 17500/35720 Training loss: 0.8455 0.2290 sec/batch\n",
      "Epoch 10/20  Iteration 17501/35720 Training loss: 0.8455 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 17502/35720 Training loss: 0.8456 0.2276 sec/batch\n",
      "Epoch 10/20  Iteration 17503/35720 Training loss: 0.8456 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 17504/35720 Training loss: 0.8455 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 17505/35720 Training loss: 0.8455 0.2169 sec/batch\n",
      "Epoch 10/20  Iteration 17506/35720 Training loss: 0.8455 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 17507/35720 Training loss: 0.8454 0.2242 sec/batch\n",
      "Epoch 10/20  Iteration 17508/35720 Training loss: 0.8454 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 17509/35720 Training loss: 0.8454 0.2224 sec/batch\n",
      "Epoch 10/20  Iteration 17510/35720 Training loss: 0.8454 0.2081 sec/batch\n",
      "Epoch 10/20  Iteration 17511/35720 Training loss: 0.8453 0.2211 sec/batch\n",
      "Epoch 10/20  Iteration 17512/35720 Training loss: 0.8454 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 17513/35720 Training loss: 0.8454 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 17514/35720 Training loss: 0.8454 0.2202 sec/batch\n",
      "Epoch 10/20  Iteration 17515/35720 Training loss: 0.8454 0.2070 sec/batch\n",
      "Epoch 10/20  Iteration 17516/35720 Training loss: 0.8454 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 17517/35720 Training loss: 0.8454 0.2166 sec/batch\n",
      "Epoch 10/20  Iteration 17518/35720 Training loss: 0.8454 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17519/35720 Training loss: 0.8453 0.2170 sec/batch\n",
      "Epoch 10/20  Iteration 17520/35720 Training loss: 0.8454 0.2306 sec/batch\n",
      "Epoch 10/20  Iteration 17521/35720 Training loss: 0.8453 0.2078 sec/batch\n",
      "Epoch 10/20  Iteration 17522/35720 Training loss: 0.8453 0.2226 sec/batch\n",
      "Epoch 10/20  Iteration 17523/35720 Training loss: 0.8453 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 17524/35720 Training loss: 0.8453 0.2152 sec/batch\n",
      "Epoch 10/20  Iteration 17525/35720 Training loss: 0.8452 0.2246 sec/batch\n",
      "Epoch 10/20  Iteration 17526/35720 Training loss: 0.8452 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 17527/35720 Training loss: 0.8452 0.2081 sec/batch\n",
      "Epoch 10/20  Iteration 17528/35720 Training loss: 0.8452 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 17529/35720 Training loss: 0.8451 0.2175 sec/batch\n",
      "Epoch 10/20  Iteration 17530/35720 Training loss: 0.8452 0.2282 sec/batch\n",
      "Epoch 10/20  Iteration 17531/35720 Training loss: 0.8452 0.2144 sec/batch\n",
      "Epoch 10/20  Iteration 17532/35720 Training loss: 0.8452 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 17533/35720 Training loss: 0.8452 0.2335 sec/batch\n",
      "Epoch 10/20  Iteration 17534/35720 Training loss: 0.8453 0.2181 sec/batch\n",
      "Epoch 10/20  Iteration 17535/35720 Training loss: 0.8453 0.2385 sec/batch\n",
      "Epoch 10/20  Iteration 17536/35720 Training loss: 0.8453 0.2073 sec/batch\n",
      "Epoch 10/20  Iteration 17537/35720 Training loss: 0.8453 0.2110 sec/batch\n",
      "Epoch 10/20  Iteration 17538/35720 Training loss: 0.8453 0.2147 sec/batch\n",
      "Epoch 10/20  Iteration 17539/35720 Training loss: 0.8452 0.2314 sec/batch\n",
      "Epoch 10/20  Iteration 17540/35720 Training loss: 0.8452 0.2267 sec/batch\n",
      "Epoch 10/20  Iteration 17541/35720 Training loss: 0.8452 0.2264 sec/batch\n",
      "Epoch 10/20  Iteration 17542/35720 Training loss: 0.8452 0.2058 sec/batch\n",
      "Epoch 10/20  Iteration 17543/35720 Training loss: 0.8452 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 17544/35720 Training loss: 0.8452 0.2232 sec/batch\n",
      "Epoch 10/20  Iteration 17545/35720 Training loss: 0.8452 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 17546/35720 Training loss: 0.8451 0.2073 sec/batch\n",
      "Epoch 10/20  Iteration 17547/35720 Training loss: 0.8451 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 17548/35720 Training loss: 0.8451 0.2129 sec/batch\n",
      "Epoch 10/20  Iteration 17549/35720 Training loss: 0.8450 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 17550/35720 Training loss: 0.8449 0.2214 sec/batch\n",
      "Epoch 10/20  Iteration 17551/35720 Training loss: 0.8448 0.2142 sec/batch\n",
      "Epoch 10/20  Iteration 17552/35720 Training loss: 0.8448 0.2177 sec/batch\n",
      "Epoch 10/20  Iteration 17553/35720 Training loss: 0.8448 0.2192 sec/batch\n",
      "Epoch 10/20  Iteration 17554/35720 Training loss: 0.8448 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 17555/35720 Training loss: 0.8448 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 17556/35720 Training loss: 0.8447 0.2147 sec/batch\n",
      "Epoch 10/20  Iteration 17557/35720 Training loss: 0.8447 0.2240 sec/batch\n",
      "Epoch 10/20  Iteration 17558/35720 Training loss: 0.8447 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 17559/35720 Training loss: 0.8446 0.2069 sec/batch\n",
      "Epoch 10/20  Iteration 17560/35720 Training loss: 0.8446 0.2244 sec/batch\n",
      "Epoch 10/20  Iteration 17561/35720 Training loss: 0.8446 0.2157 sec/batch\n",
      "Epoch 10/20  Iteration 17562/35720 Training loss: 0.8446 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17563/35720 Training loss: 0.8445 0.2227 sec/batch\n",
      "Epoch 10/20  Iteration 17564/35720 Training loss: 0.8445 0.2194 sec/batch\n",
      "Epoch 10/20  Iteration 17565/35720 Training loss: 0.8445 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 17566/35720 Training loss: 0.8444 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 17567/35720 Training loss: 0.8444 0.2251 sec/batch\n",
      "Epoch 10/20  Iteration 17568/35720 Training loss: 0.8444 0.2351 sec/batch\n",
      "Epoch 10/20  Iteration 17569/35720 Training loss: 0.8444 0.2197 sec/batch\n",
      "Epoch 10/20  Iteration 17570/35720 Training loss: 0.8444 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 17571/35720 Training loss: 0.8444 0.2118 sec/batch\n",
      "Epoch 10/20  Iteration 17572/35720 Training loss: 0.8443 0.2172 sec/batch\n",
      "Epoch 10/20  Iteration 17573/35720 Training loss: 0.8443 0.2128 sec/batch\n",
      "Epoch 10/20  Iteration 17574/35720 Training loss: 0.8443 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 17575/35720 Training loss: 0.8442 0.2201 sec/batch\n",
      "Epoch 10/20  Iteration 17576/35720 Training loss: 0.8442 0.2099 sec/batch\n",
      "Epoch 10/20  Iteration 17577/35720 Training loss: 0.8442 0.2078 sec/batch\n",
      "Epoch 10/20  Iteration 17578/35720 Training loss: 0.8442 0.2146 sec/batch\n",
      "Epoch 10/20  Iteration 17579/35720 Training loss: 0.8442 0.2136 sec/batch\n",
      "Epoch 10/20  Iteration 17580/35720 Training loss: 0.8443 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 17581/35720 Training loss: 0.8442 0.2120 sec/batch\n",
      "Epoch 10/20  Iteration 17582/35720 Training loss: 0.8442 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 17583/35720 Training loss: 0.8442 0.2171 sec/batch\n",
      "Epoch 10/20  Iteration 17584/35720 Training loss: 0.8442 0.2178 sec/batch\n",
      "Epoch 10/20  Iteration 17585/35720 Training loss: 0.8442 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 17586/35720 Training loss: 0.8441 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 17587/35720 Training loss: 0.8441 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 17588/35720 Training loss: 0.8442 0.2197 sec/batch\n",
      "Epoch 10/20  Iteration 17589/35720 Training loss: 0.8442 0.2214 sec/batch\n",
      "Epoch 10/20  Iteration 17590/35720 Training loss: 0.8442 0.2372 sec/batch\n",
      "Epoch 10/20  Iteration 17591/35720 Training loss: 0.8442 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17592/35720 Training loss: 0.8442 0.2189 sec/batch\n",
      "Epoch 10/20  Iteration 17593/35720 Training loss: 0.8442 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 17594/35720 Training loss: 0.8442 0.2213 sec/batch\n",
      "Epoch 10/20  Iteration 17595/35720 Training loss: 0.8442 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 17596/35720 Training loss: 0.8442 0.2203 sec/batch\n",
      "Epoch 10/20  Iteration 17597/35720 Training loss: 0.8443 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 17598/35720 Training loss: 0.8443 0.2226 sec/batch\n",
      "Epoch 10/20  Iteration 17599/35720 Training loss: 0.8443 0.2170 sec/batch\n",
      "Epoch 10/20  Iteration 17600/35720 Training loss: 0.8443 0.2223 sec/batch\n",
      "Validation loss: 1.42254 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 17601/35720 Training loss: 0.8445 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 17602/35720 Training loss: 0.8445 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 17603/35720 Training loss: 0.8445 0.2079 sec/batch\n",
      "Epoch 10/20  Iteration 17604/35720 Training loss: 0.8446 0.2196 sec/batch\n",
      "Epoch 10/20  Iteration 17605/35720 Training loss: 0.8446 0.2190 sec/batch\n",
      "Epoch 10/20  Iteration 17606/35720 Training loss: 0.8446 0.2122 sec/batch\n",
      "Epoch 10/20  Iteration 17607/35720 Training loss: 0.8446 0.2238 sec/batch\n",
      "Epoch 10/20  Iteration 17608/35720 Training loss: 0.8446 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 17609/35720 Training loss: 0.8446 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 17610/35720 Training loss: 0.8446 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 17611/35720 Training loss: 0.8445 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 17612/35720 Training loss: 0.8445 0.2242 sec/batch\n",
      "Epoch 10/20  Iteration 17613/35720 Training loss: 0.8445 0.2165 sec/batch\n",
      "Epoch 10/20  Iteration 17614/35720 Training loss: 0.8444 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 17615/35720 Training loss: 0.8444 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 17616/35720 Training loss: 0.8444 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 17617/35720 Training loss: 0.8444 0.2232 sec/batch\n",
      "Epoch 10/20  Iteration 17618/35720 Training loss: 0.8443 0.2150 sec/batch\n",
      "Epoch 10/20  Iteration 17619/35720 Training loss: 0.8443 0.2103 sec/batch\n",
      "Epoch 10/20  Iteration 17620/35720 Training loss: 0.8442 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 17621/35720 Training loss: 0.8443 0.2225 sec/batch\n",
      "Epoch 10/20  Iteration 17622/35720 Training loss: 0.8443 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17623/35720 Training loss: 0.8443 0.2159 sec/batch\n",
      "Epoch 10/20  Iteration 17624/35720 Training loss: 0.8443 0.2196 sec/batch\n",
      "Epoch 10/20  Iteration 17625/35720 Training loss: 0.8442 0.2100 sec/batch\n",
      "Epoch 10/20  Iteration 17626/35720 Training loss: 0.8442 0.2256 sec/batch\n",
      "Epoch 10/20  Iteration 17627/35720 Training loss: 0.8442 0.2319 sec/batch\n",
      "Epoch 10/20  Iteration 17628/35720 Training loss: 0.8442 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 17629/35720 Training loss: 0.8441 0.2202 sec/batch\n",
      "Epoch 10/20  Iteration 17630/35720 Training loss: 0.8441 0.2104 sec/batch\n",
      "Epoch 10/20  Iteration 17631/35720 Training loss: 0.8441 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17632/35720 Training loss: 0.8440 0.2251 sec/batch\n",
      "Epoch 10/20  Iteration 17633/35720 Training loss: 0.8440 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 17634/35720 Training loss: 0.8439 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 17635/35720 Training loss: 0.8439 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 17636/35720 Training loss: 0.8438 0.2053 sec/batch\n",
      "Epoch 10/20  Iteration 17637/35720 Training loss: 0.8438 0.2090 sec/batch\n",
      "Epoch 10/20  Iteration 17638/35720 Training loss: 0.8438 0.2278 sec/batch\n",
      "Epoch 10/20  Iteration 17639/35720 Training loss: 0.8438 0.2132 sec/batch\n",
      "Epoch 10/20  Iteration 17640/35720 Training loss: 0.8438 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 17641/35720 Training loss: 0.8437 0.2137 sec/batch\n",
      "Epoch 10/20  Iteration 17642/35720 Training loss: 0.8438 0.2120 sec/batch\n",
      "Epoch 10/20  Iteration 17643/35720 Training loss: 0.8437 0.2182 sec/batch\n",
      "Epoch 10/20  Iteration 17644/35720 Training loss: 0.8437 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 17645/35720 Training loss: 0.8437 0.2158 sec/batch\n",
      "Epoch 10/20  Iteration 17646/35720 Training loss: 0.8436 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 17647/35720 Training loss: 0.8436 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 17648/35720 Training loss: 0.8436 0.2095 sec/batch\n",
      "Epoch 10/20  Iteration 17649/35720 Training loss: 0.8436 0.2168 sec/batch\n",
      "Epoch 10/20  Iteration 17650/35720 Training loss: 0.8435 0.2118 sec/batch\n",
      "Epoch 10/20  Iteration 17651/35720 Training loss: 0.8435 0.2261 sec/batch\n",
      "Epoch 10/20  Iteration 17652/35720 Training loss: 0.8435 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 17653/35720 Training loss: 0.8435 0.2108 sec/batch\n",
      "Epoch 10/20  Iteration 17654/35720 Training loss: 0.8435 0.2269 sec/batch\n",
      "Epoch 10/20  Iteration 17655/35720 Training loss: 0.8435 0.2213 sec/batch\n",
      "Epoch 10/20  Iteration 17656/35720 Training loss: 0.8435 0.2118 sec/batch\n",
      "Epoch 10/20  Iteration 17657/35720 Training loss: 0.8434 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 17658/35720 Training loss: 0.8434 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 17659/35720 Training loss: 0.8434 0.2094 sec/batch\n",
      "Epoch 10/20  Iteration 17660/35720 Training loss: 0.8434 0.2234 sec/batch\n",
      "Epoch 10/20  Iteration 17661/35720 Training loss: 0.8433 0.2212 sec/batch\n",
      "Epoch 10/20  Iteration 17662/35720 Training loss: 0.8433 0.2297 sec/batch\n",
      "Epoch 10/20  Iteration 17663/35720 Training loss: 0.8432 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 17664/35720 Training loss: 0.8432 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 17665/35720 Training loss: 0.8432 0.2287 sec/batch\n",
      "Epoch 10/20  Iteration 17666/35720 Training loss: 0.8432 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17667/35720 Training loss: 0.8432 0.2286 sec/batch\n",
      "Epoch 10/20  Iteration 17668/35720 Training loss: 0.8432 0.2241 sec/batch\n",
      "Epoch 10/20  Iteration 17669/35720 Training loss: 0.8431 0.2069 sec/batch\n",
      "Epoch 10/20  Iteration 17670/35720 Training loss: 0.8431 0.2088 sec/batch\n",
      "Epoch 10/20  Iteration 17671/35720 Training loss: 0.8431 0.2188 sec/batch\n",
      "Epoch 10/20  Iteration 17672/35720 Training loss: 0.8430 0.2241 sec/batch\n",
      "Epoch 10/20  Iteration 17673/35720 Training loss: 0.8430 0.2282 sec/batch\n",
      "Epoch 10/20  Iteration 17674/35720 Training loss: 0.8430 0.2203 sec/batch\n",
      "Epoch 10/20  Iteration 17675/35720 Training loss: 0.8430 0.2073 sec/batch\n",
      "Epoch 10/20  Iteration 17676/35720 Training loss: 0.8429 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 17677/35720 Training loss: 0.8429 0.2207 sec/batch\n",
      "Epoch 10/20  Iteration 17678/35720 Training loss: 0.8429 0.2292 sec/batch\n",
      "Epoch 10/20  Iteration 17679/35720 Training loss: 0.8429 0.2164 sec/batch\n",
      "Epoch 10/20  Iteration 17680/35720 Training loss: 0.8429 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 17681/35720 Training loss: 0.8428 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 17682/35720 Training loss: 0.8428 0.2244 sec/batch\n",
      "Epoch 10/20  Iteration 17683/35720 Training loss: 0.8428 0.2295 sec/batch\n",
      "Epoch 10/20  Iteration 17684/35720 Training loss: 0.8427 0.2292 sec/batch\n",
      "Epoch 10/20  Iteration 17685/35720 Training loss: 0.8427 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 17686/35720 Training loss: 0.8427 0.2115 sec/batch\n",
      "Epoch 10/20  Iteration 17687/35720 Training loss: 0.8426 0.2135 sec/batch\n",
      "Epoch 10/20  Iteration 17688/35720 Training loss: 0.8426 0.2290 sec/batch\n",
      "Epoch 10/20  Iteration 17689/35720 Training loss: 0.8426 0.2280 sec/batch\n",
      "Epoch 10/20  Iteration 17690/35720 Training loss: 0.8426 0.2066 sec/batch\n",
      "Epoch 10/20  Iteration 17691/35720 Training loss: 0.8426 0.2074 sec/batch\n",
      "Epoch 10/20  Iteration 17692/35720 Training loss: 0.8426 0.2210 sec/batch\n",
      "Epoch 10/20  Iteration 17693/35720 Training loss: 0.8426 0.2200 sec/batch\n",
      "Epoch 10/20  Iteration 17694/35720 Training loss: 0.8425 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 17695/35720 Training loss: 0.8425 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 17696/35720 Training loss: 0.8425 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 17697/35720 Training loss: 0.8425 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 17698/35720 Training loss: 0.8424 0.2096 sec/batch\n",
      "Epoch 10/20  Iteration 17699/35720 Training loss: 0.8425 0.2181 sec/batch\n",
      "Epoch 10/20  Iteration 17700/35720 Training loss: 0.8425 0.2170 sec/batch\n",
      "Epoch 10/20  Iteration 17701/35720 Training loss: 0.8425 0.2159 sec/batch\n",
      "Epoch 10/20  Iteration 17702/35720 Training loss: 0.8425 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 17703/35720 Training loss: 0.8424 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17704/35720 Training loss: 0.8424 0.2089 sec/batch\n",
      "Epoch 10/20  Iteration 17705/35720 Training loss: 0.8424 0.2263 sec/batch\n",
      "Epoch 10/20  Iteration 17706/35720 Training loss: 0.8424 0.2151 sec/batch\n",
      "Epoch 10/20  Iteration 17707/35720 Training loss: 0.8424 0.2072 sec/batch\n",
      "Epoch 10/20  Iteration 17708/35720 Training loss: 0.8424 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17709/35720 Training loss: 0.8424 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17710/35720 Training loss: 0.8424 0.2309 sec/batch\n",
      "Epoch 10/20  Iteration 17711/35720 Training loss: 0.8424 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17712/35720 Training loss: 0.8423 0.2157 sec/batch\n",
      "Epoch 10/20  Iteration 17713/35720 Training loss: 0.8423 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 17714/35720 Training loss: 0.8423 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 17715/35720 Training loss: 0.8423 0.2075 sec/batch\n",
      "Epoch 10/20  Iteration 17716/35720 Training loss: 0.8423 0.2261 sec/batch\n",
      "Epoch 10/20  Iteration 17717/35720 Training loss: 0.8423 0.2228 sec/batch\n",
      "Epoch 10/20  Iteration 17718/35720 Training loss: 0.8423 0.2233 sec/batch\n",
      "Epoch 10/20  Iteration 17719/35720 Training loss: 0.8423 0.2056 sec/batch\n",
      "Epoch 10/20  Iteration 17720/35720 Training loss: 0.8423 0.2086 sec/batch\n",
      "Epoch 10/20  Iteration 17721/35720 Training loss: 0.8423 0.2171 sec/batch\n",
      "Epoch 10/20  Iteration 17722/35720 Training loss: 0.8423 0.2247 sec/batch\n",
      "Epoch 10/20  Iteration 17723/35720 Training loss: 0.8423 0.2294 sec/batch\n",
      "Epoch 10/20  Iteration 17724/35720 Training loss: 0.8423 0.2064 sec/batch\n",
      "Epoch 10/20  Iteration 17725/35720 Training loss: 0.8423 0.2195 sec/batch\n",
      "Epoch 10/20  Iteration 17726/35720 Training loss: 0.8423 0.2211 sec/batch\n",
      "Epoch 10/20  Iteration 17727/35720 Training loss: 0.8424 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 17728/35720 Training loss: 0.8424 0.2183 sec/batch\n",
      "Epoch 10/20  Iteration 17729/35720 Training loss: 0.8424 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 17730/35720 Training loss: 0.8424 0.2062 sec/batch\n",
      "Epoch 10/20  Iteration 17731/35720 Training loss: 0.8424 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 17732/35720 Training loss: 0.8424 0.2191 sec/batch\n",
      "Epoch 10/20  Iteration 17733/35720 Training loss: 0.8424 0.2171 sec/batch\n",
      "Epoch 10/20  Iteration 17734/35720 Training loss: 0.8424 0.2307 sec/batch\n",
      "Epoch 10/20  Iteration 17735/35720 Training loss: 0.8424 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 17736/35720 Training loss: 0.8423 0.2071 sec/batch\n",
      "Epoch 10/20  Iteration 17737/35720 Training loss: 0.8424 0.2095 sec/batch\n",
      "Epoch 10/20  Iteration 17738/35720 Training loss: 0.8423 0.2110 sec/batch\n",
      "Epoch 10/20  Iteration 17739/35720 Training loss: 0.8424 0.2238 sec/batch\n",
      "Epoch 10/20  Iteration 17740/35720 Training loss: 0.8424 0.2257 sec/batch\n",
      "Epoch 10/20  Iteration 17741/35720 Training loss: 0.8424 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 17742/35720 Training loss: 0.8424 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17743/35720 Training loss: 0.8424 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 17744/35720 Training loss: 0.8425 0.2238 sec/batch\n",
      "Epoch 10/20  Iteration 17745/35720 Training loss: 0.8425 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 17746/35720 Training loss: 0.8425 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 17747/35720 Training loss: 0.8425 0.2068 sec/batch\n",
      "Epoch 10/20  Iteration 17748/35720 Training loss: 0.8425 0.2095 sec/batch\n",
      "Epoch 10/20  Iteration 17749/35720 Training loss: 0.8425 0.2222 sec/batch\n",
      "Epoch 10/20  Iteration 17750/35720 Training loss: 0.8425 0.2272 sec/batch\n",
      "Epoch 10/20  Iteration 17751/35720 Training loss: 0.8425 0.2142 sec/batch\n",
      "Epoch 10/20  Iteration 17752/35720 Training loss: 0.8424 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 17753/35720 Training loss: 0.8424 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17754/35720 Training loss: 0.8424 0.2125 sec/batch\n",
      "Epoch 10/20  Iteration 17755/35720 Training loss: 0.8424 0.2297 sec/batch\n",
      "Epoch 10/20  Iteration 17756/35720 Training loss: 0.8423 0.2153 sec/batch\n",
      "Epoch 10/20  Iteration 17757/35720 Training loss: 0.8423 0.2262 sec/batch\n",
      "Epoch 10/20  Iteration 17758/35720 Training loss: 0.8424 0.2179 sec/batch\n",
      "Epoch 10/20  Iteration 17759/35720 Training loss: 0.8423 0.2087 sec/batch\n",
      "Epoch 10/20  Iteration 17760/35720 Training loss: 0.8424 0.2193 sec/batch\n",
      "Epoch 10/20  Iteration 17761/35720 Training loss: 0.8424 0.2272 sec/batch\n",
      "Epoch 10/20  Iteration 17762/35720 Training loss: 0.8424 0.2381 sec/batch\n",
      "Epoch 10/20  Iteration 17763/35720 Training loss: 0.8424 0.2069 sec/batch\n",
      "Epoch 10/20  Iteration 17764/35720 Training loss: 0.8423 0.2079 sec/batch\n",
      "Epoch 10/20  Iteration 17765/35720 Training loss: 0.8423 0.2228 sec/batch\n",
      "Epoch 10/20  Iteration 17766/35720 Training loss: 0.8423 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 17767/35720 Training loss: 0.8422 0.2102 sec/batch\n",
      "Epoch 10/20  Iteration 17768/35720 Training loss: 0.8422 0.2133 sec/batch\n",
      "Epoch 10/20  Iteration 17769/35720 Training loss: 0.8422 0.2476 sec/batch\n",
      "Epoch 10/20  Iteration 17770/35720 Training loss: 0.8422 0.2209 sec/batch\n",
      "Epoch 10/20  Iteration 17771/35720 Training loss: 0.8422 0.2207 sec/batch\n",
      "Epoch 10/20  Iteration 17772/35720 Training loss: 0.8422 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17773/35720 Training loss: 0.8422 0.2166 sec/batch\n",
      "Epoch 10/20  Iteration 17774/35720 Training loss: 0.8422 0.2111 sec/batch\n",
      "Epoch 10/20  Iteration 17775/35720 Training loss: 0.8422 0.2136 sec/batch\n",
      "Epoch 10/20  Iteration 17776/35720 Training loss: 0.8421 0.2318 sec/batch\n",
      "Epoch 10/20  Iteration 17777/35720 Training loss: 0.8421 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 17778/35720 Training loss: 0.8421 0.2288 sec/batch\n",
      "Epoch 10/20  Iteration 17779/35720 Training loss: 0.8421 0.2117 sec/batch\n",
      "Epoch 10/20  Iteration 17780/35720 Training loss: 0.8421 0.2114 sec/batch\n",
      "Epoch 10/20  Iteration 17781/35720 Training loss: 0.8420 0.2238 sec/batch\n",
      "Epoch 10/20  Iteration 17782/35720 Training loss: 0.8420 0.2203 sec/batch\n",
      "Epoch 10/20  Iteration 17783/35720 Training loss: 0.8420 0.2218 sec/batch\n",
      "Epoch 10/20  Iteration 17784/35720 Training loss: 0.8421 0.2262 sec/batch\n",
      "Epoch 10/20  Iteration 17785/35720 Training loss: 0.8421 0.2084 sec/batch\n",
      "Epoch 10/20  Iteration 17786/35720 Training loss: 0.8421 0.2121 sec/batch\n",
      "Epoch 10/20  Iteration 17787/35720 Training loss: 0.8422 0.2253 sec/batch\n",
      "Epoch 10/20  Iteration 17788/35720 Training loss: 0.8422 0.2107 sec/batch\n",
      "Epoch 10/20  Iteration 17789/35720 Training loss: 0.8422 0.2285 sec/batch\n",
      "Epoch 10/20  Iteration 17790/35720 Training loss: 0.8421 0.2371 sec/batch\n",
      "Epoch 10/20  Iteration 17791/35720 Training loss: 0.8421 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17792/35720 Training loss: 0.8421 0.2134 sec/batch\n",
      "Epoch 10/20  Iteration 17793/35720 Training loss: 0.8421 0.2294 sec/batch\n",
      "Epoch 10/20  Iteration 17794/35720 Training loss: 0.8421 0.2339 sec/batch\n",
      "Epoch 10/20  Iteration 17795/35720 Training loss: 0.8421 0.2269 sec/batch\n",
      "Epoch 10/20  Iteration 17796/35720 Training loss: 0.8421 0.2082 sec/batch\n",
      "Epoch 10/20  Iteration 17797/35720 Training loss: 0.8421 0.2266 sec/batch\n",
      "Epoch 10/20  Iteration 17798/35720 Training loss: 0.8421 0.2232 sec/batch\n",
      "Epoch 10/20  Iteration 17799/35720 Training loss: 0.8420 0.2234 sec/batch\n",
      "Epoch 10/20  Iteration 17800/35720 Training loss: 0.8421 0.2175 sec/batch\n",
      "Validation loss: 1.42811 Saving checkpoint!\n",
      "Epoch 10/20  Iteration 17801/35720 Training loss: 0.8422 0.2092 sec/batch\n",
      "Epoch 10/20  Iteration 17802/35720 Training loss: 0.8422 0.2123 sec/batch\n",
      "Epoch 10/20  Iteration 17803/35720 Training loss: 0.8422 0.2299 sec/batch\n",
      "Epoch 10/20  Iteration 17804/35720 Training loss: 0.8422 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17805/35720 Training loss: 0.8422 0.2154 sec/batch\n",
      "Epoch 10/20  Iteration 17806/35720 Training loss: 0.8422 0.2196 sec/batch\n",
      "Epoch 10/20  Iteration 17807/35720 Training loss: 0.8422 0.2231 sec/batch\n",
      "Epoch 10/20  Iteration 17808/35720 Training loss: 0.8422 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 17809/35720 Training loss: 0.8422 0.2161 sec/batch\n",
      "Epoch 10/20  Iteration 17810/35720 Training loss: 0.8422 0.2216 sec/batch\n",
      "Epoch 10/20  Iteration 17811/35720 Training loss: 0.8423 0.2155 sec/batch\n",
      "Epoch 10/20  Iteration 17812/35720 Training loss: 0.8422 0.2154 sec/batch\n",
      "Epoch 10/20  Iteration 17813/35720 Training loss: 0.8423 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 17814/35720 Training loss: 0.8423 0.2163 sec/batch\n",
      "Epoch 10/20  Iteration 17815/35720 Training loss: 0.8423 0.2101 sec/batch\n",
      "Epoch 10/20  Iteration 17816/35720 Training loss: 0.8422 0.2244 sec/batch\n",
      "Epoch 10/20  Iteration 17817/35720 Training loss: 0.8422 0.2246 sec/batch\n",
      "Epoch 10/20  Iteration 17818/35720 Training loss: 0.8423 0.2067 sec/batch\n",
      "Epoch 10/20  Iteration 17819/35720 Training loss: 0.8423 0.2091 sec/batch\n",
      "Epoch 10/20  Iteration 17820/35720 Training loss: 0.8424 0.2213 sec/batch\n",
      "Epoch 10/20  Iteration 17821/35720 Training loss: 0.8424 0.2216 sec/batch\n",
      "Epoch 10/20  Iteration 17822/35720 Training loss: 0.8424 0.2106 sec/batch\n",
      "Epoch 10/20  Iteration 17823/35720 Training loss: 0.8424 0.2063 sec/batch\n",
      "Epoch 10/20  Iteration 17824/35720 Training loss: 0.8424 0.2093 sec/batch\n",
      "Epoch 10/20  Iteration 17825/35720 Training loss: 0.8424 0.2337 sec/batch\n",
      "Epoch 10/20  Iteration 17826/35720 Training loss: 0.8423 0.2098 sec/batch\n",
      "Epoch 10/20  Iteration 17827/35720 Training loss: 0.8423 0.2308 sec/batch\n",
      "Epoch 10/20  Iteration 17828/35720 Training loss: 0.8423 0.2056 sec/batch\n",
      "Epoch 10/20  Iteration 17829/35720 Training loss: 0.8423 0.2065 sec/batch\n",
      "Epoch 10/20  Iteration 17830/35720 Training loss: 0.8424 0.2143 sec/batch\n",
      "Epoch 10/20  Iteration 17831/35720 Training loss: 0.8423 0.2303 sec/batch\n",
      "Epoch 10/20  Iteration 17832/35720 Training loss: 0.8423 0.2280 sec/batch\n",
      "Epoch 10/20  Iteration 17833/35720 Training loss: 0.8423 0.2277 sec/batch\n",
      "Epoch 10/20  Iteration 17834/35720 Training loss: 0.8423 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 17835/35720 Training loss: 0.8423 0.2141 sec/batch\n",
      "Epoch 10/20  Iteration 17836/35720 Training loss: 0.8423 0.2195 sec/batch\n",
      "Epoch 10/20  Iteration 17837/35720 Training loss: 0.8423 0.2173 sec/batch\n",
      "Epoch 10/20  Iteration 17838/35720 Training loss: 0.8423 0.2282 sec/batch\n",
      "Epoch 10/20  Iteration 17839/35720 Training loss: 0.8423 0.2071 sec/batch\n",
      "Epoch 10/20  Iteration 17840/35720 Training loss: 0.8423 0.2061 sec/batch\n",
      "Epoch 10/20  Iteration 17841/35720 Training loss: 0.8423 0.2124 sec/batch\n",
      "Epoch 10/20  Iteration 17842/35720 Training loss: 0.8423 0.2174 sec/batch\n",
      "Epoch 10/20  Iteration 17843/35720 Training loss: 0.8423 0.2264 sec/batch\n",
      "Epoch 10/20  Iteration 17844/35720 Training loss: 0.8423 0.2333 sec/batch\n",
      "Epoch 10/20  Iteration 17845/35720 Training loss: 0.8423 0.2074 sec/batch\n",
      "Epoch 10/20  Iteration 17846/35720 Training loss: 0.8422 0.2138 sec/batch\n",
      "Epoch 10/20  Iteration 17847/35720 Training loss: 0.8422 0.2217 sec/batch\n",
      "Epoch 10/20  Iteration 17848/35720 Training loss: 0.8421 0.2109 sec/batch\n",
      "Epoch 10/20  Iteration 17849/35720 Training loss: 0.8421 0.2269 sec/batch\n",
      "Epoch 10/20  Iteration 17850/35720 Training loss: 0.8421 0.2239 sec/batch\n",
      "Epoch 10/20  Iteration 17851/35720 Training loss: 0.8421 0.2058 sec/batch\n",
      "Epoch 10/20  Iteration 17852/35720 Training loss: 0.8421 0.2234 sec/batch\n",
      "Epoch 10/20  Iteration 17853/35720 Training loss: 0.8421 0.2238 sec/batch\n",
      "Epoch 10/20  Iteration 17854/35720 Training loss: 0.8420 0.2237 sec/batch\n",
      "Epoch 10/20  Iteration 17855/35720 Training loss: 0.8420 0.2244 sec/batch\n",
      "Epoch 10/20  Iteration 17856/35720 Training loss: 0.8420 0.2072 sec/batch\n",
      "Epoch 10/20  Iteration 17857/35720 Training loss: 0.8420 0.2085 sec/batch\n",
      "Epoch 10/20  Iteration 17858/35720 Training loss: 0.8419 0.2208 sec/batch\n",
      "Epoch 10/20  Iteration 17859/35720 Training loss: 0.8419 0.2223 sec/batch\n",
      "Epoch 10/20  Iteration 17860/35720 Training loss: 0.8419 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 17861/35720 Training loss: 0.8772 0.2080 sec/batch\n",
      "Epoch 11/20  Iteration 17862/35720 Training loss: 0.8619 0.2057 sec/batch\n",
      "Epoch 11/20  Iteration 17863/35720 Training loss: 0.8523 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 17864/35720 Training loss: 0.8514 0.2208 sec/batch\n",
      "Epoch 11/20  Iteration 17865/35720 Training loss: 0.8558 0.2172 sec/batch\n",
      "Epoch 11/20  Iteration 17866/35720 Training loss: 0.8450 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 17867/35720 Training loss: 0.8464 0.2218 sec/batch\n",
      "Epoch 11/20  Iteration 17868/35720 Training loss: 0.8382 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 17869/35720 Training loss: 0.8338 0.2124 sec/batch\n",
      "Epoch 11/20  Iteration 17870/35720 Training loss: 0.8358 0.2226 sec/batch\n",
      "Epoch 11/20  Iteration 17871/35720 Training loss: 0.8359 0.2289 sec/batch\n",
      "Epoch 11/20  Iteration 17872/35720 Training loss: 0.8330 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 17873/35720 Training loss: 0.8345 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 17874/35720 Training loss: 0.8406 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 17875/35720 Training loss: 0.8424 0.2181 sec/batch\n",
      "Epoch 11/20  Iteration 17876/35720 Training loss: 0.8413 0.2267 sec/batch\n",
      "Epoch 11/20  Iteration 17877/35720 Training loss: 0.8412 0.2268 sec/batch\n",
      "Epoch 11/20  Iteration 17878/35720 Training loss: 0.8381 0.2113 sec/batch\n",
      "Epoch 11/20  Iteration 17879/35720 Training loss: 0.8356 0.2239 sec/batch\n",
      "Epoch 11/20  Iteration 17880/35720 Training loss: 0.8363 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 17881/35720 Training loss: 0.8362 0.2367 sec/batch\n",
      "Epoch 11/20  Iteration 17882/35720 Training loss: 0.8334 0.2283 sec/batch\n",
      "Epoch 11/20  Iteration 17883/35720 Training loss: 0.8317 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 17884/35720 Training loss: 0.8326 0.2073 sec/batch\n",
      "Epoch 11/20  Iteration 17885/35720 Training loss: 0.8331 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 17886/35720 Training loss: 0.8316 0.2272 sec/batch\n",
      "Epoch 11/20  Iteration 17887/35720 Training loss: 0.8338 0.2732 sec/batch\n",
      "Epoch 11/20  Iteration 17888/35720 Training loss: 0.8343 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 17889/35720 Training loss: 0.8336 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 17890/35720 Training loss: 0.8343 0.2228 sec/batch\n",
      "Epoch 11/20  Iteration 17891/35720 Training loss: 0.8364 0.2235 sec/batch\n",
      "Epoch 11/20  Iteration 17892/35720 Training loss: 0.8351 0.2346 sec/batch\n",
      "Epoch 11/20  Iteration 17893/35720 Training loss: 0.8359 0.2174 sec/batch\n",
      "Epoch 11/20  Iteration 17894/35720 Training loss: 0.8377 0.2109 sec/batch\n",
      "Epoch 11/20  Iteration 17895/35720 Training loss: 0.8406 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 17896/35720 Training loss: 0.8412 0.2125 sec/batch\n",
      "Epoch 11/20  Iteration 17897/35720 Training loss: 0.8416 0.2259 sec/batch\n",
      "Epoch 11/20  Iteration 17898/35720 Training loss: 0.8406 0.2250 sec/batch\n",
      "Epoch 11/20  Iteration 17899/35720 Training loss: 0.8395 0.2285 sec/batch\n",
      "Epoch 11/20  Iteration 17900/35720 Training loss: 0.8403 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 17901/35720 Training loss: 0.8401 0.2155 sec/batch\n",
      "Epoch 11/20  Iteration 17902/35720 Training loss: 0.8390 0.2194 sec/batch\n",
      "Epoch 11/20  Iteration 17903/35720 Training loss: 0.8373 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 17904/35720 Training loss: 0.8367 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 17905/35720 Training loss: 0.8361 0.2068 sec/batch\n",
      "Epoch 11/20  Iteration 17906/35720 Training loss: 0.8355 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 17907/35720 Training loss: 0.8349 0.2206 sec/batch\n",
      "Epoch 11/20  Iteration 17908/35720 Training loss: 0.8348 0.2319 sec/batch\n",
      "Epoch 11/20  Iteration 17909/35720 Training loss: 0.8347 0.2128 sec/batch\n",
      "Epoch 11/20  Iteration 17910/35720 Training loss: 0.8338 0.2048 sec/batch\n",
      "Epoch 11/20  Iteration 17911/35720 Training loss: 0.8347 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 17912/35720 Training loss: 0.8352 0.2160 sec/batch\n",
      "Epoch 11/20  Iteration 17913/35720 Training loss: 0.8355 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 17914/35720 Training loss: 0.8338 0.2258 sec/batch\n",
      "Epoch 11/20  Iteration 17915/35720 Training loss: 0.8333 0.2194 sec/batch\n",
      "Epoch 11/20  Iteration 17916/35720 Training loss: 0.8329 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 17917/35720 Training loss: 0.8332 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 17918/35720 Training loss: 0.8328 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 17919/35720 Training loss: 0.8321 0.2283 sec/batch\n",
      "Epoch 11/20  Iteration 17920/35720 Training loss: 0.8310 0.2281 sec/batch\n",
      "Epoch 11/20  Iteration 17921/35720 Training loss: 0.8302 0.2256 sec/batch\n",
      "Epoch 11/20  Iteration 17922/35720 Training loss: 0.8285 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 17923/35720 Training loss: 0.8291 0.2278 sec/batch\n",
      "Epoch 11/20  Iteration 17924/35720 Training loss: 0.8289 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 17925/35720 Training loss: 0.8295 0.2243 sec/batch\n",
      "Epoch 11/20  Iteration 17926/35720 Training loss: 0.8297 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 17927/35720 Training loss: 0.8291 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 17928/35720 Training loss: 0.8285 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 17929/35720 Training loss: 0.8290 0.2370 sec/batch\n",
      "Epoch 11/20  Iteration 17930/35720 Training loss: 0.8288 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 17931/35720 Training loss: 0.8291 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 17932/35720 Training loss: 0.8298 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 17933/35720 Training loss: 0.8300 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 17934/35720 Training loss: 0.8299 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 17935/35720 Training loss: 0.8290 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 17936/35720 Training loss: 0.8286 0.2236 sec/batch\n",
      "Epoch 11/20  Iteration 17937/35720 Training loss: 0.8278 0.2235 sec/batch\n",
      "Epoch 11/20  Iteration 17938/35720 Training loss: 0.8283 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 17939/35720 Training loss: 0.8279 0.2055 sec/batch\n",
      "Epoch 11/20  Iteration 17940/35720 Training loss: 0.8289 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 17941/35720 Training loss: 0.8294 0.2209 sec/batch\n",
      "Epoch 11/20  Iteration 17942/35720 Training loss: 0.8293 0.2289 sec/batch\n",
      "Epoch 11/20  Iteration 17943/35720 Training loss: 0.8291 0.2236 sec/batch\n",
      "Epoch 11/20  Iteration 17944/35720 Training loss: 0.8292 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 17945/35720 Training loss: 0.8290 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 17946/35720 Training loss: 0.8290 0.2184 sec/batch\n",
      "Epoch 11/20  Iteration 17947/35720 Training loss: 0.8292 0.2127 sec/batch\n",
      "Epoch 11/20  Iteration 17948/35720 Training loss: 0.8294 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 17949/35720 Training loss: 0.8286 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 17950/35720 Training loss: 0.8277 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 17951/35720 Training loss: 0.8278 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 17952/35720 Training loss: 0.8274 0.2100 sec/batch\n",
      "Epoch 11/20  Iteration 17953/35720 Training loss: 0.8274 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 17954/35720 Training loss: 0.8278 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 17955/35720 Training loss: 0.8277 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 17956/35720 Training loss: 0.8270 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 17957/35720 Training loss: 0.8274 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 17958/35720 Training loss: 0.8271 0.2183 sec/batch\n",
      "Epoch 11/20  Iteration 17959/35720 Training loss: 0.8271 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 17960/35720 Training loss: 0.8268 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 17961/35720 Training loss: 0.8265 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 17962/35720 Training loss: 0.8267 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 17963/35720 Training loss: 0.8264 0.2181 sec/batch\n",
      "Epoch 11/20  Iteration 17964/35720 Training loss: 0.8262 0.2240 sec/batch\n",
      "Epoch 11/20  Iteration 17965/35720 Training loss: 0.8261 0.2210 sec/batch\n",
      "Epoch 11/20  Iteration 17966/35720 Training loss: 0.8257 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 17967/35720 Training loss: 0.8257 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 17968/35720 Training loss: 0.8259 0.2080 sec/batch\n",
      "Epoch 11/20  Iteration 17969/35720 Training loss: 0.8263 0.2272 sec/batch\n",
      "Epoch 11/20  Iteration 17970/35720 Training loss: 0.8262 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 17971/35720 Training loss: 0.8265 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 17972/35720 Training loss: 0.8270 0.2513 sec/batch\n",
      "Epoch 11/20  Iteration 17973/35720 Training loss: 0.8271 0.2113 sec/batch\n",
      "Epoch 11/20  Iteration 17974/35720 Training loss: 0.8274 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 17975/35720 Training loss: 0.8272 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 17976/35720 Training loss: 0.8273 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 17977/35720 Training loss: 0.8272 0.2277 sec/batch\n",
      "Epoch 11/20  Iteration 17978/35720 Training loss: 0.8275 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 17979/35720 Training loss: 0.8275 0.2257 sec/batch\n",
      "Epoch 11/20  Iteration 17980/35720 Training loss: 0.8283 0.2329 sec/batch\n",
      "Epoch 11/20  Iteration 17981/35720 Training loss: 0.8287 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 17982/35720 Training loss: 0.8282 0.2185 sec/batch\n",
      "Epoch 11/20  Iteration 17983/35720 Training loss: 0.8283 0.2070 sec/batch\n",
      "Epoch 11/20  Iteration 17984/35720 Training loss: 0.8289 0.2051 sec/batch\n",
      "Epoch 11/20  Iteration 17985/35720 Training loss: 0.8287 0.2154 sec/batch\n",
      "Epoch 11/20  Iteration 17986/35720 Training loss: 0.8284 0.2279 sec/batch\n",
      "Epoch 11/20  Iteration 17987/35720 Training loss: 0.8287 0.2170 sec/batch\n",
      "Epoch 11/20  Iteration 17988/35720 Training loss: 0.8287 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 17989/35720 Training loss: 0.8284 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 17990/35720 Training loss: 0.8283 0.2286 sec/batch\n",
      "Epoch 11/20  Iteration 17991/35720 Training loss: 0.8281 0.3329 sec/batch\n",
      "Epoch 11/20  Iteration 17992/35720 Training loss: 0.8277 0.3061 sec/batch\n",
      "Epoch 11/20  Iteration 17993/35720 Training loss: 0.8278 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 17994/35720 Training loss: 0.8278 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 17995/35720 Training loss: 0.8278 0.2173 sec/batch\n",
      "Epoch 11/20  Iteration 17996/35720 Training loss: 0.8279 0.2217 sec/batch\n",
      "Epoch 11/20  Iteration 17997/35720 Training loss: 0.8287 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 17998/35720 Training loss: 0.8289 0.2239 sec/batch\n",
      "Epoch 11/20  Iteration 17999/35720 Training loss: 0.8290 0.2104 sec/batch\n",
      "Epoch 11/20  Iteration 18000/35720 Training loss: 0.8291 0.2107 sec/batch\n",
      "Validation loss: 1.41784 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 18001/35720 Training loss: 0.8310 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18002/35720 Training loss: 0.8309 0.2083 sec/batch\n",
      "Epoch 11/20  Iteration 18003/35720 Training loss: 0.8305 0.2305 sec/batch\n",
      "Epoch 11/20  Iteration 18004/35720 Training loss: 0.8299 0.2059 sec/batch\n",
      "Epoch 11/20  Iteration 18005/35720 Training loss: 0.8300 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 18006/35720 Training loss: 0.8304 0.2202 sec/batch\n",
      "Epoch 11/20  Iteration 18007/35720 Training loss: 0.8302 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 18008/35720 Training loss: 0.8300 0.2318 sec/batch\n",
      "Epoch 11/20  Iteration 18009/35720 Training loss: 0.8300 0.2211 sec/batch\n",
      "Epoch 11/20  Iteration 18010/35720 Training loss: 0.8294 0.2177 sec/batch\n",
      "Epoch 11/20  Iteration 18011/35720 Training loss: 0.8293 0.2180 sec/batch\n",
      "Epoch 11/20  Iteration 18012/35720 Training loss: 0.8293 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 18013/35720 Training loss: 0.8291 0.2293 sec/batch\n",
      "Epoch 11/20  Iteration 18014/35720 Training loss: 0.8294 0.2171 sec/batch\n",
      "Epoch 11/20  Iteration 18015/35720 Training loss: 0.8295 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 18016/35720 Training loss: 0.8298 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18017/35720 Training loss: 0.8301 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 18018/35720 Training loss: 0.8304 0.2349 sec/batch\n",
      "Epoch 11/20  Iteration 18019/35720 Training loss: 0.8301 0.2281 sec/batch\n",
      "Epoch 11/20  Iteration 18020/35720 Training loss: 0.8300 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18021/35720 Training loss: 0.8298 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 18022/35720 Training loss: 0.8298 0.2207 sec/batch\n",
      "Epoch 11/20  Iteration 18023/35720 Training loss: 0.8299 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 18024/35720 Training loss: 0.8299 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 18025/35720 Training loss: 0.8302 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 18026/35720 Training loss: 0.8303 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 18027/35720 Training loss: 0.8305 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 18028/35720 Training loss: 0.8309 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 18029/35720 Training loss: 0.8312 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 18030/35720 Training loss: 0.8314 0.2232 sec/batch\n",
      "Epoch 11/20  Iteration 18031/35720 Training loss: 0.8319 0.2234 sec/batch\n",
      "Epoch 11/20  Iteration 18032/35720 Training loss: 0.8323 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18033/35720 Training loss: 0.8326 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18034/35720 Training loss: 0.8331 0.2289 sec/batch\n",
      "Epoch 11/20  Iteration 18035/35720 Training loss: 0.8332 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18036/35720 Training loss: 0.8333 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 18037/35720 Training loss: 0.8335 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 18038/35720 Training loss: 0.8333 0.2059 sec/batch\n",
      "Epoch 11/20  Iteration 18039/35720 Training loss: 0.8331 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18040/35720 Training loss: 0.8330 0.2174 sec/batch\n",
      "Epoch 11/20  Iteration 18041/35720 Training loss: 0.8331 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18042/35720 Training loss: 0.8332 0.2216 sec/batch\n",
      "Epoch 11/20  Iteration 18043/35720 Training loss: 0.8332 0.2061 sec/batch\n",
      "Epoch 11/20  Iteration 18044/35720 Training loss: 0.8334 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 18045/35720 Training loss: 0.8331 0.2147 sec/batch\n",
      "Epoch 11/20  Iteration 18046/35720 Training loss: 0.8331 0.2257 sec/batch\n",
      "Epoch 11/20  Iteration 18047/35720 Training loss: 0.8329 0.2222 sec/batch\n",
      "Epoch 11/20  Iteration 18048/35720 Training loss: 0.8332 0.2226 sec/batch\n",
      "Epoch 11/20  Iteration 18049/35720 Training loss: 0.8333 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 18050/35720 Training loss: 0.8334 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 18051/35720 Training loss: 0.8335 0.2259 sec/batch\n",
      "Epoch 11/20  Iteration 18052/35720 Training loss: 0.8339 0.2127 sec/batch\n",
      "Epoch 11/20  Iteration 18053/35720 Training loss: 0.8342 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 18054/35720 Training loss: 0.8343 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18055/35720 Training loss: 0.8343 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18056/35720 Training loss: 0.8344 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 18057/35720 Training loss: 0.8340 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 18058/35720 Training loss: 0.8339 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 18059/35720 Training loss: 0.8341 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 18060/35720 Training loss: 0.8343 0.2061 sec/batch\n",
      "Epoch 11/20  Iteration 18061/35720 Training loss: 0.8343 0.2097 sec/batch\n",
      "Epoch 11/20  Iteration 18062/35720 Training loss: 0.8342 0.2227 sec/batch\n",
      "Epoch 11/20  Iteration 18063/35720 Training loss: 0.8343 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 18064/35720 Training loss: 0.8342 0.2212 sec/batch\n",
      "Epoch 11/20  Iteration 18065/35720 Training loss: 0.8342 0.2286 sec/batch\n",
      "Epoch 11/20  Iteration 18066/35720 Training loss: 0.8342 0.2100 sec/batch\n",
      "Epoch 11/20  Iteration 18067/35720 Training loss: 0.8346 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 18068/35720 Training loss: 0.8351 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 18069/35720 Training loss: 0.8354 0.2194 sec/batch\n",
      "Epoch 11/20  Iteration 18070/35720 Training loss: 0.8355 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 18071/35720 Training loss: 0.8357 0.2099 sec/batch\n",
      "Epoch 11/20  Iteration 18072/35720 Training loss: 0.8357 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 18073/35720 Training loss: 0.8356 0.2222 sec/batch\n",
      "Epoch 11/20  Iteration 18074/35720 Training loss: 0.8356 0.2077 sec/batch\n",
      "Epoch 11/20  Iteration 18075/35720 Training loss: 0.8356 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 18076/35720 Training loss: 0.8354 0.2125 sec/batch\n",
      "Epoch 11/20  Iteration 18077/35720 Training loss: 0.8353 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 18078/35720 Training loss: 0.8351 0.2108 sec/batch\n",
      "Epoch 11/20  Iteration 18079/35720 Training loss: 0.8351 0.2187 sec/batch\n",
      "Epoch 11/20  Iteration 18080/35720 Training loss: 0.8352 0.2182 sec/batch\n",
      "Epoch 11/20  Iteration 18081/35720 Training loss: 0.8354 0.2245 sec/batch\n",
      "Epoch 11/20  Iteration 18082/35720 Training loss: 0.8354 0.2101 sec/batch\n",
      "Epoch 11/20  Iteration 18083/35720 Training loss: 0.8358 0.2080 sec/batch\n",
      "Epoch 11/20  Iteration 18084/35720 Training loss: 0.8360 0.2257 sec/batch\n",
      "Epoch 11/20  Iteration 18085/35720 Training loss: 0.8361 0.2341 sec/batch\n",
      "Epoch 11/20  Iteration 18086/35720 Training loss: 0.8361 0.2240 sec/batch\n",
      "Epoch 11/20  Iteration 18087/35720 Training loss: 0.8359 0.2057 sec/batch\n",
      "Epoch 11/20  Iteration 18088/35720 Training loss: 0.8359 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 18089/35720 Training loss: 0.8356 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 18090/35720 Training loss: 0.8357 0.2199 sec/batch\n",
      "Epoch 11/20  Iteration 18091/35720 Training loss: 0.8361 0.2315 sec/batch\n",
      "Epoch 11/20  Iteration 18092/35720 Training loss: 0.8360 0.2265 sec/batch\n",
      "Epoch 11/20  Iteration 18093/35720 Training loss: 0.8360 0.2060 sec/batch\n",
      "Epoch 11/20  Iteration 18094/35720 Training loss: 0.8360 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 18095/35720 Training loss: 0.8360 0.2335 sec/batch\n",
      "Epoch 11/20  Iteration 18096/35720 Training loss: 0.8360 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 18097/35720 Training loss: 0.8362 0.2285 sec/batch\n",
      "Epoch 11/20  Iteration 18098/35720 Training loss: 0.8360 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18099/35720 Training loss: 0.8358 0.2122 sec/batch\n",
      "Epoch 11/20  Iteration 18100/35720 Training loss: 0.8357 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 18101/35720 Training loss: 0.8356 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18102/35720 Training loss: 0.8355 0.2190 sec/batch\n",
      "Epoch 11/20  Iteration 18103/35720 Training loss: 0.8355 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 18104/35720 Training loss: 0.8355 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 18105/35720 Training loss: 0.8352 0.2143 sec/batch\n",
      "Epoch 11/20  Iteration 18106/35720 Training loss: 0.8353 0.2108 sec/batch\n",
      "Epoch 11/20  Iteration 18107/35720 Training loss: 0.8353 0.2336 sec/batch\n",
      "Epoch 11/20  Iteration 18108/35720 Training loss: 0.8351 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 18109/35720 Training loss: 0.8350 0.2071 sec/batch\n",
      "Epoch 11/20  Iteration 18110/35720 Training loss: 0.8348 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 18111/35720 Training loss: 0.8348 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 18112/35720 Training loss: 0.8348 0.2337 sec/batch\n",
      "Epoch 11/20  Iteration 18113/35720 Training loss: 0.8345 0.2190 sec/batch\n",
      "Epoch 11/20  Iteration 18114/35720 Training loss: 0.8347 0.2254 sec/batch\n",
      "Epoch 11/20  Iteration 18115/35720 Training loss: 0.8349 0.2075 sec/batch\n",
      "Epoch 11/20  Iteration 18116/35720 Training loss: 0.8351 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 18117/35720 Training loss: 0.8350 0.2302 sec/batch\n",
      "Epoch 11/20  Iteration 18118/35720 Training loss: 0.8350 0.2080 sec/batch\n",
      "Epoch 11/20  Iteration 18119/35720 Training loss: 0.8352 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 18120/35720 Training loss: 0.8353 0.2285 sec/batch\n",
      "Epoch 11/20  Iteration 18121/35720 Training loss: 0.8352 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18122/35720 Training loss: 0.8351 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18123/35720 Training loss: 0.8350 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 18124/35720 Training loss: 0.8351 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18125/35720 Training loss: 0.8351 0.2241 sec/batch\n",
      "Epoch 11/20  Iteration 18126/35720 Training loss: 0.8353 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 18127/35720 Training loss: 0.8352 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18128/35720 Training loss: 0.8352 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 18129/35720 Training loss: 0.8351 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 18130/35720 Training loss: 0.8347 0.2131 sec/batch\n",
      "Epoch 11/20  Iteration 18131/35720 Training loss: 0.8344 0.2138 sec/batch\n",
      "Epoch 11/20  Iteration 18132/35720 Training loss: 0.8344 0.2287 sec/batch\n",
      "Epoch 11/20  Iteration 18133/35720 Training loss: 0.8345 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 18134/35720 Training loss: 0.8345 0.2307 sec/batch\n",
      "Epoch 11/20  Iteration 18135/35720 Training loss: 0.8344 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 18136/35720 Training loss: 0.8342 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 18137/35720 Training loss: 0.8340 0.2294 sec/batch\n",
      "Epoch 11/20  Iteration 18138/35720 Training loss: 0.8338 0.2123 sec/batch\n",
      "Epoch 11/20  Iteration 18139/35720 Training loss: 0.8336 0.2218 sec/batch\n",
      "Epoch 11/20  Iteration 18140/35720 Training loss: 0.8336 0.2138 sec/batch\n",
      "Epoch 11/20  Iteration 18141/35720 Training loss: 0.8335 0.2349 sec/batch\n",
      "Epoch 11/20  Iteration 18142/35720 Training loss: 0.8332 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 18143/35720 Training loss: 0.8329 0.2104 sec/batch\n",
      "Epoch 11/20  Iteration 18144/35720 Training loss: 0.8328 0.2147 sec/batch\n",
      "Epoch 11/20  Iteration 18145/35720 Training loss: 0.8330 0.2291 sec/batch\n",
      "Epoch 11/20  Iteration 18146/35720 Training loss: 0.8330 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 18147/35720 Training loss: 0.8327 0.2221 sec/batch\n",
      "Epoch 11/20  Iteration 18148/35720 Training loss: 0.8327 0.2154 sec/batch\n",
      "Epoch 11/20  Iteration 18149/35720 Training loss: 0.8327 0.2190 sec/batch\n",
      "Epoch 11/20  Iteration 18150/35720 Training loss: 0.8327 0.2171 sec/batch\n",
      "Epoch 11/20  Iteration 18151/35720 Training loss: 0.8327 0.2184 sec/batch\n",
      "Epoch 11/20  Iteration 18152/35720 Training loss: 0.8326 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 18153/35720 Training loss: 0.8326 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 18154/35720 Training loss: 0.8326 0.2170 sec/batch\n",
      "Epoch 11/20  Iteration 18155/35720 Training loss: 0.8329 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 18156/35720 Training loss: 0.8328 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 18157/35720 Training loss: 0.8328 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 18158/35720 Training loss: 0.8330 0.2187 sec/batch\n",
      "Epoch 11/20  Iteration 18159/35720 Training loss: 0.8328 0.2263 sec/batch\n",
      "Epoch 11/20  Iteration 18160/35720 Training loss: 0.8328 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 18161/35720 Training loss: 0.8328 0.2217 sec/batch\n",
      "Epoch 11/20  Iteration 18162/35720 Training loss: 0.8326 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 18163/35720 Training loss: 0.8327 0.2173 sec/batch\n",
      "Epoch 11/20  Iteration 18164/35720 Training loss: 0.8327 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 18165/35720 Training loss: 0.8326 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 18166/35720 Training loss: 0.8325 0.2176 sec/batch\n",
      "Epoch 11/20  Iteration 18167/35720 Training loss: 0.8324 0.2210 sec/batch\n",
      "Epoch 11/20  Iteration 18168/35720 Training loss: 0.8325 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 18169/35720 Training loss: 0.8323 0.2273 sec/batch\n",
      "Epoch 11/20  Iteration 18170/35720 Training loss: 0.8322 0.2078 sec/batch\n",
      "Epoch 11/20  Iteration 18171/35720 Training loss: 0.8322 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 18172/35720 Training loss: 0.8320 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 18173/35720 Training loss: 0.8319 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 18174/35720 Training loss: 0.8319 0.2281 sec/batch\n",
      "Epoch 11/20  Iteration 18175/35720 Training loss: 0.8319 0.2274 sec/batch\n",
      "Epoch 11/20  Iteration 18176/35720 Training loss: 0.8319 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 18177/35720 Training loss: 0.8317 0.2218 sec/batch\n",
      "Epoch 11/20  Iteration 18178/35720 Training loss: 0.8318 0.2206 sec/batch\n",
      "Epoch 11/20  Iteration 18179/35720 Training loss: 0.8319 0.2182 sec/batch\n",
      "Epoch 11/20  Iteration 18180/35720 Training loss: 0.8318 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 18181/35720 Training loss: 0.8319 0.2203 sec/batch\n",
      "Epoch 11/20  Iteration 18182/35720 Training loss: 0.8319 0.2100 sec/batch\n",
      "Epoch 11/20  Iteration 18183/35720 Training loss: 0.8320 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 18184/35720 Training loss: 0.8320 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 18185/35720 Training loss: 0.8318 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 18186/35720 Training loss: 0.8321 0.2170 sec/batch\n",
      "Epoch 11/20  Iteration 18187/35720 Training loss: 0.8322 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 18188/35720 Training loss: 0.8322 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18189/35720 Training loss: 0.8323 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 18190/35720 Training loss: 0.8324 0.2198 sec/batch\n",
      "Epoch 11/20  Iteration 18191/35720 Training loss: 0.8324 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 18192/35720 Training loss: 0.8325 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 18193/35720 Training loss: 0.8325 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 18194/35720 Training loss: 0.8325 0.2158 sec/batch\n",
      "Epoch 11/20  Iteration 18195/35720 Training loss: 0.8325 0.2215 sec/batch\n",
      "Epoch 11/20  Iteration 18196/35720 Training loss: 0.8323 0.2280 sec/batch\n",
      "Epoch 11/20  Iteration 18197/35720 Training loss: 0.8323 0.2256 sec/batch\n",
      "Epoch 11/20  Iteration 18198/35720 Training loss: 0.8322 0.2071 sec/batch\n",
      "Epoch 11/20  Iteration 18199/35720 Training loss: 0.8322 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 18200/35720 Training loss: 0.8322 0.2109 sec/batch\n",
      "Validation loss: 1.42578 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 18201/35720 Training loss: 0.8329 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 18202/35720 Training loss: 0.8329 0.2074 sec/batch\n",
      "Epoch 11/20  Iteration 18203/35720 Training loss: 0.8330 0.2072 sec/batch\n",
      "Epoch 11/20  Iteration 18204/35720 Training loss: 0.8331 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 18205/35720 Training loss: 0.8328 0.2182 sec/batch\n",
      "Epoch 11/20  Iteration 18206/35720 Training loss: 0.8329 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 18207/35720 Training loss: 0.8330 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 18208/35720 Training loss: 0.8331 0.2230 sec/batch\n",
      "Epoch 11/20  Iteration 18209/35720 Training loss: 0.8332 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 18210/35720 Training loss: 0.8332 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18211/35720 Training loss: 0.8333 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 18212/35720 Training loss: 0.8332 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 18213/35720 Training loss: 0.8332 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18214/35720 Training loss: 0.8332 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 18215/35720 Training loss: 0.8333 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 18216/35720 Training loss: 0.8334 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 18217/35720 Training loss: 0.8335 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 18218/35720 Training loss: 0.8335 0.2210 sec/batch\n",
      "Epoch 11/20  Iteration 18219/35720 Training loss: 0.8334 0.2180 sec/batch\n",
      "Epoch 11/20  Iteration 18220/35720 Training loss: 0.8334 0.2149 sec/batch\n",
      "Epoch 11/20  Iteration 18221/35720 Training loss: 0.8334 0.2338 sec/batch\n",
      "Epoch 11/20  Iteration 18222/35720 Training loss: 0.8334 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18223/35720 Training loss: 0.8335 0.2058 sec/batch\n",
      "Epoch 11/20  Iteration 18224/35720 Training loss: 0.8335 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 18225/35720 Training loss: 0.8335 0.2220 sec/batch\n",
      "Epoch 11/20  Iteration 18226/35720 Training loss: 0.8334 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 18227/35720 Training loss: 0.8334 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 18228/35720 Training loss: 0.8332 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18229/35720 Training loss: 0.8331 0.2331 sec/batch\n",
      "Epoch 11/20  Iteration 18230/35720 Training loss: 0.8330 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 18231/35720 Training loss: 0.8329 0.2075 sec/batch\n",
      "Epoch 11/20  Iteration 18232/35720 Training loss: 0.8328 0.2127 sec/batch\n",
      "Epoch 11/20  Iteration 18233/35720 Training loss: 0.8328 0.2302 sec/batch\n",
      "Epoch 11/20  Iteration 18234/35720 Training loss: 0.8327 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 18235/35720 Training loss: 0.8327 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 18236/35720 Training loss: 0.8328 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18237/35720 Training loss: 0.8328 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18238/35720 Training loss: 0.8328 0.2286 sec/batch\n",
      "Epoch 11/20  Iteration 18239/35720 Training loss: 0.8326 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 18240/35720 Training loss: 0.8325 0.2238 sec/batch\n",
      "Epoch 11/20  Iteration 18241/35720 Training loss: 0.8324 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18242/35720 Training loss: 0.8323 0.2256 sec/batch\n",
      "Epoch 11/20  Iteration 18243/35720 Training loss: 0.8323 0.2190 sec/batch\n",
      "Epoch 11/20  Iteration 18244/35720 Training loss: 0.8324 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 18245/35720 Training loss: 0.8324 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 18246/35720 Training loss: 0.8324 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 18247/35720 Training loss: 0.8323 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 18248/35720 Training loss: 0.8324 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 18249/35720 Training loss: 0.8324 0.2275 sec/batch\n",
      "Epoch 11/20  Iteration 18250/35720 Training loss: 0.8323 0.2103 sec/batch\n",
      "Epoch 11/20  Iteration 18251/35720 Training loss: 0.8323 0.2252 sec/batch\n",
      "Epoch 11/20  Iteration 18252/35720 Training loss: 0.8321 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 18253/35720 Training loss: 0.8321 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18254/35720 Training loss: 0.8319 0.2124 sec/batch\n",
      "Epoch 11/20  Iteration 18255/35720 Training loss: 0.8319 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 18256/35720 Training loss: 0.8319 0.2259 sec/batch\n",
      "Epoch 11/20  Iteration 18257/35720 Training loss: 0.8318 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 18258/35720 Training loss: 0.8316 0.2269 sec/batch\n",
      "Epoch 11/20  Iteration 18259/35720 Training loss: 0.8315 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 18260/35720 Training loss: 0.8314 0.2267 sec/batch\n",
      "Epoch 11/20  Iteration 18261/35720 Training loss: 0.8314 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18262/35720 Training loss: 0.8315 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 18263/35720 Training loss: 0.8315 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 18264/35720 Training loss: 0.8314 0.2149 sec/batch\n",
      "Epoch 11/20  Iteration 18265/35720 Training loss: 0.8313 0.2183 sec/batch\n",
      "Epoch 11/20  Iteration 18266/35720 Training loss: 0.8312 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18267/35720 Training loss: 0.8311 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 18268/35720 Training loss: 0.8311 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 18269/35720 Training loss: 0.8310 0.2070 sec/batch\n",
      "Epoch 11/20  Iteration 18270/35720 Training loss: 0.8308 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18271/35720 Training loss: 0.8308 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 18272/35720 Training loss: 0.8307 0.2147 sec/batch\n",
      "Epoch 11/20  Iteration 18273/35720 Training loss: 0.8304 0.2232 sec/batch\n",
      "Epoch 11/20  Iteration 18274/35720 Training loss: 0.8304 0.2235 sec/batch\n",
      "Epoch 11/20  Iteration 18275/35720 Training loss: 0.8303 0.2076 sec/batch\n",
      "Epoch 11/20  Iteration 18276/35720 Training loss: 0.8303 0.2231 sec/batch\n",
      "Epoch 11/20  Iteration 18277/35720 Training loss: 0.8302 0.2127 sec/batch\n",
      "Epoch 11/20  Iteration 18278/35720 Training loss: 0.8300 0.2371 sec/batch\n",
      "Epoch 11/20  Iteration 18279/35720 Training loss: 0.8299 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 18280/35720 Training loss: 0.8298 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 18281/35720 Training loss: 0.8299 0.2080 sec/batch\n",
      "Epoch 11/20  Iteration 18282/35720 Training loss: 0.8299 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 18283/35720 Training loss: 0.8300 0.2207 sec/batch\n",
      "Epoch 11/20  Iteration 18284/35720 Training loss: 0.8300 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 18285/35720 Training loss: 0.8301 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 18286/35720 Training loss: 0.8298 0.2058 sec/batch\n",
      "Epoch 11/20  Iteration 18287/35720 Training loss: 0.8297 0.2143 sec/batch\n",
      "Epoch 11/20  Iteration 18288/35720 Training loss: 0.8298 0.2259 sec/batch\n",
      "Epoch 11/20  Iteration 18289/35720 Training loss: 0.8297 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 18290/35720 Training loss: 0.8298 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 18291/35720 Training loss: 0.8299 0.2222 sec/batch\n",
      "Epoch 11/20  Iteration 18292/35720 Training loss: 0.8299 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18293/35720 Training loss: 0.8300 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 18294/35720 Training loss: 0.8301 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 18295/35720 Training loss: 0.8301 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 18296/35720 Training loss: 0.8300 0.2254 sec/batch\n",
      "Epoch 11/20  Iteration 18297/35720 Training loss: 0.8302 0.2209 sec/batch\n",
      "Epoch 11/20  Iteration 18298/35720 Training loss: 0.8303 0.2136 sec/batch\n",
      "Epoch 11/20  Iteration 18299/35720 Training loss: 0.8304 0.2185 sec/batch\n",
      "Epoch 11/20  Iteration 18300/35720 Training loss: 0.8304 0.2176 sec/batch\n",
      "Epoch 11/20  Iteration 18301/35720 Training loss: 0.8305 0.2215 sec/batch\n",
      "Epoch 11/20  Iteration 18302/35720 Training loss: 0.8306 0.2359 sec/batch\n",
      "Epoch 11/20  Iteration 18303/35720 Training loss: 0.8307 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18304/35720 Training loss: 0.8307 0.2125 sec/batch\n",
      "Epoch 11/20  Iteration 18305/35720 Training loss: 0.8307 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 18306/35720 Training loss: 0.8308 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 18307/35720 Training loss: 0.8311 0.2258 sec/batch\n",
      "Epoch 11/20  Iteration 18308/35720 Training loss: 0.8313 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 18309/35720 Training loss: 0.8314 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18310/35720 Training loss: 0.8313 0.2302 sec/batch\n",
      "Epoch 11/20  Iteration 18311/35720 Training loss: 0.8311 0.2144 sec/batch\n",
      "Epoch 11/20  Iteration 18312/35720 Training loss: 0.8311 0.2233 sec/batch\n",
      "Epoch 11/20  Iteration 18313/35720 Training loss: 0.8310 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18314/35720 Training loss: 0.8312 0.2057 sec/batch\n",
      "Epoch 11/20  Iteration 18315/35720 Training loss: 0.8313 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 18316/35720 Training loss: 0.8315 0.2239 sec/batch\n",
      "Epoch 11/20  Iteration 18317/35720 Training loss: 0.8317 0.2212 sec/batch\n",
      "Epoch 11/20  Iteration 18318/35720 Training loss: 0.8317 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 18319/35720 Training loss: 0.8316 0.2154 sec/batch\n",
      "Epoch 11/20  Iteration 18320/35720 Training loss: 0.8315 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 18321/35720 Training loss: 0.8316 0.2250 sec/batch\n",
      "Epoch 11/20  Iteration 18322/35720 Training loss: 0.8316 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18323/35720 Training loss: 0.8317 0.2267 sec/batch\n",
      "Epoch 11/20  Iteration 18324/35720 Training loss: 0.8317 0.2113 sec/batch\n",
      "Epoch 11/20  Iteration 18325/35720 Training loss: 0.8318 0.2303 sec/batch\n",
      "Epoch 11/20  Iteration 18326/35720 Training loss: 0.8318 0.2100 sec/batch\n",
      "Epoch 11/20  Iteration 18327/35720 Training loss: 0.8318 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18328/35720 Training loss: 0.8318 0.2154 sec/batch\n",
      "Epoch 11/20  Iteration 18329/35720 Training loss: 0.8317 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18330/35720 Training loss: 0.8316 0.2079 sec/batch\n",
      "Epoch 11/20  Iteration 18331/35720 Training loss: 0.8316 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18332/35720 Training loss: 0.8316 0.2158 sec/batch\n",
      "Epoch 11/20  Iteration 18333/35720 Training loss: 0.8316 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 18334/35720 Training loss: 0.8314 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 18335/35720 Training loss: 0.8315 0.2287 sec/batch\n",
      "Epoch 11/20  Iteration 18336/35720 Training loss: 0.8315 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 18337/35720 Training loss: 0.8315 0.2147 sec/batch\n",
      "Epoch 11/20  Iteration 18338/35720 Training loss: 0.8314 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 18339/35720 Training loss: 0.8315 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 18340/35720 Training loss: 0.8313 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18341/35720 Training loss: 0.8312 0.2204 sec/batch\n",
      "Epoch 11/20  Iteration 18342/35720 Training loss: 0.8311 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 18343/35720 Training loss: 0.8311 0.2242 sec/batch\n",
      "Epoch 11/20  Iteration 18344/35720 Training loss: 0.8310 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18345/35720 Training loss: 0.8310 0.2308 sec/batch\n",
      "Epoch 11/20  Iteration 18346/35720 Training loss: 0.8309 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 18347/35720 Training loss: 0.8308 0.2186 sec/batch\n",
      "Epoch 11/20  Iteration 18348/35720 Training loss: 0.8307 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 18349/35720 Training loss: 0.8307 0.2242 sec/batch\n",
      "Epoch 11/20  Iteration 18350/35720 Training loss: 0.8306 0.2258 sec/batch\n",
      "Epoch 11/20  Iteration 18351/35720 Training loss: 0.8306 0.2101 sec/batch\n",
      "Epoch 11/20  Iteration 18352/35720 Training loss: 0.8307 0.2170 sec/batch\n",
      "Epoch 11/20  Iteration 18353/35720 Training loss: 0.8307 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 18354/35720 Training loss: 0.8306 0.2296 sec/batch\n",
      "Epoch 11/20  Iteration 18355/35720 Training loss: 0.8306 0.2136 sec/batch\n",
      "Epoch 11/20  Iteration 18356/35720 Training loss: 0.8305 0.2308 sec/batch\n",
      "Epoch 11/20  Iteration 18357/35720 Training loss: 0.8305 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 18358/35720 Training loss: 0.8306 0.2209 sec/batch\n",
      "Epoch 11/20  Iteration 18359/35720 Training loss: 0.8305 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 18360/35720 Training loss: 0.8306 0.2305 sec/batch\n",
      "Epoch 11/20  Iteration 18361/35720 Training loss: 0.8305 0.2229 sec/batch\n",
      "Epoch 11/20  Iteration 18362/35720 Training loss: 0.8304 0.2124 sec/batch\n",
      "Epoch 11/20  Iteration 18363/35720 Training loss: 0.8302 0.2221 sec/batch\n",
      "Epoch 11/20  Iteration 18364/35720 Training loss: 0.8301 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 18365/35720 Training loss: 0.8301 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 18366/35720 Training loss: 0.8301 0.2183 sec/batch\n",
      "Epoch 11/20  Iteration 18367/35720 Training loss: 0.8301 0.2259 sec/batch\n",
      "Epoch 11/20  Iteration 18368/35720 Training loss: 0.8300 0.2213 sec/batch\n",
      "Epoch 11/20  Iteration 18369/35720 Training loss: 0.8301 0.2252 sec/batch\n",
      "Epoch 11/20  Iteration 18370/35720 Training loss: 0.8301 0.2201 sec/batch\n",
      "Epoch 11/20  Iteration 18371/35720 Training loss: 0.8299 0.2196 sec/batch\n",
      "Epoch 11/20  Iteration 18372/35720 Training loss: 0.8299 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 18373/35720 Training loss: 0.8299 0.2059 sec/batch\n",
      "Epoch 11/20  Iteration 18374/35720 Training loss: 0.8299 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 18375/35720 Training loss: 0.8299 0.2054 sec/batch\n",
      "Epoch 11/20  Iteration 18376/35720 Training loss: 0.8299 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 18377/35720 Training loss: 0.8299 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 18378/35720 Training loss: 0.8299 0.2328 sec/batch\n",
      "Epoch 11/20  Iteration 18379/35720 Training loss: 0.8299 0.2200 sec/batch\n",
      "Epoch 11/20  Iteration 18380/35720 Training loss: 0.8300 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 18381/35720 Training loss: 0.8299 0.2101 sec/batch\n",
      "Epoch 11/20  Iteration 18382/35720 Training loss: 0.8299 0.2187 sec/batch\n",
      "Epoch 11/20  Iteration 18383/35720 Training loss: 0.8299 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 18384/35720 Training loss: 0.8298 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 18385/35720 Training loss: 0.8298 0.2109 sec/batch\n",
      "Epoch 11/20  Iteration 18386/35720 Training loss: 0.8298 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 18387/35720 Training loss: 0.8298 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 18388/35720 Training loss: 0.8297 0.2185 sec/batch\n",
      "Epoch 11/20  Iteration 18389/35720 Training loss: 0.8298 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 18390/35720 Training loss: 0.8297 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 18391/35720 Training loss: 0.8297 0.2170 sec/batch\n",
      "Epoch 11/20  Iteration 18392/35720 Training loss: 0.8296 0.2184 sec/batch\n",
      "Epoch 11/20  Iteration 18393/35720 Training loss: 0.8296 0.2191 sec/batch\n",
      "Epoch 11/20  Iteration 18394/35720 Training loss: 0.8296 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 18395/35720 Training loss: 0.8295 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18396/35720 Training loss: 0.8294 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 18397/35720 Training loss: 0.8294 0.2068 sec/batch\n",
      "Epoch 11/20  Iteration 18398/35720 Training loss: 0.8293 0.2078 sec/batch\n",
      "Epoch 11/20  Iteration 18399/35720 Training loss: 0.8293 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 18400/35720 Training loss: 0.8292 0.2180 sec/batch\n",
      "Validation loss: 1.42722 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 18401/35720 Training loss: 0.8297 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 18402/35720 Training loss: 0.8296 0.2085 sec/batch\n",
      "Epoch 11/20  Iteration 18403/35720 Training loss: 0.8295 0.2216 sec/batch\n",
      "Epoch 11/20  Iteration 18404/35720 Training loss: 0.8295 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18405/35720 Training loss: 0.8295 0.2172 sec/batch\n",
      "Epoch 11/20  Iteration 18406/35720 Training loss: 0.8296 0.2076 sec/batch\n",
      "Epoch 11/20  Iteration 18407/35720 Training loss: 0.8296 0.2109 sec/batch\n",
      "Epoch 11/20  Iteration 18408/35720 Training loss: 0.8296 0.2257 sec/batch\n",
      "Epoch 11/20  Iteration 18409/35720 Training loss: 0.8295 0.2186 sec/batch\n",
      "Epoch 11/20  Iteration 18410/35720 Training loss: 0.8296 0.2204 sec/batch\n",
      "Epoch 11/20  Iteration 18411/35720 Training loss: 0.8295 0.2182 sec/batch\n",
      "Epoch 11/20  Iteration 18412/35720 Training loss: 0.8295 0.2230 sec/batch\n",
      "Epoch 11/20  Iteration 18413/35720 Training loss: 0.8294 0.2075 sec/batch\n",
      "Epoch 11/20  Iteration 18414/35720 Training loss: 0.8294 0.2072 sec/batch\n",
      "Epoch 11/20  Iteration 18415/35720 Training loss: 0.8294 0.2104 sec/batch\n",
      "Epoch 11/20  Iteration 18416/35720 Training loss: 0.8295 0.2312 sec/batch\n",
      "Epoch 11/20  Iteration 18417/35720 Training loss: 0.8294 0.2171 sec/batch\n",
      "Epoch 11/20  Iteration 18418/35720 Training loss: 0.8295 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 18419/35720 Training loss: 0.8295 0.2097 sec/batch\n",
      "Epoch 11/20  Iteration 18420/35720 Training loss: 0.8295 0.2257 sec/batch\n",
      "Epoch 11/20  Iteration 18421/35720 Training loss: 0.8296 0.2267 sec/batch\n",
      "Epoch 11/20  Iteration 18422/35720 Training loss: 0.8295 0.2123 sec/batch\n",
      "Epoch 11/20  Iteration 18423/35720 Training loss: 0.8294 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 18424/35720 Training loss: 0.8295 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18425/35720 Training loss: 0.8293 0.2242 sec/batch\n",
      "Epoch 11/20  Iteration 18426/35720 Training loss: 0.8292 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 18427/35720 Training loss: 0.8293 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 18428/35720 Training loss: 0.8291 0.2073 sec/batch\n",
      "Epoch 11/20  Iteration 18429/35720 Training loss: 0.8291 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 18430/35720 Training loss: 0.8290 0.2259 sec/batch\n",
      "Epoch 11/20  Iteration 18431/35720 Training loss: 0.8290 0.2276 sec/batch\n",
      "Epoch 11/20  Iteration 18432/35720 Training loss: 0.8290 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 18433/35720 Training loss: 0.8291 0.2287 sec/batch\n",
      "Epoch 11/20  Iteration 18434/35720 Training loss: 0.8292 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 18435/35720 Training loss: 0.8292 0.2261 sec/batch\n",
      "Epoch 11/20  Iteration 18436/35720 Training loss: 0.8292 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18437/35720 Training loss: 0.8292 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 18438/35720 Training loss: 0.8292 0.2224 sec/batch\n",
      "Epoch 11/20  Iteration 18439/35720 Training loss: 0.8292 0.2176 sec/batch\n",
      "Epoch 11/20  Iteration 18440/35720 Training loss: 0.8291 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 18441/35720 Training loss: 0.8292 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18442/35720 Training loss: 0.8292 0.2216 sec/batch\n",
      "Epoch 11/20  Iteration 18443/35720 Training loss: 0.8292 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18444/35720 Training loss: 0.8291 0.2204 sec/batch\n",
      "Epoch 11/20  Iteration 18445/35720 Training loss: 0.8291 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 18446/35720 Training loss: 0.8289 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18447/35720 Training loss: 0.8288 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18448/35720 Training loss: 0.8287 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 18449/35720 Training loss: 0.8286 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 18450/35720 Training loss: 0.8285 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 18451/35720 Training loss: 0.8285 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 18452/35720 Training loss: 0.8285 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 18453/35720 Training loss: 0.8285 0.2293 sec/batch\n",
      "Epoch 11/20  Iteration 18454/35720 Training loss: 0.8285 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 18455/35720 Training loss: 0.8284 0.2245 sec/batch\n",
      "Epoch 11/20  Iteration 18456/35720 Training loss: 0.8283 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 18457/35720 Training loss: 0.8282 0.2219 sec/batch\n",
      "Epoch 11/20  Iteration 18458/35720 Training loss: 0.8283 0.2160 sec/batch\n",
      "Epoch 11/20  Iteration 18459/35720 Training loss: 0.8282 0.2104 sec/batch\n",
      "Epoch 11/20  Iteration 18460/35720 Training loss: 0.8280 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18461/35720 Training loss: 0.8280 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 18462/35720 Training loss: 0.8278 0.2217 sec/batch\n",
      "Epoch 11/20  Iteration 18463/35720 Training loss: 0.8278 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 18464/35720 Training loss: 0.8277 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 18465/35720 Training loss: 0.8277 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18466/35720 Training loss: 0.8277 0.2054 sec/batch\n",
      "Epoch 11/20  Iteration 18467/35720 Training loss: 0.8276 0.2131 sec/batch\n",
      "Epoch 11/20  Iteration 18468/35720 Training loss: 0.8276 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 18469/35720 Training loss: 0.8277 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18470/35720 Training loss: 0.8276 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 18471/35720 Training loss: 0.8276 0.2085 sec/batch\n",
      "Epoch 11/20  Iteration 18472/35720 Training loss: 0.8274 0.2131 sec/batch\n",
      "Epoch 11/20  Iteration 18473/35720 Training loss: 0.8274 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18474/35720 Training loss: 0.8274 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 18475/35720 Training loss: 0.8274 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 18476/35720 Training loss: 0.8273 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18477/35720 Training loss: 0.8272 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 18478/35720 Training loss: 0.8272 0.2227 sec/batch\n",
      "Epoch 11/20  Iteration 18479/35720 Training loss: 0.8271 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 18480/35720 Training loss: 0.8270 0.2299 sec/batch\n",
      "Epoch 11/20  Iteration 18481/35720 Training loss: 0.8270 0.2123 sec/batch\n",
      "Epoch 11/20  Iteration 18482/35720 Training loss: 0.8270 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 18483/35720 Training loss: 0.8269 0.2239 sec/batch\n",
      "Epoch 11/20  Iteration 18484/35720 Training loss: 0.8269 0.2078 sec/batch\n",
      "Epoch 11/20  Iteration 18485/35720 Training loss: 0.8268 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18486/35720 Training loss: 0.8268 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18487/35720 Training loss: 0.8268 0.2262 sec/batch\n",
      "Epoch 11/20  Iteration 18488/35720 Training loss: 0.8267 0.2245 sec/batch\n",
      "Epoch 11/20  Iteration 18489/35720 Training loss: 0.8267 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18490/35720 Training loss: 0.8266 0.2049 sec/batch\n",
      "Epoch 11/20  Iteration 18491/35720 Training loss: 0.8267 0.2100 sec/batch\n",
      "Epoch 11/20  Iteration 18492/35720 Training loss: 0.8266 0.2155 sec/batch\n",
      "Epoch 11/20  Iteration 18493/35720 Training loss: 0.8265 0.2188 sec/batch\n",
      "Epoch 11/20  Iteration 18494/35720 Training loss: 0.8265 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 18495/35720 Training loss: 0.8265 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 18496/35720 Training loss: 0.8265 0.2216 sec/batch\n",
      "Epoch 11/20  Iteration 18497/35720 Training loss: 0.8264 0.2123 sec/batch\n",
      "Epoch 11/20  Iteration 18498/35720 Training loss: 0.8263 0.2220 sec/batch\n",
      "Epoch 11/20  Iteration 18499/35720 Training loss: 0.8263 0.2225 sec/batch\n",
      "Epoch 11/20  Iteration 18500/35720 Training loss: 0.8263 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 18501/35720 Training loss: 0.8264 0.2069 sec/batch\n",
      "Epoch 11/20  Iteration 18502/35720 Training loss: 0.8264 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18503/35720 Training loss: 0.8264 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 18504/35720 Training loss: 0.8263 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18505/35720 Training loss: 0.8263 0.2167 sec/batch\n",
      "Epoch 11/20  Iteration 18506/35720 Training loss: 0.8264 0.2193 sec/batch\n",
      "Epoch 11/20  Iteration 18507/35720 Training loss: 0.8263 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 18508/35720 Training loss: 0.8263 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18509/35720 Training loss: 0.8262 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 18510/35720 Training loss: 0.8261 0.2192 sec/batch\n",
      "Epoch 11/20  Iteration 18511/35720 Training loss: 0.8261 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 18512/35720 Training loss: 0.8261 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 18513/35720 Training loss: 0.8261 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 18514/35720 Training loss: 0.8262 0.2177 sec/batch\n",
      "Epoch 11/20  Iteration 18515/35720 Training loss: 0.8261 0.2199 sec/batch\n",
      "Epoch 11/20  Iteration 18516/35720 Training loss: 0.8262 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18517/35720 Training loss: 0.8263 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 18518/35720 Training loss: 0.8263 0.2099 sec/batch\n",
      "Epoch 11/20  Iteration 18519/35720 Training loss: 0.8263 0.2144 sec/batch\n",
      "Epoch 11/20  Iteration 18520/35720 Training loss: 0.8264 0.2221 sec/batch\n",
      "Epoch 11/20  Iteration 18521/35720 Training loss: 0.8264 0.2138 sec/batch\n",
      "Epoch 11/20  Iteration 18522/35720 Training loss: 0.8265 0.2188 sec/batch\n",
      "Epoch 11/20  Iteration 18523/35720 Training loss: 0.8265 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 18524/35720 Training loss: 0.8265 0.2103 sec/batch\n",
      "Epoch 11/20  Iteration 18525/35720 Training loss: 0.8265 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18526/35720 Training loss: 0.8265 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 18527/35720 Training loss: 0.8267 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18528/35720 Training loss: 0.8266 0.2247 sec/batch\n",
      "Epoch 11/20  Iteration 18529/35720 Training loss: 0.8265 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18530/35720 Training loss: 0.8265 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18531/35720 Training loss: 0.8263 0.2128 sec/batch\n",
      "Epoch 11/20  Iteration 18532/35720 Training loss: 0.8263 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18533/35720 Training loss: 0.8264 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 18534/35720 Training loss: 0.8263 0.2248 sec/batch\n",
      "Epoch 11/20  Iteration 18535/35720 Training loss: 0.8263 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 18536/35720 Training loss: 0.8262 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18537/35720 Training loss: 0.8262 0.2258 sec/batch\n",
      "Epoch 11/20  Iteration 18538/35720 Training loss: 0.8261 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18539/35720 Training loss: 0.8262 0.2191 sec/batch\n",
      "Epoch 11/20  Iteration 18540/35720 Training loss: 0.8261 0.2144 sec/batch\n",
      "Epoch 11/20  Iteration 18541/35720 Training loss: 0.8261 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 18542/35720 Training loss: 0.8261 0.2201 sec/batch\n",
      "Epoch 11/20  Iteration 18543/35720 Training loss: 0.8260 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18544/35720 Training loss: 0.8260 0.2295 sec/batch\n",
      "Epoch 11/20  Iteration 18545/35720 Training loss: 0.8260 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 18546/35720 Training loss: 0.8260 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 18547/35720 Training loss: 0.8259 0.2230 sec/batch\n",
      "Epoch 11/20  Iteration 18548/35720 Training loss: 0.8259 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 18549/35720 Training loss: 0.8259 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 18550/35720 Training loss: 0.8258 0.2225 sec/batch\n",
      "Epoch 11/20  Iteration 18551/35720 Training loss: 0.8259 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 18552/35720 Training loss: 0.8259 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 18553/35720 Training loss: 0.8260 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18554/35720 Training loss: 0.8260 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 18555/35720 Training loss: 0.8260 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 18556/35720 Training loss: 0.8260 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 18557/35720 Training loss: 0.8260 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 18558/35720 Training loss: 0.8260 0.2193 sec/batch\n",
      "Epoch 11/20  Iteration 18559/35720 Training loss: 0.8259 0.2207 sec/batch\n",
      "Epoch 11/20  Iteration 18560/35720 Training loss: 0.8259 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 18561/35720 Training loss: 0.8258 0.2149 sec/batch\n",
      "Epoch 11/20  Iteration 18562/35720 Training loss: 0.8259 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 18563/35720 Training loss: 0.8258 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 18564/35720 Training loss: 0.8259 0.2299 sec/batch\n",
      "Epoch 11/20  Iteration 18565/35720 Training loss: 0.8258 0.2268 sec/batch\n",
      "Epoch 11/20  Iteration 18566/35720 Training loss: 0.8259 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18567/35720 Training loss: 0.8260 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 18568/35720 Training loss: 0.8260 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 18569/35720 Training loss: 0.8261 0.2128 sec/batch\n",
      "Epoch 11/20  Iteration 18570/35720 Training loss: 0.8262 0.2178 sec/batch\n",
      "Epoch 11/20  Iteration 18571/35720 Training loss: 0.8262 0.2200 sec/batch\n",
      "Epoch 11/20  Iteration 18572/35720 Training loss: 0.8262 0.2138 sec/batch\n",
      "Epoch 11/20  Iteration 18573/35720 Training loss: 0.8263 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 18574/35720 Training loss: 0.8263 0.2073 sec/batch\n",
      "Epoch 11/20  Iteration 18575/35720 Training loss: 0.8264 0.2104 sec/batch\n",
      "Epoch 11/20  Iteration 18576/35720 Training loss: 0.8264 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18577/35720 Training loss: 0.8265 0.2294 sec/batch\n",
      "Epoch 11/20  Iteration 18578/35720 Training loss: 0.8266 0.2281 sec/batch\n",
      "Epoch 11/20  Iteration 18579/35720 Training loss: 0.8265 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 18580/35720 Training loss: 0.8266 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18581/35720 Training loss: 0.8266 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 18582/35720 Training loss: 0.8267 0.2177 sec/batch\n",
      "Epoch 11/20  Iteration 18583/35720 Training loss: 0.8268 0.2281 sec/batch\n",
      "Epoch 11/20  Iteration 18584/35720 Training loss: 0.8267 0.2383 sec/batch\n",
      "Epoch 11/20  Iteration 18585/35720 Training loss: 0.8267 0.2100 sec/batch\n",
      "Epoch 11/20  Iteration 18586/35720 Training loss: 0.8267 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 18587/35720 Training loss: 0.8269 0.2184 sec/batch\n",
      "Epoch 11/20  Iteration 18588/35720 Training loss: 0.8269 0.2217 sec/batch\n",
      "Epoch 11/20  Iteration 18589/35720 Training loss: 0.8270 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 18590/35720 Training loss: 0.8270 0.2223 sec/batch\n",
      "Epoch 11/20  Iteration 18591/35720 Training loss: 0.8269 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 18592/35720 Training loss: 0.8269 0.2227 sec/batch\n",
      "Epoch 11/20  Iteration 18593/35720 Training loss: 0.8270 0.2222 sec/batch\n",
      "Epoch 11/20  Iteration 18594/35720 Training loss: 0.8269 0.2187 sec/batch\n",
      "Epoch 11/20  Iteration 18595/35720 Training loss: 0.8269 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 18596/35720 Training loss: 0.8269 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 18597/35720 Training loss: 0.8269 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 18598/35720 Training loss: 0.8269 0.2253 sec/batch\n",
      "Epoch 11/20  Iteration 18599/35720 Training loss: 0.8269 0.2211 sec/batch\n",
      "Epoch 11/20  Iteration 18600/35720 Training loss: 0.8269 0.2196 sec/batch\n",
      "Validation loss: 1.42929 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 18601/35720 Training loss: 0.8273 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18602/35720 Training loss: 0.8274 0.2294 sec/batch\n",
      "Epoch 11/20  Iteration 18603/35720 Training loss: 0.8273 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 18604/35720 Training loss: 0.8273 0.2257 sec/batch\n",
      "Epoch 11/20  Iteration 18605/35720 Training loss: 0.8273 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 18606/35720 Training loss: 0.8273 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 18607/35720 Training loss: 0.8273 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 18608/35720 Training loss: 0.8272 0.2289 sec/batch\n",
      "Epoch 11/20  Iteration 18609/35720 Training loss: 0.8272 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 18610/35720 Training loss: 0.8272 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 18611/35720 Training loss: 0.8272 0.2113 sec/batch\n",
      "Epoch 11/20  Iteration 18612/35720 Training loss: 0.8273 0.2234 sec/batch\n",
      "Epoch 11/20  Iteration 18613/35720 Training loss: 0.8272 0.2275 sec/batch\n",
      "Epoch 11/20  Iteration 18614/35720 Training loss: 0.8272 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18615/35720 Training loss: 0.8271 0.2184 sec/batch\n",
      "Epoch 11/20  Iteration 18616/35720 Training loss: 0.8271 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 18617/35720 Training loss: 0.8271 0.2128 sec/batch\n",
      "Epoch 11/20  Iteration 18618/35720 Training loss: 0.8271 0.2290 sec/batch\n",
      "Epoch 11/20  Iteration 18619/35720 Training loss: 0.8272 0.2264 sec/batch\n",
      "Epoch 11/20  Iteration 18620/35720 Training loss: 0.8272 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 18621/35720 Training loss: 0.8273 0.2240 sec/batch\n",
      "Epoch 11/20  Iteration 18622/35720 Training loss: 0.8273 0.2186 sec/batch\n",
      "Epoch 11/20  Iteration 18623/35720 Training loss: 0.8273 0.2229 sec/batch\n",
      "Epoch 11/20  Iteration 18624/35720 Training loss: 0.8272 0.2100 sec/batch\n",
      "Epoch 11/20  Iteration 18625/35720 Training loss: 0.8272 0.2170 sec/batch\n",
      "Epoch 11/20  Iteration 18626/35720 Training loss: 0.8273 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 18627/35720 Training loss: 0.8273 0.2104 sec/batch\n",
      "Epoch 11/20  Iteration 18628/35720 Training loss: 0.8272 0.2076 sec/batch\n",
      "Epoch 11/20  Iteration 18629/35720 Training loss: 0.8273 0.2229 sec/batch\n",
      "Epoch 11/20  Iteration 18630/35720 Training loss: 0.8274 0.2279 sec/batch\n",
      "Epoch 11/20  Iteration 18631/35720 Training loss: 0.8275 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18632/35720 Training loss: 0.8276 0.2245 sec/batch\n",
      "Epoch 11/20  Iteration 18633/35720 Training loss: 0.8276 0.2108 sec/batch\n",
      "Epoch 11/20  Iteration 18634/35720 Training loss: 0.8275 0.2078 sec/batch\n",
      "Epoch 11/20  Iteration 18635/35720 Training loss: 0.8274 0.2282 sec/batch\n",
      "Epoch 11/20  Iteration 18636/35720 Training loss: 0.8273 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 18637/35720 Training loss: 0.8273 0.2136 sec/batch\n",
      "Epoch 11/20  Iteration 18638/35720 Training loss: 0.8273 0.2057 sec/batch\n",
      "Epoch 11/20  Iteration 18639/35720 Training loss: 0.8273 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 18640/35720 Training loss: 0.8273 0.2185 sec/batch\n",
      "Epoch 11/20  Iteration 18641/35720 Training loss: 0.8273 0.2176 sec/batch\n",
      "Epoch 11/20  Iteration 18642/35720 Training loss: 0.8273 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18643/35720 Training loss: 0.8274 0.2186 sec/batch\n",
      "Epoch 11/20  Iteration 18644/35720 Training loss: 0.8273 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18645/35720 Training loss: 0.8273 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 18646/35720 Training loss: 0.8272 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 18647/35720 Training loss: 0.8273 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 18648/35720 Training loss: 0.8273 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 18649/35720 Training loss: 0.8273 0.2211 sec/batch\n",
      "Epoch 11/20  Iteration 18650/35720 Training loss: 0.8273 0.2101 sec/batch\n",
      "Epoch 11/20  Iteration 18651/35720 Training loss: 0.8274 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 18652/35720 Training loss: 0.8275 0.2340 sec/batch\n",
      "Epoch 11/20  Iteration 18653/35720 Training loss: 0.8274 0.2283 sec/batch\n",
      "Epoch 11/20  Iteration 18654/35720 Training loss: 0.8273 0.2220 sec/batch\n",
      "Epoch 11/20  Iteration 18655/35720 Training loss: 0.8274 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18656/35720 Training loss: 0.8274 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 18657/35720 Training loss: 0.8273 0.2097 sec/batch\n",
      "Epoch 11/20  Iteration 18658/35720 Training loss: 0.8273 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 18659/35720 Training loss: 0.8273 0.2183 sec/batch\n",
      "Epoch 11/20  Iteration 18660/35720 Training loss: 0.8273 0.2154 sec/batch\n",
      "Epoch 11/20  Iteration 18661/35720 Training loss: 0.8273 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 18662/35720 Training loss: 0.8274 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18663/35720 Training loss: 0.8274 0.2307 sec/batch\n",
      "Epoch 11/20  Iteration 18664/35720 Training loss: 0.8275 0.2208 sec/batch\n",
      "Epoch 11/20  Iteration 18665/35720 Training loss: 0.8276 0.2239 sec/batch\n",
      "Epoch 11/20  Iteration 18666/35720 Training loss: 0.8275 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18667/35720 Training loss: 0.8276 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18668/35720 Training loss: 0.8275 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18669/35720 Training loss: 0.8276 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 18670/35720 Training loss: 0.8277 0.2123 sec/batch\n",
      "Epoch 11/20  Iteration 18671/35720 Training loss: 0.8277 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 18672/35720 Training loss: 0.8277 0.2252 sec/batch\n",
      "Epoch 11/20  Iteration 18673/35720 Training loss: 0.8277 0.2083 sec/batch\n",
      "Epoch 11/20  Iteration 18674/35720 Training loss: 0.8278 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 18675/35720 Training loss: 0.8278 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 18676/35720 Training loss: 0.8278 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18677/35720 Training loss: 0.8278 0.2182 sec/batch\n",
      "Epoch 11/20  Iteration 18678/35720 Training loss: 0.8278 0.2074 sec/batch\n",
      "Epoch 11/20  Iteration 18679/35720 Training loss: 0.8278 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 18680/35720 Training loss: 0.8278 0.2254 sec/batch\n",
      "Epoch 11/20  Iteration 18681/35720 Training loss: 0.8278 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 18682/35720 Training loss: 0.8278 0.2218 sec/batch\n",
      "Epoch 11/20  Iteration 18683/35720 Training loss: 0.8277 0.2155 sec/batch\n",
      "Epoch 11/20  Iteration 18684/35720 Training loss: 0.8276 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 18685/35720 Training loss: 0.8275 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18686/35720 Training loss: 0.8275 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18687/35720 Training loss: 0.8274 0.2099 sec/batch\n",
      "Epoch 11/20  Iteration 18688/35720 Training loss: 0.8273 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 18689/35720 Training loss: 0.8273 0.2099 sec/batch\n",
      "Epoch 11/20  Iteration 18690/35720 Training loss: 0.8273 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 18691/35720 Training loss: 0.8272 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 18692/35720 Training loss: 0.8273 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18693/35720 Training loss: 0.8273 0.2336 sec/batch\n",
      "Epoch 11/20  Iteration 18694/35720 Training loss: 0.8273 0.2233 sec/batch\n",
      "Epoch 11/20  Iteration 18695/35720 Training loss: 0.8273 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 18696/35720 Training loss: 0.8273 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18697/35720 Training loss: 0.8273 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 18698/35720 Training loss: 0.8273 0.2097 sec/batch\n",
      "Epoch 11/20  Iteration 18699/35720 Training loss: 0.8273 0.2243 sec/batch\n",
      "Epoch 11/20  Iteration 18700/35720 Training loss: 0.8273 0.2258 sec/batch\n",
      "Epoch 11/20  Iteration 18701/35720 Training loss: 0.8273 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 18702/35720 Training loss: 0.8273 0.2296 sec/batch\n",
      "Epoch 11/20  Iteration 18703/35720 Training loss: 0.8273 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18704/35720 Training loss: 0.8273 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 18705/35720 Training loss: 0.8273 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 18706/35720 Training loss: 0.8273 0.2077 sec/batch\n",
      "Epoch 11/20  Iteration 18707/35720 Training loss: 0.8272 0.2083 sec/batch\n",
      "Epoch 11/20  Iteration 18708/35720 Training loss: 0.8272 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 18709/35720 Training loss: 0.8272 0.2186 sec/batch\n",
      "Epoch 11/20  Iteration 18710/35720 Training loss: 0.8272 0.2099 sec/batch\n",
      "Epoch 11/20  Iteration 18711/35720 Training loss: 0.8272 0.2203 sec/batch\n",
      "Epoch 11/20  Iteration 18712/35720 Training loss: 0.8272 0.2069 sec/batch\n",
      "Epoch 11/20  Iteration 18713/35720 Training loss: 0.8271 0.2192 sec/batch\n",
      "Epoch 11/20  Iteration 18714/35720 Training loss: 0.8271 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 18715/35720 Training loss: 0.8272 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 18716/35720 Training loss: 0.8271 0.2069 sec/batch\n",
      "Epoch 11/20  Iteration 18717/35720 Training loss: 0.8270 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 18718/35720 Training loss: 0.8271 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 18719/35720 Training loss: 0.8270 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 18720/35720 Training loss: 0.8270 0.2171 sec/batch\n",
      "Epoch 11/20  Iteration 18721/35720 Training loss: 0.8270 0.2217 sec/batch\n",
      "Epoch 11/20  Iteration 18722/35720 Training loss: 0.8269 0.2305 sec/batch\n",
      "Epoch 11/20  Iteration 18723/35720 Training loss: 0.8269 0.2056 sec/batch\n",
      "Epoch 11/20  Iteration 18724/35720 Training loss: 0.8269 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18725/35720 Training loss: 0.8268 0.2283 sec/batch\n",
      "Epoch 11/20  Iteration 18726/35720 Training loss: 0.8268 0.2200 sec/batch\n",
      "Epoch 11/20  Iteration 18727/35720 Training loss: 0.8269 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 18728/35720 Training loss: 0.8269 0.2238 sec/batch\n",
      "Epoch 11/20  Iteration 18729/35720 Training loss: 0.8268 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 18730/35720 Training loss: 0.8268 0.2143 sec/batch\n",
      "Epoch 11/20  Iteration 18731/35720 Training loss: 0.8268 0.2127 sec/batch\n",
      "Epoch 11/20  Iteration 18732/35720 Training loss: 0.8267 0.2176 sec/batch\n",
      "Epoch 11/20  Iteration 18733/35720 Training loss: 0.8267 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 18734/35720 Training loss: 0.8267 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18735/35720 Training loss: 0.8266 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 18736/35720 Training loss: 0.8266 0.2306 sec/batch\n",
      "Epoch 11/20  Iteration 18737/35720 Training loss: 0.8266 0.2269 sec/batch\n",
      "Epoch 11/20  Iteration 18738/35720 Training loss: 0.8266 0.2269 sec/batch\n",
      "Epoch 11/20  Iteration 18739/35720 Training loss: 0.8266 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18740/35720 Training loss: 0.8265 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18741/35720 Training loss: 0.8265 0.2292 sec/batch\n",
      "Epoch 11/20  Iteration 18742/35720 Training loss: 0.8265 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 18743/35720 Training loss: 0.8265 0.2196 sec/batch\n",
      "Epoch 11/20  Iteration 18744/35720 Training loss: 0.8265 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18745/35720 Training loss: 0.8265 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 18746/35720 Training loss: 0.8264 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 18747/35720 Training loss: 0.8264 0.2193 sec/batch\n",
      "Epoch 11/20  Iteration 18748/35720 Training loss: 0.8264 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 18749/35720 Training loss: 0.8263 0.2213 sec/batch\n",
      "Epoch 11/20  Iteration 18750/35720 Training loss: 0.8263 0.2274 sec/batch\n",
      "Epoch 11/20  Iteration 18751/35720 Training loss: 0.8262 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 18752/35720 Training loss: 0.8261 0.2263 sec/batch\n",
      "Epoch 11/20  Iteration 18753/35720 Training loss: 0.8261 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18754/35720 Training loss: 0.8260 0.2238 sec/batch\n",
      "Epoch 11/20  Iteration 18755/35720 Training loss: 0.8260 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 18756/35720 Training loss: 0.8259 0.2172 sec/batch\n",
      "Epoch 11/20  Iteration 18757/35720 Training loss: 0.8258 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 18758/35720 Training loss: 0.8257 0.2300 sec/batch\n",
      "Epoch 11/20  Iteration 18759/35720 Training loss: 0.8256 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 18760/35720 Training loss: 0.8256 0.2184 sec/batch\n",
      "Epoch 11/20  Iteration 18761/35720 Training loss: 0.8255 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 18762/35720 Training loss: 0.8254 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 18763/35720 Training loss: 0.8253 0.2180 sec/batch\n",
      "Epoch 11/20  Iteration 18764/35720 Training loss: 0.8253 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 18765/35720 Training loss: 0.8252 0.2285 sec/batch\n",
      "Epoch 11/20  Iteration 18766/35720 Training loss: 0.8252 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 18767/35720 Training loss: 0.8252 0.2231 sec/batch\n",
      "Epoch 11/20  Iteration 18768/35720 Training loss: 0.8252 0.2144 sec/batch\n",
      "Epoch 11/20  Iteration 18769/35720 Training loss: 0.8252 0.2290 sec/batch\n",
      "Epoch 11/20  Iteration 18770/35720 Training loss: 0.8252 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18771/35720 Training loss: 0.8252 0.2228 sec/batch\n",
      "Epoch 11/20  Iteration 18772/35720 Training loss: 0.8252 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18773/35720 Training loss: 0.8252 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 18774/35720 Training loss: 0.8252 0.2181 sec/batch\n",
      "Epoch 11/20  Iteration 18775/35720 Training loss: 0.8253 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18776/35720 Training loss: 0.8253 0.2070 sec/batch\n",
      "Epoch 11/20  Iteration 18777/35720 Training loss: 0.8253 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 18778/35720 Training loss: 0.8253 0.2147 sec/batch\n",
      "Epoch 11/20  Iteration 18779/35720 Training loss: 0.8252 0.2074 sec/batch\n",
      "Epoch 11/20  Iteration 18780/35720 Training loss: 0.8252 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 18781/35720 Training loss: 0.8252 0.2207 sec/batch\n",
      "Epoch 11/20  Iteration 18782/35720 Training loss: 0.8252 0.2297 sec/batch\n",
      "Epoch 11/20  Iteration 18783/35720 Training loss: 0.8252 0.2291 sec/batch\n",
      "Epoch 11/20  Iteration 18784/35720 Training loss: 0.8252 0.2080 sec/batch\n",
      "Epoch 11/20  Iteration 18785/35720 Training loss: 0.8252 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 18786/35720 Training loss: 0.8252 0.2263 sec/batch\n",
      "Epoch 11/20  Iteration 18787/35720 Training loss: 0.8252 0.2232 sec/batch\n",
      "Epoch 11/20  Iteration 18788/35720 Training loss: 0.8252 0.2076 sec/batch\n",
      "Epoch 11/20  Iteration 18789/35720 Training loss: 0.8253 0.2194 sec/batch\n",
      "Epoch 11/20  Iteration 18790/35720 Training loss: 0.8253 0.2253 sec/batch\n",
      "Epoch 11/20  Iteration 18791/35720 Training loss: 0.8253 0.2230 sec/batch\n",
      "Epoch 11/20  Iteration 18792/35720 Training loss: 0.8253 0.2147 sec/batch\n",
      "Epoch 11/20  Iteration 18793/35720 Training loss: 0.8253 0.2255 sec/batch\n",
      "Epoch 11/20  Iteration 18794/35720 Training loss: 0.8253 0.2182 sec/batch\n",
      "Epoch 11/20  Iteration 18795/35720 Training loss: 0.8252 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 18796/35720 Training loss: 0.8251 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 18797/35720 Training loss: 0.8250 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 18798/35720 Training loss: 0.8250 0.2136 sec/batch\n",
      "Epoch 11/20  Iteration 18799/35720 Training loss: 0.8249 0.2236 sec/batch\n",
      "Epoch 11/20  Iteration 18800/35720 Training loss: 0.8248 0.2161 sec/batch\n",
      "Validation loss: 1.42279 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 18801/35720 Training loss: 0.8252 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 18802/35720 Training loss: 0.8253 0.2212 sec/batch\n",
      "Epoch 11/20  Iteration 18803/35720 Training loss: 0.8252 0.2276 sec/batch\n",
      "Epoch 11/20  Iteration 18804/35720 Training loss: 0.8253 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18805/35720 Training loss: 0.8252 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 18806/35720 Training loss: 0.8252 0.2078 sec/batch\n",
      "Epoch 11/20  Iteration 18807/35720 Training loss: 0.8252 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 18808/35720 Training loss: 0.8251 0.2184 sec/batch\n",
      "Epoch 11/20  Iteration 18809/35720 Training loss: 0.8251 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 18810/35720 Training loss: 0.8251 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18811/35720 Training loss: 0.8250 0.2061 sec/batch\n",
      "Epoch 11/20  Iteration 18812/35720 Training loss: 0.8249 0.2213 sec/batch\n",
      "Epoch 11/20  Iteration 18813/35720 Training loss: 0.8248 0.2238 sec/batch\n",
      "Epoch 11/20  Iteration 18814/35720 Training loss: 0.8248 0.2274 sec/batch\n",
      "Epoch 11/20  Iteration 18815/35720 Training loss: 0.8248 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 18816/35720 Training loss: 0.8247 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 18817/35720 Training loss: 0.8246 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 18818/35720 Training loss: 0.8246 0.2167 sec/batch\n",
      "Epoch 11/20  Iteration 18819/35720 Training loss: 0.8245 0.2200 sec/batch\n",
      "Epoch 11/20  Iteration 18820/35720 Training loss: 0.8245 0.2737 sec/batch\n",
      "Epoch 11/20  Iteration 18821/35720 Training loss: 0.8244 0.2439 sec/batch\n",
      "Epoch 11/20  Iteration 18822/35720 Training loss: 0.8244 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 18823/35720 Training loss: 0.8243 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 18824/35720 Training loss: 0.8243 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 18825/35720 Training loss: 0.8243 0.2178 sec/batch\n",
      "Epoch 11/20  Iteration 18826/35720 Training loss: 0.8242 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 18827/35720 Training loss: 0.8242 0.2061 sec/batch\n",
      "Epoch 11/20  Iteration 18828/35720 Training loss: 0.8241 0.2124 sec/batch\n",
      "Epoch 11/20  Iteration 18829/35720 Training loss: 0.8242 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 18830/35720 Training loss: 0.8244 0.2124 sec/batch\n",
      "Epoch 11/20  Iteration 18831/35720 Training loss: 0.8243 0.2256 sec/batch\n",
      "Epoch 11/20  Iteration 18832/35720 Training loss: 0.8242 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 18833/35720 Training loss: 0.8242 0.2053 sec/batch\n",
      "Epoch 11/20  Iteration 18834/35720 Training loss: 0.8241 0.2124 sec/batch\n",
      "Epoch 11/20  Iteration 18835/35720 Training loss: 0.8241 0.2236 sec/batch\n",
      "Epoch 11/20  Iteration 18836/35720 Training loss: 0.8241 0.2278 sec/batch\n",
      "Epoch 11/20  Iteration 18837/35720 Training loss: 0.8241 0.2075 sec/batch\n",
      "Epoch 11/20  Iteration 18838/35720 Training loss: 0.8241 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 18839/35720 Training loss: 0.8241 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 18840/35720 Training loss: 0.8241 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 18841/35720 Training loss: 0.8240 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18842/35720 Training loss: 0.8240 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 18843/35720 Training loss: 0.8239 0.2197 sec/batch\n",
      "Epoch 11/20  Iteration 18844/35720 Training loss: 0.8238 0.2060 sec/batch\n",
      "Epoch 11/20  Iteration 18845/35720 Training loss: 0.8238 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18846/35720 Training loss: 0.8238 0.2238 sec/batch\n",
      "Epoch 11/20  Iteration 18847/35720 Training loss: 0.8238 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 18848/35720 Training loss: 0.8237 0.2254 sec/batch\n",
      "Epoch 11/20  Iteration 18849/35720 Training loss: 0.8237 0.2170 sec/batch\n",
      "Epoch 11/20  Iteration 18850/35720 Training loss: 0.8237 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18851/35720 Training loss: 0.8237 0.2335 sec/batch\n",
      "Epoch 11/20  Iteration 18852/35720 Training loss: 0.8237 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 18853/35720 Training loss: 0.8237 0.2275 sec/batch\n",
      "Epoch 11/20  Iteration 18854/35720 Training loss: 0.8237 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 18855/35720 Training loss: 0.8237 0.2248 sec/batch\n",
      "Epoch 11/20  Iteration 18856/35720 Training loss: 0.8237 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18857/35720 Training loss: 0.8238 0.2255 sec/batch\n",
      "Epoch 11/20  Iteration 18858/35720 Training loss: 0.8238 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 18859/35720 Training loss: 0.8238 0.2221 sec/batch\n",
      "Epoch 11/20  Iteration 18860/35720 Training loss: 0.8238 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18861/35720 Training loss: 0.8237 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 18862/35720 Training loss: 0.8236 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 18863/35720 Training loss: 0.8235 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 18864/35720 Training loss: 0.8234 0.2208 sec/batch\n",
      "Epoch 11/20  Iteration 18865/35720 Training loss: 0.8234 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18866/35720 Training loss: 0.8233 0.2195 sec/batch\n",
      "Epoch 11/20  Iteration 18867/35720 Training loss: 0.8233 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 18868/35720 Training loss: 0.8232 0.2220 sec/batch\n",
      "Epoch 11/20  Iteration 18869/35720 Training loss: 0.8232 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 18870/35720 Training loss: 0.8231 0.2253 sec/batch\n",
      "Epoch 11/20  Iteration 18871/35720 Training loss: 0.8231 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 18872/35720 Training loss: 0.8231 0.2185 sec/batch\n",
      "Epoch 11/20  Iteration 18873/35720 Training loss: 0.8230 0.2272 sec/batch\n",
      "Epoch 11/20  Iteration 18874/35720 Training loss: 0.8230 0.2143 sec/batch\n",
      "Epoch 11/20  Iteration 18875/35720 Training loss: 0.8230 0.2225 sec/batch\n",
      "Epoch 11/20  Iteration 18876/35720 Training loss: 0.8230 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 18877/35720 Training loss: 0.8230 0.2078 sec/batch\n",
      "Epoch 11/20  Iteration 18878/35720 Training loss: 0.8230 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 18879/35720 Training loss: 0.8229 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 18880/35720 Training loss: 0.8228 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18881/35720 Training loss: 0.8228 0.2279 sec/batch\n",
      "Epoch 11/20  Iteration 18882/35720 Training loss: 0.8229 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 18883/35720 Training loss: 0.8229 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 18884/35720 Training loss: 0.8229 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 18885/35720 Training loss: 0.8230 0.2185 sec/batch\n",
      "Epoch 11/20  Iteration 18886/35720 Training loss: 0.8229 0.2250 sec/batch\n",
      "Epoch 11/20  Iteration 18887/35720 Training loss: 0.8229 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18888/35720 Training loss: 0.8229 0.2282 sec/batch\n",
      "Epoch 11/20  Iteration 18889/35720 Training loss: 0.8228 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 18890/35720 Training loss: 0.8228 0.2149 sec/batch\n",
      "Epoch 11/20  Iteration 18891/35720 Training loss: 0.8228 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 18892/35720 Training loss: 0.8228 0.2319 sec/batch\n",
      "Epoch 11/20  Iteration 18893/35720 Training loss: 0.8228 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 18894/35720 Training loss: 0.8228 0.2265 sec/batch\n",
      "Epoch 11/20  Iteration 18895/35720 Training loss: 0.8228 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 18896/35720 Training loss: 0.8228 0.2083 sec/batch\n",
      "Epoch 11/20  Iteration 18897/35720 Training loss: 0.8229 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 18898/35720 Training loss: 0.8229 0.2158 sec/batch\n",
      "Epoch 11/20  Iteration 18899/35720 Training loss: 0.8228 0.2073 sec/batch\n",
      "Epoch 11/20  Iteration 18900/35720 Training loss: 0.8228 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18901/35720 Training loss: 0.8229 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 18902/35720 Training loss: 0.8229 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 18903/35720 Training loss: 0.8228 0.2358 sec/batch\n",
      "Epoch 11/20  Iteration 18904/35720 Training loss: 0.8228 0.2182 sec/batch\n",
      "Epoch 11/20  Iteration 18905/35720 Training loss: 0.8228 0.2240 sec/batch\n",
      "Epoch 11/20  Iteration 18906/35720 Training loss: 0.8228 0.2190 sec/batch\n",
      "Epoch 11/20  Iteration 18907/35720 Training loss: 0.8228 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18908/35720 Training loss: 0.8228 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 18909/35720 Training loss: 0.8227 0.2108 sec/batch\n",
      "Epoch 11/20  Iteration 18910/35720 Training loss: 0.8228 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 18911/35720 Training loss: 0.8227 0.2386 sec/batch\n",
      "Epoch 11/20  Iteration 18912/35720 Training loss: 0.8228 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 18913/35720 Training loss: 0.8229 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 18914/35720 Training loss: 0.8229 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18915/35720 Training loss: 0.8229 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 18916/35720 Training loss: 0.8229 0.2187 sec/batch\n",
      "Epoch 11/20  Iteration 18917/35720 Training loss: 0.8229 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 18918/35720 Training loss: 0.8229 0.2287 sec/batch\n",
      "Epoch 11/20  Iteration 18919/35720 Training loss: 0.8229 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 18920/35720 Training loss: 0.8228 0.2367 sec/batch\n",
      "Epoch 11/20  Iteration 18921/35720 Training loss: 0.8229 0.2280 sec/batch\n",
      "Epoch 11/20  Iteration 18922/35720 Training loss: 0.8229 0.2127 sec/batch\n",
      "Epoch 11/20  Iteration 18923/35720 Training loss: 0.8230 0.2245 sec/batch\n",
      "Epoch 11/20  Iteration 18924/35720 Training loss: 0.8229 0.2123 sec/batch\n",
      "Epoch 11/20  Iteration 18925/35720 Training loss: 0.8229 0.2268 sec/batch\n",
      "Epoch 11/20  Iteration 18926/35720 Training loss: 0.8229 0.2079 sec/batch\n",
      "Epoch 11/20  Iteration 18927/35720 Training loss: 0.8229 0.2079 sec/batch\n",
      "Epoch 11/20  Iteration 18928/35720 Training loss: 0.8229 0.2264 sec/batch\n",
      "Epoch 11/20  Iteration 18929/35720 Training loss: 0.8229 0.2234 sec/batch\n",
      "Epoch 11/20  Iteration 18930/35720 Training loss: 0.8229 0.2368 sec/batch\n",
      "Epoch 11/20  Iteration 18931/35720 Training loss: 0.8229 0.2055 sec/batch\n",
      "Epoch 11/20  Iteration 18932/35720 Training loss: 0.8228 0.2230 sec/batch\n",
      "Epoch 11/20  Iteration 18933/35720 Training loss: 0.8229 0.2210 sec/batch\n",
      "Epoch 11/20  Iteration 18934/35720 Training loss: 0.8229 0.2275 sec/batch\n",
      "Epoch 11/20  Iteration 18935/35720 Training loss: 0.8229 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 18936/35720 Training loss: 0.8229 0.2260 sec/batch\n",
      "Epoch 11/20  Iteration 18937/35720 Training loss: 0.8229 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 18938/35720 Training loss: 0.8229 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 18939/35720 Training loss: 0.8228 0.2138 sec/batch\n",
      "Epoch 11/20  Iteration 18940/35720 Training loss: 0.8228 0.2203 sec/batch\n",
      "Epoch 11/20  Iteration 18941/35720 Training loss: 0.8228 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 18942/35720 Training loss: 0.8227 0.2071 sec/batch\n",
      "Epoch 11/20  Iteration 18943/35720 Training loss: 0.8228 0.2071 sec/batch\n",
      "Epoch 11/20  Iteration 18944/35720 Training loss: 0.8227 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 18945/35720 Training loss: 0.8227 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 18946/35720 Training loss: 0.8227 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 18947/35720 Training loss: 0.8227 0.2261 sec/batch\n",
      "Epoch 11/20  Iteration 18948/35720 Training loss: 0.8228 0.2070 sec/batch\n",
      "Epoch 11/20  Iteration 18949/35720 Training loss: 0.8228 0.2136 sec/batch\n",
      "Epoch 11/20  Iteration 18950/35720 Training loss: 0.8228 0.2247 sec/batch\n",
      "Epoch 11/20  Iteration 18951/35720 Training loss: 0.8228 0.2223 sec/batch\n",
      "Epoch 11/20  Iteration 18952/35720 Training loss: 0.8227 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 18953/35720 Training loss: 0.8227 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 18954/35720 Training loss: 0.8227 0.2143 sec/batch\n",
      "Epoch 11/20  Iteration 18955/35720 Training loss: 0.8227 0.2205 sec/batch\n",
      "Epoch 11/20  Iteration 18956/35720 Training loss: 0.8227 0.2188 sec/batch\n",
      "Epoch 11/20  Iteration 18957/35720 Training loss: 0.8226 0.2206 sec/batch\n",
      "Epoch 11/20  Iteration 18958/35720 Training loss: 0.8226 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 18959/35720 Training loss: 0.8227 0.2055 sec/batch\n",
      "Epoch 11/20  Iteration 18960/35720 Training loss: 0.8227 0.2212 sec/batch\n",
      "Epoch 11/20  Iteration 18961/35720 Training loss: 0.8227 0.2122 sec/batch\n",
      "Epoch 11/20  Iteration 18962/35720 Training loss: 0.8228 0.2154 sec/batch\n",
      "Epoch 11/20  Iteration 18963/35720 Training loss: 0.8228 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 18964/35720 Training loss: 0.8228 0.2242 sec/batch\n",
      "Epoch 11/20  Iteration 18965/35720 Training loss: 0.8228 0.2253 sec/batch\n",
      "Epoch 11/20  Iteration 18966/35720 Training loss: 0.8227 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 18967/35720 Training loss: 0.8227 0.2223 sec/batch\n",
      "Epoch 11/20  Iteration 18968/35720 Training loss: 0.8228 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 18969/35720 Training loss: 0.8227 0.2253 sec/batch\n",
      "Epoch 11/20  Iteration 18970/35720 Training loss: 0.8228 0.2149 sec/batch\n",
      "Epoch 11/20  Iteration 18971/35720 Training loss: 0.8227 0.2128 sec/batch\n",
      "Epoch 11/20  Iteration 18972/35720 Training loss: 0.8226 0.2278 sec/batch\n",
      "Epoch 11/20  Iteration 18973/35720 Training loss: 0.8226 0.2280 sec/batch\n",
      "Epoch 11/20  Iteration 18974/35720 Training loss: 0.8226 0.2235 sec/batch\n",
      "Epoch 11/20  Iteration 18975/35720 Training loss: 0.8226 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 18976/35720 Training loss: 0.8226 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 18977/35720 Training loss: 0.8226 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 18978/35720 Training loss: 0.8225 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 18979/35720 Training loss: 0.8225 0.2256 sec/batch\n",
      "Epoch 11/20  Iteration 18980/35720 Training loss: 0.8226 0.2154 sec/batch\n",
      "Epoch 11/20  Iteration 18981/35720 Training loss: 0.8226 0.2069 sec/batch\n",
      "Epoch 11/20  Iteration 18982/35720 Training loss: 0.8226 0.2125 sec/batch\n",
      "Epoch 11/20  Iteration 18983/35720 Training loss: 0.8227 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 18984/35720 Training loss: 0.8227 0.2288 sec/batch\n",
      "Epoch 11/20  Iteration 18985/35720 Training loss: 0.8227 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 18986/35720 Training loss: 0.8226 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 18987/35720 Training loss: 0.8226 0.2059 sec/batch\n",
      "Epoch 11/20  Iteration 18988/35720 Training loss: 0.8226 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 18989/35720 Training loss: 0.8226 0.2158 sec/batch\n",
      "Epoch 11/20  Iteration 18990/35720 Training loss: 0.8226 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 18991/35720 Training loss: 0.8226 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 18992/35720 Training loss: 0.8225 0.2134 sec/batch\n",
      "Epoch 11/20  Iteration 18993/35720 Training loss: 0.8224 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 18994/35720 Training loss: 0.8225 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 18995/35720 Training loss: 0.8224 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 18996/35720 Training loss: 0.8224 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 18997/35720 Training loss: 0.8223 0.2222 sec/batch\n",
      "Epoch 11/20  Iteration 18998/35720 Training loss: 0.8223 0.2188 sec/batch\n",
      "Epoch 11/20  Iteration 18999/35720 Training loss: 0.8222 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 19000/35720 Training loss: 0.8222 0.2224 sec/batch\n",
      "Validation loss: 1.42472 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 19001/35720 Training loss: 0.8225 0.2124 sec/batch\n",
      "Epoch 11/20  Iteration 19002/35720 Training loss: 0.8225 0.2194 sec/batch\n",
      "Epoch 11/20  Iteration 19003/35720 Training loss: 0.8224 0.2069 sec/batch\n",
      "Epoch 11/20  Iteration 19004/35720 Training loss: 0.8224 0.2258 sec/batch\n",
      "Epoch 11/20  Iteration 19005/35720 Training loss: 0.8224 0.2074 sec/batch\n",
      "Epoch 11/20  Iteration 19006/35720 Training loss: 0.8223 0.2193 sec/batch\n",
      "Epoch 11/20  Iteration 19007/35720 Training loss: 0.8223 0.2097 sec/batch\n",
      "Epoch 11/20  Iteration 19008/35720 Training loss: 0.8223 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 19009/35720 Training loss: 0.8223 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 19010/35720 Training loss: 0.8222 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 19011/35720 Training loss: 0.8223 0.2274 sec/batch\n",
      "Epoch 11/20  Iteration 19012/35720 Training loss: 0.8223 0.2122 sec/batch\n",
      "Epoch 11/20  Iteration 19013/35720 Training loss: 0.8223 0.2203 sec/batch\n",
      "Epoch 11/20  Iteration 19014/35720 Training loss: 0.8223 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 19015/35720 Training loss: 0.8223 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 19016/35720 Training loss: 0.8222 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 19017/35720 Training loss: 0.8222 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 19018/35720 Training loss: 0.8222 0.2108 sec/batch\n",
      "Epoch 11/20  Iteration 19019/35720 Training loss: 0.8222 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 19020/35720 Training loss: 0.8222 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 19021/35720 Training loss: 0.8221 0.2085 sec/batch\n",
      "Epoch 11/20  Iteration 19022/35720 Training loss: 0.8221 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 19023/35720 Training loss: 0.8221 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 19024/35720 Training loss: 0.8222 0.2243 sec/batch\n",
      "Epoch 11/20  Iteration 19025/35720 Training loss: 0.8221 0.2197 sec/batch\n",
      "Epoch 11/20  Iteration 19026/35720 Training loss: 0.8221 0.2068 sec/batch\n",
      "Epoch 11/20  Iteration 19027/35720 Training loss: 0.8220 0.2125 sec/batch\n",
      "Epoch 11/20  Iteration 19028/35720 Training loss: 0.8221 0.2301 sec/batch\n",
      "Epoch 11/20  Iteration 19029/35720 Training loss: 0.8221 0.2181 sec/batch\n",
      "Epoch 11/20  Iteration 19030/35720 Training loss: 0.8221 0.2131 sec/batch\n",
      "Epoch 11/20  Iteration 19031/35720 Training loss: 0.8221 0.2212 sec/batch\n",
      "Epoch 11/20  Iteration 19032/35720 Training loss: 0.8221 0.2134 sec/batch\n",
      "Epoch 11/20  Iteration 19033/35720 Training loss: 0.8221 0.2245 sec/batch\n",
      "Epoch 11/20  Iteration 19034/35720 Training loss: 0.8221 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 19035/35720 Training loss: 0.8221 0.2281 sec/batch\n",
      "Epoch 11/20  Iteration 19036/35720 Training loss: 0.8221 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 19037/35720 Training loss: 0.8221 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 19038/35720 Training loss: 0.8221 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 19039/35720 Training loss: 0.8221 0.2363 sec/batch\n",
      "Epoch 11/20  Iteration 19040/35720 Training loss: 0.8221 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 19041/35720 Training loss: 0.8221 0.2283 sec/batch\n",
      "Epoch 11/20  Iteration 19042/35720 Training loss: 0.8221 0.2440 sec/batch\n",
      "Epoch 11/20  Iteration 19043/35720 Training loss: 0.8221 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 19044/35720 Training loss: 0.8220 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 19045/35720 Training loss: 0.8220 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 19046/35720 Training loss: 0.8219 0.2327 sec/batch\n",
      "Epoch 11/20  Iteration 19047/35720 Training loss: 0.8219 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 19048/35720 Training loss: 0.8220 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 19049/35720 Training loss: 0.8219 0.2181 sec/batch\n",
      "Epoch 11/20  Iteration 19050/35720 Training loss: 0.8219 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 19051/35720 Training loss: 0.8219 0.2081 sec/batch\n",
      "Epoch 11/20  Iteration 19052/35720 Training loss: 0.8219 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 19053/35720 Training loss: 0.8219 0.2131 sec/batch\n",
      "Epoch 11/20  Iteration 19054/35720 Training loss: 0.8219 0.2085 sec/batch\n",
      "Epoch 11/20  Iteration 19055/35720 Training loss: 0.8219 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 19056/35720 Training loss: 0.8219 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 19057/35720 Training loss: 0.8220 0.2171 sec/batch\n",
      "Epoch 11/20  Iteration 19058/35720 Training loss: 0.8220 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 19059/35720 Training loss: 0.8220 0.2276 sec/batch\n",
      "Epoch 11/20  Iteration 19060/35720 Training loss: 0.8219 0.2229 sec/batch\n",
      "Epoch 11/20  Iteration 19061/35720 Training loss: 0.8219 0.2251 sec/batch\n",
      "Epoch 11/20  Iteration 19062/35720 Training loss: 0.8218 0.2074 sec/batch\n",
      "Epoch 11/20  Iteration 19063/35720 Training loss: 0.8218 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 19064/35720 Training loss: 0.8218 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 19065/35720 Training loss: 0.8217 0.2109 sec/batch\n",
      "Epoch 11/20  Iteration 19066/35720 Training loss: 0.8217 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 19067/35720 Training loss: 0.8217 0.2261 sec/batch\n",
      "Epoch 11/20  Iteration 19068/35720 Training loss: 0.8217 0.2267 sec/batch\n",
      "Epoch 11/20  Iteration 19069/35720 Training loss: 0.8217 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 19070/35720 Training loss: 0.8217 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 19071/35720 Training loss: 0.8216 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 19072/35720 Training loss: 0.8216 0.2324 sec/batch\n",
      "Epoch 11/20  Iteration 19073/35720 Training loss: 0.8216 0.2272 sec/batch\n",
      "Epoch 11/20  Iteration 19074/35720 Training loss: 0.8216 0.2258 sec/batch\n",
      "Epoch 11/20  Iteration 19075/35720 Training loss: 0.8216 0.2103 sec/batch\n",
      "Epoch 11/20  Iteration 19076/35720 Training loss: 0.8216 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 19077/35720 Training loss: 0.8216 0.2199 sec/batch\n",
      "Epoch 11/20  Iteration 19078/35720 Training loss: 0.8216 0.2427 sec/batch\n",
      "Epoch 11/20  Iteration 19079/35720 Training loss: 0.8215 0.2277 sec/batch\n",
      "Epoch 11/20  Iteration 19080/35720 Training loss: 0.8215 0.2057 sec/batch\n",
      "Epoch 11/20  Iteration 19081/35720 Training loss: 0.8215 0.2143 sec/batch\n",
      "Epoch 11/20  Iteration 19082/35720 Training loss: 0.8215 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 19083/35720 Training loss: 0.8215 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 19084/35720 Training loss: 0.8214 0.2169 sec/batch\n",
      "Epoch 11/20  Iteration 19085/35720 Training loss: 0.8214 0.2173 sec/batch\n",
      "Epoch 11/20  Iteration 19086/35720 Training loss: 0.8214 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 19087/35720 Training loss: 0.8214 0.2104 sec/batch\n",
      "Epoch 11/20  Iteration 19088/35720 Training loss: 0.8214 0.2219 sec/batch\n",
      "Epoch 11/20  Iteration 19089/35720 Training loss: 0.8215 0.2269 sec/batch\n",
      "Epoch 11/20  Iteration 19090/35720 Training loss: 0.8215 0.2204 sec/batch\n",
      "Epoch 11/20  Iteration 19091/35720 Training loss: 0.8215 0.2051 sec/batch\n",
      "Epoch 11/20  Iteration 19092/35720 Training loss: 0.8214 0.2208 sec/batch\n",
      "Epoch 11/20  Iteration 19093/35720 Training loss: 0.8214 0.2083 sec/batch\n",
      "Epoch 11/20  Iteration 19094/35720 Training loss: 0.8214 0.2269 sec/batch\n",
      "Epoch 11/20  Iteration 19095/35720 Training loss: 0.8214 0.2332 sec/batch\n",
      "Epoch 11/20  Iteration 19096/35720 Training loss: 0.8214 0.2267 sec/batch\n",
      "Epoch 11/20  Iteration 19097/35720 Training loss: 0.8213 0.2099 sec/batch\n",
      "Epoch 11/20  Iteration 19098/35720 Training loss: 0.8213 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 19099/35720 Training loss: 0.8212 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 19100/35720 Training loss: 0.8212 0.2256 sec/batch\n",
      "Epoch 11/20  Iteration 19101/35720 Training loss: 0.8211 0.2193 sec/batch\n",
      "Epoch 11/20  Iteration 19102/35720 Training loss: 0.8211 0.2072 sec/batch\n",
      "Epoch 11/20  Iteration 19103/35720 Training loss: 0.8211 0.2128 sec/batch\n",
      "Epoch 11/20  Iteration 19104/35720 Training loss: 0.8211 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 19105/35720 Training loss: 0.8211 0.2197 sec/batch\n",
      "Epoch 11/20  Iteration 19106/35720 Training loss: 0.8210 0.2277 sec/batch\n",
      "Epoch 11/20  Iteration 19107/35720 Training loss: 0.8210 0.2253 sec/batch\n",
      "Epoch 11/20  Iteration 19108/35720 Training loss: 0.8210 0.2122 sec/batch\n",
      "Epoch 11/20  Iteration 19109/35720 Training loss: 0.8209 0.2254 sec/batch\n",
      "Epoch 11/20  Iteration 19110/35720 Training loss: 0.8209 0.2293 sec/batch\n",
      "Epoch 11/20  Iteration 19111/35720 Training loss: 0.8209 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 19112/35720 Training loss: 0.8208 0.2210 sec/batch\n",
      "Epoch 11/20  Iteration 19113/35720 Training loss: 0.8208 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19114/35720 Training loss: 0.8208 0.2075 sec/batch\n",
      "Epoch 11/20  Iteration 19115/35720 Training loss: 0.8208 0.2097 sec/batch\n",
      "Epoch 11/20  Iteration 19116/35720 Training loss: 0.8208 0.2261 sec/batch\n",
      "Epoch 11/20  Iteration 19117/35720 Training loss: 0.8208 0.2192 sec/batch\n",
      "Epoch 11/20  Iteration 19118/35720 Training loss: 0.8207 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 19119/35720 Training loss: 0.8207 0.2244 sec/batch\n",
      "Epoch 11/20  Iteration 19120/35720 Training loss: 0.8207 0.2125 sec/batch\n",
      "Epoch 11/20  Iteration 19121/35720 Training loss: 0.8207 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 19122/35720 Training loss: 0.8207 0.2134 sec/batch\n",
      "Epoch 11/20  Iteration 19123/35720 Training loss: 0.8206 0.2252 sec/batch\n",
      "Epoch 11/20  Iteration 19124/35720 Training loss: 0.8206 0.2251 sec/batch\n",
      "Epoch 11/20  Iteration 19125/35720 Training loss: 0.8205 0.2180 sec/batch\n",
      "Epoch 11/20  Iteration 19126/35720 Training loss: 0.8205 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 19127/35720 Training loss: 0.8204 0.2141 sec/batch\n",
      "Epoch 11/20  Iteration 19128/35720 Training loss: 0.8204 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 19129/35720 Training loss: 0.8204 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 19130/35720 Training loss: 0.8204 0.2204 sec/batch\n",
      "Epoch 11/20  Iteration 19131/35720 Training loss: 0.8204 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 19132/35720 Training loss: 0.8203 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 19133/35720 Training loss: 0.8203 0.2160 sec/batch\n",
      "Epoch 11/20  Iteration 19134/35720 Training loss: 0.8203 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 19135/35720 Training loss: 0.8203 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 19136/35720 Training loss: 0.8203 0.2072 sec/batch\n",
      "Epoch 11/20  Iteration 19137/35720 Training loss: 0.8202 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19138/35720 Training loss: 0.8201 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 19139/35720 Training loss: 0.8201 0.2081 sec/batch\n",
      "Epoch 11/20  Iteration 19140/35720 Training loss: 0.8200 0.2225 sec/batch\n",
      "Epoch 11/20  Iteration 19141/35720 Training loss: 0.8200 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 19142/35720 Training loss: 0.8200 0.2273 sec/batch\n",
      "Epoch 11/20  Iteration 19143/35720 Training loss: 0.8200 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 19144/35720 Training loss: 0.8199 0.2144 sec/batch\n",
      "Epoch 11/20  Iteration 19145/35720 Training loss: 0.8199 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 19146/35720 Training loss: 0.8198 0.2208 sec/batch\n",
      "Epoch 11/20  Iteration 19147/35720 Training loss: 0.8198 0.2283 sec/batch\n",
      "Epoch 11/20  Iteration 19148/35720 Training loss: 0.8198 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 19149/35720 Training loss: 0.8198 0.2276 sec/batch\n",
      "Epoch 11/20  Iteration 19150/35720 Training loss: 0.8198 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 19151/35720 Training loss: 0.8197 0.2218 sec/batch\n",
      "Epoch 11/20  Iteration 19152/35720 Training loss: 0.8197 0.2203 sec/batch\n",
      "Epoch 11/20  Iteration 19153/35720 Training loss: 0.8197 0.2058 sec/batch\n",
      "Epoch 11/20  Iteration 19154/35720 Training loss: 0.8197 0.2085 sec/batch\n",
      "Epoch 11/20  Iteration 19155/35720 Training loss: 0.8197 0.2257 sec/batch\n",
      "Epoch 11/20  Iteration 19156/35720 Training loss: 0.8197 0.2249 sec/batch\n",
      "Epoch 11/20  Iteration 19157/35720 Training loss: 0.8197 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 19158/35720 Training loss: 0.8197 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 19159/35720 Training loss: 0.8196 0.2097 sec/batch\n",
      "Epoch 11/20  Iteration 19160/35720 Training loss: 0.8196 0.2372 sec/batch\n",
      "Epoch 11/20  Iteration 19161/35720 Training loss: 0.8195 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19162/35720 Training loss: 0.8195 0.2204 sec/batch\n",
      "Epoch 11/20  Iteration 19163/35720 Training loss: 0.8195 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 19164/35720 Training loss: 0.8195 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 19165/35720 Training loss: 0.8194 0.2083 sec/batch\n",
      "Epoch 11/20  Iteration 19166/35720 Training loss: 0.8194 0.2233 sec/batch\n",
      "Epoch 11/20  Iteration 19167/35720 Training loss: 0.8194 0.2226 sec/batch\n",
      "Epoch 11/20  Iteration 19168/35720 Training loss: 0.8193 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 19169/35720 Training loss: 0.8193 0.2081 sec/batch\n",
      "Epoch 11/20  Iteration 19170/35720 Training loss: 0.8192 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 19171/35720 Training loss: 0.8192 0.2174 sec/batch\n",
      "Epoch 11/20  Iteration 19172/35720 Training loss: 0.8192 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 19173/35720 Training loss: 0.8192 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 19174/35720 Training loss: 0.8192 0.2073 sec/batch\n",
      "Epoch 11/20  Iteration 19175/35720 Training loss: 0.8192 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 19176/35720 Training loss: 0.8192 0.2257 sec/batch\n",
      "Epoch 11/20  Iteration 19177/35720 Training loss: 0.8191 0.2149 sec/batch\n",
      "Epoch 11/20  Iteration 19178/35720 Training loss: 0.8191 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19179/35720 Training loss: 0.8191 0.2149 sec/batch\n",
      "Epoch 11/20  Iteration 19180/35720 Training loss: 0.8191 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 19181/35720 Training loss: 0.8191 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 19182/35720 Training loss: 0.8191 0.2192 sec/batch\n",
      "Epoch 11/20  Iteration 19183/35720 Training loss: 0.8190 0.2190 sec/batch\n",
      "Epoch 11/20  Iteration 19184/35720 Training loss: 0.8190 0.2198 sec/batch\n",
      "Epoch 11/20  Iteration 19185/35720 Training loss: 0.8191 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 19186/35720 Training loss: 0.8191 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 19187/35720 Training loss: 0.8191 0.2270 sec/batch\n",
      "Epoch 11/20  Iteration 19188/35720 Training loss: 0.8191 0.2185 sec/batch\n",
      "Epoch 11/20  Iteration 19189/35720 Training loss: 0.8191 0.2235 sec/batch\n",
      "Epoch 11/20  Iteration 19190/35720 Training loss: 0.8190 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 19191/35720 Training loss: 0.8190 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 19192/35720 Training loss: 0.8190 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 19193/35720 Training loss: 0.8191 0.2188 sec/batch\n",
      "Epoch 11/20  Iteration 19194/35720 Training loss: 0.8190 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 19195/35720 Training loss: 0.8191 0.2178 sec/batch\n",
      "Epoch 11/20  Iteration 19196/35720 Training loss: 0.8191 0.2215 sec/batch\n",
      "Epoch 11/20  Iteration 19197/35720 Training loss: 0.8191 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 19198/35720 Training loss: 0.8191 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 19199/35720 Training loss: 0.8191 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 19200/35720 Training loss: 0.8191 0.2212 sec/batch\n",
      "Validation loss: 1.43393 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 19201/35720 Training loss: 0.8193 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 19202/35720 Training loss: 0.8193 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 19203/35720 Training loss: 0.8193 0.2081 sec/batch\n",
      "Epoch 11/20  Iteration 19204/35720 Training loss: 0.8192 0.2206 sec/batch\n",
      "Epoch 11/20  Iteration 19205/35720 Training loss: 0.8192 0.2143 sec/batch\n",
      "Epoch 11/20  Iteration 19206/35720 Training loss: 0.8192 0.2174 sec/batch\n",
      "Epoch 11/20  Iteration 19207/35720 Training loss: 0.8192 0.2189 sec/batch\n",
      "Epoch 11/20  Iteration 19208/35720 Training loss: 0.8193 0.2294 sec/batch\n",
      "Epoch 11/20  Iteration 19209/35720 Training loss: 0.8192 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 19210/35720 Training loss: 0.8193 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 19211/35720 Training loss: 0.8193 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 19212/35720 Training loss: 0.8193 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 19213/35720 Training loss: 0.8194 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 19214/35720 Training loss: 0.8193 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 19215/35720 Training loss: 0.8193 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 19216/35720 Training loss: 0.8192 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 19217/35720 Training loss: 0.8192 0.2232 sec/batch\n",
      "Epoch 11/20  Iteration 19218/35720 Training loss: 0.8192 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 19219/35720 Training loss: 0.8192 0.2179 sec/batch\n",
      "Epoch 11/20  Iteration 19220/35720 Training loss: 0.8192 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 19221/35720 Training loss: 0.8192 0.2177 sec/batch\n",
      "Epoch 11/20  Iteration 19222/35720 Training loss: 0.8191 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 19223/35720 Training loss: 0.8191 0.2121 sec/batch\n",
      "Epoch 11/20  Iteration 19224/35720 Training loss: 0.8191 0.2113 sec/batch\n",
      "Epoch 11/20  Iteration 19225/35720 Training loss: 0.8191 0.2250 sec/batch\n",
      "Epoch 11/20  Iteration 19226/35720 Training loss: 0.8191 0.2224 sec/batch\n",
      "Epoch 11/20  Iteration 19227/35720 Training loss: 0.8191 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 19228/35720 Training loss: 0.8191 0.2236 sec/batch\n",
      "Epoch 11/20  Iteration 19229/35720 Training loss: 0.8191 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 19230/35720 Training loss: 0.8191 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 19231/35720 Training loss: 0.8191 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 19232/35720 Training loss: 0.8191 0.2195 sec/batch\n",
      "Epoch 11/20  Iteration 19233/35720 Training loss: 0.8191 0.2173 sec/batch\n",
      "Epoch 11/20  Iteration 19234/35720 Training loss: 0.8191 0.2050 sec/batch\n",
      "Epoch 11/20  Iteration 19235/35720 Training loss: 0.8191 0.2061 sec/batch\n",
      "Epoch 11/20  Iteration 19236/35720 Training loss: 0.8191 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 19237/35720 Training loss: 0.8191 0.2104 sec/batch\n",
      "Epoch 11/20  Iteration 19238/35720 Training loss: 0.8191 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 19239/35720 Training loss: 0.8191 0.2268 sec/batch\n",
      "Epoch 11/20  Iteration 19240/35720 Training loss: 0.8191 0.2058 sec/batch\n",
      "Epoch 11/20  Iteration 19241/35720 Training loss: 0.8191 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 19242/35720 Training loss: 0.8191 0.2176 sec/batch\n",
      "Epoch 11/20  Iteration 19243/35720 Training loss: 0.8191 0.2277 sec/batch\n",
      "Epoch 11/20  Iteration 19244/35720 Training loss: 0.8190 0.2210 sec/batch\n",
      "Epoch 11/20  Iteration 19245/35720 Training loss: 0.8191 0.2174 sec/batch\n",
      "Epoch 11/20  Iteration 19246/35720 Training loss: 0.8191 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19247/35720 Training loss: 0.8191 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 19248/35720 Training loss: 0.8191 0.2099 sec/batch\n",
      "Epoch 11/20  Iteration 19249/35720 Training loss: 0.8191 0.2171 sec/batch\n",
      "Epoch 11/20  Iteration 19250/35720 Training loss: 0.8190 0.2167 sec/batch\n",
      "Epoch 11/20  Iteration 19251/35720 Training loss: 0.8190 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 19252/35720 Training loss: 0.8190 0.2059 sec/batch\n",
      "Epoch 11/20  Iteration 19253/35720 Training loss: 0.8189 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 19254/35720 Training loss: 0.8189 0.2231 sec/batch\n",
      "Epoch 11/20  Iteration 19255/35720 Training loss: 0.8188 0.2195 sec/batch\n",
      "Epoch 11/20  Iteration 19256/35720 Training loss: 0.8188 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 19257/35720 Training loss: 0.8188 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 19258/35720 Training loss: 0.8187 0.2079 sec/batch\n",
      "Epoch 11/20  Iteration 19259/35720 Training loss: 0.8187 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 19260/35720 Training loss: 0.8186 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 19261/35720 Training loss: 0.8186 0.2185 sec/batch\n",
      "Epoch 11/20  Iteration 19262/35720 Training loss: 0.8186 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 19263/35720 Training loss: 0.8186 0.3040 sec/batch\n",
      "Epoch 11/20  Iteration 19264/35720 Training loss: 0.8186 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 19265/35720 Training loss: 0.8186 0.2131 sec/batch\n",
      "Epoch 11/20  Iteration 19266/35720 Training loss: 0.8185 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 19267/35720 Training loss: 0.8185 0.2172 sec/batch\n",
      "Epoch 11/20  Iteration 19268/35720 Training loss: 0.8185 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 19269/35720 Training loss: 0.8186 0.2056 sec/batch\n",
      "Epoch 11/20  Iteration 19270/35720 Training loss: 0.8185 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 19271/35720 Training loss: 0.8185 0.2134 sec/batch\n",
      "Epoch 11/20  Iteration 19272/35720 Training loss: 0.8185 0.2316 sec/batch\n",
      "Epoch 11/20  Iteration 19273/35720 Training loss: 0.8185 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 19274/35720 Training loss: 0.8185 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 19275/35720 Training loss: 0.8185 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 19276/35720 Training loss: 0.8185 0.2176 sec/batch\n",
      "Epoch 11/20  Iteration 19277/35720 Training loss: 0.8184 0.2090 sec/batch\n",
      "Epoch 11/20  Iteration 19278/35720 Training loss: 0.8185 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 19279/35720 Training loss: 0.8185 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 19280/35720 Training loss: 0.8185 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 19281/35720 Training loss: 0.8185 0.2212 sec/batch\n",
      "Epoch 11/20  Iteration 19282/35720 Training loss: 0.8184 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 19283/35720 Training loss: 0.8184 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 19284/35720 Training loss: 0.8184 0.2203 sec/batch\n",
      "Epoch 11/20  Iteration 19285/35720 Training loss: 0.8185 0.2150 sec/batch\n",
      "Epoch 11/20  Iteration 19286/35720 Training loss: 0.8185 0.2195 sec/batch\n",
      "Epoch 11/20  Iteration 19287/35720 Training loss: 0.8185 0.2227 sec/batch\n",
      "Epoch 11/20  Iteration 19288/35720 Training loss: 0.8185 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 19289/35720 Training loss: 0.8185 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 19290/35720 Training loss: 0.8185 0.2119 sec/batch\n",
      "Epoch 11/20  Iteration 19291/35720 Training loss: 0.8184 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 19292/35720 Training loss: 0.8184 0.2191 sec/batch\n",
      "Epoch 11/20  Iteration 19293/35720 Training loss: 0.8184 0.2196 sec/batch\n",
      "Epoch 11/20  Iteration 19294/35720 Training loss: 0.8184 0.2075 sec/batch\n",
      "Epoch 11/20  Iteration 19295/35720 Training loss: 0.8184 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 19296/35720 Training loss: 0.8183 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 19297/35720 Training loss: 0.8183 0.2109 sec/batch\n",
      "Epoch 11/20  Iteration 19298/35720 Training loss: 0.8183 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 19299/35720 Training loss: 0.8183 0.2171 sec/batch\n",
      "Epoch 11/20  Iteration 19300/35720 Training loss: 0.8183 0.2198 sec/batch\n",
      "Epoch 11/20  Iteration 19301/35720 Training loss: 0.8184 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 19302/35720 Training loss: 0.8184 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 19303/35720 Training loss: 0.8184 0.2283 sec/batch\n",
      "Epoch 11/20  Iteration 19304/35720 Training loss: 0.8183 0.2218 sec/batch\n",
      "Epoch 11/20  Iteration 19305/35720 Training loss: 0.8183 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19306/35720 Training loss: 0.8183 0.2219 sec/batch\n",
      "Epoch 11/20  Iteration 19307/35720 Training loss: 0.8183 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 19308/35720 Training loss: 0.8183 0.2107 sec/batch\n",
      "Epoch 11/20  Iteration 19309/35720 Training loss: 0.8183 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 19310/35720 Training loss: 0.8182 0.2261 sec/batch\n",
      "Epoch 11/20  Iteration 19311/35720 Training loss: 0.8182 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 19312/35720 Training loss: 0.8182 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 19313/35720 Training loss: 0.8182 0.2068 sec/batch\n",
      "Epoch 11/20  Iteration 19314/35720 Training loss: 0.8182 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 19315/35720 Training loss: 0.8182 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 19316/35720 Training loss: 0.8182 0.2060 sec/batch\n",
      "Epoch 11/20  Iteration 19317/35720 Training loss: 0.8182 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 19318/35720 Training loss: 0.8182 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 19319/35720 Training loss: 0.8183 0.2071 sec/batch\n",
      "Epoch 11/20  Iteration 19320/35720 Training loss: 0.8183 0.2184 sec/batch\n",
      "Epoch 11/20  Iteration 19321/35720 Training loss: 0.8183 0.2196 sec/batch\n",
      "Epoch 11/20  Iteration 19322/35720 Training loss: 0.8184 0.2227 sec/batch\n",
      "Epoch 11/20  Iteration 19323/35720 Training loss: 0.8184 0.2278 sec/batch\n",
      "Epoch 11/20  Iteration 19324/35720 Training loss: 0.8183 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 19325/35720 Training loss: 0.8183 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 19326/35720 Training loss: 0.8183 0.2273 sec/batch\n",
      "Epoch 11/20  Iteration 19327/35720 Training loss: 0.8183 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 19328/35720 Training loss: 0.8183 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 19329/35720 Training loss: 0.8183 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 19330/35720 Training loss: 0.8183 0.2186 sec/batch\n",
      "Epoch 11/20  Iteration 19331/35720 Training loss: 0.8183 0.2144 sec/batch\n",
      "Epoch 11/20  Iteration 19332/35720 Training loss: 0.8182 0.2142 sec/batch\n",
      "Epoch 11/20  Iteration 19333/35720 Training loss: 0.8182 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 19334/35720 Training loss: 0.8182 0.2326 sec/batch\n",
      "Epoch 11/20  Iteration 19335/35720 Training loss: 0.8181 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 19336/35720 Training loss: 0.8180 0.2113 sec/batch\n",
      "Epoch 11/20  Iteration 19337/35720 Training loss: 0.8179 0.2265 sec/batch\n",
      "Epoch 11/20  Iteration 19338/35720 Training loss: 0.8179 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 19339/35720 Training loss: 0.8179 0.2140 sec/batch\n",
      "Epoch 11/20  Iteration 19340/35720 Training loss: 0.8179 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 19341/35720 Training loss: 0.8179 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 19342/35720 Training loss: 0.8178 0.2081 sec/batch\n",
      "Epoch 11/20  Iteration 19343/35720 Training loss: 0.8178 0.2288 sec/batch\n",
      "Epoch 11/20  Iteration 19344/35720 Training loss: 0.8178 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 19345/35720 Training loss: 0.8177 0.2212 sec/batch\n",
      "Epoch 11/20  Iteration 19346/35720 Training loss: 0.8177 0.2226 sec/batch\n",
      "Epoch 11/20  Iteration 19347/35720 Training loss: 0.8177 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 19348/35720 Training loss: 0.8177 0.2095 sec/batch\n",
      "Epoch 11/20  Iteration 19349/35720 Training loss: 0.8177 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 19350/35720 Training loss: 0.8176 0.2186 sec/batch\n",
      "Epoch 11/20  Iteration 19351/35720 Training loss: 0.8176 0.2265 sec/batch\n",
      "Epoch 11/20  Iteration 19352/35720 Training loss: 0.8175 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 19353/35720 Training loss: 0.8176 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 19354/35720 Training loss: 0.8176 0.2223 sec/batch\n",
      "Epoch 11/20  Iteration 19355/35720 Training loss: 0.8175 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 19356/35720 Training loss: 0.8175 0.2352 sec/batch\n",
      "Epoch 11/20  Iteration 19357/35720 Training loss: 0.8175 0.2194 sec/batch\n",
      "Epoch 11/20  Iteration 19358/35720 Training loss: 0.8174 0.2080 sec/batch\n",
      "Epoch 11/20  Iteration 19359/35720 Training loss: 0.8174 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 19360/35720 Training loss: 0.8174 0.2127 sec/batch\n",
      "Epoch 11/20  Iteration 19361/35720 Training loss: 0.8173 0.2328 sec/batch\n",
      "Epoch 11/20  Iteration 19362/35720 Training loss: 0.8173 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 19363/35720 Training loss: 0.8173 0.2247 sec/batch\n",
      "Epoch 11/20  Iteration 19364/35720 Training loss: 0.8173 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 19365/35720 Training loss: 0.8173 0.2318 sec/batch\n",
      "Epoch 11/20  Iteration 19366/35720 Training loss: 0.8173 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 19367/35720 Training loss: 0.8173 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 19368/35720 Training loss: 0.8173 0.2154 sec/batch\n",
      "Epoch 11/20  Iteration 19369/35720 Training loss: 0.8173 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 19370/35720 Training loss: 0.8173 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 19371/35720 Training loss: 0.8173 0.2151 sec/batch\n",
      "Epoch 11/20  Iteration 19372/35720 Training loss: 0.8172 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 19373/35720 Training loss: 0.8172 0.2211 sec/batch\n",
      "Epoch 11/20  Iteration 19374/35720 Training loss: 0.8173 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 19375/35720 Training loss: 0.8173 0.2211 sec/batch\n",
      "Epoch 11/20  Iteration 19376/35720 Training loss: 0.8173 0.2218 sec/batch\n",
      "Epoch 11/20  Iteration 19377/35720 Training loss: 0.8173 0.2214 sec/batch\n",
      "Epoch 11/20  Iteration 19378/35720 Training loss: 0.8172 0.2517 sec/batch\n",
      "Epoch 11/20  Iteration 19379/35720 Training loss: 0.8173 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 19380/35720 Training loss: 0.8173 0.2172 sec/batch\n",
      "Epoch 11/20  Iteration 19381/35720 Training loss: 0.8173 0.2273 sec/batch\n",
      "Epoch 11/20  Iteration 19382/35720 Training loss: 0.8173 0.2217 sec/batch\n",
      "Epoch 11/20  Iteration 19383/35720 Training loss: 0.8173 0.2145 sec/batch\n",
      "Epoch 11/20  Iteration 19384/35720 Training loss: 0.8173 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 19385/35720 Training loss: 0.8174 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 19386/35720 Training loss: 0.8173 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19387/35720 Training loss: 0.8174 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 19388/35720 Training loss: 0.8173 0.2202 sec/batch\n",
      "Epoch 11/20  Iteration 19389/35720 Training loss: 0.8174 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 19390/35720 Training loss: 0.8174 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 19391/35720 Training loss: 0.8174 0.2244 sec/batch\n",
      "Epoch 11/20  Iteration 19392/35720 Training loss: 0.8174 0.2216 sec/batch\n",
      "Epoch 11/20  Iteration 19393/35720 Training loss: 0.8174 0.2109 sec/batch\n",
      "Epoch 11/20  Iteration 19394/35720 Training loss: 0.8175 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 19395/35720 Training loss: 0.8175 0.2265 sec/batch\n",
      "Epoch 11/20  Iteration 19396/35720 Training loss: 0.8174 0.2206 sec/batch\n",
      "Epoch 11/20  Iteration 19397/35720 Training loss: 0.8174 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 19398/35720 Training loss: 0.8174 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 19399/35720 Training loss: 0.8174 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 19400/35720 Training loss: 0.8173 0.2192 sec/batch\n",
      "Validation loss: 1.43608 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 19401/35720 Training loss: 0.8175 0.2092 sec/batch\n",
      "Epoch 11/20  Iteration 19402/35720 Training loss: 0.8176 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 19403/35720 Training loss: 0.8175 0.2304 sec/batch\n",
      "Epoch 11/20  Iteration 19404/35720 Training loss: 0.8175 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19405/35720 Training loss: 0.8175 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 19406/35720 Training loss: 0.8174 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 19407/35720 Training loss: 0.8174 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 19408/35720 Training loss: 0.8174 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 19409/35720 Training loss: 0.8175 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 19410/35720 Training loss: 0.8175 0.2227 sec/batch\n",
      "Epoch 11/20  Iteration 19411/35720 Training loss: 0.8175 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 19412/35720 Training loss: 0.8174 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 19413/35720 Training loss: 0.8174 0.2246 sec/batch\n",
      "Epoch 11/20  Iteration 19414/35720 Training loss: 0.8174 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 19415/35720 Training loss: 0.8173 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 19416/35720 Training loss: 0.8173 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 19417/35720 Training loss: 0.8173 0.2249 sec/batch\n",
      "Epoch 11/20  Iteration 19418/35720 Training loss: 0.8172 0.2180 sec/batch\n",
      "Epoch 11/20  Iteration 19419/35720 Training loss: 0.8172 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 19420/35720 Training loss: 0.8171 0.2272 sec/batch\n",
      "Epoch 11/20  Iteration 19421/35720 Training loss: 0.8171 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 19422/35720 Training loss: 0.8171 0.2260 sec/batch\n",
      "Epoch 11/20  Iteration 19423/35720 Training loss: 0.8171 0.2100 sec/batch\n",
      "Epoch 11/20  Iteration 19424/35720 Training loss: 0.8170 0.2157 sec/batch\n",
      "Epoch 11/20  Iteration 19425/35720 Training loss: 0.8170 0.2266 sec/batch\n",
      "Epoch 11/20  Iteration 19426/35720 Training loss: 0.8170 0.2209 sec/batch\n",
      "Epoch 11/20  Iteration 19427/35720 Training loss: 0.8169 0.2304 sec/batch\n",
      "Epoch 11/20  Iteration 19428/35720 Training loss: 0.8170 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 19429/35720 Training loss: 0.8170 0.2135 sec/batch\n",
      "Epoch 11/20  Iteration 19430/35720 Training loss: 0.8170 0.2108 sec/batch\n",
      "Epoch 11/20  Iteration 19431/35720 Training loss: 0.8169 0.2160 sec/batch\n",
      "Epoch 11/20  Iteration 19432/35720 Training loss: 0.8169 0.2174 sec/batch\n",
      "Epoch 11/20  Iteration 19433/35720 Training loss: 0.8169 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 19434/35720 Training loss: 0.8168 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 19435/35720 Training loss: 0.8168 0.2207 sec/batch\n",
      "Epoch 11/20  Iteration 19436/35720 Training loss: 0.8168 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 19437/35720 Training loss: 0.8168 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 19438/35720 Training loss: 0.8167 0.2369 sec/batch\n",
      "Epoch 11/20  Iteration 19439/35720 Training loss: 0.8168 0.2116 sec/batch\n",
      "Epoch 11/20  Iteration 19440/35720 Training loss: 0.8167 0.2122 sec/batch\n",
      "Epoch 11/20  Iteration 19441/35720 Training loss: 0.8168 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 19442/35720 Training loss: 0.8167 0.2253 sec/batch\n",
      "Epoch 11/20  Iteration 19443/35720 Training loss: 0.8167 0.2331 sec/batch\n",
      "Epoch 11/20  Iteration 19444/35720 Training loss: 0.8167 0.2264 sec/batch\n",
      "Epoch 11/20  Iteration 19445/35720 Training loss: 0.8167 0.2062 sec/batch\n",
      "Epoch 11/20  Iteration 19446/35720 Training loss: 0.8167 0.2081 sec/batch\n",
      "Epoch 11/20  Iteration 19447/35720 Training loss: 0.8166 0.2274 sec/batch\n",
      "Epoch 11/20  Iteration 19448/35720 Training loss: 0.8166 0.2310 sec/batch\n",
      "Epoch 11/20  Iteration 19449/35720 Training loss: 0.8165 0.2201 sec/batch\n",
      "Epoch 11/20  Iteration 19450/35720 Training loss: 0.8165 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 19451/35720 Training loss: 0.8165 0.2113 sec/batch\n",
      "Epoch 11/20  Iteration 19452/35720 Training loss: 0.8165 0.2159 sec/batch\n",
      "Epoch 11/20  Iteration 19453/35720 Training loss: 0.8165 0.2294 sec/batch\n",
      "Epoch 11/20  Iteration 19454/35720 Training loss: 0.8165 0.2179 sec/batch\n",
      "Epoch 11/20  Iteration 19455/35720 Training loss: 0.8164 0.2244 sec/batch\n",
      "Epoch 11/20  Iteration 19456/35720 Training loss: 0.8164 0.2073 sec/batch\n",
      "Epoch 11/20  Iteration 19457/35720 Training loss: 0.8164 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 19458/35720 Training loss: 0.8163 0.2178 sec/batch\n",
      "Epoch 11/20  Iteration 19459/35720 Training loss: 0.8163 0.2202 sec/batch\n",
      "Epoch 11/20  Iteration 19460/35720 Training loss: 0.8163 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 19461/35720 Training loss: 0.8163 0.2112 sec/batch\n",
      "Epoch 11/20  Iteration 19462/35720 Training loss: 0.8163 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 19463/35720 Training loss: 0.8163 0.2144 sec/batch\n",
      "Epoch 11/20  Iteration 19464/35720 Training loss: 0.8163 0.2167 sec/batch\n",
      "Epoch 11/20  Iteration 19465/35720 Training loss: 0.8162 0.2286 sec/batch\n",
      "Epoch 11/20  Iteration 19466/35720 Training loss: 0.8162 0.2247 sec/batch\n",
      "Epoch 11/20  Iteration 19467/35720 Training loss: 0.8162 0.2227 sec/batch\n",
      "Epoch 11/20  Iteration 19468/35720 Training loss: 0.8162 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 19469/35720 Training loss: 0.8161 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 19470/35720 Training loss: 0.8161 0.2074 sec/batch\n",
      "Epoch 11/20  Iteration 19471/35720 Training loss: 0.8160 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 19472/35720 Training loss: 0.8160 0.2108 sec/batch\n",
      "Epoch 11/20  Iteration 19473/35720 Training loss: 0.8160 0.2060 sec/batch\n",
      "Epoch 11/20  Iteration 19474/35720 Training loss: 0.8160 0.2093 sec/batch\n",
      "Epoch 11/20  Iteration 19475/35720 Training loss: 0.8160 0.2199 sec/batch\n",
      "Epoch 11/20  Iteration 19476/35720 Training loss: 0.8160 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 19477/35720 Training loss: 0.8159 0.2160 sec/batch\n",
      "Epoch 11/20  Iteration 19478/35720 Training loss: 0.8159 0.2132 sec/batch\n",
      "Epoch 11/20  Iteration 19479/35720 Training loss: 0.8159 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 19480/35720 Training loss: 0.8159 0.2099 sec/batch\n",
      "Epoch 11/20  Iteration 19481/35720 Training loss: 0.8158 0.2122 sec/batch\n",
      "Epoch 11/20  Iteration 19482/35720 Training loss: 0.8158 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 19483/35720 Training loss: 0.8158 0.2198 sec/batch\n",
      "Epoch 11/20  Iteration 19484/35720 Training loss: 0.8158 0.2236 sec/batch\n",
      "Epoch 11/20  Iteration 19485/35720 Training loss: 0.8158 0.2170 sec/batch\n",
      "Epoch 11/20  Iteration 19486/35720 Training loss: 0.8158 0.2525 sec/batch\n",
      "Epoch 11/20  Iteration 19487/35720 Training loss: 0.8158 0.2225 sec/batch\n",
      "Epoch 11/20  Iteration 19488/35720 Training loss: 0.8158 0.2372 sec/batch\n",
      "Epoch 11/20  Iteration 19489/35720 Training loss: 0.8158 0.2263 sec/batch\n",
      "Epoch 11/20  Iteration 19490/35720 Training loss: 0.8158 0.2068 sec/batch\n",
      "Epoch 11/20  Iteration 19491/35720 Training loss: 0.8158 0.2051 sec/batch\n",
      "Epoch 11/20  Iteration 19492/35720 Training loss: 0.8157 0.2124 sec/batch\n",
      "Epoch 11/20  Iteration 19493/35720 Training loss: 0.8157 0.2152 sec/batch\n",
      "Epoch 11/20  Iteration 19494/35720 Training loss: 0.8157 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 19495/35720 Training loss: 0.8157 0.2155 sec/batch\n",
      "Epoch 11/20  Iteration 19496/35720 Training loss: 0.8157 0.2082 sec/batch\n",
      "Epoch 11/20  Iteration 19497/35720 Training loss: 0.8157 0.2190 sec/batch\n",
      "Epoch 11/20  Iteration 19498/35720 Training loss: 0.8156 0.2111 sec/batch\n",
      "Epoch 11/20  Iteration 19499/35720 Training loss: 0.8156 0.2309 sec/batch\n",
      "Epoch 11/20  Iteration 19500/35720 Training loss: 0.8156 0.2136 sec/batch\n",
      "Epoch 11/20  Iteration 19501/35720 Training loss: 0.8156 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 19502/35720 Training loss: 0.8156 0.2559 sec/batch\n",
      "Epoch 11/20  Iteration 19503/35720 Training loss: 0.8156 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 19504/35720 Training loss: 0.8156 0.2222 sec/batch\n",
      "Epoch 11/20  Iteration 19505/35720 Training loss: 0.8156 0.2109 sec/batch\n",
      "Epoch 11/20  Iteration 19506/35720 Training loss: 0.8156 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 19507/35720 Training loss: 0.8156 0.2183 sec/batch\n",
      "Epoch 11/20  Iteration 19508/35720 Training loss: 0.8156 0.2120 sec/batch\n",
      "Epoch 11/20  Iteration 19509/35720 Training loss: 0.8156 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 19510/35720 Training loss: 0.8156 0.2131 sec/batch\n",
      "Epoch 11/20  Iteration 19511/35720 Training loss: 0.8156 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 19512/35720 Training loss: 0.8156 0.2106 sec/batch\n",
      "Epoch 11/20  Iteration 19513/35720 Training loss: 0.8156 0.2182 sec/batch\n",
      "Epoch 11/20  Iteration 19514/35720 Training loss: 0.8156 0.2168 sec/batch\n",
      "Epoch 11/20  Iteration 19515/35720 Training loss: 0.8157 0.2147 sec/batch\n",
      "Epoch 11/20  Iteration 19516/35720 Training loss: 0.8157 0.2158 sec/batch\n",
      "Epoch 11/20  Iteration 19517/35720 Training loss: 0.8157 0.2102 sec/batch\n",
      "Epoch 11/20  Iteration 19518/35720 Training loss: 0.8157 0.2115 sec/batch\n",
      "Epoch 11/20  Iteration 19519/35720 Training loss: 0.8157 0.2174 sec/batch\n",
      "Epoch 11/20  Iteration 19520/35720 Training loss: 0.8157 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 19521/35720 Training loss: 0.8156 0.2297 sec/batch\n",
      "Epoch 11/20  Iteration 19522/35720 Training loss: 0.8156 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 19523/35720 Training loss: 0.8157 0.2103 sec/batch\n",
      "Epoch 11/20  Iteration 19524/35720 Training loss: 0.8156 0.2260 sec/batch\n",
      "Epoch 11/20  Iteration 19525/35720 Training loss: 0.8157 0.2161 sec/batch\n",
      "Epoch 11/20  Iteration 19526/35720 Training loss: 0.8156 0.2180 sec/batch\n",
      "Epoch 11/20  Iteration 19527/35720 Training loss: 0.8157 0.2237 sec/batch\n",
      "Epoch 11/20  Iteration 19528/35720 Training loss: 0.8157 0.2066 sec/batch\n",
      "Epoch 11/20  Iteration 19529/35720 Training loss: 0.8157 0.2156 sec/batch\n",
      "Epoch 11/20  Iteration 19530/35720 Training loss: 0.8157 0.2192 sec/batch\n",
      "Epoch 11/20  Iteration 19531/35720 Training loss: 0.8158 0.2301 sec/batch\n",
      "Epoch 11/20  Iteration 19532/35720 Training loss: 0.8158 0.2442 sec/batch\n",
      "Epoch 11/20  Iteration 19533/35720 Training loss: 0.8158 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 19534/35720 Training loss: 0.8158 0.2195 sec/batch\n",
      "Epoch 11/20  Iteration 19535/35720 Training loss: 0.8157 0.2139 sec/batch\n",
      "Epoch 11/20  Iteration 19536/35720 Training loss: 0.8157 0.2077 sec/batch\n",
      "Epoch 11/20  Iteration 19537/35720 Training loss: 0.8157 0.2275 sec/batch\n",
      "Epoch 11/20  Iteration 19538/35720 Training loss: 0.8157 0.2256 sec/batch\n",
      "Epoch 11/20  Iteration 19539/35720 Training loss: 0.8157 0.2061 sec/batch\n",
      "Epoch 11/20  Iteration 19540/35720 Training loss: 0.8156 0.2117 sec/batch\n",
      "Epoch 11/20  Iteration 19541/35720 Training loss: 0.8156 0.2138 sec/batch\n",
      "Epoch 11/20  Iteration 19542/35720 Training loss: 0.8155 0.2269 sec/batch\n",
      "Epoch 11/20  Iteration 19543/35720 Training loss: 0.8155 0.2385 sec/batch\n",
      "Epoch 11/20  Iteration 19544/35720 Training loss: 0.8156 0.2079 sec/batch\n",
      "Epoch 11/20  Iteration 19545/35720 Training loss: 0.8155 0.2056 sec/batch\n",
      "Epoch 11/20  Iteration 19546/35720 Training loss: 0.8156 0.2098 sec/batch\n",
      "Epoch 11/20  Iteration 19547/35720 Training loss: 0.8156 0.2253 sec/batch\n",
      "Epoch 11/20  Iteration 19548/35720 Training loss: 0.8156 0.2264 sec/batch\n",
      "Epoch 11/20  Iteration 19549/35720 Training loss: 0.8156 0.2265 sec/batch\n",
      "Epoch 11/20  Iteration 19550/35720 Training loss: 0.8155 0.2072 sec/batch\n",
      "Epoch 11/20  Iteration 19551/35720 Training loss: 0.8155 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 19552/35720 Training loss: 0.8155 0.2270 sec/batch\n",
      "Epoch 11/20  Iteration 19553/35720 Training loss: 0.8155 0.2110 sec/batch\n",
      "Epoch 11/20  Iteration 19554/35720 Training loss: 0.8155 0.2179 sec/batch\n",
      "Epoch 11/20  Iteration 19555/35720 Training loss: 0.8154 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 19556/35720 Training loss: 0.8154 0.2067 sec/batch\n",
      "Epoch 11/20  Iteration 19557/35720 Training loss: 0.8154 0.2197 sec/batch\n",
      "Epoch 11/20  Iteration 19558/35720 Training loss: 0.8154 0.2209 sec/batch\n",
      "Epoch 11/20  Iteration 19559/35720 Training loss: 0.8154 0.2173 sec/batch\n",
      "Epoch 11/20  Iteration 19560/35720 Training loss: 0.8154 0.2307 sec/batch\n",
      "Epoch 11/20  Iteration 19561/35720 Training loss: 0.8154 0.2081 sec/batch\n",
      "Epoch 11/20  Iteration 19562/35720 Training loss: 0.8154 0.2166 sec/batch\n",
      "Epoch 11/20  Iteration 19563/35720 Training loss: 0.8154 0.2200 sec/batch\n",
      "Epoch 11/20  Iteration 19564/35720 Training loss: 0.8153 0.2165 sec/batch\n",
      "Epoch 11/20  Iteration 19565/35720 Training loss: 0.8153 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 19566/35720 Training loss: 0.8153 0.2259 sec/batch\n",
      "Epoch 11/20  Iteration 19567/35720 Training loss: 0.8153 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 19568/35720 Training loss: 0.8152 0.2096 sec/batch\n",
      "Epoch 11/20  Iteration 19569/35720 Training loss: 0.8153 0.2144 sec/batch\n",
      "Epoch 11/20  Iteration 19570/35720 Training loss: 0.8153 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 19571/35720 Training loss: 0.8154 0.2265 sec/batch\n",
      "Epoch 11/20  Iteration 19572/35720 Training loss: 0.8154 0.2218 sec/batch\n",
      "Epoch 11/20  Iteration 19573/35720 Training loss: 0.8154 0.2243 sec/batch\n",
      "Epoch 11/20  Iteration 19574/35720 Training loss: 0.8154 0.2147 sec/batch\n",
      "Epoch 11/20  Iteration 19575/35720 Training loss: 0.8154 0.2149 sec/batch\n",
      "Epoch 11/20  Iteration 19576/35720 Training loss: 0.8154 0.2063 sec/batch\n",
      "Epoch 11/20  Iteration 19577/35720 Training loss: 0.8154 0.2163 sec/batch\n",
      "Epoch 11/20  Iteration 19578/35720 Training loss: 0.8153 0.2058 sec/batch\n",
      "Epoch 11/20  Iteration 19579/35720 Training loss: 0.8153 0.2137 sec/batch\n",
      "Epoch 11/20  Iteration 19580/35720 Training loss: 0.8153 0.2215 sec/batch\n",
      "Epoch 11/20  Iteration 19581/35720 Training loss: 0.8153 0.2083 sec/batch\n",
      "Epoch 11/20  Iteration 19582/35720 Training loss: 0.8153 0.2351 sec/batch\n",
      "Epoch 11/20  Iteration 19583/35720 Training loss: 0.8153 0.2373 sec/batch\n",
      "Epoch 11/20  Iteration 19584/35720 Training loss: 0.8153 0.2114 sec/batch\n",
      "Epoch 11/20  Iteration 19585/35720 Training loss: 0.8152 0.2269 sec/batch\n",
      "Epoch 11/20  Iteration 19586/35720 Training loss: 0.8153 0.2087 sec/batch\n",
      "Epoch 11/20  Iteration 19587/35720 Training loss: 0.8152 0.2251 sec/batch\n",
      "Epoch 11/20  Iteration 19588/35720 Training loss: 0.8152 0.2177 sec/batch\n",
      "Epoch 11/20  Iteration 19589/35720 Training loss: 0.8152 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 19590/35720 Training loss: 0.8152 0.2241 sec/batch\n",
      "Epoch 11/20  Iteration 19591/35720 Training loss: 0.8152 0.2153 sec/batch\n",
      "Epoch 11/20  Iteration 19592/35720 Training loss: 0.8152 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 19593/35720 Training loss: 0.8152 0.2231 sec/batch\n",
      "Epoch 11/20  Iteration 19594/35720 Training loss: 0.8152 0.2177 sec/batch\n",
      "Epoch 11/20  Iteration 19595/35720 Training loss: 0.8153 0.2091 sec/batch\n",
      "Epoch 11/20  Iteration 19596/35720 Training loss: 0.8153 0.2088 sec/batch\n",
      "Epoch 11/20  Iteration 19597/35720 Training loss: 0.8153 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 19598/35720 Training loss: 0.8153 0.2138 sec/batch\n",
      "Epoch 11/20  Iteration 19599/35720 Training loss: 0.8153 0.2136 sec/batch\n",
      "Epoch 11/20  Iteration 19600/35720 Training loss: 0.8153 0.2104 sec/batch\n",
      "Validation loss: 1.44319 Saving checkpoint!\n",
      "Epoch 11/20  Iteration 19601/35720 Training loss: 0.8155 0.2118 sec/batch\n",
      "Epoch 11/20  Iteration 19602/35720 Training loss: 0.8155 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 19603/35720 Training loss: 0.8154 0.2126 sec/batch\n",
      "Epoch 11/20  Iteration 19604/35720 Training loss: 0.8155 0.2085 sec/batch\n",
      "Epoch 11/20  Iteration 19605/35720 Training loss: 0.8155 0.2105 sec/batch\n",
      "Epoch 11/20  Iteration 19606/35720 Training loss: 0.8155 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 19607/35720 Training loss: 0.8155 0.2233 sec/batch\n",
      "Epoch 11/20  Iteration 19608/35720 Training loss: 0.8156 0.2101 sec/batch\n",
      "Epoch 11/20  Iteration 19609/35720 Training loss: 0.8155 0.2240 sec/batch\n",
      "Epoch 11/20  Iteration 19610/35720 Training loss: 0.8155 0.2242 sec/batch\n",
      "Epoch 11/20  Iteration 19611/35720 Training loss: 0.8155 0.2070 sec/batch\n",
      "Epoch 11/20  Iteration 19612/35720 Training loss: 0.8155 0.2084 sec/batch\n",
      "Epoch 11/20  Iteration 19613/35720 Training loss: 0.8155 0.2301 sec/batch\n",
      "Epoch 11/20  Iteration 19614/35720 Training loss: 0.8155 0.2268 sec/batch\n",
      "Epoch 11/20  Iteration 19615/35720 Training loss: 0.8155 0.2158 sec/batch\n",
      "Epoch 11/20  Iteration 19616/35720 Training loss: 0.8155 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 19617/35720 Training loss: 0.8155 0.2130 sec/batch\n",
      "Epoch 11/20  Iteration 19618/35720 Training loss: 0.8155 0.2122 sec/batch\n",
      "Epoch 11/20  Iteration 19619/35720 Training loss: 0.8155 0.2101 sec/batch\n",
      "Epoch 11/20  Iteration 19620/35720 Training loss: 0.8155 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 19621/35720 Training loss: 0.8155 0.2174 sec/batch\n",
      "Epoch 11/20  Iteration 19622/35720 Training loss: 0.8155 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 19623/35720 Training loss: 0.8155 0.2094 sec/batch\n",
      "Epoch 11/20  Iteration 19624/35720 Training loss: 0.8154 0.2146 sec/batch\n",
      "Epoch 11/20  Iteration 19625/35720 Training loss: 0.8154 0.2086 sec/batch\n",
      "Epoch 11/20  Iteration 19626/35720 Training loss: 0.8154 0.2188 sec/batch\n",
      "Epoch 11/20  Iteration 19627/35720 Training loss: 0.8154 0.2175 sec/batch\n",
      "Epoch 11/20  Iteration 19628/35720 Training loss: 0.8154 0.2065 sec/batch\n",
      "Epoch 11/20  Iteration 19629/35720 Training loss: 0.8154 0.2089 sec/batch\n",
      "Epoch 11/20  Iteration 19630/35720 Training loss: 0.8154 0.2164 sec/batch\n",
      "Epoch 11/20  Iteration 19631/35720 Training loss: 0.8154 0.2251 sec/batch\n",
      "Epoch 11/20  Iteration 19632/35720 Training loss: 0.8154 0.2055 sec/batch\n",
      "Epoch 11/20  Iteration 19633/35720 Training loss: 0.8154 0.2148 sec/batch\n",
      "Epoch 11/20  Iteration 19634/35720 Training loss: 0.8154 0.2180 sec/batch\n",
      "Epoch 11/20  Iteration 19635/35720 Training loss: 0.8154 0.2264 sec/batch\n",
      "Epoch 11/20  Iteration 19636/35720 Training loss: 0.8153 0.2250 sec/batch\n",
      "Epoch 11/20  Iteration 19637/35720 Training loss: 0.8154 0.2220 sec/batch\n",
      "Epoch 11/20  Iteration 19638/35720 Training loss: 0.8153 0.2133 sec/batch\n",
      "Epoch 11/20  Iteration 19639/35720 Training loss: 0.8153 0.2305 sec/batch\n",
      "Epoch 11/20  Iteration 19640/35720 Training loss: 0.8153 0.2129 sec/batch\n",
      "Epoch 11/20  Iteration 19641/35720 Training loss: 0.8153 0.2232 sec/batch\n",
      "Epoch 11/20  Iteration 19642/35720 Training loss: 0.8152 0.2263 sec/batch\n",
      "Epoch 11/20  Iteration 19643/35720 Training loss: 0.8152 0.2162 sec/batch\n",
      "Epoch 11/20  Iteration 19644/35720 Training loss: 0.8152 0.2064 sec/batch\n",
      "Epoch 11/20  Iteration 19645/35720 Training loss: 0.8152 0.2083 sec/batch\n",
      "Epoch 11/20  Iteration 19646/35720 Training loss: 0.8152 0.2143 sec/batch\n",
      "Epoch 12/20  Iteration 19647/35720 Training loss: 0.8358 0.2208 sec/batch\n",
      "Epoch 12/20  Iteration 19648/35720 Training loss: 0.8450 0.2331 sec/batch\n",
      "Epoch 12/20  Iteration 19649/35720 Training loss: 0.8336 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 19650/35720 Training loss: 0.8295 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 19651/35720 Training loss: 0.8365 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 19652/35720 Training loss: 0.8242 0.2309 sec/batch\n",
      "Epoch 12/20  Iteration 19653/35720 Training loss: 0.8218 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 19654/35720 Training loss: 0.8141 0.2177 sec/batch\n",
      "Epoch 12/20  Iteration 19655/35720 Training loss: 0.8077 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 19656/35720 Training loss: 0.8088 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 19657/35720 Training loss: 0.8097 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 19658/35720 Training loss: 0.8067 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 19659/35720 Training loss: 0.8092 0.2155 sec/batch\n",
      "Epoch 12/20  Iteration 19660/35720 Training loss: 0.8132 0.2216 sec/batch\n",
      "Epoch 12/20  Iteration 19661/35720 Training loss: 0.8145 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 19662/35720 Training loss: 0.8146 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 19663/35720 Training loss: 0.8142 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 19664/35720 Training loss: 0.8110 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 19665/35720 Training loss: 0.8096 0.2241 sec/batch\n",
      "Epoch 12/20  Iteration 19666/35720 Training loss: 0.8097 0.2299 sec/batch\n",
      "Epoch 12/20  Iteration 19667/35720 Training loss: 0.8098 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 19668/35720 Training loss: 0.8074 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 19669/35720 Training loss: 0.8057 0.2191 sec/batch\n",
      "Epoch 12/20  Iteration 19670/35720 Training loss: 0.8067 0.2242 sec/batch\n",
      "Epoch 12/20  Iteration 19671/35720 Training loss: 0.8062 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 19672/35720 Training loss: 0.8060 0.2180 sec/batch\n",
      "Epoch 12/20  Iteration 19673/35720 Training loss: 0.8083 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 19674/35720 Training loss: 0.8085 0.2209 sec/batch\n",
      "Epoch 12/20  Iteration 19675/35720 Training loss: 0.8076 0.2134 sec/batch\n",
      "Epoch 12/20  Iteration 19676/35720 Training loss: 0.8081 0.2246 sec/batch\n",
      "Epoch 12/20  Iteration 19677/35720 Training loss: 0.8104 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 19678/35720 Training loss: 0.8091 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 19679/35720 Training loss: 0.8105 0.2079 sec/batch\n",
      "Epoch 12/20  Iteration 19680/35720 Training loss: 0.8122 0.2276 sec/batch\n",
      "Epoch 12/20  Iteration 19681/35720 Training loss: 0.8145 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 19682/35720 Training loss: 0.8150 0.2340 sec/batch\n",
      "Epoch 12/20  Iteration 19683/35720 Training loss: 0.8156 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 19684/35720 Training loss: 0.8150 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 19685/35720 Training loss: 0.8139 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 19686/35720 Training loss: 0.8153 0.2270 sec/batch\n",
      "Epoch 12/20  Iteration 19687/35720 Training loss: 0.8151 0.2281 sec/batch\n",
      "Epoch 12/20  Iteration 19688/35720 Training loss: 0.8142 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 19689/35720 Training loss: 0.8123 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 19690/35720 Training loss: 0.8117 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 19691/35720 Training loss: 0.8114 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 19692/35720 Training loss: 0.8106 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 19693/35720 Training loss: 0.8096 0.2332 sec/batch\n",
      "Epoch 12/20  Iteration 19694/35720 Training loss: 0.8093 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 19695/35720 Training loss: 0.8087 0.2244 sec/batch\n",
      "Epoch 12/20  Iteration 19696/35720 Training loss: 0.8078 0.2228 sec/batch\n",
      "Epoch 12/20  Iteration 19697/35720 Training loss: 0.8081 0.2158 sec/batch\n",
      "Epoch 12/20  Iteration 19698/35720 Training loss: 0.8081 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 19699/35720 Training loss: 0.8080 0.2294 sec/batch\n",
      "Epoch 12/20  Iteration 19700/35720 Training loss: 0.8062 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 19701/35720 Training loss: 0.8052 0.2126 sec/batch\n",
      "Epoch 12/20  Iteration 19702/35720 Training loss: 0.8049 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 19703/35720 Training loss: 0.8050 0.2263 sec/batch\n",
      "Epoch 12/20  Iteration 19704/35720 Training loss: 0.8049 0.2135 sec/batch\n",
      "Epoch 12/20  Iteration 19705/35720 Training loss: 0.8043 0.2055 sec/batch\n",
      "Epoch 12/20  Iteration 19706/35720 Training loss: 0.8032 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 19707/35720 Training loss: 0.8027 0.2120 sec/batch\n",
      "Epoch 12/20  Iteration 19708/35720 Training loss: 0.8011 0.2191 sec/batch\n",
      "Epoch 12/20  Iteration 19709/35720 Training loss: 0.8018 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 19710/35720 Training loss: 0.8017 0.2372 sec/batch\n",
      "Epoch 12/20  Iteration 19711/35720 Training loss: 0.8024 0.2235 sec/batch\n",
      "Epoch 12/20  Iteration 19712/35720 Training loss: 0.8027 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 19713/35720 Training loss: 0.8026 0.2137 sec/batch\n",
      "Epoch 12/20  Iteration 19714/35720 Training loss: 0.8019 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 19715/35720 Training loss: 0.8026 0.2204 sec/batch\n",
      "Epoch 12/20  Iteration 19716/35720 Training loss: 0.8023 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 19717/35720 Training loss: 0.8027 0.2236 sec/batch\n",
      "Epoch 12/20  Iteration 19718/35720 Training loss: 0.8036 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 19719/35720 Training loss: 0.8038 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 19720/35720 Training loss: 0.8040 0.2155 sec/batch\n",
      "Epoch 12/20  Iteration 19721/35720 Training loss: 0.8032 0.2201 sec/batch\n",
      "Epoch 12/20  Iteration 19722/35720 Training loss: 0.8034 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 19723/35720 Training loss: 0.8024 0.2079 sec/batch\n",
      "Epoch 12/20  Iteration 19724/35720 Training loss: 0.8028 0.2304 sec/batch\n",
      "Epoch 12/20  Iteration 19725/35720 Training loss: 0.8025 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 19726/35720 Training loss: 0.8037 0.2243 sec/batch\n",
      "Epoch 12/20  Iteration 19727/35720 Training loss: 0.8042 0.2070 sec/batch\n",
      "Epoch 12/20  Iteration 19728/35720 Training loss: 0.8041 0.2104 sec/batch\n",
      "Epoch 12/20  Iteration 19729/35720 Training loss: 0.8043 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 19730/35720 Training loss: 0.8044 0.2251 sec/batch\n",
      "Epoch 12/20  Iteration 19731/35720 Training loss: 0.8042 0.2231 sec/batch\n",
      "Epoch 12/20  Iteration 19732/35720 Training loss: 0.8043 0.2228 sec/batch\n",
      "Epoch 12/20  Iteration 19733/35720 Training loss: 0.8044 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 19734/35720 Training loss: 0.8046 0.2191 sec/batch\n",
      "Epoch 12/20  Iteration 19735/35720 Training loss: 0.8036 0.2221 sec/batch\n",
      "Epoch 12/20  Iteration 19736/35720 Training loss: 0.8029 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 19737/35720 Training loss: 0.8032 0.2348 sec/batch\n",
      "Epoch 12/20  Iteration 19738/35720 Training loss: 0.8030 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 19739/35720 Training loss: 0.8033 0.2190 sec/batch\n",
      "Epoch 12/20  Iteration 19740/35720 Training loss: 0.8037 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 19741/35720 Training loss: 0.8038 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 19742/35720 Training loss: 0.8032 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 19743/35720 Training loss: 0.8036 0.2120 sec/batch\n",
      "Epoch 12/20  Iteration 19744/35720 Training loss: 0.8035 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 19745/35720 Training loss: 0.8037 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 19746/35720 Training loss: 0.8034 0.2297 sec/batch\n",
      "Epoch 12/20  Iteration 19747/35720 Training loss: 0.8030 0.2130 sec/batch\n",
      "Epoch 12/20  Iteration 19748/35720 Training loss: 0.8030 0.2138 sec/batch\n",
      "Epoch 12/20  Iteration 19749/35720 Training loss: 0.8024 0.2070 sec/batch\n",
      "Epoch 12/20  Iteration 19750/35720 Training loss: 0.8021 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 19751/35720 Training loss: 0.8019 0.2302 sec/batch\n",
      "Epoch 12/20  Iteration 19752/35720 Training loss: 0.8015 0.2254 sec/batch\n",
      "Epoch 12/20  Iteration 19753/35720 Training loss: 0.8014 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 19754/35720 Training loss: 0.8015 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 19755/35720 Training loss: 0.8019 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 19756/35720 Training loss: 0.8017 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 19757/35720 Training loss: 0.8019 0.2190 sec/batch\n",
      "Epoch 12/20  Iteration 19758/35720 Training loss: 0.8021 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 19759/35720 Training loss: 0.8022 0.2169 sec/batch\n",
      "Epoch 12/20  Iteration 19760/35720 Training loss: 0.8023 0.2059 sec/batch\n",
      "Epoch 12/20  Iteration 19761/35720 Training loss: 0.8022 0.2063 sec/batch\n",
      "Epoch 12/20  Iteration 19762/35720 Training loss: 0.8020 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 19763/35720 Training loss: 0.8020 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 19764/35720 Training loss: 0.8023 0.2241 sec/batch\n",
      "Epoch 12/20  Iteration 19765/35720 Training loss: 0.8022 0.2263 sec/batch\n",
      "Epoch 12/20  Iteration 19766/35720 Training loss: 0.8027 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 19767/35720 Training loss: 0.8031 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 19768/35720 Training loss: 0.8028 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 19769/35720 Training loss: 0.8028 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 19770/35720 Training loss: 0.8034 0.2141 sec/batch\n",
      "Epoch 12/20  Iteration 19771/35720 Training loss: 0.8031 0.2343 sec/batch\n",
      "Epoch 12/20  Iteration 19772/35720 Training loss: 0.8031 0.2071 sec/batch\n",
      "Epoch 12/20  Iteration 19773/35720 Training loss: 0.8033 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 19774/35720 Training loss: 0.8032 0.2233 sec/batch\n",
      "Epoch 12/20  Iteration 19775/35720 Training loss: 0.8028 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 19776/35720 Training loss: 0.8028 0.2237 sec/batch\n",
      "Epoch 12/20  Iteration 19777/35720 Training loss: 0.8027 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 19778/35720 Training loss: 0.8024 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 19779/35720 Training loss: 0.8025 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 19780/35720 Training loss: 0.8026 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 19781/35720 Training loss: 0.8026 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 19782/35720 Training loss: 0.8026 0.2205 sec/batch\n",
      "Epoch 12/20  Iteration 19783/35720 Training loss: 0.8033 0.2287 sec/batch\n",
      "Epoch 12/20  Iteration 19784/35720 Training loss: 0.8034 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 19785/35720 Training loss: 0.8035 0.2150 sec/batch\n",
      "Epoch 12/20  Iteration 19786/35720 Training loss: 0.8037 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 19787/35720 Training loss: 0.8038 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 19788/35720 Training loss: 0.8034 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 19789/35720 Training loss: 0.8031 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 19790/35720 Training loss: 0.8026 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 19791/35720 Training loss: 0.8027 0.2209 sec/batch\n",
      "Epoch 12/20  Iteration 19792/35720 Training loss: 0.8031 0.2132 sec/batch\n",
      "Epoch 12/20  Iteration 19793/35720 Training loss: 0.8028 0.2257 sec/batch\n",
      "Epoch 12/20  Iteration 19794/35720 Training loss: 0.8026 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 19795/35720 Training loss: 0.8027 0.2401 sec/batch\n",
      "Epoch 12/20  Iteration 19796/35720 Training loss: 0.8021 0.2141 sec/batch\n",
      "Epoch 12/20  Iteration 19797/35720 Training loss: 0.8020 0.2079 sec/batch\n",
      "Epoch 12/20  Iteration 19798/35720 Training loss: 0.8020 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 19799/35720 Training loss: 0.8019 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 19800/35720 Training loss: 0.8023 0.2116 sec/batch\n",
      "Validation loss: 1.45226 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 19801/35720 Training loss: 0.8051 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 19802/35720 Training loss: 0.8056 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 19803/35720 Training loss: 0.8059 0.2324 sec/batch\n",
      "Epoch 12/20  Iteration 19804/35720 Training loss: 0.8063 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 19805/35720 Training loss: 0.8058 0.2075 sec/batch\n",
      "Epoch 12/20  Iteration 19806/35720 Training loss: 0.8058 0.2080 sec/batch\n",
      "Epoch 12/20  Iteration 19807/35720 Training loss: 0.8057 0.2220 sec/batch\n",
      "Epoch 12/20  Iteration 19808/35720 Training loss: 0.8057 0.2278 sec/batch\n",
      "Epoch 12/20  Iteration 19809/35720 Training loss: 0.8057 0.2262 sec/batch\n",
      "Epoch 12/20  Iteration 19810/35720 Training loss: 0.8058 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 19811/35720 Training loss: 0.8058 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 19812/35720 Training loss: 0.8058 0.2305 sec/batch\n",
      "Epoch 12/20  Iteration 19813/35720 Training loss: 0.8059 0.2237 sec/batch\n",
      "Epoch 12/20  Iteration 19814/35720 Training loss: 0.8062 0.2298 sec/batch\n",
      "Epoch 12/20  Iteration 19815/35720 Training loss: 0.8066 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 19816/35720 Training loss: 0.8067 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 19817/35720 Training loss: 0.8073 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 19818/35720 Training loss: 0.8077 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 19819/35720 Training loss: 0.8079 0.2198 sec/batch\n",
      "Epoch 12/20  Iteration 19820/35720 Training loss: 0.8085 0.2124 sec/batch\n",
      "Epoch 12/20  Iteration 19821/35720 Training loss: 0.8086 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 19822/35720 Training loss: 0.8087 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 19823/35720 Training loss: 0.8088 0.2152 sec/batch\n",
      "Epoch 12/20  Iteration 19824/35720 Training loss: 0.8086 0.2313 sec/batch\n",
      "Epoch 12/20  Iteration 19825/35720 Training loss: 0.8085 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 19826/35720 Training loss: 0.8083 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 19827/35720 Training loss: 0.8084 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 19828/35720 Training loss: 0.8085 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 19829/35720 Training loss: 0.8085 0.2212 sec/batch\n",
      "Epoch 12/20  Iteration 19830/35720 Training loss: 0.8087 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 19831/35720 Training loss: 0.8086 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 19832/35720 Training loss: 0.8084 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 19833/35720 Training loss: 0.8083 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 19834/35720 Training loss: 0.8085 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 19835/35720 Training loss: 0.8086 0.2173 sec/batch\n",
      "Epoch 12/20  Iteration 19836/35720 Training loss: 0.8086 0.2400 sec/batch\n",
      "Epoch 12/20  Iteration 19837/35720 Training loss: 0.8088 0.2186 sec/batch\n",
      "Epoch 12/20  Iteration 19838/35720 Training loss: 0.8091 0.2173 sec/batch\n",
      "Epoch 12/20  Iteration 19839/35720 Training loss: 0.8094 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 19840/35720 Training loss: 0.8096 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 19841/35720 Training loss: 0.8096 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 19842/35720 Training loss: 0.8097 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 19843/35720 Training loss: 0.8094 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 19844/35720 Training loss: 0.8093 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 19845/35720 Training loss: 0.8094 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 19846/35720 Training loss: 0.8094 0.2286 sec/batch\n",
      "Epoch 12/20  Iteration 19847/35720 Training loss: 0.8093 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 19848/35720 Training loss: 0.8093 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 19849/35720 Training loss: 0.8094 0.2215 sec/batch\n",
      "Epoch 12/20  Iteration 19850/35720 Training loss: 0.8092 0.2192 sec/batch\n",
      "Epoch 12/20  Iteration 19851/35720 Training loss: 0.8092 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 19852/35720 Training loss: 0.8091 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 19853/35720 Training loss: 0.8095 0.2168 sec/batch\n",
      "Epoch 12/20  Iteration 19854/35720 Training loss: 0.8098 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 19855/35720 Training loss: 0.8101 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 19856/35720 Training loss: 0.8101 0.2239 sec/batch\n",
      "Epoch 12/20  Iteration 19857/35720 Training loss: 0.8103 0.2384 sec/batch\n",
      "Epoch 12/20  Iteration 19858/35720 Training loss: 0.8103 0.2287 sec/batch\n",
      "Epoch 12/20  Iteration 19859/35720 Training loss: 0.8102 0.2256 sec/batch\n",
      "Epoch 12/20  Iteration 19860/35720 Training loss: 0.8103 0.2079 sec/batch\n",
      "Epoch 12/20  Iteration 19861/35720 Training loss: 0.8103 0.2216 sec/batch\n",
      "Epoch 12/20  Iteration 19862/35720 Training loss: 0.8103 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 19863/35720 Training loss: 0.8101 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 19864/35720 Training loss: 0.8099 0.2292 sec/batch\n",
      "Epoch 12/20  Iteration 19865/35720 Training loss: 0.8099 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 19866/35720 Training loss: 0.8100 0.2071 sec/batch\n",
      "Epoch 12/20  Iteration 19867/35720 Training loss: 0.8101 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 19868/35720 Training loss: 0.8102 0.2209 sec/batch\n",
      "Epoch 12/20  Iteration 19869/35720 Training loss: 0.8105 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 19870/35720 Training loss: 0.8107 0.2205 sec/batch\n",
      "Epoch 12/20  Iteration 19871/35720 Training loss: 0.8108 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 19872/35720 Training loss: 0.8107 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 19873/35720 Training loss: 0.8106 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 19874/35720 Training loss: 0.8105 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 19875/35720 Training loss: 0.8102 0.2188 sec/batch\n",
      "Epoch 12/20  Iteration 19876/35720 Training loss: 0.8103 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 19877/35720 Training loss: 0.8106 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 19878/35720 Training loss: 0.8105 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 19879/35720 Training loss: 0.8107 0.2210 sec/batch\n",
      "Epoch 12/20  Iteration 19880/35720 Training loss: 0.8107 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 19881/35720 Training loss: 0.8107 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 19882/35720 Training loss: 0.8107 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 19883/35720 Training loss: 0.8108 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 19884/35720 Training loss: 0.8106 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 19885/35720 Training loss: 0.8105 0.2143 sec/batch\n",
      "Epoch 12/20  Iteration 19886/35720 Training loss: 0.8106 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 19887/35720 Training loss: 0.8104 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 19888/35720 Training loss: 0.8103 0.2120 sec/batch\n",
      "Epoch 12/20  Iteration 19889/35720 Training loss: 0.8102 0.2265 sec/batch\n",
      "Epoch 12/20  Iteration 19890/35720 Training loss: 0.8102 0.2206 sec/batch\n",
      "Epoch 12/20  Iteration 19891/35720 Training loss: 0.8100 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 19892/35720 Training loss: 0.8101 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 19893/35720 Training loss: 0.8100 0.2104 sec/batch\n",
      "Epoch 12/20  Iteration 19894/35720 Training loss: 0.8098 0.2120 sec/batch\n",
      "Epoch 12/20  Iteration 19895/35720 Training loss: 0.8097 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 19896/35720 Training loss: 0.8094 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 19897/35720 Training loss: 0.8095 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 19898/35720 Training loss: 0.8094 0.2269 sec/batch\n",
      "Epoch 12/20  Iteration 19899/35720 Training loss: 0.8092 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 19900/35720 Training loss: 0.8094 0.2285 sec/batch\n",
      "Epoch 12/20  Iteration 19901/35720 Training loss: 0.8097 0.2267 sec/batch\n",
      "Epoch 12/20  Iteration 19902/35720 Training loss: 0.8098 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 19903/35720 Training loss: 0.8098 0.2199 sec/batch\n",
      "Epoch 12/20  Iteration 19904/35720 Training loss: 0.8097 0.2054 sec/batch\n",
      "Epoch 12/20  Iteration 19905/35720 Training loss: 0.8099 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 19906/35720 Training loss: 0.8099 0.2238 sec/batch\n",
      "Epoch 12/20  Iteration 19907/35720 Training loss: 0.8098 0.2331 sec/batch\n",
      "Epoch 12/20  Iteration 19908/35720 Training loss: 0.8096 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 19909/35720 Training loss: 0.8096 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 19910/35720 Training loss: 0.8096 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 19911/35720 Training loss: 0.8096 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 19912/35720 Training loss: 0.8098 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 19913/35720 Training loss: 0.8098 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 19914/35720 Training loss: 0.8098 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 19915/35720 Training loss: 0.8096 0.2057 sec/batch\n",
      "Epoch 12/20  Iteration 19916/35720 Training loss: 0.8093 0.2102 sec/batch\n",
      "Epoch 12/20  Iteration 19917/35720 Training loss: 0.8090 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 19918/35720 Training loss: 0.8090 0.2160 sec/batch\n",
      "Epoch 12/20  Iteration 19919/35720 Training loss: 0.8091 0.2180 sec/batch\n",
      "Epoch 12/20  Iteration 19920/35720 Training loss: 0.8091 0.2147 sec/batch\n",
      "Epoch 12/20  Iteration 19921/35720 Training loss: 0.8089 0.2106 sec/batch\n",
      "Epoch 12/20  Iteration 19922/35720 Training loss: 0.8088 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 19923/35720 Training loss: 0.8085 0.2182 sec/batch\n",
      "Epoch 12/20  Iteration 19924/35720 Training loss: 0.8084 0.2186 sec/batch\n",
      "Epoch 12/20  Iteration 19925/35720 Training loss: 0.8082 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 19926/35720 Training loss: 0.8082 0.2213 sec/batch\n",
      "Epoch 12/20  Iteration 19927/35720 Training loss: 0.8080 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 19928/35720 Training loss: 0.8077 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 19929/35720 Training loss: 0.8075 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 19930/35720 Training loss: 0.8074 0.2280 sec/batch\n",
      "Epoch 12/20  Iteration 19931/35720 Training loss: 0.8076 0.2243 sec/batch\n",
      "Epoch 12/20  Iteration 19932/35720 Training loss: 0.8075 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 19933/35720 Training loss: 0.8073 0.2135 sec/batch\n",
      "Epoch 12/20  Iteration 19934/35720 Training loss: 0.8073 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 19935/35720 Training loss: 0.8074 0.2247 sec/batch\n",
      "Epoch 12/20  Iteration 19936/35720 Training loss: 0.8074 0.2250 sec/batch\n",
      "Epoch 12/20  Iteration 19937/35720 Training loss: 0.8073 0.2143 sec/batch\n",
      "Epoch 12/20  Iteration 19938/35720 Training loss: 0.8074 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 19939/35720 Training loss: 0.8073 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 19940/35720 Training loss: 0.8074 0.2138 sec/batch\n",
      "Epoch 12/20  Iteration 19941/35720 Training loss: 0.8076 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 19942/35720 Training loss: 0.8076 0.2255 sec/batch\n",
      "Epoch 12/20  Iteration 19943/35720 Training loss: 0.8075 0.2078 sec/batch\n",
      "Epoch 12/20  Iteration 19944/35720 Training loss: 0.8077 0.2057 sec/batch\n",
      "Epoch 12/20  Iteration 19945/35720 Training loss: 0.8076 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 19946/35720 Training loss: 0.8076 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 19947/35720 Training loss: 0.8076 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 19948/35720 Training loss: 0.8075 0.2246 sec/batch\n",
      "Epoch 12/20  Iteration 19949/35720 Training loss: 0.8076 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 19950/35720 Training loss: 0.8076 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 19951/35720 Training loss: 0.8075 0.2130 sec/batch\n",
      "Epoch 12/20  Iteration 19952/35720 Training loss: 0.8073 0.2131 sec/batch\n",
      "Epoch 12/20  Iteration 19953/35720 Training loss: 0.8073 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 19954/35720 Training loss: 0.8074 0.2181 sec/batch\n",
      "Epoch 12/20  Iteration 19955/35720 Training loss: 0.8072 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 19956/35720 Training loss: 0.8071 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 19957/35720 Training loss: 0.8071 0.2226 sec/batch\n",
      "Epoch 12/20  Iteration 19958/35720 Training loss: 0.8070 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 19959/35720 Training loss: 0.8069 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 19960/35720 Training loss: 0.8068 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 19961/35720 Training loss: 0.8068 0.2297 sec/batch\n",
      "Epoch 12/20  Iteration 19962/35720 Training loss: 0.8067 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 19963/35720 Training loss: 0.8065 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 19964/35720 Training loss: 0.8065 0.2154 sec/batch\n",
      "Epoch 12/20  Iteration 19965/35720 Training loss: 0.8066 0.2168 sec/batch\n",
      "Epoch 12/20  Iteration 19966/35720 Training loss: 0.8065 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 19967/35720 Training loss: 0.8066 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 19968/35720 Training loss: 0.8066 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 19969/35720 Training loss: 0.8066 0.2135 sec/batch\n",
      "Epoch 12/20  Iteration 19970/35720 Training loss: 0.8067 0.2307 sec/batch\n",
      "Epoch 12/20  Iteration 19971/35720 Training loss: 0.8066 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 19972/35720 Training loss: 0.8067 0.2323 sec/batch\n",
      "Epoch 12/20  Iteration 19973/35720 Training loss: 0.8068 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 19974/35720 Training loss: 0.8068 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 19975/35720 Training loss: 0.8068 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 19976/35720 Training loss: 0.8068 0.2137 sec/batch\n",
      "Epoch 12/20  Iteration 19977/35720 Training loss: 0.8069 0.2164 sec/batch\n",
      "Epoch 12/20  Iteration 19978/35720 Training loss: 0.8069 0.2199 sec/batch\n",
      "Epoch 12/20  Iteration 19979/35720 Training loss: 0.8069 0.2227 sec/batch\n",
      "Epoch 12/20  Iteration 19980/35720 Training loss: 0.8070 0.2099 sec/batch\n",
      "Epoch 12/20  Iteration 19981/35720 Training loss: 0.8070 0.2233 sec/batch\n",
      "Epoch 12/20  Iteration 19982/35720 Training loss: 0.8068 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 19983/35720 Training loss: 0.8067 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 19984/35720 Training loss: 0.8067 0.2148 sec/batch\n",
      "Epoch 12/20  Iteration 19985/35720 Training loss: 0.8066 0.2137 sec/batch\n",
      "Epoch 12/20  Iteration 19986/35720 Training loss: 0.8067 0.2204 sec/batch\n",
      "Epoch 12/20  Iteration 19987/35720 Training loss: 0.8066 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 19988/35720 Training loss: 0.8066 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 19989/35720 Training loss: 0.8066 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 19990/35720 Training loss: 0.8066 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 19991/35720 Training loss: 0.8063 0.2267 sec/batch\n",
      "Epoch 12/20  Iteration 19992/35720 Training loss: 0.8065 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 19993/35720 Training loss: 0.8065 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 19994/35720 Training loss: 0.8067 0.2058 sec/batch\n",
      "Epoch 12/20  Iteration 19995/35720 Training loss: 0.8067 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 19996/35720 Training loss: 0.8067 0.2202 sec/batch\n",
      "Epoch 12/20  Iteration 19997/35720 Training loss: 0.8068 0.2226 sec/batch\n",
      "Epoch 12/20  Iteration 19998/35720 Training loss: 0.8066 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 19999/35720 Training loss: 0.8067 0.2070 sec/batch\n",
      "Epoch 12/20  Iteration 20000/35720 Training loss: 0.8068 0.2072 sec/batch\n",
      "Validation loss: 1.45114 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 20001/35720 Training loss: 0.8078 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20002/35720 Training loss: 0.8079 0.2143 sec/batch\n",
      "Epoch 12/20  Iteration 20003/35720 Training loss: 0.8080 0.2249 sec/batch\n",
      "Epoch 12/20  Iteration 20004/35720 Training loss: 0.8081 0.2059 sec/batch\n",
      "Epoch 12/20  Iteration 20005/35720 Training loss: 0.8080 0.2055 sec/batch\n",
      "Epoch 12/20  Iteration 20006/35720 Training loss: 0.8080 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20007/35720 Training loss: 0.8080 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 20008/35720 Training loss: 0.8081 0.2319 sec/batch\n",
      "Epoch 12/20  Iteration 20009/35720 Training loss: 0.8081 0.2250 sec/batch\n",
      "Epoch 12/20  Iteration 20010/35720 Training loss: 0.8082 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20011/35720 Training loss: 0.8081 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20012/35720 Training loss: 0.8081 0.2322 sec/batch\n",
      "Epoch 12/20  Iteration 20013/35720 Training loss: 0.8081 0.2319 sec/batch\n",
      "Epoch 12/20  Iteration 20014/35720 Training loss: 0.8080 0.2274 sec/batch\n",
      "Epoch 12/20  Iteration 20015/35720 Training loss: 0.8079 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20016/35720 Training loss: 0.8077 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 20017/35720 Training loss: 0.8077 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 20018/35720 Training loss: 0.8076 0.2164 sec/batch\n",
      "Epoch 12/20  Iteration 20019/35720 Training loss: 0.8076 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 20020/35720 Training loss: 0.8076 0.2163 sec/batch\n",
      "Epoch 12/20  Iteration 20021/35720 Training loss: 0.8076 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 20022/35720 Training loss: 0.8076 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 20023/35720 Training loss: 0.8077 0.2274 sec/batch\n",
      "Epoch 12/20  Iteration 20024/35720 Training loss: 0.8077 0.2291 sec/batch\n",
      "Epoch 12/20  Iteration 20025/35720 Training loss: 0.8075 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 20026/35720 Training loss: 0.8074 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 20027/35720 Training loss: 0.8073 0.2104 sec/batch\n",
      "Epoch 12/20  Iteration 20028/35720 Training loss: 0.8072 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20029/35720 Training loss: 0.8072 0.2235 sec/batch\n",
      "Epoch 12/20  Iteration 20030/35720 Training loss: 0.8073 0.2172 sec/batch\n",
      "Epoch 12/20  Iteration 20031/35720 Training loss: 0.8073 0.2169 sec/batch\n",
      "Epoch 12/20  Iteration 20032/35720 Training loss: 0.8072 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20033/35720 Training loss: 0.8072 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 20034/35720 Training loss: 0.8073 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 20035/35720 Training loss: 0.8073 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 20036/35720 Training loss: 0.8071 0.2309 sec/batch\n",
      "Epoch 12/20  Iteration 20037/35720 Training loss: 0.8072 0.2151 sec/batch\n",
      "Epoch 12/20  Iteration 20038/35720 Training loss: 0.8070 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 20039/35720 Training loss: 0.8071 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 20040/35720 Training loss: 0.8068 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 20041/35720 Training loss: 0.8068 0.2202 sec/batch\n",
      "Epoch 12/20  Iteration 20042/35720 Training loss: 0.8067 0.2126 sec/batch\n",
      "Epoch 12/20  Iteration 20043/35720 Training loss: 0.8067 0.2192 sec/batch\n",
      "Epoch 12/20  Iteration 20044/35720 Training loss: 0.8065 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20045/35720 Training loss: 0.8064 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 20046/35720 Training loss: 0.8063 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20047/35720 Training loss: 0.8063 0.2181 sec/batch\n",
      "Epoch 12/20  Iteration 20048/35720 Training loss: 0.8063 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20049/35720 Training loss: 0.8063 0.2117 sec/batch\n",
      "Epoch 12/20  Iteration 20050/35720 Training loss: 0.8062 0.2148 sec/batch\n",
      "Epoch 12/20  Iteration 20051/35720 Training loss: 0.8061 0.2212 sec/batch\n",
      "Epoch 12/20  Iteration 20052/35720 Training loss: 0.8060 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 20053/35720 Training loss: 0.8060 0.2257 sec/batch\n",
      "Epoch 12/20  Iteration 20054/35720 Training loss: 0.8060 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 20055/35720 Training loss: 0.8059 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20056/35720 Training loss: 0.8057 0.2162 sec/batch\n",
      "Epoch 12/20  Iteration 20057/35720 Training loss: 0.8058 0.2190 sec/batch\n",
      "Epoch 12/20  Iteration 20058/35720 Training loss: 0.8056 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 20059/35720 Training loss: 0.8055 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 20060/35720 Training loss: 0.8054 0.2116 sec/batch\n",
      "Epoch 12/20  Iteration 20061/35720 Training loss: 0.8054 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 20062/35720 Training loss: 0.8053 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 20063/35720 Training loss: 0.8052 0.2354 sec/batch\n",
      "Epoch 12/20  Iteration 20064/35720 Training loss: 0.8051 0.2272 sec/batch\n",
      "Epoch 12/20  Iteration 20065/35720 Training loss: 0.8050 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 20066/35720 Training loss: 0.8049 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 20067/35720 Training loss: 0.8049 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 20068/35720 Training loss: 0.8049 0.2330 sec/batch\n",
      "Epoch 12/20  Iteration 20069/35720 Training loss: 0.8049 0.2204 sec/batch\n",
      "Epoch 12/20  Iteration 20070/35720 Training loss: 0.8050 0.2120 sec/batch\n",
      "Epoch 12/20  Iteration 20071/35720 Training loss: 0.8049 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 20072/35720 Training loss: 0.8048 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 20073/35720 Training loss: 0.8047 0.2322 sec/batch\n",
      "Epoch 12/20  Iteration 20074/35720 Training loss: 0.8047 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20075/35720 Training loss: 0.8046 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20076/35720 Training loss: 0.8047 0.2056 sec/batch\n",
      "Epoch 12/20  Iteration 20077/35720 Training loss: 0.8048 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 20078/35720 Training loss: 0.8047 0.2208 sec/batch\n",
      "Epoch 12/20  Iteration 20079/35720 Training loss: 0.8049 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 20080/35720 Training loss: 0.8050 0.2221 sec/batch\n",
      "Epoch 12/20  Iteration 20081/35720 Training loss: 0.8051 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 20082/35720 Training loss: 0.8050 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 20083/35720 Training loss: 0.8051 0.2082 sec/batch\n",
      "Epoch 12/20  Iteration 20084/35720 Training loss: 0.8053 0.2272 sec/batch\n",
      "Epoch 12/20  Iteration 20085/35720 Training loss: 0.8053 0.2223 sec/batch\n",
      "Epoch 12/20  Iteration 20086/35720 Training loss: 0.8053 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 20087/35720 Training loss: 0.8054 0.2240 sec/batch\n",
      "Epoch 12/20  Iteration 20088/35720 Training loss: 0.8055 0.2057 sec/batch\n",
      "Epoch 12/20  Iteration 20089/35720 Training loss: 0.8056 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 20090/35720 Training loss: 0.8056 0.2160 sec/batch\n",
      "Epoch 12/20  Iteration 20091/35720 Training loss: 0.8057 0.2247 sec/batch\n",
      "Epoch 12/20  Iteration 20092/35720 Training loss: 0.8057 0.2197 sec/batch\n",
      "Epoch 12/20  Iteration 20093/35720 Training loss: 0.8060 0.2292 sec/batch\n",
      "Epoch 12/20  Iteration 20094/35720 Training loss: 0.8062 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20095/35720 Training loss: 0.8062 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20096/35720 Training loss: 0.8062 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 20097/35720 Training loss: 0.8060 0.2058 sec/batch\n",
      "Epoch 12/20  Iteration 20098/35720 Training loss: 0.8060 0.2141 sec/batch\n",
      "Epoch 12/20  Iteration 20099/35720 Training loss: 0.8060 0.2299 sec/batch\n",
      "Epoch 12/20  Iteration 20100/35720 Training loss: 0.8061 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 20101/35720 Training loss: 0.8063 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20102/35720 Training loss: 0.8064 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 20103/35720 Training loss: 0.8066 0.2213 sec/batch\n",
      "Epoch 12/20  Iteration 20104/35720 Training loss: 0.8066 0.2231 sec/batch\n",
      "Epoch 12/20  Iteration 20105/35720 Training loss: 0.8066 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20106/35720 Training loss: 0.8065 0.2207 sec/batch\n",
      "Epoch 12/20  Iteration 20107/35720 Training loss: 0.8065 0.2270 sec/batch\n",
      "Epoch 12/20  Iteration 20108/35720 Training loss: 0.8066 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 20109/35720 Training loss: 0.8067 0.2104 sec/batch\n",
      "Epoch 12/20  Iteration 20110/35720 Training loss: 0.8067 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20111/35720 Training loss: 0.8067 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20112/35720 Training loss: 0.8066 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20113/35720 Training loss: 0.8067 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 20114/35720 Training loss: 0.8067 0.2277 sec/batch\n",
      "Epoch 12/20  Iteration 20115/35720 Training loss: 0.8066 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 20116/35720 Training loss: 0.8065 0.2228 sec/batch\n",
      "Epoch 12/20  Iteration 20117/35720 Training loss: 0.8065 0.2199 sec/batch\n",
      "Epoch 12/20  Iteration 20118/35720 Training loss: 0.8065 0.2200 sec/batch\n",
      "Epoch 12/20  Iteration 20119/35720 Training loss: 0.8064 0.2215 sec/batch\n",
      "Epoch 12/20  Iteration 20120/35720 Training loss: 0.8062 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 20121/35720 Training loss: 0.8063 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20122/35720 Training loss: 0.8063 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 20123/35720 Training loss: 0.8063 0.2235 sec/batch\n",
      "Epoch 12/20  Iteration 20124/35720 Training loss: 0.8063 0.2297 sec/batch\n",
      "Epoch 12/20  Iteration 20125/35720 Training loss: 0.8063 0.2271 sec/batch\n",
      "Epoch 12/20  Iteration 20126/35720 Training loss: 0.8062 0.2221 sec/batch\n",
      "Epoch 12/20  Iteration 20127/35720 Training loss: 0.8061 0.2063 sec/batch\n",
      "Epoch 12/20  Iteration 20128/35720 Training loss: 0.8060 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 20129/35720 Training loss: 0.8060 0.2162 sec/batch\n",
      "Epoch 12/20  Iteration 20130/35720 Training loss: 0.8059 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 20131/35720 Training loss: 0.8059 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 20132/35720 Training loss: 0.8058 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 20133/35720 Training loss: 0.8057 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20134/35720 Training loss: 0.8056 0.2329 sec/batch\n",
      "Epoch 12/20  Iteration 20135/35720 Training loss: 0.8056 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20136/35720 Training loss: 0.8055 0.2219 sec/batch\n",
      "Epoch 12/20  Iteration 20137/35720 Training loss: 0.8055 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20138/35720 Training loss: 0.8056 0.2055 sec/batch\n",
      "Epoch 12/20  Iteration 20139/35720 Training loss: 0.8056 0.2129 sec/batch\n",
      "Epoch 12/20  Iteration 20140/35720 Training loss: 0.8055 0.2239 sec/batch\n",
      "Epoch 12/20  Iteration 20141/35720 Training loss: 0.8055 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 20142/35720 Training loss: 0.8053 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 20143/35720 Training loss: 0.8054 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 20144/35720 Training loss: 0.8055 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 20145/35720 Training loss: 0.8054 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 20146/35720 Training loss: 0.8054 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20147/35720 Training loss: 0.8053 0.2138 sec/batch\n",
      "Epoch 12/20  Iteration 20148/35720 Training loss: 0.8052 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 20149/35720 Training loss: 0.8050 0.2226 sec/batch\n",
      "Epoch 12/20  Iteration 20150/35720 Training loss: 0.8049 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20151/35720 Training loss: 0.8049 0.2321 sec/batch\n",
      "Epoch 12/20  Iteration 20152/35720 Training loss: 0.8049 0.2179 sec/batch\n",
      "Epoch 12/20  Iteration 20153/35720 Training loss: 0.8049 0.2290 sec/batch\n",
      "Epoch 12/20  Iteration 20154/35720 Training loss: 0.8049 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20155/35720 Training loss: 0.8050 0.2847 sec/batch\n",
      "Epoch 12/20  Iteration 20156/35720 Training loss: 0.8049 0.2225 sec/batch\n",
      "Epoch 12/20  Iteration 20157/35720 Training loss: 0.8048 0.2273 sec/batch\n",
      "Epoch 12/20  Iteration 20158/35720 Training loss: 0.8049 0.2323 sec/batch\n",
      "Epoch 12/20  Iteration 20159/35720 Training loss: 0.8048 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20160/35720 Training loss: 0.8049 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 20161/35720 Training loss: 0.8048 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20162/35720 Training loss: 0.8048 0.2186 sec/batch\n",
      "Epoch 12/20  Iteration 20163/35720 Training loss: 0.8048 0.2295 sec/batch\n",
      "Epoch 12/20  Iteration 20164/35720 Training loss: 0.8048 0.2260 sec/batch\n",
      "Epoch 12/20  Iteration 20165/35720 Training loss: 0.8048 0.2049 sec/batch\n",
      "Epoch 12/20  Iteration 20166/35720 Training loss: 0.8049 0.2116 sec/batch\n",
      "Epoch 12/20  Iteration 20167/35720 Training loss: 0.8048 0.2201 sec/batch\n",
      "Epoch 12/20  Iteration 20168/35720 Training loss: 0.8048 0.2210 sec/batch\n",
      "Epoch 12/20  Iteration 20169/35720 Training loss: 0.8048 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 20170/35720 Training loss: 0.8046 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 20171/35720 Training loss: 0.8046 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 20172/35720 Training loss: 0.8046 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 20173/35720 Training loss: 0.8046 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 20174/35720 Training loss: 0.8046 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20175/35720 Training loss: 0.8046 0.2295 sec/batch\n",
      "Epoch 12/20  Iteration 20176/35720 Training loss: 0.8045 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20177/35720 Training loss: 0.8045 0.2130 sec/batch\n",
      "Epoch 12/20  Iteration 20178/35720 Training loss: 0.8045 0.2195 sec/batch\n",
      "Epoch 12/20  Iteration 20179/35720 Training loss: 0.8045 0.2198 sec/batch\n",
      "Epoch 12/20  Iteration 20180/35720 Training loss: 0.8045 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20181/35720 Training loss: 0.8043 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 20182/35720 Training loss: 0.8042 0.2230 sec/batch\n",
      "Epoch 12/20  Iteration 20183/35720 Training loss: 0.8042 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20184/35720 Training loss: 0.8041 0.2208 sec/batch\n",
      "Epoch 12/20  Iteration 20185/35720 Training loss: 0.8041 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 20186/35720 Training loss: 0.8040 0.2347 sec/batch\n",
      "Epoch 12/20  Iteration 20187/35720 Training loss: 0.8040 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 20188/35720 Training loss: 0.8039 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 20189/35720 Training loss: 0.8038 0.2232 sec/batch\n",
      "Epoch 12/20  Iteration 20190/35720 Training loss: 0.8037 0.2147 sec/batch\n",
      "Epoch 12/20  Iteration 20191/35720 Training loss: 0.8038 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 20192/35720 Training loss: 0.8039 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 20193/35720 Training loss: 0.8039 0.2295 sec/batch\n",
      "Epoch 12/20  Iteration 20194/35720 Training loss: 0.8039 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 20195/35720 Training loss: 0.8038 0.2230 sec/batch\n",
      "Epoch 12/20  Iteration 20196/35720 Training loss: 0.8039 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 20197/35720 Training loss: 0.8038 0.2216 sec/batch\n",
      "Epoch 12/20  Iteration 20198/35720 Training loss: 0.8037 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 20199/35720 Training loss: 0.8037 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 20200/35720 Training loss: 0.8037 0.2271 sec/batch\n",
      "Validation loss: 1.45298 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 20201/35720 Training loss: 0.8044 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20202/35720 Training loss: 0.8045 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 20203/35720 Training loss: 0.8044 0.2058 sec/batch\n",
      "Epoch 12/20  Iteration 20204/35720 Training loss: 0.8045 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 20205/35720 Training loss: 0.8045 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 20206/35720 Training loss: 0.8045 0.2151 sec/batch\n",
      "Epoch 12/20  Iteration 20207/35720 Training loss: 0.8046 0.2291 sec/batch\n",
      "Epoch 12/20  Iteration 20208/35720 Training loss: 0.8045 0.2131 sec/batch\n",
      "Epoch 12/20  Iteration 20209/35720 Training loss: 0.8044 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20210/35720 Training loss: 0.8044 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20211/35720 Training loss: 0.8043 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 20212/35720 Training loss: 0.8043 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 20213/35720 Training loss: 0.8043 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 20214/35720 Training loss: 0.8042 0.2226 sec/batch\n",
      "Epoch 12/20  Iteration 20215/35720 Training loss: 0.8041 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 20216/35720 Training loss: 0.8041 0.2261 sec/batch\n",
      "Epoch 12/20  Iteration 20217/35720 Training loss: 0.8041 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 20218/35720 Training loss: 0.8041 0.2351 sec/batch\n",
      "Epoch 12/20  Iteration 20219/35720 Training loss: 0.8042 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20220/35720 Training loss: 0.8042 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 20221/35720 Training loss: 0.8043 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20222/35720 Training loss: 0.8043 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20223/35720 Training loss: 0.8043 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20224/35720 Training loss: 0.8044 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 20225/35720 Training loss: 0.8043 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 20226/35720 Training loss: 0.8043 0.2169 sec/batch\n",
      "Epoch 12/20  Iteration 20227/35720 Training loss: 0.8044 0.2099 sec/batch\n",
      "Epoch 12/20  Iteration 20228/35720 Training loss: 0.8044 0.2214 sec/batch\n",
      "Epoch 12/20  Iteration 20229/35720 Training loss: 0.8044 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 20230/35720 Training loss: 0.8043 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20231/35720 Training loss: 0.8042 0.2198 sec/batch\n",
      "Epoch 12/20  Iteration 20232/35720 Training loss: 0.8041 0.2223 sec/batch\n",
      "Epoch 12/20  Iteration 20233/35720 Training loss: 0.8040 0.2239 sec/batch\n",
      "Epoch 12/20  Iteration 20234/35720 Training loss: 0.8039 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 20235/35720 Training loss: 0.8038 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20236/35720 Training loss: 0.8037 0.2106 sec/batch\n",
      "Epoch 12/20  Iteration 20237/35720 Training loss: 0.8038 0.2340 sec/batch\n",
      "Epoch 12/20  Iteration 20238/35720 Training loss: 0.8038 0.2099 sec/batch\n",
      "Epoch 12/20  Iteration 20239/35720 Training loss: 0.8038 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20240/35720 Training loss: 0.8037 0.2231 sec/batch\n",
      "Epoch 12/20  Iteration 20241/35720 Training loss: 0.8036 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 20242/35720 Training loss: 0.8036 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 20243/35720 Training loss: 0.8035 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 20244/35720 Training loss: 0.8036 0.2200 sec/batch\n",
      "Epoch 12/20  Iteration 20245/35720 Training loss: 0.8035 0.2217 sec/batch\n",
      "Epoch 12/20  Iteration 20246/35720 Training loss: 0.8033 0.2180 sec/batch\n",
      "Epoch 12/20  Iteration 20247/35720 Training loss: 0.8032 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 20248/35720 Training loss: 0.8031 0.2075 sec/batch\n",
      "Epoch 12/20  Iteration 20249/35720 Training loss: 0.8030 0.2102 sec/batch\n",
      "Epoch 12/20  Iteration 20250/35720 Training loss: 0.8030 0.2440 sec/batch\n",
      "Epoch 12/20  Iteration 20251/35720 Training loss: 0.8030 0.2147 sec/batch\n",
      "Epoch 12/20  Iteration 20252/35720 Training loss: 0.8031 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 20253/35720 Training loss: 0.8030 0.2071 sec/batch\n",
      "Epoch 12/20  Iteration 20254/35720 Training loss: 0.8030 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20255/35720 Training loss: 0.8030 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 20256/35720 Training loss: 0.8029 0.2286 sec/batch\n",
      "Epoch 12/20  Iteration 20257/35720 Training loss: 0.8028 0.2285 sec/batch\n",
      "Epoch 12/20  Iteration 20258/35720 Training loss: 0.8027 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 20259/35720 Training loss: 0.8026 0.2226 sec/batch\n",
      "Epoch 12/20  Iteration 20260/35720 Training loss: 0.8026 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 20261/35720 Training loss: 0.8026 0.2369 sec/batch\n",
      "Epoch 12/20  Iteration 20262/35720 Training loss: 0.8025 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 20263/35720 Training loss: 0.8024 0.2273 sec/batch\n",
      "Epoch 12/20  Iteration 20264/35720 Training loss: 0.8023 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 20265/35720 Training loss: 0.8023 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 20266/35720 Training loss: 0.8022 0.2154 sec/batch\n",
      "Epoch 12/20  Iteration 20267/35720 Training loss: 0.8022 0.2133 sec/batch\n",
      "Epoch 12/20  Iteration 20268/35720 Training loss: 0.8023 0.2198 sec/batch\n",
      "Epoch 12/20  Iteration 20269/35720 Training loss: 0.8022 0.2234 sec/batch\n",
      "Epoch 12/20  Iteration 20270/35720 Training loss: 0.8022 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 20271/35720 Training loss: 0.8021 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 20272/35720 Training loss: 0.8021 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 20273/35720 Training loss: 0.8021 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 20274/35720 Training loss: 0.8020 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 20275/35720 Training loss: 0.8021 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20276/35720 Training loss: 0.8020 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 20277/35720 Training loss: 0.8021 0.2117 sec/batch\n",
      "Epoch 12/20  Iteration 20278/35720 Training loss: 0.8020 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20279/35720 Training loss: 0.8019 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 20280/35720 Training loss: 0.8019 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 20281/35720 Training loss: 0.8018 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20282/35720 Training loss: 0.8018 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 20283/35720 Training loss: 0.8017 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 20284/35720 Training loss: 0.8017 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20285/35720 Training loss: 0.8017 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20286/35720 Training loss: 0.8017 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 20287/35720 Training loss: 0.8017 0.2151 sec/batch\n",
      "Epoch 12/20  Iteration 20288/35720 Training loss: 0.8018 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20289/35720 Training loss: 0.8018 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 20290/35720 Training loss: 0.8017 0.2189 sec/batch\n",
      "Epoch 12/20  Iteration 20291/35720 Training loss: 0.8017 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 20292/35720 Training loss: 0.8017 0.2071 sec/batch\n",
      "Epoch 12/20  Iteration 20293/35720 Training loss: 0.8016 0.2234 sec/batch\n",
      "Epoch 12/20  Iteration 20294/35720 Training loss: 0.8016 0.2271 sec/batch\n",
      "Epoch 12/20  Iteration 20295/35720 Training loss: 0.8015 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 20296/35720 Training loss: 0.8014 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20297/35720 Training loss: 0.8015 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 20298/35720 Training loss: 0.8014 0.2133 sec/batch\n",
      "Epoch 12/20  Iteration 20299/35720 Training loss: 0.8014 0.2329 sec/batch\n",
      "Epoch 12/20  Iteration 20300/35720 Training loss: 0.8015 0.2230 sec/batch\n",
      "Epoch 12/20  Iteration 20301/35720 Training loss: 0.8015 0.2169 sec/batch\n",
      "Epoch 12/20  Iteration 20302/35720 Training loss: 0.8015 0.2173 sec/batch\n",
      "Epoch 12/20  Iteration 20303/35720 Training loss: 0.8016 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 20304/35720 Training loss: 0.8017 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20305/35720 Training loss: 0.8016 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20306/35720 Training loss: 0.8017 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 20307/35720 Training loss: 0.8017 0.2229 sec/batch\n",
      "Epoch 12/20  Iteration 20308/35720 Training loss: 0.8018 0.2216 sec/batch\n",
      "Epoch 12/20  Iteration 20309/35720 Training loss: 0.8018 0.2059 sec/batch\n",
      "Epoch 12/20  Iteration 20310/35720 Training loss: 0.8018 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 20311/35720 Training loss: 0.8018 0.2308 sec/batch\n",
      "Epoch 12/20  Iteration 20312/35720 Training loss: 0.8018 0.2221 sec/batch\n",
      "Epoch 12/20  Iteration 20313/35720 Training loss: 0.8019 0.2220 sec/batch\n",
      "Epoch 12/20  Iteration 20314/35720 Training loss: 0.8018 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 20315/35720 Training loss: 0.8018 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 20316/35720 Training loss: 0.8018 0.2082 sec/batch\n",
      "Epoch 12/20  Iteration 20317/35720 Training loss: 0.8016 0.2206 sec/batch\n",
      "Epoch 12/20  Iteration 20318/35720 Training loss: 0.8016 0.2143 sec/batch\n",
      "Epoch 12/20  Iteration 20319/35720 Training loss: 0.8017 0.2229 sec/batch\n",
      "Epoch 12/20  Iteration 20320/35720 Training loss: 0.8016 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 20321/35720 Training loss: 0.8016 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20322/35720 Training loss: 0.8015 0.2150 sec/batch\n",
      "Epoch 12/20  Iteration 20323/35720 Training loss: 0.8015 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20324/35720 Training loss: 0.8015 0.2226 sec/batch\n",
      "Epoch 12/20  Iteration 20325/35720 Training loss: 0.8016 0.2188 sec/batch\n",
      "Epoch 12/20  Iteration 20326/35720 Training loss: 0.8015 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 20327/35720 Training loss: 0.8015 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 20328/35720 Training loss: 0.8015 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 20329/35720 Training loss: 0.8015 0.2179 sec/batch\n",
      "Epoch 12/20  Iteration 20330/35720 Training loss: 0.8014 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 20331/35720 Training loss: 0.8014 0.2116 sec/batch\n",
      "Epoch 12/20  Iteration 20332/35720 Training loss: 0.8014 0.2275 sec/batch\n",
      "Epoch 12/20  Iteration 20333/35720 Training loss: 0.8014 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 20334/35720 Training loss: 0.8014 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 20335/35720 Training loss: 0.8014 0.2177 sec/batch\n",
      "Epoch 12/20  Iteration 20336/35720 Training loss: 0.8014 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 20337/35720 Training loss: 0.8014 0.2058 sec/batch\n",
      "Epoch 12/20  Iteration 20338/35720 Training loss: 0.8015 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 20339/35720 Training loss: 0.8016 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 20340/35720 Training loss: 0.8016 0.2172 sec/batch\n",
      "Epoch 12/20  Iteration 20341/35720 Training loss: 0.8015 0.2224 sec/batch\n",
      "Epoch 12/20  Iteration 20342/35720 Training loss: 0.8015 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20343/35720 Training loss: 0.8015 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 20344/35720 Training loss: 0.8015 0.2304 sec/batch\n",
      "Epoch 12/20  Iteration 20345/35720 Training loss: 0.8014 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 20346/35720 Training loss: 0.8014 0.2164 sec/batch\n",
      "Epoch 12/20  Iteration 20347/35720 Training loss: 0.8013 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 20348/35720 Training loss: 0.8014 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 20349/35720 Training loss: 0.8014 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 20350/35720 Training loss: 0.8014 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 20351/35720 Training loss: 0.8014 0.2250 sec/batch\n",
      "Epoch 12/20  Iteration 20352/35720 Training loss: 0.8014 0.2319 sec/batch\n",
      "Epoch 12/20  Iteration 20353/35720 Training loss: 0.8015 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 20354/35720 Training loss: 0.8015 0.2076 sec/batch\n",
      "Epoch 12/20  Iteration 20355/35720 Training loss: 0.8016 0.2416 sec/batch\n",
      "Epoch 12/20  Iteration 20356/35720 Training loss: 0.8017 0.2200 sec/batch\n",
      "Epoch 12/20  Iteration 20357/35720 Training loss: 0.8017 0.2182 sec/batch\n",
      "Epoch 12/20  Iteration 20358/35720 Training loss: 0.8017 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 20359/35720 Training loss: 0.8018 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 20360/35720 Training loss: 0.8018 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 20361/35720 Training loss: 0.8019 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 20362/35720 Training loss: 0.8019 0.2360 sec/batch\n",
      "Epoch 12/20  Iteration 20363/35720 Training loss: 0.8020 0.2275 sec/batch\n",
      "Epoch 12/20  Iteration 20364/35720 Training loss: 0.8021 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20365/35720 Training loss: 0.8020 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 20366/35720 Training loss: 0.8021 0.2380 sec/batch\n",
      "Epoch 12/20  Iteration 20367/35720 Training loss: 0.8021 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 20368/35720 Training loss: 0.8022 0.2179 sec/batch\n",
      "Epoch 12/20  Iteration 20369/35720 Training loss: 0.8023 0.2254 sec/batch\n",
      "Epoch 12/20  Iteration 20370/35720 Training loss: 0.8022 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 20371/35720 Training loss: 0.8022 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 20372/35720 Training loss: 0.8022 0.2271 sec/batch\n",
      "Epoch 12/20  Iteration 20373/35720 Training loss: 0.8024 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 20374/35720 Training loss: 0.8024 0.2203 sec/batch\n",
      "Epoch 12/20  Iteration 20375/35720 Training loss: 0.8025 0.2191 sec/batch\n",
      "Epoch 12/20  Iteration 20376/35720 Training loss: 0.8025 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 20377/35720 Training loss: 0.8025 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20378/35720 Training loss: 0.8025 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 20379/35720 Training loss: 0.8025 0.2154 sec/batch\n",
      "Epoch 12/20  Iteration 20380/35720 Training loss: 0.8024 0.2204 sec/batch\n",
      "Epoch 12/20  Iteration 20381/35720 Training loss: 0.8024 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20382/35720 Training loss: 0.8024 0.2190 sec/batch\n",
      "Epoch 12/20  Iteration 20383/35720 Training loss: 0.8024 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20384/35720 Training loss: 0.8024 0.2116 sec/batch\n",
      "Epoch 12/20  Iteration 20385/35720 Training loss: 0.8024 0.2247 sec/batch\n",
      "Epoch 12/20  Iteration 20386/35720 Training loss: 0.8024 0.2053 sec/batch\n",
      "Epoch 12/20  Iteration 20387/35720 Training loss: 0.8024 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 20388/35720 Training loss: 0.8024 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 20389/35720 Training loss: 0.8024 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 20390/35720 Training loss: 0.8024 0.2168 sec/batch\n",
      "Epoch 12/20  Iteration 20391/35720 Training loss: 0.8024 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 20392/35720 Training loss: 0.8024 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 20393/35720 Training loss: 0.8024 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 20394/35720 Training loss: 0.8023 0.2228 sec/batch\n",
      "Epoch 12/20  Iteration 20395/35720 Training loss: 0.8022 0.2325 sec/batch\n",
      "Epoch 12/20  Iteration 20396/35720 Training loss: 0.8023 0.2160 sec/batch\n",
      "Epoch 12/20  Iteration 20397/35720 Training loss: 0.8023 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 20398/35720 Training loss: 0.8023 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 20399/35720 Training loss: 0.8022 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 20400/35720 Training loss: 0.8022 0.2115 sec/batch\n",
      "Validation loss: 1.45431 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 20401/35720 Training loss: 0.8028 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20402/35720 Training loss: 0.8028 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 20403/35720 Training loss: 0.8028 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 20404/35720 Training loss: 0.8028 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20405/35720 Training loss: 0.8028 0.2339 sec/batch\n",
      "Epoch 12/20  Iteration 20406/35720 Training loss: 0.8028 0.2207 sec/batch\n",
      "Epoch 12/20  Iteration 20407/35720 Training loss: 0.8028 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 20408/35720 Training loss: 0.8029 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 20409/35720 Training loss: 0.8028 0.2181 sec/batch\n",
      "Epoch 12/20  Iteration 20410/35720 Training loss: 0.8028 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 20411/35720 Training loss: 0.8027 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 20412/35720 Training loss: 0.8028 0.2270 sec/batch\n",
      "Epoch 12/20  Iteration 20413/35720 Training loss: 0.8028 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 20414/35720 Training loss: 0.8027 0.2199 sec/batch\n",
      "Epoch 12/20  Iteration 20415/35720 Training loss: 0.8028 0.2099 sec/batch\n",
      "Epoch 12/20  Iteration 20416/35720 Training loss: 0.8029 0.2238 sec/batch\n",
      "Epoch 12/20  Iteration 20417/35720 Training loss: 0.8029 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 20418/35720 Training loss: 0.8030 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20419/35720 Training loss: 0.8030 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20420/35720 Training loss: 0.8030 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 20421/35720 Training loss: 0.8029 0.2299 sec/batch\n",
      "Epoch 12/20  Iteration 20422/35720 Training loss: 0.8028 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 20423/35720 Training loss: 0.8028 0.2078 sec/batch\n",
      "Epoch 12/20  Iteration 20424/35720 Training loss: 0.8028 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 20425/35720 Training loss: 0.8028 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20426/35720 Training loss: 0.8028 0.2228 sec/batch\n",
      "Epoch 12/20  Iteration 20427/35720 Training loss: 0.8027 0.2300 sec/batch\n",
      "Epoch 12/20  Iteration 20428/35720 Training loss: 0.8028 0.2078 sec/batch\n",
      "Epoch 12/20  Iteration 20429/35720 Training loss: 0.8028 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20430/35720 Training loss: 0.8028 0.2063 sec/batch\n",
      "Epoch 12/20  Iteration 20431/35720 Training loss: 0.8027 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20432/35720 Training loss: 0.8027 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 20433/35720 Training loss: 0.8028 0.2226 sec/batch\n",
      "Epoch 12/20  Iteration 20434/35720 Training loss: 0.8027 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 20435/35720 Training loss: 0.8028 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 20436/35720 Training loss: 0.8028 0.2104 sec/batch\n",
      "Epoch 12/20  Iteration 20437/35720 Training loss: 0.8028 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 20438/35720 Training loss: 0.8028 0.2253 sec/batch\n",
      "Epoch 12/20  Iteration 20439/35720 Training loss: 0.8028 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20440/35720 Training loss: 0.8027 0.2179 sec/batch\n",
      "Epoch 12/20  Iteration 20441/35720 Training loss: 0.8028 0.2163 sec/batch\n",
      "Epoch 12/20  Iteration 20442/35720 Training loss: 0.8028 0.2223 sec/batch\n",
      "Epoch 12/20  Iteration 20443/35720 Training loss: 0.8027 0.2277 sec/batch\n",
      "Epoch 12/20  Iteration 20444/35720 Training loss: 0.8027 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 20445/35720 Training loss: 0.8027 0.2367 sec/batch\n",
      "Epoch 12/20  Iteration 20446/35720 Training loss: 0.8027 0.2168 sec/batch\n",
      "Epoch 12/20  Iteration 20447/35720 Training loss: 0.8028 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 20448/35720 Training loss: 0.8028 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20449/35720 Training loss: 0.8028 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20450/35720 Training loss: 0.8029 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20451/35720 Training loss: 0.8030 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 20452/35720 Training loss: 0.8030 0.2263 sec/batch\n",
      "Epoch 12/20  Iteration 20453/35720 Training loss: 0.8030 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20454/35720 Training loss: 0.8030 0.2193 sec/batch\n",
      "Epoch 12/20  Iteration 20455/35720 Training loss: 0.8031 0.2203 sec/batch\n",
      "Epoch 12/20  Iteration 20456/35720 Training loss: 0.8031 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 20457/35720 Training loss: 0.8031 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20458/35720 Training loss: 0.8031 0.2050 sec/batch\n",
      "Epoch 12/20  Iteration 20459/35720 Training loss: 0.8032 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20460/35720 Training loss: 0.8032 0.2141 sec/batch\n",
      "Epoch 12/20  Iteration 20461/35720 Training loss: 0.8032 0.2162 sec/batch\n",
      "Epoch 12/20  Iteration 20462/35720 Training loss: 0.8032 0.2172 sec/batch\n",
      "Epoch 12/20  Iteration 20463/35720 Training loss: 0.8032 0.2080 sec/batch\n",
      "Epoch 12/20  Iteration 20464/35720 Training loss: 0.8032 0.2072 sec/batch\n",
      "Epoch 12/20  Iteration 20465/35720 Training loss: 0.8032 0.2268 sec/batch\n",
      "Epoch 12/20  Iteration 20466/35720 Training loss: 0.8032 0.2210 sec/batch\n",
      "Epoch 12/20  Iteration 20467/35720 Training loss: 0.8032 0.2126 sec/batch\n",
      "Epoch 12/20  Iteration 20468/35720 Training loss: 0.8032 0.2228 sec/batch\n",
      "Epoch 12/20  Iteration 20469/35720 Training loss: 0.8031 0.2058 sec/batch\n",
      "Epoch 12/20  Iteration 20470/35720 Training loss: 0.8030 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 20471/35720 Training loss: 0.8029 0.2213 sec/batch\n",
      "Epoch 12/20  Iteration 20472/35720 Training loss: 0.8029 0.2203 sec/batch\n",
      "Epoch 12/20  Iteration 20473/35720 Training loss: 0.8029 0.2158 sec/batch\n",
      "Epoch 12/20  Iteration 20474/35720 Training loss: 0.8028 0.2059 sec/batch\n",
      "Epoch 12/20  Iteration 20475/35720 Training loss: 0.8028 0.2193 sec/batch\n",
      "Epoch 12/20  Iteration 20476/35720 Training loss: 0.8028 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20477/35720 Training loss: 0.8027 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20478/35720 Training loss: 0.8028 0.2274 sec/batch\n",
      "Epoch 12/20  Iteration 20479/35720 Training loss: 0.8028 0.2238 sec/batch\n",
      "Epoch 12/20  Iteration 20480/35720 Training loss: 0.8028 0.2137 sec/batch\n",
      "Epoch 12/20  Iteration 20481/35720 Training loss: 0.8028 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 20482/35720 Training loss: 0.8028 0.2241 sec/batch\n",
      "Epoch 12/20  Iteration 20483/35720 Training loss: 0.8028 0.2232 sec/batch\n",
      "Epoch 12/20  Iteration 20484/35720 Training loss: 0.8028 0.2276 sec/batch\n",
      "Epoch 12/20  Iteration 20485/35720 Training loss: 0.8028 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 20486/35720 Training loss: 0.8028 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 20487/35720 Training loss: 0.8028 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20488/35720 Training loss: 0.8028 0.2164 sec/batch\n",
      "Epoch 12/20  Iteration 20489/35720 Training loss: 0.8028 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20490/35720 Training loss: 0.8028 0.2117 sec/batch\n",
      "Epoch 12/20  Iteration 20491/35720 Training loss: 0.8029 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20492/35720 Training loss: 0.8028 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 20493/35720 Training loss: 0.8028 0.2134 sec/batch\n",
      "Epoch 12/20  Iteration 20494/35720 Training loss: 0.8028 0.2679 sec/batch\n",
      "Epoch 12/20  Iteration 20495/35720 Training loss: 0.8027 0.2433 sec/batch\n",
      "Epoch 12/20  Iteration 20496/35720 Training loss: 0.8028 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 20497/35720 Training loss: 0.8027 0.2186 sec/batch\n",
      "Epoch 12/20  Iteration 20498/35720 Training loss: 0.8027 0.2078 sec/batch\n",
      "Epoch 12/20  Iteration 20499/35720 Training loss: 0.8026 0.2416 sec/batch\n",
      "Epoch 12/20  Iteration 20500/35720 Training loss: 0.8026 0.2150 sec/batch\n",
      "Epoch 12/20  Iteration 20501/35720 Training loss: 0.8027 0.2269 sec/batch\n",
      "Epoch 12/20  Iteration 20502/35720 Training loss: 0.8026 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 20503/35720 Training loss: 0.8025 0.2284 sec/batch\n",
      "Epoch 12/20  Iteration 20504/35720 Training loss: 0.8026 0.2368 sec/batch\n",
      "Epoch 12/20  Iteration 20505/35720 Training loss: 0.8025 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 20506/35720 Training loss: 0.8025 0.2200 sec/batch\n",
      "Epoch 12/20  Iteration 20507/35720 Training loss: 0.8025 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 20508/35720 Training loss: 0.8024 0.2296 sec/batch\n",
      "Epoch 12/20  Iteration 20509/35720 Training loss: 0.8025 0.2294 sec/batch\n",
      "Epoch 12/20  Iteration 20510/35720 Training loss: 0.8024 0.2117 sec/batch\n",
      "Epoch 12/20  Iteration 20511/35720 Training loss: 0.8024 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20512/35720 Training loss: 0.8024 0.2104 sec/batch\n",
      "Epoch 12/20  Iteration 20513/35720 Training loss: 0.8025 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 20514/35720 Training loss: 0.8024 0.2181 sec/batch\n",
      "Epoch 12/20  Iteration 20515/35720 Training loss: 0.8024 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 20516/35720 Training loss: 0.8024 0.2102 sec/batch\n",
      "Epoch 12/20  Iteration 20517/35720 Training loss: 0.8024 0.2198 sec/batch\n",
      "Epoch 12/20  Iteration 20518/35720 Training loss: 0.8024 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 20519/35720 Training loss: 0.8023 0.2249 sec/batch\n",
      "Epoch 12/20  Iteration 20520/35720 Training loss: 0.8023 0.2197 sec/batch\n",
      "Epoch 12/20  Iteration 20521/35720 Training loss: 0.8023 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 20522/35720 Training loss: 0.8022 0.2245 sec/batch\n",
      "Epoch 12/20  Iteration 20523/35720 Training loss: 0.8022 0.2180 sec/batch\n",
      "Epoch 12/20  Iteration 20524/35720 Training loss: 0.8022 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 20525/35720 Training loss: 0.8022 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 20526/35720 Training loss: 0.8021 0.2278 sec/batch\n",
      "Epoch 12/20  Iteration 20527/35720 Training loss: 0.8021 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 20528/35720 Training loss: 0.8021 0.2322 sec/batch\n",
      "Epoch 12/20  Iteration 20529/35720 Training loss: 0.8021 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 20530/35720 Training loss: 0.8020 0.2080 sec/batch\n",
      "Epoch 12/20  Iteration 20531/35720 Training loss: 0.8021 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 20532/35720 Training loss: 0.8020 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20533/35720 Training loss: 0.8020 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 20534/35720 Training loss: 0.8020 0.2172 sec/batch\n",
      "Epoch 12/20  Iteration 20535/35720 Training loss: 0.8019 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 20536/35720 Training loss: 0.8018 0.2130 sec/batch\n",
      "Epoch 12/20  Iteration 20537/35720 Training loss: 0.8018 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 20538/35720 Training loss: 0.8017 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 20539/35720 Training loss: 0.8016 0.2057 sec/batch\n",
      "Epoch 12/20  Iteration 20540/35720 Training loss: 0.8015 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20541/35720 Training loss: 0.8015 0.2169 sec/batch\n",
      "Epoch 12/20  Iteration 20542/35720 Training loss: 0.8015 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 20543/35720 Training loss: 0.8014 0.2160 sec/batch\n",
      "Epoch 12/20  Iteration 20544/35720 Training loss: 0.8013 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20545/35720 Training loss: 0.8012 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 20546/35720 Training loss: 0.8011 0.2074 sec/batch\n",
      "Epoch 12/20  Iteration 20547/35720 Training loss: 0.8011 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20548/35720 Training loss: 0.8010 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20549/35720 Training loss: 0.8009 0.2241 sec/batch\n",
      "Epoch 12/20  Iteration 20550/35720 Training loss: 0.8009 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 20551/35720 Training loss: 0.8008 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 20552/35720 Training loss: 0.8008 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 20553/35720 Training loss: 0.8008 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20554/35720 Training loss: 0.8008 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20555/35720 Training loss: 0.8008 0.2283 sec/batch\n",
      "Epoch 12/20  Iteration 20556/35720 Training loss: 0.8008 0.2242 sec/batch\n",
      "Epoch 12/20  Iteration 20557/35720 Training loss: 0.8008 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 20558/35720 Training loss: 0.8008 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20559/35720 Training loss: 0.8008 0.2203 sec/batch\n",
      "Epoch 12/20  Iteration 20560/35720 Training loss: 0.8008 0.2262 sec/batch\n",
      "Epoch 12/20  Iteration 20561/35720 Training loss: 0.8009 0.2126 sec/batch\n",
      "Epoch 12/20  Iteration 20562/35720 Training loss: 0.8009 0.2164 sec/batch\n",
      "Epoch 12/20  Iteration 20563/35720 Training loss: 0.8009 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 20564/35720 Training loss: 0.8009 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 20565/35720 Training loss: 0.8008 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 20566/35720 Training loss: 0.8008 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20567/35720 Training loss: 0.8008 0.2158 sec/batch\n",
      "Epoch 12/20  Iteration 20568/35720 Training loss: 0.8008 0.2272 sec/batch\n",
      "Epoch 12/20  Iteration 20569/35720 Training loss: 0.8008 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 20570/35720 Training loss: 0.8008 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 20571/35720 Training loss: 0.8008 0.2221 sec/batch\n",
      "Epoch 12/20  Iteration 20572/35720 Training loss: 0.8008 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 20573/35720 Training loss: 0.8008 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 20574/35720 Training loss: 0.8007 0.2163 sec/batch\n",
      "Epoch 12/20  Iteration 20575/35720 Training loss: 0.8008 0.2063 sec/batch\n",
      "Epoch 12/20  Iteration 20576/35720 Training loss: 0.8008 0.2179 sec/batch\n",
      "Epoch 12/20  Iteration 20577/35720 Training loss: 0.8009 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 20578/35720 Training loss: 0.8009 0.2318 sec/batch\n",
      "Epoch 12/20  Iteration 20579/35720 Training loss: 0.8009 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 20580/35720 Training loss: 0.8009 0.2079 sec/batch\n",
      "Epoch 12/20  Iteration 20581/35720 Training loss: 0.8007 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 20582/35720 Training loss: 0.8007 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 20583/35720 Training loss: 0.8006 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 20584/35720 Training loss: 0.8006 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20585/35720 Training loss: 0.8005 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 20586/35720 Training loss: 0.8004 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 20587/35720 Training loss: 0.8005 0.2213 sec/batch\n",
      "Epoch 12/20  Iteration 20588/35720 Training loss: 0.8005 0.2218 sec/batch\n",
      "Epoch 12/20  Iteration 20589/35720 Training loss: 0.8004 0.2233 sec/batch\n",
      "Epoch 12/20  Iteration 20590/35720 Training loss: 0.8004 0.2152 sec/batch\n",
      "Epoch 12/20  Iteration 20591/35720 Training loss: 0.8004 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 20592/35720 Training loss: 0.8004 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 20593/35720 Training loss: 0.8004 0.2193 sec/batch\n",
      "Epoch 12/20  Iteration 20594/35720 Training loss: 0.8003 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20595/35720 Training loss: 0.8003 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 20596/35720 Training loss: 0.8002 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 20597/35720 Training loss: 0.8002 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 20598/35720 Training loss: 0.8001 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 20599/35720 Training loss: 0.8001 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 20600/35720 Training loss: 0.8000 0.2513 sec/batch\n",
      "Validation loss: 1.45924 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 20601/35720 Training loss: 0.8003 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 20602/35720 Training loss: 0.8004 0.2070 sec/batch\n",
      "Epoch 12/20  Iteration 20603/35720 Training loss: 0.8003 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 20604/35720 Training loss: 0.8003 0.2173 sec/batch\n",
      "Epoch 12/20  Iteration 20605/35720 Training loss: 0.8002 0.2523 sec/batch\n",
      "Epoch 12/20  Iteration 20606/35720 Training loss: 0.8002 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 20607/35720 Training loss: 0.8001 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 20608/35720 Training loss: 0.8001 0.2234 sec/batch\n",
      "Epoch 12/20  Iteration 20609/35720 Training loss: 0.8000 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 20610/35720 Training loss: 0.8000 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 20611/35720 Training loss: 0.8000 0.2261 sec/batch\n",
      "Epoch 12/20  Iteration 20612/35720 Training loss: 0.8000 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20613/35720 Training loss: 0.8000 0.2076 sec/batch\n",
      "Epoch 12/20  Iteration 20614/35720 Training loss: 0.7999 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 20615/35720 Training loss: 0.8000 0.2221 sec/batch\n",
      "Epoch 12/20  Iteration 20616/35720 Training loss: 0.8002 0.2322 sec/batch\n",
      "Epoch 12/20  Iteration 20617/35720 Training loss: 0.8001 0.2074 sec/batch\n",
      "Epoch 12/20  Iteration 20618/35720 Training loss: 0.8000 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 20619/35720 Training loss: 0.8000 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20620/35720 Training loss: 0.7999 0.2120 sec/batch\n",
      "Epoch 12/20  Iteration 20621/35720 Training loss: 0.7999 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 20622/35720 Training loss: 0.7999 0.2253 sec/batch\n",
      "Epoch 12/20  Iteration 20623/35720 Training loss: 0.7998 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 20624/35720 Training loss: 0.7999 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 20625/35720 Training loss: 0.7998 0.2187 sec/batch\n",
      "Epoch 12/20  Iteration 20626/35720 Training loss: 0.7998 0.2251 sec/batch\n",
      "Epoch 12/20  Iteration 20627/35720 Training loss: 0.7998 0.2220 sec/batch\n",
      "Epoch 12/20  Iteration 20628/35720 Training loss: 0.7997 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 20629/35720 Training loss: 0.7996 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 20630/35720 Training loss: 0.7996 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20631/35720 Training loss: 0.7996 0.2209 sec/batch\n",
      "Epoch 12/20  Iteration 20632/35720 Training loss: 0.7995 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 20633/35720 Training loss: 0.7995 0.2214 sec/batch\n",
      "Epoch 12/20  Iteration 20634/35720 Training loss: 0.7995 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20635/35720 Training loss: 0.7994 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20636/35720 Training loss: 0.7994 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20637/35720 Training loss: 0.7993 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20638/35720 Training loss: 0.7993 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 20639/35720 Training loss: 0.7993 0.2348 sec/batch\n",
      "Epoch 12/20  Iteration 20640/35720 Training loss: 0.7993 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 20641/35720 Training loss: 0.7994 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 20642/35720 Training loss: 0.7994 0.2147 sec/batch\n",
      "Epoch 12/20  Iteration 20643/35720 Training loss: 0.7994 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20644/35720 Training loss: 0.7995 0.2126 sec/batch\n",
      "Epoch 12/20  Iteration 20645/35720 Training loss: 0.7995 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 20646/35720 Training loss: 0.7994 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 20647/35720 Training loss: 0.7993 0.2195 sec/batch\n",
      "Epoch 12/20  Iteration 20648/35720 Training loss: 0.7992 0.2189 sec/batch\n",
      "Epoch 12/20  Iteration 20649/35720 Training loss: 0.7991 0.2275 sec/batch\n",
      "Epoch 12/20  Iteration 20650/35720 Training loss: 0.7991 0.2234 sec/batch\n",
      "Epoch 12/20  Iteration 20651/35720 Training loss: 0.7990 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 20652/35720 Training loss: 0.7990 0.2056 sec/batch\n",
      "Epoch 12/20  Iteration 20653/35720 Training loss: 0.7989 0.2230 sec/batch\n",
      "Epoch 12/20  Iteration 20654/35720 Training loss: 0.7989 0.2273 sec/batch\n",
      "Epoch 12/20  Iteration 20655/35720 Training loss: 0.7989 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20656/35720 Training loss: 0.7988 0.2117 sec/batch\n",
      "Epoch 12/20  Iteration 20657/35720 Training loss: 0.7988 0.2072 sec/batch\n",
      "Epoch 12/20  Iteration 20658/35720 Training loss: 0.7988 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20659/35720 Training loss: 0.7988 0.2228 sec/batch\n",
      "Epoch 12/20  Iteration 20660/35720 Training loss: 0.7987 0.2169 sec/batch\n",
      "Epoch 12/20  Iteration 20661/35720 Training loss: 0.7987 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 20662/35720 Training loss: 0.7987 0.2199 sec/batch\n",
      "Epoch 12/20  Iteration 20663/35720 Training loss: 0.7987 0.2082 sec/batch\n",
      "Epoch 12/20  Iteration 20664/35720 Training loss: 0.7987 0.2124 sec/batch\n",
      "Epoch 12/20  Iteration 20665/35720 Training loss: 0.7986 0.2214 sec/batch\n",
      "Epoch 12/20  Iteration 20666/35720 Training loss: 0.7986 0.2160 sec/batch\n",
      "Epoch 12/20  Iteration 20667/35720 Training loss: 0.7985 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 20668/35720 Training loss: 0.7986 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20669/35720 Training loss: 0.7987 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 20670/35720 Training loss: 0.7987 0.2311 sec/batch\n",
      "Epoch 12/20  Iteration 20671/35720 Training loss: 0.7987 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 20672/35720 Training loss: 0.7986 0.2177 sec/batch\n",
      "Epoch 12/20  Iteration 20673/35720 Training loss: 0.7986 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20674/35720 Training loss: 0.7986 0.2191 sec/batch\n",
      "Epoch 12/20  Iteration 20675/35720 Training loss: 0.7985 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20676/35720 Training loss: 0.7985 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20677/35720 Training loss: 0.7985 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20678/35720 Training loss: 0.7985 0.2135 sec/batch\n",
      "Epoch 12/20  Iteration 20679/35720 Training loss: 0.7985 0.2099 sec/batch\n",
      "Epoch 12/20  Iteration 20680/35720 Training loss: 0.7985 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20681/35720 Training loss: 0.7985 0.2295 sec/batch\n",
      "Epoch 12/20  Iteration 20682/35720 Training loss: 0.7985 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 20683/35720 Training loss: 0.7986 0.2120 sec/batch\n",
      "Epoch 12/20  Iteration 20684/35720 Training loss: 0.7986 0.2209 sec/batch\n",
      "Epoch 12/20  Iteration 20685/35720 Training loss: 0.7986 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 20686/35720 Training loss: 0.7986 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 20687/35720 Training loss: 0.7986 0.2289 sec/batch\n",
      "Epoch 12/20  Iteration 20688/35720 Training loss: 0.7986 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 20689/35720 Training loss: 0.7986 0.2258 sec/batch\n",
      "Epoch 12/20  Iteration 20690/35720 Training loss: 0.7986 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 20691/35720 Training loss: 0.7986 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 20692/35720 Training loss: 0.7986 0.2223 sec/batch\n",
      "Epoch 12/20  Iteration 20693/35720 Training loss: 0.7986 0.2302 sec/batch\n",
      "Epoch 12/20  Iteration 20694/35720 Training loss: 0.7985 0.2312 sec/batch\n",
      "Epoch 12/20  Iteration 20695/35720 Training loss: 0.7985 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 20696/35720 Training loss: 0.7986 0.2072 sec/batch\n",
      "Epoch 12/20  Iteration 20697/35720 Training loss: 0.7985 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20698/35720 Training loss: 0.7986 0.2231 sec/batch\n",
      "Epoch 12/20  Iteration 20699/35720 Training loss: 0.7987 0.2305 sec/batch\n",
      "Epoch 12/20  Iteration 20700/35720 Training loss: 0.7987 0.2155 sec/batch\n",
      "Epoch 12/20  Iteration 20701/35720 Training loss: 0.7987 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 20702/35720 Training loss: 0.7987 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 20703/35720 Training loss: 0.7987 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 20704/35720 Training loss: 0.7987 0.2163 sec/batch\n",
      "Epoch 12/20  Iteration 20705/35720 Training loss: 0.7987 0.2276 sec/batch\n",
      "Epoch 12/20  Iteration 20706/35720 Training loss: 0.7986 0.2212 sec/batch\n",
      "Epoch 12/20  Iteration 20707/35720 Training loss: 0.7987 0.2188 sec/batch\n",
      "Epoch 12/20  Iteration 20708/35720 Training loss: 0.7987 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 20709/35720 Training loss: 0.7988 0.2246 sec/batch\n",
      "Epoch 12/20  Iteration 20710/35720 Training loss: 0.7987 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 20711/35720 Training loss: 0.7987 0.2217 sec/batch\n",
      "Epoch 12/20  Iteration 20712/35720 Training loss: 0.7987 0.2265 sec/batch\n",
      "Epoch 12/20  Iteration 20713/35720 Training loss: 0.7987 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 20714/35720 Training loss: 0.7987 0.2261 sec/batch\n",
      "Epoch 12/20  Iteration 20715/35720 Training loss: 0.7987 0.2150 sec/batch\n",
      "Epoch 12/20  Iteration 20716/35720 Training loss: 0.7987 0.2279 sec/batch\n",
      "Epoch 12/20  Iteration 20717/35720 Training loss: 0.7987 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 20718/35720 Training loss: 0.7987 0.2155 sec/batch\n",
      "Epoch 12/20  Iteration 20719/35720 Training loss: 0.7987 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20720/35720 Training loss: 0.7987 0.2259 sec/batch\n",
      "Epoch 12/20  Iteration 20721/35720 Training loss: 0.7987 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 20722/35720 Training loss: 0.7987 0.2213 sec/batch\n",
      "Epoch 12/20  Iteration 20723/35720 Training loss: 0.7987 0.2262 sec/batch\n",
      "Epoch 12/20  Iteration 20724/35720 Training loss: 0.7987 0.2133 sec/batch\n",
      "Epoch 12/20  Iteration 20725/35720 Training loss: 0.7987 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 20726/35720 Training loss: 0.7987 0.2210 sec/batch\n",
      "Epoch 12/20  Iteration 20727/35720 Training loss: 0.7986 0.2305 sec/batch\n",
      "Epoch 12/20  Iteration 20728/35720 Training loss: 0.7986 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 20729/35720 Training loss: 0.7986 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20730/35720 Training loss: 0.7986 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20731/35720 Training loss: 0.7986 0.2257 sec/batch\n",
      "Epoch 12/20  Iteration 20732/35720 Training loss: 0.7986 0.2143 sec/batch\n",
      "Epoch 12/20  Iteration 20733/35720 Training loss: 0.7986 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 20734/35720 Training loss: 0.7986 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 20735/35720 Training loss: 0.7986 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 20736/35720 Training loss: 0.7986 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20737/35720 Training loss: 0.7986 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 20738/35720 Training loss: 0.7986 0.2286 sec/batch\n",
      "Epoch 12/20  Iteration 20739/35720 Training loss: 0.7986 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20740/35720 Training loss: 0.7986 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20741/35720 Training loss: 0.7986 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 20742/35720 Training loss: 0.7986 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20743/35720 Training loss: 0.7985 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 20744/35720 Training loss: 0.7985 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 20745/35720 Training loss: 0.7986 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 20746/35720 Training loss: 0.7986 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 20747/35720 Training loss: 0.7987 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 20748/35720 Training loss: 0.7986 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 20749/35720 Training loss: 0.7987 0.2151 sec/batch\n",
      "Epoch 12/20  Iteration 20750/35720 Training loss: 0.7987 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20751/35720 Training loss: 0.7987 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 20752/35720 Training loss: 0.7986 0.2077 sec/batch\n",
      "Epoch 12/20  Iteration 20753/35720 Training loss: 0.7986 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 20754/35720 Training loss: 0.7986 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20755/35720 Training loss: 0.7986 0.2230 sec/batch\n",
      "Epoch 12/20  Iteration 20756/35720 Training loss: 0.7986 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 20757/35720 Training loss: 0.7986 0.2063 sec/batch\n",
      "Epoch 12/20  Iteration 20758/35720 Training loss: 0.7985 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 20759/35720 Training loss: 0.7985 0.2326 sec/batch\n",
      "Epoch 12/20  Iteration 20760/35720 Training loss: 0.7985 0.2189 sec/batch\n",
      "Epoch 12/20  Iteration 20761/35720 Training loss: 0.7985 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 20762/35720 Training loss: 0.7985 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20763/35720 Training loss: 0.7985 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 20764/35720 Training loss: 0.7985 0.2186 sec/batch\n",
      "Epoch 12/20  Iteration 20765/35720 Training loss: 0.7985 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20766/35720 Training loss: 0.7985 0.2162 sec/batch\n",
      "Epoch 12/20  Iteration 20767/35720 Training loss: 0.7985 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 20768/35720 Training loss: 0.7986 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20769/35720 Training loss: 0.7986 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 20770/35720 Training loss: 0.7986 0.2296 sec/batch\n",
      "Epoch 12/20  Iteration 20771/35720 Training loss: 0.7986 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 20772/35720 Training loss: 0.7985 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 20773/35720 Training loss: 0.7985 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 20774/35720 Training loss: 0.7985 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20775/35720 Training loss: 0.7985 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 20776/35720 Training loss: 0.7984 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 20777/35720 Training loss: 0.7984 0.2198 sec/batch\n",
      "Epoch 12/20  Iteration 20778/35720 Training loss: 0.7983 0.2058 sec/batch\n",
      "Epoch 12/20  Iteration 20779/35720 Training loss: 0.7983 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 20780/35720 Training loss: 0.7983 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20781/35720 Training loss: 0.7982 0.2303 sec/batch\n",
      "Epoch 12/20  Iteration 20782/35720 Training loss: 0.7982 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 20783/35720 Training loss: 0.7982 0.2212 sec/batch\n",
      "Epoch 12/20  Iteration 20784/35720 Training loss: 0.7981 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 20785/35720 Training loss: 0.7981 0.2076 sec/batch\n",
      "Epoch 12/20  Iteration 20786/35720 Training loss: 0.7981 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 20787/35720 Training loss: 0.7981 0.2240 sec/batch\n",
      "Epoch 12/20  Iteration 20788/35720 Training loss: 0.7981 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 20789/35720 Training loss: 0.7981 0.2146 sec/batch\n",
      "Epoch 12/20  Iteration 20790/35720 Training loss: 0.7981 0.2134 sec/batch\n",
      "Epoch 12/20  Iteration 20791/35720 Training loss: 0.7980 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 20792/35720 Training loss: 0.7980 0.2200 sec/batch\n",
      "Epoch 12/20  Iteration 20793/35720 Training loss: 0.7980 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 20794/35720 Training loss: 0.7979 0.2207 sec/batch\n",
      "Epoch 12/20  Iteration 20795/35720 Training loss: 0.7979 0.2131 sec/batch\n",
      "Epoch 12/20  Iteration 20796/35720 Training loss: 0.7979 0.2063 sec/batch\n",
      "Epoch 12/20  Iteration 20797/35720 Training loss: 0.7979 0.2133 sec/batch\n",
      "Epoch 12/20  Iteration 20798/35720 Training loss: 0.7979 0.2271 sec/batch\n",
      "Epoch 12/20  Iteration 20799/35720 Training loss: 0.7979 0.2082 sec/batch\n",
      "Epoch 12/20  Iteration 20800/35720 Training loss: 0.7979 0.2247 sec/batch\n",
      "Validation loss: 1.45635 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 20801/35720 Training loss: 0.7981 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 20802/35720 Training loss: 0.7981 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 20803/35720 Training loss: 0.7981 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20804/35720 Training loss: 0.7980 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 20805/35720 Training loss: 0.7980 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 20806/35720 Training loss: 0.7980 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 20807/35720 Training loss: 0.7979 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 20808/35720 Training loss: 0.7979 0.2160 sec/batch\n",
      "Epoch 12/20  Iteration 20809/35720 Training loss: 0.7980 0.2253 sec/batch\n",
      "Epoch 12/20  Iteration 20810/35720 Training loss: 0.7980 0.2054 sec/batch\n",
      "Epoch 12/20  Iteration 20811/35720 Training loss: 0.7980 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 20812/35720 Training loss: 0.7979 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 20813/35720 Training loss: 0.7979 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 20814/35720 Training loss: 0.7979 0.2347 sec/batch\n",
      "Epoch 12/20  Iteration 20815/35720 Training loss: 0.7980 0.2288 sec/batch\n",
      "Epoch 12/20  Iteration 20816/35720 Training loss: 0.7980 0.2059 sec/batch\n",
      "Epoch 12/20  Iteration 20817/35720 Training loss: 0.7980 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 20818/35720 Training loss: 0.7980 0.2101 sec/batch\n",
      "Epoch 12/20  Iteration 20819/35720 Training loss: 0.7979 0.2214 sec/batch\n",
      "Epoch 12/20  Iteration 20820/35720 Training loss: 0.7979 0.2263 sec/batch\n",
      "Epoch 12/20  Iteration 20821/35720 Training loss: 0.7979 0.2261 sec/batch\n",
      "Epoch 12/20  Iteration 20822/35720 Training loss: 0.7979 0.2055 sec/batch\n",
      "Epoch 12/20  Iteration 20823/35720 Training loss: 0.7980 0.2154 sec/batch\n",
      "Epoch 12/20  Iteration 20824/35720 Training loss: 0.7979 0.2206 sec/batch\n",
      "Epoch 12/20  Iteration 20825/35720 Training loss: 0.7980 0.2219 sec/batch\n",
      "Epoch 12/20  Iteration 20826/35720 Training loss: 0.7979 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 20827/35720 Training loss: 0.7980 0.2074 sec/batch\n",
      "Epoch 12/20  Iteration 20828/35720 Training loss: 0.7980 0.2059 sec/batch\n",
      "Epoch 12/20  Iteration 20829/35720 Training loss: 0.7979 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 20830/35720 Training loss: 0.7978 0.2216 sec/batch\n",
      "Epoch 12/20  Iteration 20831/35720 Training loss: 0.7978 0.2235 sec/batch\n",
      "Epoch 12/20  Iteration 20832/35720 Training loss: 0.7978 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20833/35720 Training loss: 0.7978 0.2202 sec/batch\n",
      "Epoch 12/20  Iteration 20834/35720 Training loss: 0.7979 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 20835/35720 Training loss: 0.7978 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 20836/35720 Training loss: 0.7978 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 20837/35720 Training loss: 0.7978 0.2209 sec/batch\n",
      "Epoch 12/20  Iteration 20838/35720 Training loss: 0.7979 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20839/35720 Training loss: 0.7978 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 20840/35720 Training loss: 0.7978 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20841/35720 Training loss: 0.7978 0.2201 sec/batch\n",
      "Epoch 12/20  Iteration 20842/35720 Training loss: 0.7978 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20843/35720 Training loss: 0.7979 0.2164 sec/batch\n",
      "Epoch 12/20  Iteration 20844/35720 Training loss: 0.7979 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 20845/35720 Training loss: 0.7979 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 20846/35720 Training loss: 0.7978 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 20847/35720 Training loss: 0.7978 0.2152 sec/batch\n",
      "Epoch 12/20  Iteration 20848/35720 Training loss: 0.7977 0.2283 sec/batch\n",
      "Epoch 12/20  Iteration 20849/35720 Training loss: 0.7977 0.2172 sec/batch\n",
      "Epoch 12/20  Iteration 20850/35720 Training loss: 0.7977 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20851/35720 Training loss: 0.7976 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 20852/35720 Training loss: 0.7976 0.2214 sec/batch\n",
      "Epoch 12/20  Iteration 20853/35720 Training loss: 0.7976 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20854/35720 Training loss: 0.7976 0.2220 sec/batch\n",
      "Epoch 12/20  Iteration 20855/35720 Training loss: 0.7976 0.2181 sec/batch\n",
      "Epoch 12/20  Iteration 20856/35720 Training loss: 0.7976 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 20857/35720 Training loss: 0.7975 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 20858/35720 Training loss: 0.7975 0.2147 sec/batch\n",
      "Epoch 12/20  Iteration 20859/35720 Training loss: 0.7975 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 20860/35720 Training loss: 0.7975 0.2263 sec/batch\n",
      "Epoch 12/20  Iteration 20861/35720 Training loss: 0.7975 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 20862/35720 Training loss: 0.7975 0.2260 sec/batch\n",
      "Epoch 12/20  Iteration 20863/35720 Training loss: 0.7975 0.2134 sec/batch\n",
      "Epoch 12/20  Iteration 20864/35720 Training loss: 0.7974 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 20865/35720 Training loss: 0.7974 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 20866/35720 Training loss: 0.7974 0.2079 sec/batch\n",
      "Epoch 12/20  Iteration 20867/35720 Training loss: 0.7974 0.2106 sec/batch\n",
      "Epoch 12/20  Iteration 20868/35720 Training loss: 0.7974 0.2279 sec/batch\n",
      "Epoch 12/20  Iteration 20869/35720 Training loss: 0.7974 0.2277 sec/batch\n",
      "Epoch 12/20  Iteration 20870/35720 Training loss: 0.7973 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 20871/35720 Training loss: 0.7973 0.2265 sec/batch\n",
      "Epoch 12/20  Iteration 20872/35720 Training loss: 0.7973 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 20873/35720 Training loss: 0.7973 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 20874/35720 Training loss: 0.7972 0.2190 sec/batch\n",
      "Epoch 12/20  Iteration 20875/35720 Training loss: 0.7973 0.2188 sec/batch\n",
      "Epoch 12/20  Iteration 20876/35720 Training loss: 0.7973 0.2283 sec/batch\n",
      "Epoch 12/20  Iteration 20877/35720 Training loss: 0.7973 0.2104 sec/batch\n",
      "Epoch 12/20  Iteration 20878/35720 Training loss: 0.7973 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 20879/35720 Training loss: 0.7973 0.2124 sec/batch\n",
      "Epoch 12/20  Iteration 20880/35720 Training loss: 0.7973 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20881/35720 Training loss: 0.7972 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 20882/35720 Training loss: 0.7972 0.2195 sec/batch\n",
      "Epoch 12/20  Iteration 20883/35720 Training loss: 0.7971 0.2070 sec/batch\n",
      "Epoch 12/20  Iteration 20884/35720 Training loss: 0.7971 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20885/35720 Training loss: 0.7970 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 20886/35720 Training loss: 0.7970 0.2180 sec/batch\n",
      "Epoch 12/20  Iteration 20887/35720 Training loss: 0.7969 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 20888/35720 Training loss: 0.7969 0.2210 sec/batch\n",
      "Epoch 12/20  Iteration 20889/35720 Training loss: 0.7969 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 20890/35720 Training loss: 0.7969 0.2181 sec/batch\n",
      "Epoch 12/20  Iteration 20891/35720 Training loss: 0.7968 0.2254 sec/batch\n",
      "Epoch 12/20  Iteration 20892/35720 Training loss: 0.7968 0.2155 sec/batch\n",
      "Epoch 12/20  Iteration 20893/35720 Training loss: 0.7968 0.2238 sec/batch\n",
      "Epoch 12/20  Iteration 20894/35720 Training loss: 0.7968 0.2143 sec/batch\n",
      "Epoch 12/20  Iteration 20895/35720 Training loss: 0.7967 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 20896/35720 Training loss: 0.7967 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 20897/35720 Training loss: 0.7967 0.2241 sec/batch\n",
      "Epoch 12/20  Iteration 20898/35720 Training loss: 0.7966 0.2321 sec/batch\n",
      "Epoch 12/20  Iteration 20899/35720 Training loss: 0.7966 0.2135 sec/batch\n",
      "Epoch 12/20  Iteration 20900/35720 Training loss: 0.7967 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 20901/35720 Training loss: 0.7967 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 20902/35720 Training loss: 0.7966 0.2264 sec/batch\n",
      "Epoch 12/20  Iteration 20903/35720 Training loss: 0.7966 0.2217 sec/batch\n",
      "Epoch 12/20  Iteration 20904/35720 Training loss: 0.7965 0.2266 sec/batch\n",
      "Epoch 12/20  Iteration 20905/35720 Training loss: 0.7965 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 20906/35720 Training loss: 0.7966 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 20907/35720 Training loss: 0.7965 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 20908/35720 Training loss: 0.7965 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 20909/35720 Training loss: 0.7964 0.2238 sec/batch\n",
      "Epoch 12/20  Iteration 20910/35720 Training loss: 0.7964 0.2134 sec/batch\n",
      "Epoch 12/20  Iteration 20911/35720 Training loss: 0.7963 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 20912/35720 Training loss: 0.7963 0.2215 sec/batch\n",
      "Epoch 12/20  Iteration 20913/35720 Training loss: 0.7963 0.2247 sec/batch\n",
      "Epoch 12/20  Iteration 20914/35720 Training loss: 0.7962 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 20915/35720 Training loss: 0.7962 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 20916/35720 Training loss: 0.7962 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 20917/35720 Training loss: 0.7962 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 20918/35720 Training loss: 0.7961 0.2080 sec/batch\n",
      "Epoch 12/20  Iteration 20919/35720 Training loss: 0.7961 0.2186 sec/batch\n",
      "Epoch 12/20  Iteration 20920/35720 Training loss: 0.7961 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 20921/35720 Training loss: 0.7961 0.2249 sec/batch\n",
      "Epoch 12/20  Iteration 20922/35720 Training loss: 0.7961 0.2070 sec/batch\n",
      "Epoch 12/20  Iteration 20923/35720 Training loss: 0.7960 0.2120 sec/batch\n",
      "Epoch 12/20  Iteration 20924/35720 Training loss: 0.7959 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 20925/35720 Training loss: 0.7959 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 20926/35720 Training loss: 0.7958 0.2338 sec/batch\n",
      "Epoch 12/20  Iteration 20927/35720 Training loss: 0.7958 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 20928/35720 Training loss: 0.7958 0.2124 sec/batch\n",
      "Epoch 12/20  Iteration 20929/35720 Training loss: 0.7957 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 20930/35720 Training loss: 0.7957 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20931/35720 Training loss: 0.7957 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 20932/35720 Training loss: 0.7956 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 20933/35720 Training loss: 0.7956 0.2258 sec/batch\n",
      "Epoch 12/20  Iteration 20934/35720 Training loss: 0.7956 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 20935/35720 Training loss: 0.7955 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 20936/35720 Training loss: 0.7956 0.2102 sec/batch\n",
      "Epoch 12/20  Iteration 20937/35720 Training loss: 0.7955 0.2143 sec/batch\n",
      "Epoch 12/20  Iteration 20938/35720 Training loss: 0.7954 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 20939/35720 Training loss: 0.7955 0.2078 sec/batch\n",
      "Epoch 12/20  Iteration 20940/35720 Training loss: 0.7955 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 20941/35720 Training loss: 0.7955 0.2182 sec/batch\n",
      "Epoch 12/20  Iteration 20942/35720 Training loss: 0.7955 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 20943/35720 Training loss: 0.7955 0.2129 sec/batch\n",
      "Epoch 12/20  Iteration 20944/35720 Training loss: 0.7954 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 20945/35720 Training loss: 0.7954 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 20946/35720 Training loss: 0.7954 0.2198 sec/batch\n",
      "Epoch 12/20  Iteration 20947/35720 Training loss: 0.7953 0.2168 sec/batch\n",
      "Epoch 12/20  Iteration 20948/35720 Training loss: 0.7953 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 20949/35720 Training loss: 0.7953 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 20950/35720 Training loss: 0.7952 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 20951/35720 Training loss: 0.7952 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 20952/35720 Training loss: 0.7952 0.2319 sec/batch\n",
      "Epoch 12/20  Iteration 20953/35720 Training loss: 0.7952 0.2241 sec/batch\n",
      "Epoch 12/20  Iteration 20954/35720 Training loss: 0.7951 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 20955/35720 Training loss: 0.7951 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 20956/35720 Training loss: 0.7951 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 20957/35720 Training loss: 0.7951 0.2131 sec/batch\n",
      "Epoch 12/20  Iteration 20958/35720 Training loss: 0.7950 0.2323 sec/batch\n",
      "Epoch 12/20  Iteration 20959/35720 Training loss: 0.7951 0.2239 sec/batch\n",
      "Epoch 12/20  Iteration 20960/35720 Training loss: 0.7950 0.2293 sec/batch\n",
      "Epoch 12/20  Iteration 20961/35720 Training loss: 0.7950 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 20962/35720 Training loss: 0.7950 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 20963/35720 Training loss: 0.7950 0.2285 sec/batch\n",
      "Epoch 12/20  Iteration 20964/35720 Training loss: 0.7949 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 20965/35720 Training loss: 0.7949 0.2294 sec/batch\n",
      "Epoch 12/20  Iteration 20966/35720 Training loss: 0.7949 0.2071 sec/batch\n",
      "Epoch 12/20  Iteration 20967/35720 Training loss: 0.7949 0.2132 sec/batch\n",
      "Epoch 12/20  Iteration 20968/35720 Training loss: 0.7949 0.2188 sec/batch\n",
      "Epoch 12/20  Iteration 20969/35720 Training loss: 0.7949 0.2260 sec/batch\n",
      "Epoch 12/20  Iteration 20970/35720 Training loss: 0.7949 0.2134 sec/batch\n",
      "Epoch 12/20  Iteration 20971/35720 Training loss: 0.7949 0.2181 sec/batch\n",
      "Epoch 12/20  Iteration 20972/35720 Training loss: 0.7949 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 20973/35720 Training loss: 0.7949 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20974/35720 Training loss: 0.7949 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 20975/35720 Training loss: 0.7949 0.2294 sec/batch\n",
      "Epoch 12/20  Iteration 20976/35720 Training loss: 0.7949 0.2357 sec/batch\n",
      "Epoch 12/20  Iteration 20977/35720 Training loss: 0.7949 0.2055 sec/batch\n",
      "Epoch 12/20  Iteration 20978/35720 Training loss: 0.7949 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 20979/35720 Training loss: 0.7949 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 20980/35720 Training loss: 0.7949 0.2296 sec/batch\n",
      "Epoch 12/20  Iteration 20981/35720 Training loss: 0.7949 0.2269 sec/batch\n",
      "Epoch 12/20  Iteration 20982/35720 Training loss: 0.7949 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 20983/35720 Training loss: 0.7950 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 20984/35720 Training loss: 0.7950 0.2193 sec/batch\n",
      "Epoch 12/20  Iteration 20985/35720 Training loss: 0.7949 0.2261 sec/batch\n",
      "Epoch 12/20  Iteration 20986/35720 Training loss: 0.7949 0.2124 sec/batch\n",
      "Epoch 12/20  Iteration 20987/35720 Training loss: 0.7950 0.2181 sec/batch\n",
      "Epoch 12/20  Iteration 20988/35720 Training loss: 0.7949 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 20989/35720 Training loss: 0.7949 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 20990/35720 Training loss: 0.7949 0.2141 sec/batch\n",
      "Epoch 12/20  Iteration 20991/35720 Training loss: 0.7949 0.2278 sec/batch\n",
      "Epoch 12/20  Iteration 20992/35720 Training loss: 0.7949 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 20993/35720 Training loss: 0.7949 0.2244 sec/batch\n",
      "Epoch 12/20  Iteration 20994/35720 Training loss: 0.7949 0.2257 sec/batch\n",
      "Epoch 12/20  Iteration 20995/35720 Training loss: 0.7949 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 20996/35720 Training loss: 0.7949 0.2365 sec/batch\n",
      "Epoch 12/20  Iteration 20997/35720 Training loss: 0.7949 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 20998/35720 Training loss: 0.7949 0.2147 sec/batch\n",
      "Epoch 12/20  Iteration 20999/35720 Training loss: 0.7949 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 21000/35720 Training loss: 0.7949 0.2255 sec/batch\n",
      "Validation loss: 1.45995 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 21001/35720 Training loss: 0.7951 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 21002/35720 Training loss: 0.7951 0.2294 sec/batch\n",
      "Epoch 12/20  Iteration 21003/35720 Training loss: 0.7951 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 21004/35720 Training loss: 0.7951 0.2059 sec/batch\n",
      "Epoch 12/20  Iteration 21005/35720 Training loss: 0.7950 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 21006/35720 Training loss: 0.7950 0.2268 sec/batch\n",
      "Epoch 12/20  Iteration 21007/35720 Training loss: 0.7950 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21008/35720 Training loss: 0.7949 0.2174 sec/batch\n",
      "Epoch 12/20  Iteration 21009/35720 Training loss: 0.7949 0.2276 sec/batch\n",
      "Epoch 12/20  Iteration 21010/35720 Training loss: 0.7949 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 21011/35720 Training loss: 0.7949 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 21012/35720 Training loss: 0.7949 0.2351 sec/batch\n",
      "Epoch 12/20  Iteration 21013/35720 Training loss: 0.7949 0.2126 sec/batch\n",
      "Epoch 12/20  Iteration 21014/35720 Training loss: 0.7949 0.2245 sec/batch\n",
      "Epoch 12/20  Iteration 21015/35720 Training loss: 0.7949 0.2099 sec/batch\n",
      "Epoch 12/20  Iteration 21016/35720 Training loss: 0.7949 0.2173 sec/batch\n",
      "Epoch 12/20  Iteration 21017/35720 Training loss: 0.7949 0.2248 sec/batch\n",
      "Epoch 12/20  Iteration 21018/35720 Training loss: 0.7949 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 21019/35720 Training loss: 0.7949 0.2206 sec/batch\n",
      "Epoch 12/20  Iteration 21020/35720 Training loss: 0.7949 0.2132 sec/batch\n",
      "Epoch 12/20  Iteration 21021/35720 Training loss: 0.7949 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 21022/35720 Training loss: 0.7949 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 21023/35720 Training loss: 0.7949 0.2228 sec/batch\n",
      "Epoch 12/20  Iteration 21024/35720 Training loss: 0.7949 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 21025/35720 Training loss: 0.7949 0.2164 sec/batch\n",
      "Epoch 12/20  Iteration 21026/35720 Training loss: 0.7949 0.2074 sec/batch\n",
      "Epoch 12/20  Iteration 21027/35720 Training loss: 0.7949 0.2099 sec/batch\n",
      "Epoch 12/20  Iteration 21028/35720 Training loss: 0.7949 0.2151 sec/batch\n",
      "Epoch 12/20  Iteration 21029/35720 Training loss: 0.7949 0.2163 sec/batch\n",
      "Epoch 12/20  Iteration 21030/35720 Training loss: 0.7948 0.2102 sec/batch\n",
      "Epoch 12/20  Iteration 21031/35720 Training loss: 0.7949 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 21032/35720 Training loss: 0.7949 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 21033/35720 Training loss: 0.7949 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 21034/35720 Training loss: 0.7949 0.2275 sec/batch\n",
      "Epoch 12/20  Iteration 21035/35720 Training loss: 0.7949 0.2182 sec/batch\n",
      "Epoch 12/20  Iteration 21036/35720 Training loss: 0.7948 0.2173 sec/batch\n",
      "Epoch 12/20  Iteration 21037/35720 Training loss: 0.7948 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 21038/35720 Training loss: 0.7947 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 21039/35720 Training loss: 0.7947 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 21040/35720 Training loss: 0.7946 0.2224 sec/batch\n",
      "Epoch 12/20  Iteration 21041/35720 Training loss: 0.7946 0.2155 sec/batch\n",
      "Epoch 12/20  Iteration 21042/35720 Training loss: 0.7946 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 21043/35720 Training loss: 0.7946 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 21044/35720 Training loss: 0.7945 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 21045/35720 Training loss: 0.7945 0.2382 sec/batch\n",
      "Epoch 12/20  Iteration 21046/35720 Training loss: 0.7944 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 21047/35720 Training loss: 0.7944 0.2201 sec/batch\n",
      "Epoch 12/20  Iteration 21048/35720 Training loss: 0.7944 0.2203 sec/batch\n",
      "Epoch 12/20  Iteration 21049/35720 Training loss: 0.7944 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21050/35720 Training loss: 0.7944 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 21051/35720 Training loss: 0.7943 0.2268 sec/batch\n",
      "Epoch 12/20  Iteration 21052/35720 Training loss: 0.7943 0.2121 sec/batch\n",
      "Epoch 12/20  Iteration 21053/35720 Training loss: 0.7943 0.2245 sec/batch\n",
      "Epoch 12/20  Iteration 21054/35720 Training loss: 0.7943 0.2077 sec/batch\n",
      "Epoch 12/20  Iteration 21055/35720 Training loss: 0.7943 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 21056/35720 Training loss: 0.7943 0.2349 sec/batch\n",
      "Epoch 12/20  Iteration 21057/35720 Training loss: 0.7943 0.2280 sec/batch\n",
      "Epoch 12/20  Iteration 21058/35720 Training loss: 0.7943 0.2268 sec/batch\n",
      "Epoch 12/20  Iteration 21059/35720 Training loss: 0.7943 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 21060/35720 Training loss: 0.7943 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 21061/35720 Training loss: 0.7943 0.2245 sec/batch\n",
      "Epoch 12/20  Iteration 21062/35720 Training loss: 0.7943 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 21063/35720 Training loss: 0.7942 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 21064/35720 Training loss: 0.7942 0.2225 sec/batch\n",
      "Epoch 12/20  Iteration 21065/35720 Training loss: 0.7943 0.2129 sec/batch\n",
      "Epoch 12/20  Iteration 21066/35720 Training loss: 0.7943 0.2106 sec/batch\n",
      "Epoch 12/20  Iteration 21067/35720 Training loss: 0.7943 0.2141 sec/batch\n",
      "Epoch 12/20  Iteration 21068/35720 Training loss: 0.7943 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 21069/35720 Training loss: 0.7943 0.2102 sec/batch\n",
      "Epoch 12/20  Iteration 21070/35720 Training loss: 0.7942 0.2254 sec/batch\n",
      "Epoch 12/20  Iteration 21071/35720 Training loss: 0.7943 0.2070 sec/batch\n",
      "Epoch 12/20  Iteration 21072/35720 Training loss: 0.7943 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 21073/35720 Training loss: 0.7943 0.2158 sec/batch\n",
      "Epoch 12/20  Iteration 21074/35720 Training loss: 0.7943 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21075/35720 Training loss: 0.7944 0.2135 sec/batch\n",
      "Epoch 12/20  Iteration 21076/35720 Training loss: 0.7943 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 21077/35720 Training loss: 0.7943 0.2182 sec/batch\n",
      "Epoch 12/20  Iteration 21078/35720 Training loss: 0.7943 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21079/35720 Training loss: 0.7943 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 21080/35720 Training loss: 0.7943 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 21081/35720 Training loss: 0.7943 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 21082/35720 Training loss: 0.7942 0.2134 sec/batch\n",
      "Epoch 12/20  Iteration 21083/35720 Training loss: 0.7942 0.2231 sec/batch\n",
      "Epoch 12/20  Iteration 21084/35720 Training loss: 0.7942 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 21085/35720 Training loss: 0.7942 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 21086/35720 Training loss: 0.7942 0.2274 sec/batch\n",
      "Epoch 12/20  Iteration 21087/35720 Training loss: 0.7942 0.2076 sec/batch\n",
      "Epoch 12/20  Iteration 21088/35720 Training loss: 0.7942 0.2259 sec/batch\n",
      "Epoch 12/20  Iteration 21089/35720 Training loss: 0.7942 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 21090/35720 Training loss: 0.7942 0.2203 sec/batch\n",
      "Epoch 12/20  Iteration 21091/35720 Training loss: 0.7942 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21092/35720 Training loss: 0.7942 0.2125 sec/batch\n",
      "Epoch 12/20  Iteration 21093/35720 Training loss: 0.7942 0.2054 sec/batch\n",
      "Epoch 12/20  Iteration 21094/35720 Training loss: 0.7941 0.2083 sec/batch\n",
      "Epoch 12/20  Iteration 21095/35720 Training loss: 0.7941 0.2307 sec/batch\n",
      "Epoch 12/20  Iteration 21096/35720 Training loss: 0.7941 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 21097/35720 Training loss: 0.7941 0.2253 sec/batch\n",
      "Epoch 12/20  Iteration 21098/35720 Training loss: 0.7941 0.2361 sec/batch\n",
      "Epoch 12/20  Iteration 21099/35720 Training loss: 0.7941 0.2137 sec/batch\n",
      "Epoch 12/20  Iteration 21100/35720 Training loss: 0.7941 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 21101/35720 Training loss: 0.7940 0.2195 sec/batch\n",
      "Epoch 12/20  Iteration 21102/35720 Training loss: 0.7941 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 21103/35720 Training loss: 0.7941 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 21104/35720 Training loss: 0.7941 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 21105/35720 Training loss: 0.7941 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 21106/35720 Training loss: 0.7942 0.2223 sec/batch\n",
      "Epoch 12/20  Iteration 21107/35720 Training loss: 0.7942 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 21108/35720 Training loss: 0.7942 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 21109/35720 Training loss: 0.7942 0.2212 sec/batch\n",
      "Epoch 12/20  Iteration 21110/35720 Training loss: 0.7942 0.2188 sec/batch\n",
      "Epoch 12/20  Iteration 21111/35720 Training loss: 0.7942 0.2232 sec/batch\n",
      "Epoch 12/20  Iteration 21112/35720 Training loss: 0.7942 0.2309 sec/batch\n",
      "Epoch 12/20  Iteration 21113/35720 Training loss: 0.7942 0.2116 sec/batch\n",
      "Epoch 12/20  Iteration 21114/35720 Training loss: 0.7942 0.2200 sec/batch\n",
      "Epoch 12/20  Iteration 21115/35720 Training loss: 0.7941 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 21116/35720 Training loss: 0.7942 0.2173 sec/batch\n",
      "Epoch 12/20  Iteration 21117/35720 Training loss: 0.7941 0.2372 sec/batch\n",
      "Epoch 12/20  Iteration 21118/35720 Training loss: 0.7941 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 21119/35720 Training loss: 0.7941 0.2297 sec/batch\n",
      "Epoch 12/20  Iteration 21120/35720 Training loss: 0.7941 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 21121/35720 Training loss: 0.7940 0.2071 sec/batch\n",
      "Epoch 12/20  Iteration 21122/35720 Training loss: 0.7939 0.2085 sec/batch\n",
      "Epoch 12/20  Iteration 21123/35720 Training loss: 0.7939 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 21124/35720 Training loss: 0.7938 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 21125/35720 Training loss: 0.7938 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 21126/35720 Training loss: 0.7938 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 21127/35720 Training loss: 0.7938 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 21128/35720 Training loss: 0.7938 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21129/35720 Training loss: 0.7937 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 21130/35720 Training loss: 0.7937 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 21131/35720 Training loss: 0.7937 0.2071 sec/batch\n",
      "Epoch 12/20  Iteration 21132/35720 Training loss: 0.7937 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21133/35720 Training loss: 0.7936 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 21134/35720 Training loss: 0.7936 0.2192 sec/batch\n",
      "Epoch 12/20  Iteration 21135/35720 Training loss: 0.7936 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 21136/35720 Training loss: 0.7936 0.2225 sec/batch\n",
      "Epoch 12/20  Iteration 21137/35720 Training loss: 0.7936 0.2150 sec/batch\n",
      "Epoch 12/20  Iteration 21138/35720 Training loss: 0.7935 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 21139/35720 Training loss: 0.7935 0.2104 sec/batch\n",
      "Epoch 12/20  Iteration 21140/35720 Training loss: 0.7935 0.2231 sec/batch\n",
      "Epoch 12/20  Iteration 21141/35720 Training loss: 0.7935 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 21142/35720 Training loss: 0.7935 0.2202 sec/batch\n",
      "Epoch 12/20  Iteration 21143/35720 Training loss: 0.7934 0.2044 sec/batch\n",
      "Epoch 12/20  Iteration 21144/35720 Training loss: 0.7934 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 21145/35720 Training loss: 0.7934 0.2243 sec/batch\n",
      "Epoch 12/20  Iteration 21146/35720 Training loss: 0.7934 0.2298 sec/batch\n",
      "Epoch 12/20  Iteration 21147/35720 Training loss: 0.7934 0.2072 sec/batch\n",
      "Epoch 12/20  Iteration 21148/35720 Training loss: 0.7933 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 21149/35720 Training loss: 0.7933 0.2094 sec/batch\n",
      "Epoch 12/20  Iteration 21150/35720 Training loss: 0.7933 0.2134 sec/batch\n",
      "Epoch 12/20  Iteration 21151/35720 Training loss: 0.7933 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 21152/35720 Training loss: 0.7934 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 21153/35720 Training loss: 0.7934 0.2162 sec/batch\n",
      "Epoch 12/20  Iteration 21154/35720 Training loss: 0.7933 0.2171 sec/batch\n",
      "Epoch 12/20  Iteration 21155/35720 Training loss: 0.7933 0.2077 sec/batch\n",
      "Epoch 12/20  Iteration 21156/35720 Training loss: 0.7933 0.2078 sec/batch\n",
      "Epoch 12/20  Iteration 21157/35720 Training loss: 0.7933 0.2082 sec/batch\n",
      "Epoch 12/20  Iteration 21158/35720 Training loss: 0.7933 0.2241 sec/batch\n",
      "Epoch 12/20  Iteration 21159/35720 Training loss: 0.7933 0.2153 sec/batch\n",
      "Epoch 12/20  Iteration 21160/35720 Training loss: 0.7933 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 21161/35720 Training loss: 0.7933 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 21162/35720 Training loss: 0.7933 0.2176 sec/batch\n",
      "Epoch 12/20  Iteration 21163/35720 Training loss: 0.7933 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 21164/35720 Training loss: 0.7933 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 21165/35720 Training loss: 0.7933 0.2200 sec/batch\n",
      "Epoch 12/20  Iteration 21166/35720 Training loss: 0.7933 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 21167/35720 Training loss: 0.7933 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 21168/35720 Training loss: 0.7933 0.2282 sec/batch\n",
      "Epoch 12/20  Iteration 21169/35720 Training loss: 0.7934 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 21170/35720 Training loss: 0.7934 0.2188 sec/batch\n",
      "Epoch 12/20  Iteration 21171/35720 Training loss: 0.7934 0.2273 sec/batch\n",
      "Epoch 12/20  Iteration 21172/35720 Training loss: 0.7934 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 21173/35720 Training loss: 0.7934 0.2269 sec/batch\n",
      "Epoch 12/20  Iteration 21174/35720 Training loss: 0.7934 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 21175/35720 Training loss: 0.7934 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 21176/35720 Training loss: 0.7935 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 21177/35720 Training loss: 0.7935 0.2147 sec/batch\n",
      "Epoch 12/20  Iteration 21178/35720 Training loss: 0.7935 0.2194 sec/batch\n",
      "Epoch 12/20  Iteration 21179/35720 Training loss: 0.7935 0.2225 sec/batch\n",
      "Epoch 12/20  Iteration 21180/35720 Training loss: 0.7935 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 21181/35720 Training loss: 0.7935 0.2334 sec/batch\n",
      "Epoch 12/20  Iteration 21182/35720 Training loss: 0.7935 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 21183/35720 Training loss: 0.7934 0.2148 sec/batch\n",
      "Epoch 12/20  Iteration 21184/35720 Training loss: 0.7934 0.2211 sec/batch\n",
      "Epoch 12/20  Iteration 21185/35720 Training loss: 0.7934 0.2149 sec/batch\n",
      "Epoch 12/20  Iteration 21186/35720 Training loss: 0.7934 0.2144 sec/batch\n",
      "Epoch 12/20  Iteration 21187/35720 Training loss: 0.7934 0.2057 sec/batch\n",
      "Epoch 12/20  Iteration 21188/35720 Training loss: 0.7934 0.2187 sec/batch\n",
      "Epoch 12/20  Iteration 21189/35720 Training loss: 0.7933 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21190/35720 Training loss: 0.7933 0.2303 sec/batch\n",
      "Epoch 12/20  Iteration 21191/35720 Training loss: 0.7933 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21192/35720 Training loss: 0.7932 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 21193/35720 Training loss: 0.7933 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 21194/35720 Training loss: 0.7933 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 21195/35720 Training loss: 0.7933 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 21196/35720 Training loss: 0.7933 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 21197/35720 Training loss: 0.7932 0.2103 sec/batch\n",
      "Epoch 12/20  Iteration 21198/35720 Training loss: 0.7932 0.2129 sec/batch\n",
      "Epoch 12/20  Iteration 21199/35720 Training loss: 0.7932 0.2156 sec/batch\n",
      "Epoch 12/20  Iteration 21200/35720 Training loss: 0.7932 0.2092 sec/batch\n",
      "Validation loss: 1.46766 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 21201/35720 Training loss: 0.7934 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 21202/35720 Training loss: 0.7934 0.2187 sec/batch\n",
      "Epoch 12/20  Iteration 21203/35720 Training loss: 0.7934 0.2064 sec/batch\n",
      "Epoch 12/20  Iteration 21204/35720 Training loss: 0.7933 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 21205/35720 Training loss: 0.7933 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 21206/35720 Training loss: 0.7932 0.2127 sec/batch\n",
      "Epoch 12/20  Iteration 21207/35720 Training loss: 0.7932 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 21208/35720 Training loss: 0.7931 0.2184 sec/batch\n",
      "Epoch 12/20  Iteration 21209/35720 Training loss: 0.7931 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 21210/35720 Training loss: 0.7931 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 21211/35720 Training loss: 0.7931 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 21212/35720 Training loss: 0.7930 0.2119 sec/batch\n",
      "Epoch 12/20  Iteration 21213/35720 Training loss: 0.7930 0.2137 sec/batch\n",
      "Epoch 12/20  Iteration 21214/35720 Training loss: 0.7930 0.2757 sec/batch\n",
      "Epoch 12/20  Iteration 21215/35720 Training loss: 0.7930 0.2155 sec/batch\n",
      "Epoch 12/20  Iteration 21216/35720 Training loss: 0.7930 0.2739 sec/batch\n",
      "Epoch 12/20  Iteration 21217/35720 Training loss: 0.7929 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 21218/35720 Training loss: 0.7929 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 21219/35720 Training loss: 0.7929 0.2152 sec/batch\n",
      "Epoch 12/20  Iteration 21220/35720 Training loss: 0.7929 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 21221/35720 Training loss: 0.7929 0.2076 sec/batch\n",
      "Epoch 12/20  Iteration 21222/35720 Training loss: 0.7928 0.2084 sec/batch\n",
      "Epoch 12/20  Iteration 21223/35720 Training loss: 0.7928 0.2170 sec/batch\n",
      "Epoch 12/20  Iteration 21224/35720 Training loss: 0.7928 0.2187 sec/batch\n",
      "Epoch 12/20  Iteration 21225/35720 Training loss: 0.7928 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 21226/35720 Training loss: 0.7928 0.2073 sec/batch\n",
      "Epoch 12/20  Iteration 21227/35720 Training loss: 0.7928 0.2128 sec/batch\n",
      "Epoch 12/20  Iteration 21228/35720 Training loss: 0.7928 0.2252 sec/batch\n",
      "Epoch 12/20  Iteration 21229/35720 Training loss: 0.7927 0.2236 sec/batch\n",
      "Epoch 12/20  Iteration 21230/35720 Training loss: 0.7927 0.2252 sec/batch\n",
      "Epoch 12/20  Iteration 21231/35720 Training loss: 0.7927 0.2061 sec/batch\n",
      "Epoch 12/20  Iteration 21232/35720 Training loss: 0.7927 0.2072 sec/batch\n",
      "Epoch 12/20  Iteration 21233/35720 Training loss: 0.7927 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 21234/35720 Training loss: 0.7926 0.2231 sec/batch\n",
      "Epoch 12/20  Iteration 21235/35720 Training loss: 0.7926 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 21236/35720 Training loss: 0.7926 0.2137 sec/batch\n",
      "Epoch 12/20  Iteration 21237/35720 Training loss: 0.7925 0.2155 sec/batch\n",
      "Epoch 12/20  Iteration 21238/35720 Training loss: 0.7925 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21239/35720 Training loss: 0.7926 0.2344 sec/batch\n",
      "Epoch 12/20  Iteration 21240/35720 Training loss: 0.7926 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 21241/35720 Training loss: 0.7925 0.2339 sec/batch\n",
      "Epoch 12/20  Iteration 21242/35720 Training loss: 0.7925 0.2234 sec/batch\n",
      "Epoch 12/20  Iteration 21243/35720 Training loss: 0.7925 0.2068 sec/batch\n",
      "Epoch 12/20  Iteration 21244/35720 Training loss: 0.7924 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21245/35720 Training loss: 0.7924 0.2225 sec/batch\n",
      "Epoch 12/20  Iteration 21246/35720 Training loss: 0.7924 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 21247/35720 Training loss: 0.7923 0.2217 sec/batch\n",
      "Epoch 12/20  Iteration 21248/35720 Training loss: 0.7923 0.2208 sec/batch\n",
      "Epoch 12/20  Iteration 21249/35720 Training loss: 0.7923 0.2239 sec/batch\n",
      "Epoch 12/20  Iteration 21250/35720 Training loss: 0.7923 0.2163 sec/batch\n",
      "Epoch 12/20  Iteration 21251/35720 Training loss: 0.7923 0.2154 sec/batch\n",
      "Epoch 12/20  Iteration 21252/35720 Training loss: 0.7922 0.2299 sec/batch\n",
      "Epoch 12/20  Iteration 21253/35720 Training loss: 0.7922 0.2060 sec/batch\n",
      "Epoch 12/20  Iteration 21254/35720 Training loss: 0.7922 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 21255/35720 Training loss: 0.7921 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 21256/35720 Training loss: 0.7921 0.2220 sec/batch\n",
      "Epoch 12/20  Iteration 21257/35720 Training loss: 0.7921 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 21258/35720 Training loss: 0.7920 0.2246 sec/batch\n",
      "Epoch 12/20  Iteration 21259/35720 Training loss: 0.7920 0.2140 sec/batch\n",
      "Epoch 12/20  Iteration 21260/35720 Training loss: 0.7920 0.2251 sec/batch\n",
      "Epoch 12/20  Iteration 21261/35720 Training loss: 0.7920 0.2182 sec/batch\n",
      "Epoch 12/20  Iteration 21262/35720 Training loss: 0.7920 0.2102 sec/batch\n",
      "Epoch 12/20  Iteration 21263/35720 Training loss: 0.7920 0.2260 sec/batch\n",
      "Epoch 12/20  Iteration 21264/35720 Training loss: 0.7920 0.2082 sec/batch\n",
      "Epoch 12/20  Iteration 21265/35720 Training loss: 0.7920 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 21266/35720 Training loss: 0.7920 0.2074 sec/batch\n",
      "Epoch 12/20  Iteration 21267/35720 Training loss: 0.7919 0.2246 sec/batch\n",
      "Epoch 12/20  Iteration 21268/35720 Training loss: 0.7919 0.2281 sec/batch\n",
      "Epoch 12/20  Iteration 21269/35720 Training loss: 0.7919 0.2240 sec/batch\n",
      "Epoch 12/20  Iteration 21270/35720 Training loss: 0.7919 0.2077 sec/batch\n",
      "Epoch 12/20  Iteration 21271/35720 Training loss: 0.7919 0.2082 sec/batch\n",
      "Epoch 12/20  Iteration 21272/35720 Training loss: 0.7919 0.2211 sec/batch\n",
      "Epoch 12/20  Iteration 21273/35720 Training loss: 0.7919 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 21274/35720 Training loss: 0.7919 0.2246 sec/batch\n",
      "Epoch 12/20  Iteration 21275/35720 Training loss: 0.7918 0.2138 sec/batch\n",
      "Epoch 12/20  Iteration 21276/35720 Training loss: 0.7918 0.2189 sec/batch\n",
      "Epoch 12/20  Iteration 21277/35720 Training loss: 0.7918 0.2082 sec/batch\n",
      "Epoch 12/20  Iteration 21278/35720 Training loss: 0.7918 0.2252 sec/batch\n",
      "Epoch 12/20  Iteration 21279/35720 Training loss: 0.7918 0.2209 sec/batch\n",
      "Epoch 12/20  Iteration 21280/35720 Training loss: 0.7918 0.2057 sec/batch\n",
      "Epoch 12/20  Iteration 21281/35720 Training loss: 0.7918 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 21282/35720 Training loss: 0.7918 0.2107 sec/batch\n",
      "Epoch 12/20  Iteration 21283/35720 Training loss: 0.7918 0.2261 sec/batch\n",
      "Epoch 12/20  Iteration 21284/35720 Training loss: 0.7917 0.2145 sec/batch\n",
      "Epoch 12/20  Iteration 21285/35720 Training loss: 0.7917 0.2225 sec/batch\n",
      "Epoch 12/20  Iteration 21286/35720 Training loss: 0.7917 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 21287/35720 Training loss: 0.7917 0.2123 sec/batch\n",
      "Epoch 12/20  Iteration 21288/35720 Training loss: 0.7917 0.2216 sec/batch\n",
      "Epoch 12/20  Iteration 21289/35720 Training loss: 0.7917 0.2263 sec/batch\n",
      "Epoch 12/20  Iteration 21290/35720 Training loss: 0.7917 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21291/35720 Training loss: 0.7917 0.2201 sec/batch\n",
      "Epoch 12/20  Iteration 21292/35720 Training loss: 0.7917 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 21293/35720 Training loss: 0.7917 0.2110 sec/batch\n",
      "Epoch 12/20  Iteration 21294/35720 Training loss: 0.7917 0.2270 sec/batch\n",
      "Epoch 12/20  Iteration 21295/35720 Training loss: 0.7917 0.2220 sec/batch\n",
      "Epoch 12/20  Iteration 21296/35720 Training loss: 0.7917 0.2227 sec/batch\n",
      "Epoch 12/20  Iteration 21297/35720 Training loss: 0.7917 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 21298/35720 Training loss: 0.7917 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 21299/35720 Training loss: 0.7918 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 21300/35720 Training loss: 0.7917 0.2202 sec/batch\n",
      "Epoch 12/20  Iteration 21301/35720 Training loss: 0.7918 0.2192 sec/batch\n",
      "Epoch 12/20  Iteration 21302/35720 Training loss: 0.7918 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 21303/35720 Training loss: 0.7918 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 21304/35720 Training loss: 0.7918 0.2100 sec/batch\n",
      "Epoch 12/20  Iteration 21305/35720 Training loss: 0.7918 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 21306/35720 Training loss: 0.7917 0.2288 sec/batch\n",
      "Epoch 12/20  Iteration 21307/35720 Training loss: 0.7917 0.2167 sec/batch\n",
      "Epoch 12/20  Iteration 21308/35720 Training loss: 0.7917 0.2262 sec/batch\n",
      "Epoch 12/20  Iteration 21309/35720 Training loss: 0.7918 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 21310/35720 Training loss: 0.7917 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21311/35720 Training loss: 0.7918 0.2078 sec/batch\n",
      "Epoch 12/20  Iteration 21312/35720 Training loss: 0.7917 0.2109 sec/batch\n",
      "Epoch 12/20  Iteration 21313/35720 Training loss: 0.7917 0.2105 sec/batch\n",
      "Epoch 12/20  Iteration 21314/35720 Training loss: 0.7918 0.2178 sec/batch\n",
      "Epoch 12/20  Iteration 21315/35720 Training loss: 0.7918 0.2065 sec/batch\n",
      "Epoch 12/20  Iteration 21316/35720 Training loss: 0.7918 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 21317/35720 Training loss: 0.7918 0.2148 sec/batch\n",
      "Epoch 12/20  Iteration 21318/35720 Training loss: 0.7918 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21319/35720 Training loss: 0.7918 0.2189 sec/batch\n",
      "Epoch 12/20  Iteration 21320/35720 Training loss: 0.7918 0.2229 sec/batch\n",
      "Epoch 12/20  Iteration 21321/35720 Training loss: 0.7918 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 21322/35720 Training loss: 0.7918 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 21323/35720 Training loss: 0.7918 0.2136 sec/batch\n",
      "Epoch 12/20  Iteration 21324/35720 Training loss: 0.7918 0.2049 sec/batch\n",
      "Epoch 12/20  Iteration 21325/35720 Training loss: 0.7917 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 21326/35720 Training loss: 0.7917 0.2116 sec/batch\n",
      "Epoch 12/20  Iteration 21327/35720 Training loss: 0.7917 0.2183 sec/batch\n",
      "Epoch 12/20  Iteration 21328/35720 Training loss: 0.7916 0.2177 sec/batch\n",
      "Epoch 12/20  Iteration 21329/35720 Training loss: 0.7916 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 21330/35720 Training loss: 0.7916 0.2175 sec/batch\n",
      "Epoch 12/20  Iteration 21331/35720 Training loss: 0.7916 0.2106 sec/batch\n",
      "Epoch 12/20  Iteration 21332/35720 Training loss: 0.7916 0.2062 sec/batch\n",
      "Epoch 12/20  Iteration 21333/35720 Training loss: 0.7916 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21334/35720 Training loss: 0.7916 0.2202 sec/batch\n",
      "Epoch 12/20  Iteration 21335/35720 Training loss: 0.7916 0.2185 sec/batch\n",
      "Epoch 12/20  Iteration 21336/35720 Training loss: 0.7916 0.2163 sec/batch\n",
      "Epoch 12/20  Iteration 21337/35720 Training loss: 0.7915 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 21338/35720 Training loss: 0.7915 0.2141 sec/batch\n",
      "Epoch 12/20  Iteration 21339/35720 Training loss: 0.7915 0.2250 sec/batch\n",
      "Epoch 12/20  Iteration 21340/35720 Training loss: 0.7915 0.2196 sec/batch\n",
      "Epoch 12/20  Iteration 21341/35720 Training loss: 0.7915 0.2261 sec/batch\n",
      "Epoch 12/20  Iteration 21342/35720 Training loss: 0.7914 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 21343/35720 Training loss: 0.7914 0.2067 sec/batch\n",
      "Epoch 12/20  Iteration 21344/35720 Training loss: 0.7914 0.2091 sec/batch\n",
      "Epoch 12/20  Iteration 21345/35720 Training loss: 0.7914 0.2278 sec/batch\n",
      "Epoch 12/20  Iteration 21346/35720 Training loss: 0.7914 0.2137 sec/batch\n",
      "Epoch 12/20  Iteration 21347/35720 Training loss: 0.7915 0.2253 sec/batch\n",
      "Epoch 12/20  Iteration 21348/35720 Training loss: 0.7914 0.2190 sec/batch\n",
      "Epoch 12/20  Iteration 21349/35720 Training loss: 0.7914 0.2108 sec/batch\n",
      "Epoch 12/20  Iteration 21350/35720 Training loss: 0.7913 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 21351/35720 Training loss: 0.7913 0.2266 sec/batch\n",
      "Epoch 12/20  Iteration 21352/35720 Training loss: 0.7913 0.2366 sec/batch\n",
      "Epoch 12/20  Iteration 21353/35720 Training loss: 0.7913 0.2111 sec/batch\n",
      "Epoch 12/20  Iteration 21354/35720 Training loss: 0.7913 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 21355/35720 Training loss: 0.7913 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 21356/35720 Training loss: 0.7913 0.2363 sec/batch\n",
      "Epoch 12/20  Iteration 21357/35720 Training loss: 0.7914 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 21358/35720 Training loss: 0.7914 0.2305 sec/batch\n",
      "Epoch 12/20  Iteration 21359/35720 Training loss: 0.7914 0.2172 sec/batch\n",
      "Epoch 12/20  Iteration 21360/35720 Training loss: 0.7914 0.2097 sec/batch\n",
      "Epoch 12/20  Iteration 21361/35720 Training loss: 0.7914 0.2255 sec/batch\n",
      "Epoch 12/20  Iteration 21362/35720 Training loss: 0.7914 0.2087 sec/batch\n",
      "Epoch 12/20  Iteration 21363/35720 Training loss: 0.7914 0.2187 sec/batch\n",
      "Epoch 12/20  Iteration 21364/35720 Training loss: 0.7914 0.2069 sec/batch\n",
      "Epoch 12/20  Iteration 21365/35720 Training loss: 0.7913 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 21366/35720 Training loss: 0.7914 0.2161 sec/batch\n",
      "Epoch 12/20  Iteration 21367/35720 Training loss: 0.7913 0.2154 sec/batch\n",
      "Epoch 12/20  Iteration 21368/35720 Training loss: 0.7913 0.2189 sec/batch\n",
      "Epoch 12/20  Iteration 21369/35720 Training loss: 0.7913 0.2216 sec/batch\n",
      "Epoch 12/20  Iteration 21370/35720 Training loss: 0.7913 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 21371/35720 Training loss: 0.7913 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 21372/35720 Training loss: 0.7913 0.2099 sec/batch\n",
      "Epoch 12/20  Iteration 21373/35720 Training loss: 0.7913 0.2139 sec/batch\n",
      "Epoch 12/20  Iteration 21374/35720 Training loss: 0.7913 0.2165 sec/batch\n",
      "Epoch 12/20  Iteration 21375/35720 Training loss: 0.7913 0.2205 sec/batch\n",
      "Epoch 12/20  Iteration 21376/35720 Training loss: 0.7913 0.2058 sec/batch\n",
      "Epoch 12/20  Iteration 21377/35720 Training loss: 0.7913 0.2247 sec/batch\n",
      "Epoch 12/20  Iteration 21378/35720 Training loss: 0.7913 0.2226 sec/batch\n",
      "Epoch 12/20  Iteration 21379/35720 Training loss: 0.7913 0.2218 sec/batch\n",
      "Epoch 12/20  Iteration 21380/35720 Training loss: 0.7913 0.2126 sec/batch\n",
      "Epoch 12/20  Iteration 21381/35720 Training loss: 0.7913 0.2112 sec/batch\n",
      "Epoch 12/20  Iteration 21382/35720 Training loss: 0.7913 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 21383/35720 Training loss: 0.7914 0.2086 sec/batch\n",
      "Epoch 12/20  Iteration 21384/35720 Training loss: 0.7913 0.2270 sec/batch\n",
      "Epoch 12/20  Iteration 21385/35720 Training loss: 0.7914 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 21386/35720 Training loss: 0.7914 0.2192 sec/batch\n",
      "Epoch 12/20  Iteration 21387/35720 Training loss: 0.7914 0.2159 sec/batch\n",
      "Epoch 12/20  Iteration 21388/35720 Training loss: 0.7913 0.2142 sec/batch\n",
      "Epoch 12/20  Iteration 21389/35720 Training loss: 0.7913 0.2114 sec/batch\n",
      "Epoch 12/20  Iteration 21390/35720 Training loss: 0.7914 0.2246 sec/batch\n",
      "Epoch 12/20  Iteration 21391/35720 Training loss: 0.7914 0.2288 sec/batch\n",
      "Epoch 12/20  Iteration 21392/35720 Training loss: 0.7914 0.2177 sec/batch\n",
      "Epoch 12/20  Iteration 21393/35720 Training loss: 0.7914 0.2077 sec/batch\n",
      "Epoch 12/20  Iteration 21394/35720 Training loss: 0.7914 0.2098 sec/batch\n",
      "Epoch 12/20  Iteration 21395/35720 Training loss: 0.7914 0.2124 sec/batch\n",
      "Epoch 12/20  Iteration 21396/35720 Training loss: 0.7914 0.2095 sec/batch\n",
      "Epoch 12/20  Iteration 21397/35720 Training loss: 0.7914 0.2328 sec/batch\n",
      "Epoch 12/20  Iteration 21398/35720 Training loss: 0.7914 0.2273 sec/batch\n",
      "Epoch 12/20  Iteration 21399/35720 Training loss: 0.7913 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21400/35720 Training loss: 0.7914 0.2238 sec/batch\n",
      "Validation loss: 1.46307 Saving checkpoint!\n",
      "Epoch 12/20  Iteration 21401/35720 Training loss: 0.7916 0.2090 sec/batch\n",
      "Epoch 12/20  Iteration 21402/35720 Training loss: 0.7916 0.2092 sec/batch\n",
      "Epoch 12/20  Iteration 21403/35720 Training loss: 0.7916 0.2066 sec/batch\n",
      "Epoch 12/20  Iteration 21404/35720 Training loss: 0.7916 0.2093 sec/batch\n",
      "Epoch 12/20  Iteration 21405/35720 Training loss: 0.7915 0.2258 sec/batch\n",
      "Epoch 12/20  Iteration 21406/35720 Training loss: 0.7915 0.2166 sec/batch\n",
      "Epoch 12/20  Iteration 21407/35720 Training loss: 0.7915 0.2118 sec/batch\n",
      "Epoch 12/20  Iteration 21408/35720 Training loss: 0.7915 0.2189 sec/batch\n",
      "Epoch 12/20  Iteration 21409/35720 Training loss: 0.7915 0.2222 sec/batch\n",
      "Epoch 12/20  Iteration 21410/35720 Training loss: 0.7915 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 21411/35720 Training loss: 0.7915 0.2081 sec/batch\n",
      "Epoch 12/20  Iteration 21412/35720 Training loss: 0.7915 0.2152 sec/batch\n",
      "Epoch 12/20  Iteration 21413/35720 Training loss: 0.7915 0.2132 sec/batch\n",
      "Epoch 12/20  Iteration 21414/35720 Training loss: 0.7915 0.2203 sec/batch\n",
      "Epoch 12/20  Iteration 21415/35720 Training loss: 0.7915 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21416/35720 Training loss: 0.7915 0.2158 sec/batch\n",
      "Epoch 12/20  Iteration 21417/35720 Training loss: 0.7915 0.2089 sec/batch\n",
      "Epoch 12/20  Iteration 21418/35720 Training loss: 0.7915 0.2202 sec/batch\n",
      "Epoch 12/20  Iteration 21419/35720 Training loss: 0.7915 0.2274 sec/batch\n",
      "Epoch 12/20  Iteration 21420/35720 Training loss: 0.7914 0.2088 sec/batch\n",
      "Epoch 12/20  Iteration 21421/35720 Training loss: 0.7914 0.2199 sec/batch\n",
      "Epoch 12/20  Iteration 21422/35720 Training loss: 0.7914 0.2141 sec/batch\n",
      "Epoch 12/20  Iteration 21423/35720 Training loss: 0.7914 0.2180 sec/batch\n",
      "Epoch 12/20  Iteration 21424/35720 Training loss: 0.7914 0.2115 sec/batch\n",
      "Epoch 12/20  Iteration 21425/35720 Training loss: 0.7914 0.2187 sec/batch\n",
      "Epoch 12/20  Iteration 21426/35720 Training loss: 0.7914 0.2122 sec/batch\n",
      "Epoch 12/20  Iteration 21427/35720 Training loss: 0.7913 0.2157 sec/batch\n",
      "Epoch 12/20  Iteration 21428/35720 Training loss: 0.7913 0.2096 sec/batch\n",
      "Epoch 12/20  Iteration 21429/35720 Training loss: 0.7913 0.2151 sec/batch\n",
      "Epoch 12/20  Iteration 21430/35720 Training loss: 0.7913 0.2063 sec/batch\n",
      "Epoch 12/20  Iteration 21431/35720 Training loss: 0.7913 0.2113 sec/batch\n",
      "Epoch 12/20  Iteration 21432/35720 Training loss: 0.7913 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 21433/35720 Training loss: 0.8004 0.2207 sec/batch\n",
      "Epoch 13/20  Iteration 21434/35720 Training loss: 0.8166 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 21435/35720 Training loss: 0.8076 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 21436/35720 Training loss: 0.8077 0.2053 sec/batch\n",
      "Epoch 13/20  Iteration 21437/35720 Training loss: 0.8131 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 21438/35720 Training loss: 0.8014 0.2877 sec/batch\n",
      "Epoch 13/20  Iteration 21439/35720 Training loss: 0.8026 0.2935 sec/batch\n",
      "Epoch 13/20  Iteration 21440/35720 Training loss: 0.7952 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 21441/35720 Training loss: 0.7884 0.2075 sec/batch\n",
      "Epoch 13/20  Iteration 21442/35720 Training loss: 0.7898 0.2145 sec/batch\n",
      "Epoch 13/20  Iteration 21443/35720 Training loss: 0.7894 0.2221 sec/batch\n",
      "Epoch 13/20  Iteration 21444/35720 Training loss: 0.7847 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 21445/35720 Training loss: 0.7861 0.2327 sec/batch\n",
      "Epoch 13/20  Iteration 21446/35720 Training loss: 0.7893 0.2219 sec/batch\n",
      "Epoch 13/20  Iteration 21447/35720 Training loss: 0.7912 0.2058 sec/batch\n",
      "Epoch 13/20  Iteration 21448/35720 Training loss: 0.7900 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 21449/35720 Training loss: 0.7903 0.2246 sec/batch\n",
      "Epoch 13/20  Iteration 21450/35720 Training loss: 0.7876 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 21451/35720 Training loss: 0.7866 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 21452/35720 Training loss: 0.7872 0.2051 sec/batch\n",
      "Epoch 13/20  Iteration 21453/35720 Training loss: 0.7871 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 21454/35720 Training loss: 0.7847 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 21455/35720 Training loss: 0.7836 0.2199 sec/batch\n",
      "Epoch 13/20  Iteration 21456/35720 Training loss: 0.7848 0.2219 sec/batch\n",
      "Epoch 13/20  Iteration 21457/35720 Training loss: 0.7853 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 21458/35720 Training loss: 0.7846 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 21459/35720 Training loss: 0.7860 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 21460/35720 Training loss: 0.7868 0.2215 sec/batch\n",
      "Epoch 13/20  Iteration 21461/35720 Training loss: 0.7863 0.2173 sec/batch\n",
      "Epoch 13/20  Iteration 21462/35720 Training loss: 0.7875 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 21463/35720 Training loss: 0.7894 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 21464/35720 Training loss: 0.7877 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 21465/35720 Training loss: 0.7893 0.2152 sec/batch\n",
      "Epoch 13/20  Iteration 21466/35720 Training loss: 0.7908 0.2230 sec/batch\n",
      "Epoch 13/20  Iteration 21467/35720 Training loss: 0.7933 0.2214 sec/batch\n",
      "Epoch 13/20  Iteration 21468/35720 Training loss: 0.7938 0.2272 sec/batch\n",
      "Epoch 13/20  Iteration 21469/35720 Training loss: 0.7943 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 21470/35720 Training loss: 0.7937 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 21471/35720 Training loss: 0.7927 0.2219 sec/batch\n",
      "Epoch 13/20  Iteration 21472/35720 Training loss: 0.7938 0.2081 sec/batch\n",
      "Epoch 13/20  Iteration 21473/35720 Training loss: 0.7936 0.2207 sec/batch\n",
      "Epoch 13/20  Iteration 21474/35720 Training loss: 0.7925 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 21475/35720 Training loss: 0.7910 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 21476/35720 Training loss: 0.7905 0.2152 sec/batch\n",
      "Epoch 13/20  Iteration 21477/35720 Training loss: 0.7901 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21478/35720 Training loss: 0.7898 0.2138 sec/batch\n",
      "Epoch 13/20  Iteration 21479/35720 Training loss: 0.7890 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 21480/35720 Training loss: 0.7884 0.2187 sec/batch\n",
      "Epoch 13/20  Iteration 21481/35720 Training loss: 0.7875 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 21482/35720 Training loss: 0.7867 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 21483/35720 Training loss: 0.7871 0.2261 sec/batch\n",
      "Epoch 13/20  Iteration 21484/35720 Training loss: 0.7871 0.2277 sec/batch\n",
      "Epoch 13/20  Iteration 21485/35720 Training loss: 0.7873 0.2138 sec/batch\n",
      "Epoch 13/20  Iteration 21486/35720 Training loss: 0.7856 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 21487/35720 Training loss: 0.7848 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 21488/35720 Training loss: 0.7843 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21489/35720 Training loss: 0.7841 0.2079 sec/batch\n",
      "Epoch 13/20  Iteration 21490/35720 Training loss: 0.7840 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 21491/35720 Training loss: 0.7832 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 21492/35720 Training loss: 0.7824 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 21493/35720 Training loss: 0.7818 0.2284 sec/batch\n",
      "Epoch 13/20  Iteration 21494/35720 Training loss: 0.7801 0.2204 sec/batch\n",
      "Epoch 13/20  Iteration 21495/35720 Training loss: 0.7807 0.2271 sec/batch\n",
      "Epoch 13/20  Iteration 21496/35720 Training loss: 0.7806 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 21497/35720 Training loss: 0.7814 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 21498/35720 Training loss: 0.7817 0.2138 sec/batch\n",
      "Epoch 13/20  Iteration 21499/35720 Training loss: 0.7814 0.2239 sec/batch\n",
      "Epoch 13/20  Iteration 21500/35720 Training loss: 0.7807 0.2219 sec/batch\n",
      "Epoch 13/20  Iteration 21501/35720 Training loss: 0.7815 0.2103 sec/batch\n",
      "Epoch 13/20  Iteration 21502/35720 Training loss: 0.7814 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 21503/35720 Training loss: 0.7817 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 21504/35720 Training loss: 0.7825 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 21505/35720 Training loss: 0.7824 0.2188 sec/batch\n",
      "Epoch 13/20  Iteration 21506/35720 Training loss: 0.7826 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 21507/35720 Training loss: 0.7817 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 21508/35720 Training loss: 0.7817 0.2191 sec/batch\n",
      "Epoch 13/20  Iteration 21509/35720 Training loss: 0.7810 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 21510/35720 Training loss: 0.7817 0.2371 sec/batch\n",
      "Epoch 13/20  Iteration 21511/35720 Training loss: 0.7816 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 21512/35720 Training loss: 0.7825 0.2204 sec/batch\n",
      "Epoch 13/20  Iteration 21513/35720 Training loss: 0.7827 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 21514/35720 Training loss: 0.7828 0.2121 sec/batch\n",
      "Epoch 13/20  Iteration 21515/35720 Training loss: 0.7826 0.2172 sec/batch\n",
      "Epoch 13/20  Iteration 21516/35720 Training loss: 0.7827 0.2271 sec/batch\n",
      "Epoch 13/20  Iteration 21517/35720 Training loss: 0.7826 0.2144 sec/batch\n",
      "Epoch 13/20  Iteration 21518/35720 Training loss: 0.7825 0.2138 sec/batch\n",
      "Epoch 13/20  Iteration 21519/35720 Training loss: 0.7826 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 21520/35720 Training loss: 0.7827 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 21521/35720 Training loss: 0.7815 0.2241 sec/batch\n",
      "Epoch 13/20  Iteration 21522/35720 Training loss: 0.7809 0.2379 sec/batch\n",
      "Epoch 13/20  Iteration 21523/35720 Training loss: 0.7810 0.2332 sec/batch\n",
      "Epoch 13/20  Iteration 21524/35720 Training loss: 0.7808 0.2116 sec/batch\n",
      "Epoch 13/20  Iteration 21525/35720 Training loss: 0.7813 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 21526/35720 Training loss: 0.7817 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 21527/35720 Training loss: 0.7816 0.2222 sec/batch\n",
      "Epoch 13/20  Iteration 21528/35720 Training loss: 0.7811 0.2278 sec/batch\n",
      "Epoch 13/20  Iteration 21529/35720 Training loss: 0.7814 0.2247 sec/batch\n",
      "Epoch 13/20  Iteration 21530/35720 Training loss: 0.7811 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 21531/35720 Training loss: 0.7812 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 21532/35720 Training loss: 0.7810 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 21533/35720 Training loss: 0.7808 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 21534/35720 Training loss: 0.7809 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 21535/35720 Training loss: 0.7804 0.2135 sec/batch\n",
      "Epoch 13/20  Iteration 21536/35720 Training loss: 0.7801 0.2257 sec/batch\n",
      "Epoch 13/20  Iteration 21537/35720 Training loss: 0.7801 0.2235 sec/batch\n",
      "Epoch 13/20  Iteration 21538/35720 Training loss: 0.7796 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 21539/35720 Training loss: 0.7796 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 21540/35720 Training loss: 0.7798 0.2152 sec/batch\n",
      "Epoch 13/20  Iteration 21541/35720 Training loss: 0.7802 0.2163 sec/batch\n",
      "Epoch 13/20  Iteration 21542/35720 Training loss: 0.7799 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 21543/35720 Training loss: 0.7802 0.2173 sec/batch\n",
      "Epoch 13/20  Iteration 21544/35720 Training loss: 0.7805 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 21545/35720 Training loss: 0.7805 0.2154 sec/batch\n",
      "Epoch 13/20  Iteration 21546/35720 Training loss: 0.7806 0.2110 sec/batch\n",
      "Epoch 13/20  Iteration 21547/35720 Training loss: 0.7805 0.2209 sec/batch\n",
      "Epoch 13/20  Iteration 21548/35720 Training loss: 0.7804 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 21549/35720 Training loss: 0.7803 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 21550/35720 Training loss: 0.7805 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 21551/35720 Training loss: 0.7805 0.2195 sec/batch\n",
      "Epoch 13/20  Iteration 21552/35720 Training loss: 0.7811 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 21553/35720 Training loss: 0.7815 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 21554/35720 Training loss: 0.7810 0.2237 sec/batch\n",
      "Epoch 13/20  Iteration 21555/35720 Training loss: 0.7810 0.2310 sec/batch\n",
      "Epoch 13/20  Iteration 21556/35720 Training loss: 0.7815 0.2272 sec/batch\n",
      "Epoch 13/20  Iteration 21557/35720 Training loss: 0.7811 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 21558/35720 Training loss: 0.7808 0.2194 sec/batch\n",
      "Epoch 13/20  Iteration 21559/35720 Training loss: 0.7811 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 21560/35720 Training loss: 0.7809 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 21561/35720 Training loss: 0.7805 0.2277 sec/batch\n",
      "Epoch 13/20  Iteration 21562/35720 Training loss: 0.7804 0.2247 sec/batch\n",
      "Epoch 13/20  Iteration 21563/35720 Training loss: 0.7801 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 21564/35720 Training loss: 0.7798 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 21565/35720 Training loss: 0.7800 0.2305 sec/batch\n",
      "Epoch 13/20  Iteration 21566/35720 Training loss: 0.7801 0.2127 sec/batch\n",
      "Epoch 13/20  Iteration 21567/35720 Training loss: 0.7800 0.2175 sec/batch\n",
      "Epoch 13/20  Iteration 21568/35720 Training loss: 0.7801 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 21569/35720 Training loss: 0.7807 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 21570/35720 Training loss: 0.7809 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 21571/35720 Training loss: 0.7810 0.2226 sec/batch\n",
      "Epoch 13/20  Iteration 21572/35720 Training loss: 0.7811 0.2109 sec/batch\n",
      "Epoch 13/20  Iteration 21573/35720 Training loss: 0.7811 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 21574/35720 Training loss: 0.7807 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21575/35720 Training loss: 0.7804 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 21576/35720 Training loss: 0.7800 0.2305 sec/batch\n",
      "Epoch 13/20  Iteration 21577/35720 Training loss: 0.7800 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 21578/35720 Training loss: 0.7803 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 21579/35720 Training loss: 0.7800 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 21580/35720 Training loss: 0.7800 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 21581/35720 Training loss: 0.7801 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 21582/35720 Training loss: 0.7795 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 21583/35720 Training loss: 0.7793 0.2239 sec/batch\n",
      "Epoch 13/20  Iteration 21584/35720 Training loss: 0.7795 0.2171 sec/batch\n",
      "Epoch 13/20  Iteration 21585/35720 Training loss: 0.7795 0.2223 sec/batch\n",
      "Epoch 13/20  Iteration 21586/35720 Training loss: 0.7798 0.2135 sec/batch\n",
      "Epoch 13/20  Iteration 21587/35720 Training loss: 0.7800 0.2273 sec/batch\n",
      "Epoch 13/20  Iteration 21588/35720 Training loss: 0.7802 0.2242 sec/batch\n",
      "Epoch 13/20  Iteration 21589/35720 Training loss: 0.7803 0.2254 sec/batch\n",
      "Epoch 13/20  Iteration 21590/35720 Training loss: 0.7806 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 21591/35720 Training loss: 0.7803 0.2103 sec/batch\n",
      "Epoch 13/20  Iteration 21592/35720 Training loss: 0.7803 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 21593/35720 Training loss: 0.7800 0.2252 sec/batch\n",
      "Epoch 13/20  Iteration 21594/35720 Training loss: 0.7801 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 21595/35720 Training loss: 0.7801 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 21596/35720 Training loss: 0.7802 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 21597/35720 Training loss: 0.7803 0.2046 sec/batch\n",
      "Epoch 13/20  Iteration 21598/35720 Training loss: 0.7803 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 21599/35720 Training loss: 0.7805 0.2248 sec/batch\n",
      "Epoch 13/20  Iteration 21600/35720 Training loss: 0.7808 0.2319 sec/batch\n",
      "Validation loss: 1.47905 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 21601/35720 Training loss: 0.7840 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 21602/35720 Training loss: 0.7846 0.2146 sec/batch\n",
      "Epoch 13/20  Iteration 21603/35720 Training loss: 0.7850 0.2244 sec/batch\n",
      "Epoch 13/20  Iteration 21604/35720 Training loss: 0.7854 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 21605/35720 Training loss: 0.7857 0.2221 sec/batch\n",
      "Epoch 13/20  Iteration 21606/35720 Training loss: 0.7862 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 21607/35720 Training loss: 0.7864 0.2192 sec/batch\n",
      "Epoch 13/20  Iteration 21608/35720 Training loss: 0.7864 0.2297 sec/batch\n",
      "Epoch 13/20  Iteration 21609/35720 Training loss: 0.7867 0.2270 sec/batch\n",
      "Epoch 13/20  Iteration 21610/35720 Training loss: 0.7864 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 21611/35720 Training loss: 0.7863 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 21612/35720 Training loss: 0.7860 0.2202 sec/batch\n",
      "Epoch 13/20  Iteration 21613/35720 Training loss: 0.7862 0.2282 sec/batch\n",
      "Epoch 13/20  Iteration 21614/35720 Training loss: 0.7863 0.2271 sec/batch\n",
      "Epoch 13/20  Iteration 21615/35720 Training loss: 0.7864 0.2144 sec/batch\n",
      "Epoch 13/20  Iteration 21616/35720 Training loss: 0.7866 0.2298 sec/batch\n",
      "Epoch 13/20  Iteration 21617/35720 Training loss: 0.7865 0.2123 sec/batch\n",
      "Epoch 13/20  Iteration 21618/35720 Training loss: 0.7865 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 21619/35720 Training loss: 0.7862 0.2212 sec/batch\n",
      "Epoch 13/20  Iteration 21620/35720 Training loss: 0.7864 0.2218 sec/batch\n",
      "Epoch 13/20  Iteration 21621/35720 Training loss: 0.7864 0.2266 sec/batch\n",
      "Epoch 13/20  Iteration 21622/35720 Training loss: 0.7864 0.2056 sec/batch\n",
      "Epoch 13/20  Iteration 21623/35720 Training loss: 0.7866 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 21624/35720 Training loss: 0.7870 0.2112 sec/batch\n",
      "Epoch 13/20  Iteration 21625/35720 Training loss: 0.7873 0.2194 sec/batch\n",
      "Epoch 13/20  Iteration 21626/35720 Training loss: 0.7875 0.2257 sec/batch\n",
      "Epoch 13/20  Iteration 21627/35720 Training loss: 0.7876 0.2273 sec/batch\n",
      "Epoch 13/20  Iteration 21628/35720 Training loss: 0.7878 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 21629/35720 Training loss: 0.7874 0.2103 sec/batch\n",
      "Epoch 13/20  Iteration 21630/35720 Training loss: 0.7874 0.2110 sec/batch\n",
      "Epoch 13/20  Iteration 21631/35720 Training loss: 0.7875 0.2225 sec/batch\n",
      "Epoch 13/20  Iteration 21632/35720 Training loss: 0.7875 0.2192 sec/batch\n",
      "Epoch 13/20  Iteration 21633/35720 Training loss: 0.7874 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 21634/35720 Training loss: 0.7874 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 21635/35720 Training loss: 0.7876 0.2246 sec/batch\n",
      "Epoch 13/20  Iteration 21636/35720 Training loss: 0.7874 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 21637/35720 Training loss: 0.7875 0.2138 sec/batch\n",
      "Epoch 13/20  Iteration 21638/35720 Training loss: 0.7874 0.2216 sec/batch\n",
      "Epoch 13/20  Iteration 21639/35720 Training loss: 0.7877 0.2278 sec/batch\n",
      "Epoch 13/20  Iteration 21640/35720 Training loss: 0.7882 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 21641/35720 Training loss: 0.7885 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 21642/35720 Training loss: 0.7885 0.2249 sec/batch\n",
      "Epoch 13/20  Iteration 21643/35720 Training loss: 0.7887 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 21644/35720 Training loss: 0.7888 0.2143 sec/batch\n",
      "Epoch 13/20  Iteration 21645/35720 Training loss: 0.7886 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 21646/35720 Training loss: 0.7886 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 21647/35720 Training loss: 0.7885 0.2236 sec/batch\n",
      "Epoch 13/20  Iteration 21648/35720 Training loss: 0.7885 0.2084 sec/batch\n",
      "Epoch 13/20  Iteration 21649/35720 Training loss: 0.7883 0.2247 sec/batch\n",
      "Epoch 13/20  Iteration 21650/35720 Training loss: 0.7881 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 21651/35720 Training loss: 0.7882 0.2054 sec/batch\n",
      "Epoch 13/20  Iteration 21652/35720 Training loss: 0.7883 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 21653/35720 Training loss: 0.7884 0.2200 sec/batch\n",
      "Epoch 13/20  Iteration 21654/35720 Training loss: 0.7884 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 21655/35720 Training loss: 0.7887 0.2143 sec/batch\n",
      "Epoch 13/20  Iteration 21656/35720 Training loss: 0.7890 0.2057 sec/batch\n",
      "Epoch 13/20  Iteration 21657/35720 Training loss: 0.7891 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 21658/35720 Training loss: 0.7890 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 21659/35720 Training loss: 0.7887 0.2274 sec/batch\n",
      "Epoch 13/20  Iteration 21660/35720 Training loss: 0.7887 0.2243 sec/batch\n",
      "Epoch 13/20  Iteration 21661/35720 Training loss: 0.7884 0.2213 sec/batch\n",
      "Epoch 13/20  Iteration 21662/35720 Training loss: 0.7883 0.2519 sec/batch\n",
      "Epoch 13/20  Iteration 21663/35720 Training loss: 0.7886 0.2298 sec/batch\n",
      "Epoch 13/20  Iteration 21664/35720 Training loss: 0.7886 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 21665/35720 Training loss: 0.7887 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 21666/35720 Training loss: 0.7887 0.2260 sec/batch\n",
      "Epoch 13/20  Iteration 21667/35720 Training loss: 0.7886 0.2081 sec/batch\n",
      "Epoch 13/20  Iteration 21668/35720 Training loss: 0.7887 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 21669/35720 Training loss: 0.7888 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 21670/35720 Training loss: 0.7887 0.2135 sec/batch\n",
      "Epoch 13/20  Iteration 21671/35720 Training loss: 0.7886 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 21672/35720 Training loss: 0.7886 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 21673/35720 Training loss: 0.7884 0.2075 sec/batch\n",
      "Epoch 13/20  Iteration 21674/35720 Training loss: 0.7883 0.2134 sec/batch\n",
      "Epoch 13/20  Iteration 21675/35720 Training loss: 0.7882 0.2195 sec/batch\n",
      "Epoch 13/20  Iteration 21676/35720 Training loss: 0.7881 0.2225 sec/batch\n",
      "Epoch 13/20  Iteration 21677/35720 Training loss: 0.7878 0.2276 sec/batch\n",
      "Epoch 13/20  Iteration 21678/35720 Training loss: 0.7879 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 21679/35720 Training loss: 0.7878 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 21680/35720 Training loss: 0.7878 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 21681/35720 Training loss: 0.7877 0.2215 sec/batch\n",
      "Epoch 13/20  Iteration 21682/35720 Training loss: 0.7875 0.2331 sec/batch\n",
      "Epoch 13/20  Iteration 21683/35720 Training loss: 0.7875 0.2254 sec/batch\n",
      "Epoch 13/20  Iteration 21684/35720 Training loss: 0.7873 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 21685/35720 Training loss: 0.7871 0.2253 sec/batch\n",
      "Epoch 13/20  Iteration 21686/35720 Training loss: 0.7872 0.2181 sec/batch\n",
      "Epoch 13/20  Iteration 21687/35720 Training loss: 0.7874 0.2084 sec/batch\n",
      "Epoch 13/20  Iteration 21688/35720 Training loss: 0.7875 0.2244 sec/batch\n",
      "Epoch 13/20  Iteration 21689/35720 Training loss: 0.7874 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 21690/35720 Training loss: 0.7873 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 21691/35720 Training loss: 0.7876 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 21692/35720 Training loss: 0.7877 0.2245 sec/batch\n",
      "Epoch 13/20  Iteration 21693/35720 Training loss: 0.7876 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 21694/35720 Training loss: 0.7874 0.2188 sec/batch\n",
      "Epoch 13/20  Iteration 21695/35720 Training loss: 0.7874 0.2215 sec/batch\n",
      "Epoch 13/20  Iteration 21696/35720 Training loss: 0.7875 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 21697/35720 Training loss: 0.7874 0.2326 sec/batch\n",
      "Epoch 13/20  Iteration 21698/35720 Training loss: 0.7877 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 21699/35720 Training loss: 0.7877 0.2145 sec/batch\n",
      "Epoch 13/20  Iteration 21700/35720 Training loss: 0.7877 0.2112 sec/batch\n",
      "Epoch 13/20  Iteration 21701/35720 Training loss: 0.7876 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 21702/35720 Training loss: 0.7872 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 21703/35720 Training loss: 0.7870 0.2302 sec/batch\n",
      "Epoch 13/20  Iteration 21704/35720 Training loss: 0.7871 0.2112 sec/batch\n",
      "Epoch 13/20  Iteration 21705/35720 Training loss: 0.7870 0.2262 sec/batch\n",
      "Epoch 13/20  Iteration 21706/35720 Training loss: 0.7871 0.2057 sec/batch\n",
      "Epoch 13/20  Iteration 21707/35720 Training loss: 0.7870 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 21708/35720 Training loss: 0.7869 0.2112 sec/batch\n",
      "Epoch 13/20  Iteration 21709/35720 Training loss: 0.7867 0.2193 sec/batch\n",
      "Epoch 13/20  Iteration 21710/35720 Training loss: 0.7866 0.2365 sec/batch\n",
      "Epoch 13/20  Iteration 21711/35720 Training loss: 0.7864 0.2257 sec/batch\n",
      "Epoch 13/20  Iteration 21712/35720 Training loss: 0.7864 0.2112 sec/batch\n",
      "Epoch 13/20  Iteration 21713/35720 Training loss: 0.7862 0.2079 sec/batch\n",
      "Epoch 13/20  Iteration 21714/35720 Training loss: 0.7859 0.2335 sec/batch\n",
      "Epoch 13/20  Iteration 21715/35720 Training loss: 0.7857 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 21716/35720 Training loss: 0.7855 0.2195 sec/batch\n",
      "Epoch 13/20  Iteration 21717/35720 Training loss: 0.7857 0.2215 sec/batch\n",
      "Epoch 13/20  Iteration 21718/35720 Training loss: 0.7856 0.2064 sec/batch\n",
      "Epoch 13/20  Iteration 21719/35720 Training loss: 0.7854 0.2115 sec/batch\n",
      "Epoch 13/20  Iteration 21720/35720 Training loss: 0.7854 0.2355 sec/batch\n",
      "Epoch 13/20  Iteration 21721/35720 Training loss: 0.7854 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 21722/35720 Training loss: 0.7855 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 21723/35720 Training loss: 0.7854 0.2110 sec/batch\n",
      "Epoch 13/20  Iteration 21724/35720 Training loss: 0.7854 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 21725/35720 Training loss: 0.7854 0.2260 sec/batch\n",
      "Epoch 13/20  Iteration 21726/35720 Training loss: 0.7854 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 21727/35720 Training loss: 0.7856 0.2280 sec/batch\n",
      "Epoch 13/20  Iteration 21728/35720 Training loss: 0.7857 0.2107 sec/batch\n",
      "Epoch 13/20  Iteration 21729/35720 Training loss: 0.7856 0.2250 sec/batch\n",
      "Epoch 13/20  Iteration 21730/35720 Training loss: 0.7858 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 21731/35720 Training loss: 0.7857 0.2286 sec/batch\n",
      "Epoch 13/20  Iteration 21732/35720 Training loss: 0.7857 0.2243 sec/batch\n",
      "Epoch 13/20  Iteration 21733/35720 Training loss: 0.7857 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 21734/35720 Training loss: 0.7855 0.2058 sec/batch\n",
      "Epoch 13/20  Iteration 21735/35720 Training loss: 0.7856 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21736/35720 Training loss: 0.7856 0.2253 sec/batch\n",
      "Epoch 13/20  Iteration 21737/35720 Training loss: 0.7855 0.2288 sec/batch\n",
      "Epoch 13/20  Iteration 21738/35720 Training loss: 0.7854 0.2184 sec/batch\n",
      "Epoch 13/20  Iteration 21739/35720 Training loss: 0.7855 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 21740/35720 Training loss: 0.7855 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 21741/35720 Training loss: 0.7853 0.2141 sec/batch\n",
      "Epoch 13/20  Iteration 21742/35720 Training loss: 0.7853 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 21743/35720 Training loss: 0.7853 0.2227 sec/batch\n",
      "Epoch 13/20  Iteration 21744/35720 Training loss: 0.7852 0.2235 sec/batch\n",
      "Epoch 13/20  Iteration 21745/35720 Training loss: 0.7851 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 21746/35720 Training loss: 0.7850 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 21747/35720 Training loss: 0.7849 0.2258 sec/batch\n",
      "Epoch 13/20  Iteration 21748/35720 Training loss: 0.7849 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 21749/35720 Training loss: 0.7847 0.2191 sec/batch\n",
      "Epoch 13/20  Iteration 21750/35720 Training loss: 0.7848 0.2215 sec/batch\n",
      "Epoch 13/20  Iteration 21751/35720 Training loss: 0.7849 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 21752/35720 Training loss: 0.7848 0.2121 sec/batch\n",
      "Epoch 13/20  Iteration 21753/35720 Training loss: 0.7849 0.2358 sec/batch\n",
      "Epoch 13/20  Iteration 21754/35720 Training loss: 0.7849 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 21755/35720 Training loss: 0.7850 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 21756/35720 Training loss: 0.7850 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 21757/35720 Training loss: 0.7848 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 21758/35720 Training loss: 0.7850 0.2260 sec/batch\n",
      "Epoch 13/20  Iteration 21759/35720 Training loss: 0.7851 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 21760/35720 Training loss: 0.7851 0.2194 sec/batch\n",
      "Epoch 13/20  Iteration 21761/35720 Training loss: 0.7850 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 21762/35720 Training loss: 0.7851 0.2123 sec/batch\n",
      "Epoch 13/20  Iteration 21763/35720 Training loss: 0.7851 0.2221 sec/batch\n",
      "Epoch 13/20  Iteration 21764/35720 Training loss: 0.7851 0.2359 sec/batch\n",
      "Epoch 13/20  Iteration 21765/35720 Training loss: 0.7851 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 21766/35720 Training loss: 0.7852 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 21767/35720 Training loss: 0.7852 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 21768/35720 Training loss: 0.7849 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 21769/35720 Training loss: 0.7849 0.2208 sec/batch\n",
      "Epoch 13/20  Iteration 21770/35720 Training loss: 0.7847 0.2328 sec/batch\n",
      "Epoch 13/20  Iteration 21771/35720 Training loss: 0.7848 0.2244 sec/batch\n",
      "Epoch 13/20  Iteration 21772/35720 Training loss: 0.7848 0.2112 sec/batch\n",
      "Epoch 13/20  Iteration 21773/35720 Training loss: 0.7847 0.2128 sec/batch\n",
      "Epoch 13/20  Iteration 21774/35720 Training loss: 0.7846 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 21775/35720 Training loss: 0.7847 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 21776/35720 Training loss: 0.7848 0.2397 sec/batch\n",
      "Epoch 13/20  Iteration 21777/35720 Training loss: 0.7845 0.2279 sec/batch\n",
      "Epoch 13/20  Iteration 21778/35720 Training loss: 0.7846 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 21779/35720 Training loss: 0.7847 0.2146 sec/batch\n",
      "Epoch 13/20  Iteration 21780/35720 Training loss: 0.7848 0.2171 sec/batch\n",
      "Epoch 13/20  Iteration 21781/35720 Training loss: 0.7848 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 21782/35720 Training loss: 0.7849 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21783/35720 Training loss: 0.7849 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21784/35720 Training loss: 0.7849 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 21785/35720 Training loss: 0.7849 0.2080 sec/batch\n",
      "Epoch 13/20  Iteration 21786/35720 Training loss: 0.7849 0.2230 sec/batch\n",
      "Epoch 13/20  Iteration 21787/35720 Training loss: 0.7851 0.2231 sec/batch\n",
      "Epoch 13/20  Iteration 21788/35720 Training loss: 0.7851 0.2151 sec/batch\n",
      "Epoch 13/20  Iteration 21789/35720 Training loss: 0.7852 0.2074 sec/batch\n",
      "Epoch 13/20  Iteration 21790/35720 Training loss: 0.7853 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 21791/35720 Training loss: 0.7853 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 21792/35720 Training loss: 0.7852 0.2184 sec/batch\n",
      "Epoch 13/20  Iteration 21793/35720 Training loss: 0.7852 0.2245 sec/batch\n",
      "Epoch 13/20  Iteration 21794/35720 Training loss: 0.7853 0.2115 sec/batch\n",
      "Epoch 13/20  Iteration 21795/35720 Training loss: 0.7853 0.2204 sec/batch\n",
      "Epoch 13/20  Iteration 21796/35720 Training loss: 0.7853 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 21797/35720 Training loss: 0.7852 0.2261 sec/batch\n",
      "Epoch 13/20  Iteration 21798/35720 Training loss: 0.7852 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 21799/35720 Training loss: 0.7853 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 21800/35720 Training loss: 0.7852 0.2074 sec/batch\n",
      "Validation loss: 1.46928 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 21801/35720 Training loss: 0.7863 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 21802/35720 Training loss: 0.7863 0.2258 sec/batch\n",
      "Epoch 13/20  Iteration 21803/35720 Training loss: 0.7863 0.2084 sec/batch\n",
      "Epoch 13/20  Iteration 21804/35720 Training loss: 0.7862 0.2173 sec/batch\n",
      "Epoch 13/20  Iteration 21805/35720 Training loss: 0.7862 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 21806/35720 Training loss: 0.7862 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 21807/35720 Training loss: 0.7862 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 21808/35720 Training loss: 0.7862 0.2274 sec/batch\n",
      "Epoch 13/20  Iteration 21809/35720 Training loss: 0.7863 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 21810/35720 Training loss: 0.7863 0.2189 sec/batch\n",
      "Epoch 13/20  Iteration 21811/35720 Training loss: 0.7861 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21812/35720 Training loss: 0.7860 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 21813/35720 Training loss: 0.7859 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 21814/35720 Training loss: 0.7858 0.2127 sec/batch\n",
      "Epoch 13/20  Iteration 21815/35720 Training loss: 0.7858 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 21816/35720 Training loss: 0.7859 0.2152 sec/batch\n",
      "Epoch 13/20  Iteration 21817/35720 Training loss: 0.7859 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 21818/35720 Training loss: 0.7859 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 21819/35720 Training loss: 0.7859 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 21820/35720 Training loss: 0.7859 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 21821/35720 Training loss: 0.7859 0.2237 sec/batch\n",
      "Epoch 13/20  Iteration 21822/35720 Training loss: 0.7858 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 21823/35720 Training loss: 0.7858 0.2047 sec/batch\n",
      "Epoch 13/20  Iteration 21824/35720 Training loss: 0.7857 0.2116 sec/batch\n",
      "Epoch 13/20  Iteration 21825/35720 Training loss: 0.7857 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 21826/35720 Training loss: 0.7855 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 21827/35720 Training loss: 0.7856 0.2145 sec/batch\n",
      "Epoch 13/20  Iteration 21828/35720 Training loss: 0.7854 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 21829/35720 Training loss: 0.7855 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 21830/35720 Training loss: 0.7852 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 21831/35720 Training loss: 0.7852 0.2235 sec/batch\n",
      "Epoch 13/20  Iteration 21832/35720 Training loss: 0.7851 0.2340 sec/batch\n",
      "Epoch 13/20  Iteration 21833/35720 Training loss: 0.7852 0.2262 sec/batch\n",
      "Epoch 13/20  Iteration 21834/35720 Training loss: 0.7853 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 21835/35720 Training loss: 0.7853 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 21836/35720 Training loss: 0.7853 0.2331 sec/batch\n",
      "Epoch 13/20  Iteration 21837/35720 Training loss: 0.7852 0.2225 sec/batch\n",
      "Epoch 13/20  Iteration 21838/35720 Training loss: 0.7850 0.2217 sec/batch\n",
      "Epoch 13/20  Iteration 21839/35720 Training loss: 0.7850 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 21840/35720 Training loss: 0.7850 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 21841/35720 Training loss: 0.7849 0.2055 sec/batch\n",
      "Epoch 13/20  Iteration 21842/35720 Training loss: 0.7847 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 21843/35720 Training loss: 0.7848 0.2301 sec/batch\n",
      "Epoch 13/20  Iteration 21844/35720 Training loss: 0.7846 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 21845/35720 Training loss: 0.7845 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 21846/35720 Training loss: 0.7845 0.2213 sec/batch\n",
      "Epoch 13/20  Iteration 21847/35720 Training loss: 0.7844 0.2235 sec/batch\n",
      "Epoch 13/20  Iteration 21848/35720 Training loss: 0.7844 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 21849/35720 Training loss: 0.7843 0.2202 sec/batch\n",
      "Epoch 13/20  Iteration 21850/35720 Training loss: 0.7842 0.2057 sec/batch\n",
      "Epoch 13/20  Iteration 21851/35720 Training loss: 0.7841 0.2109 sec/batch\n",
      "Epoch 13/20  Iteration 21852/35720 Training loss: 0.7840 0.2226 sec/batch\n",
      "Epoch 13/20  Iteration 21853/35720 Training loss: 0.7840 0.2300 sec/batch\n",
      "Epoch 13/20  Iteration 21854/35720 Training loss: 0.7840 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 21855/35720 Training loss: 0.7841 0.2163 sec/batch\n",
      "Epoch 13/20  Iteration 21856/35720 Training loss: 0.7841 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 21857/35720 Training loss: 0.7842 0.2212 sec/batch\n",
      "Epoch 13/20  Iteration 21858/35720 Training loss: 0.7840 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 21859/35720 Training loss: 0.7839 0.2188 sec/batch\n",
      "Epoch 13/20  Iteration 21860/35720 Training loss: 0.7839 0.2191 sec/batch\n",
      "Epoch 13/20  Iteration 21861/35720 Training loss: 0.7838 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 21862/35720 Training loss: 0.7839 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 21863/35720 Training loss: 0.7840 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 21864/35720 Training loss: 0.7840 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 21865/35720 Training loss: 0.7841 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 21866/35720 Training loss: 0.7842 0.2346 sec/batch\n",
      "Epoch 13/20  Iteration 21867/35720 Training loss: 0.7843 0.2128 sec/batch\n",
      "Epoch 13/20  Iteration 21868/35720 Training loss: 0.7842 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 21869/35720 Training loss: 0.7844 0.2134 sec/batch\n",
      "Epoch 13/20  Iteration 21870/35720 Training loss: 0.7845 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 21871/35720 Training loss: 0.7845 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 21872/35720 Training loss: 0.7845 0.2074 sec/batch\n",
      "Epoch 13/20  Iteration 21873/35720 Training loss: 0.7845 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 21874/35720 Training loss: 0.7847 0.2163 sec/batch\n",
      "Epoch 13/20  Iteration 21875/35720 Training loss: 0.7847 0.2334 sec/batch\n",
      "Epoch 13/20  Iteration 21876/35720 Training loss: 0.7847 0.2246 sec/batch\n",
      "Epoch 13/20  Iteration 21877/35720 Training loss: 0.7848 0.2232 sec/batch\n",
      "Epoch 13/20  Iteration 21878/35720 Training loss: 0.7848 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 21879/35720 Training loss: 0.7850 0.2056 sec/batch\n",
      "Epoch 13/20  Iteration 21880/35720 Training loss: 0.7853 0.2077 sec/batch\n",
      "Epoch 13/20  Iteration 21881/35720 Training loss: 0.7853 0.2204 sec/batch\n",
      "Epoch 13/20  Iteration 21882/35720 Training loss: 0.7853 0.2182 sec/batch\n",
      "Epoch 13/20  Iteration 21883/35720 Training loss: 0.7851 0.2182 sec/batch\n",
      "Epoch 13/20  Iteration 21884/35720 Training loss: 0.7851 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 21885/35720 Training loss: 0.7851 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 21886/35720 Training loss: 0.7852 0.2148 sec/batch\n",
      "Epoch 13/20  Iteration 21887/35720 Training loss: 0.7854 0.2289 sec/batch\n",
      "Epoch 13/20  Iteration 21888/35720 Training loss: 0.7855 0.2176 sec/batch\n",
      "Epoch 13/20  Iteration 21889/35720 Training loss: 0.7857 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21890/35720 Training loss: 0.7857 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 21891/35720 Training loss: 0.7856 0.2128 sec/batch\n",
      "Epoch 13/20  Iteration 21892/35720 Training loss: 0.7855 0.2165 sec/batch\n",
      "Epoch 13/20  Iteration 21893/35720 Training loss: 0.7855 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 21894/35720 Training loss: 0.7856 0.2151 sec/batch\n",
      "Epoch 13/20  Iteration 21895/35720 Training loss: 0.7857 0.2177 sec/batch\n",
      "Epoch 13/20  Iteration 21896/35720 Training loss: 0.7857 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 21897/35720 Training loss: 0.7857 0.2291 sec/batch\n",
      "Epoch 13/20  Iteration 21898/35720 Training loss: 0.7856 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 21899/35720 Training loss: 0.7856 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 21900/35720 Training loss: 0.7856 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 21901/35720 Training loss: 0.7856 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 21902/35720 Training loss: 0.7855 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 21903/35720 Training loss: 0.7855 0.2268 sec/batch\n",
      "Epoch 13/20  Iteration 21904/35720 Training loss: 0.7854 0.2241 sec/batch\n",
      "Epoch 13/20  Iteration 21905/35720 Training loss: 0.7855 0.2193 sec/batch\n",
      "Epoch 13/20  Iteration 21906/35720 Training loss: 0.7853 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 21907/35720 Training loss: 0.7854 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 21908/35720 Training loss: 0.7853 0.2252 sec/batch\n",
      "Epoch 13/20  Iteration 21909/35720 Training loss: 0.7854 0.2177 sec/batch\n",
      "Epoch 13/20  Iteration 21910/35720 Training loss: 0.7853 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 21911/35720 Training loss: 0.7853 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 21912/35720 Training loss: 0.7852 0.2070 sec/batch\n",
      "Epoch 13/20  Iteration 21913/35720 Training loss: 0.7851 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 21914/35720 Training loss: 0.7850 0.2356 sec/batch\n",
      "Epoch 13/20  Iteration 21915/35720 Training loss: 0.7850 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 21916/35720 Training loss: 0.7850 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 21917/35720 Training loss: 0.7850 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 21918/35720 Training loss: 0.7849 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 21919/35720 Training loss: 0.7849 0.2196 sec/batch\n",
      "Epoch 13/20  Iteration 21920/35720 Training loss: 0.7848 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 21921/35720 Training loss: 0.7848 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 21922/35720 Training loss: 0.7847 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 21923/35720 Training loss: 0.7846 0.2248 sec/batch\n",
      "Epoch 13/20  Iteration 21924/35720 Training loss: 0.7847 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 21925/35720 Training loss: 0.7846 0.2304 sec/batch\n",
      "Epoch 13/20  Iteration 21926/35720 Training loss: 0.7845 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 21927/35720 Training loss: 0.7845 0.2169 sec/batch\n",
      "Epoch 13/20  Iteration 21928/35720 Training loss: 0.7844 0.2143 sec/batch\n",
      "Epoch 13/20  Iteration 21929/35720 Training loss: 0.7844 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 21930/35720 Training loss: 0.7845 0.2184 sec/batch\n",
      "Epoch 13/20  Iteration 21931/35720 Training loss: 0.7844 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 21932/35720 Training loss: 0.7844 0.2269 sec/batch\n",
      "Epoch 13/20  Iteration 21933/35720 Training loss: 0.7843 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 21934/35720 Training loss: 0.7842 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 21935/35720 Training loss: 0.7841 0.2140 sec/batch\n",
      "Epoch 13/20  Iteration 21936/35720 Training loss: 0.7839 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 21937/35720 Training loss: 0.7839 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 21938/35720 Training loss: 0.7840 0.2155 sec/batch\n",
      "Epoch 13/20  Iteration 21939/35720 Training loss: 0.7839 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 21940/35720 Training loss: 0.7839 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 21941/35720 Training loss: 0.7840 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 21942/35720 Training loss: 0.7839 0.2151 sec/batch\n",
      "Epoch 13/20  Iteration 21943/35720 Training loss: 0.7838 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 21944/35720 Training loss: 0.7839 0.2195 sec/batch\n",
      "Epoch 13/20  Iteration 21945/35720 Training loss: 0.7839 0.2171 sec/batch\n",
      "Epoch 13/20  Iteration 21946/35720 Training loss: 0.7839 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 21947/35720 Training loss: 0.7839 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 21948/35720 Training loss: 0.7839 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 21949/35720 Training loss: 0.7839 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 21950/35720 Training loss: 0.7840 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 21951/35720 Training loss: 0.7840 0.2116 sec/batch\n",
      "Epoch 13/20  Iteration 21952/35720 Training loss: 0.7840 0.2180 sec/batch\n",
      "Epoch 13/20  Iteration 21953/35720 Training loss: 0.7840 0.2402 sec/batch\n",
      "Epoch 13/20  Iteration 21954/35720 Training loss: 0.7839 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 21955/35720 Training loss: 0.7839 0.2239 sec/batch\n",
      "Epoch 13/20  Iteration 21956/35720 Training loss: 0.7838 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 21957/35720 Training loss: 0.7839 0.2206 sec/batch\n",
      "Epoch 13/20  Iteration 21958/35720 Training loss: 0.7838 0.2148 sec/batch\n",
      "Epoch 13/20  Iteration 21959/35720 Training loss: 0.7838 0.2189 sec/batch\n",
      "Epoch 13/20  Iteration 21960/35720 Training loss: 0.7838 0.2156 sec/batch\n",
      "Epoch 13/20  Iteration 21961/35720 Training loss: 0.7839 0.2143 sec/batch\n",
      "Epoch 13/20  Iteration 21962/35720 Training loss: 0.7838 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 21963/35720 Training loss: 0.7838 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 21964/35720 Training loss: 0.7838 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 21965/35720 Training loss: 0.7838 0.2107 sec/batch\n",
      "Epoch 13/20  Iteration 21966/35720 Training loss: 0.7838 0.2228 sec/batch\n",
      "Epoch 13/20  Iteration 21967/35720 Training loss: 0.7837 0.2234 sec/batch\n",
      "Epoch 13/20  Iteration 21968/35720 Training loss: 0.7836 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 21969/35720 Training loss: 0.7835 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 21970/35720 Training loss: 0.7834 0.2249 sec/batch\n",
      "Epoch 13/20  Iteration 21971/35720 Training loss: 0.7834 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 21972/35720 Training loss: 0.7833 0.2260 sec/batch\n",
      "Epoch 13/20  Iteration 21973/35720 Training loss: 0.7833 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 21974/35720 Training loss: 0.7832 0.2146 sec/batch\n",
      "Epoch 13/20  Iteration 21975/35720 Training loss: 0.7831 0.2145 sec/batch\n",
      "Epoch 13/20  Iteration 21976/35720 Training loss: 0.7830 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 21977/35720 Training loss: 0.7831 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 21978/35720 Training loss: 0.7831 0.2179 sec/batch\n",
      "Epoch 13/20  Iteration 21979/35720 Training loss: 0.7831 0.2070 sec/batch\n",
      "Epoch 13/20  Iteration 21980/35720 Training loss: 0.7832 0.2081 sec/batch\n",
      "Epoch 13/20  Iteration 21981/35720 Training loss: 0.7831 0.2236 sec/batch\n",
      "Epoch 13/20  Iteration 21982/35720 Training loss: 0.7832 0.2222 sec/batch\n",
      "Epoch 13/20  Iteration 21983/35720 Training loss: 0.7831 0.2153 sec/batch\n",
      "Epoch 13/20  Iteration 21984/35720 Training loss: 0.7831 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 21985/35720 Training loss: 0.7830 0.2143 sec/batch\n",
      "Epoch 13/20  Iteration 21986/35720 Training loss: 0.7830 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 21987/35720 Training loss: 0.7831 0.2203 sec/batch\n",
      "Epoch 13/20  Iteration 21988/35720 Training loss: 0.7832 0.2158 sec/batch\n",
      "Epoch 13/20  Iteration 21989/35720 Training loss: 0.7831 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 21990/35720 Training loss: 0.7832 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 21991/35720 Training loss: 0.7832 0.2199 sec/batch\n",
      "Epoch 13/20  Iteration 21992/35720 Training loss: 0.7831 0.2211 sec/batch\n",
      "Epoch 13/20  Iteration 21993/35720 Training loss: 0.7832 0.2223 sec/batch\n",
      "Epoch 13/20  Iteration 21994/35720 Training loss: 0.7832 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 21995/35720 Training loss: 0.7831 0.2120 sec/batch\n",
      "Epoch 13/20  Iteration 21996/35720 Training loss: 0.7832 0.2082 sec/batch\n",
      "Epoch 13/20  Iteration 21997/35720 Training loss: 0.7830 0.2074 sec/batch\n",
      "Epoch 13/20  Iteration 21998/35720 Training loss: 0.7830 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 21999/35720 Training loss: 0.7830 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 22000/35720 Training loss: 0.7829 0.2253 sec/batch\n",
      "Validation loss: 1.4708 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 22001/35720 Training loss: 0.7836 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 22002/35720 Training loss: 0.7835 0.2302 sec/batch\n",
      "Epoch 13/20  Iteration 22003/35720 Training loss: 0.7835 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 22004/35720 Training loss: 0.7835 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22005/35720 Training loss: 0.7836 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 22006/35720 Training loss: 0.7836 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 22007/35720 Training loss: 0.7836 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22008/35720 Training loss: 0.7836 0.2313 sec/batch\n",
      "Epoch 13/20  Iteration 22009/35720 Training loss: 0.7837 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 22010/35720 Training loss: 0.7837 0.2280 sec/batch\n",
      "Epoch 13/20  Iteration 22011/35720 Training loss: 0.7837 0.2052 sec/batch\n",
      "Epoch 13/20  Iteration 22012/35720 Training loss: 0.7837 0.2084 sec/batch\n",
      "Epoch 13/20  Iteration 22013/35720 Training loss: 0.7837 0.2151 sec/batch\n",
      "Epoch 13/20  Iteration 22014/35720 Training loss: 0.7837 0.2157 sec/batch\n",
      "Epoch 13/20  Iteration 22015/35720 Training loss: 0.7837 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 22016/35720 Training loss: 0.7837 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 22017/35720 Training loss: 0.7836 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 22018/35720 Training loss: 0.7835 0.2144 sec/batch\n",
      "Epoch 13/20  Iteration 22019/35720 Training loss: 0.7834 0.2338 sec/batch\n",
      "Epoch 13/20  Iteration 22020/35720 Training loss: 0.7833 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22021/35720 Training loss: 0.7832 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22022/35720 Training loss: 0.7831 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22023/35720 Training loss: 0.7831 0.2055 sec/batch\n",
      "Epoch 13/20  Iteration 22024/35720 Training loss: 0.7831 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 22025/35720 Training loss: 0.7831 0.2158 sec/batch\n",
      "Epoch 13/20  Iteration 22026/35720 Training loss: 0.7831 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22027/35720 Training loss: 0.7830 0.2156 sec/batch\n",
      "Epoch 13/20  Iteration 22028/35720 Training loss: 0.7830 0.2064 sec/batch\n",
      "Epoch 13/20  Iteration 22029/35720 Training loss: 0.7829 0.2210 sec/batch\n",
      "Epoch 13/20  Iteration 22030/35720 Training loss: 0.7829 0.2266 sec/batch\n",
      "Epoch 13/20  Iteration 22031/35720 Training loss: 0.7827 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 22032/35720 Training loss: 0.7826 0.2253 sec/batch\n",
      "Epoch 13/20  Iteration 22033/35720 Training loss: 0.7825 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22034/35720 Training loss: 0.7824 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 22035/35720 Training loss: 0.7823 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22036/35720 Training loss: 0.7823 0.2228 sec/batch\n",
      "Epoch 13/20  Iteration 22037/35720 Training loss: 0.7823 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 22038/35720 Training loss: 0.7824 0.2158 sec/batch\n",
      "Epoch 13/20  Iteration 22039/35720 Training loss: 0.7823 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 22040/35720 Training loss: 0.7823 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22041/35720 Training loss: 0.7824 0.2200 sec/batch\n",
      "Epoch 13/20  Iteration 22042/35720 Training loss: 0.7823 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 22043/35720 Training loss: 0.7822 0.2216 sec/batch\n",
      "Epoch 13/20  Iteration 22044/35720 Training loss: 0.7821 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 22045/35720 Training loss: 0.7821 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22046/35720 Training loss: 0.7820 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 22047/35720 Training loss: 0.7820 0.2373 sec/batch\n",
      "Epoch 13/20  Iteration 22048/35720 Training loss: 0.7819 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 22049/35720 Training loss: 0.7819 0.2255 sec/batch\n",
      "Epoch 13/20  Iteration 22050/35720 Training loss: 0.7818 0.2103 sec/batch\n",
      "Epoch 13/20  Iteration 22051/35720 Training loss: 0.7818 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 22052/35720 Training loss: 0.7817 0.2278 sec/batch\n",
      "Epoch 13/20  Iteration 22053/35720 Training loss: 0.7817 0.2300 sec/batch\n",
      "Epoch 13/20  Iteration 22054/35720 Training loss: 0.7818 0.2232 sec/batch\n",
      "Epoch 13/20  Iteration 22055/35720 Training loss: 0.7817 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 22056/35720 Training loss: 0.7816 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 22057/35720 Training loss: 0.7815 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 22058/35720 Training loss: 0.7815 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 22059/35720 Training loss: 0.7815 0.2236 sec/batch\n",
      "Epoch 13/20  Iteration 22060/35720 Training loss: 0.7814 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 22061/35720 Training loss: 0.7814 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 22062/35720 Training loss: 0.7814 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 22063/35720 Training loss: 0.7814 0.2077 sec/batch\n",
      "Epoch 13/20  Iteration 22064/35720 Training loss: 0.7813 0.2279 sec/batch\n",
      "Epoch 13/20  Iteration 22065/35720 Training loss: 0.7813 0.2178 sec/batch\n",
      "Epoch 13/20  Iteration 22066/35720 Training loss: 0.7812 0.2175 sec/batch\n",
      "Epoch 13/20  Iteration 22067/35720 Training loss: 0.7812 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22068/35720 Training loss: 0.7812 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22069/35720 Training loss: 0.7811 0.2211 sec/batch\n",
      "Epoch 13/20  Iteration 22070/35720 Training loss: 0.7811 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 22071/35720 Training loss: 0.7811 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 22072/35720 Training loss: 0.7811 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22073/35720 Training loss: 0.7812 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 22074/35720 Training loss: 0.7812 0.2242 sec/batch\n",
      "Epoch 13/20  Iteration 22075/35720 Training loss: 0.7812 0.2148 sec/batch\n",
      "Epoch 13/20  Iteration 22076/35720 Training loss: 0.7811 0.2225 sec/batch\n",
      "Epoch 13/20  Iteration 22077/35720 Training loss: 0.7811 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 22078/35720 Training loss: 0.7811 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22079/35720 Training loss: 0.7810 0.2213 sec/batch\n",
      "Epoch 13/20  Iteration 22080/35720 Training loss: 0.7810 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 22081/35720 Training loss: 0.7809 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 22082/35720 Training loss: 0.7808 0.2260 sec/batch\n",
      "Epoch 13/20  Iteration 22083/35720 Training loss: 0.7809 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22084/35720 Training loss: 0.7809 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 22085/35720 Training loss: 0.7808 0.2182 sec/batch\n",
      "Epoch 13/20  Iteration 22086/35720 Training loss: 0.7809 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 22087/35720 Training loss: 0.7809 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 22088/35720 Training loss: 0.7809 0.2153 sec/batch\n",
      "Epoch 13/20  Iteration 22089/35720 Training loss: 0.7810 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 22090/35720 Training loss: 0.7811 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 22091/35720 Training loss: 0.7811 0.2153 sec/batch\n",
      "Epoch 13/20  Iteration 22092/35720 Training loss: 0.7811 0.2305 sec/batch\n",
      "Epoch 13/20  Iteration 22093/35720 Training loss: 0.7811 0.2209 sec/batch\n",
      "Epoch 13/20  Iteration 22094/35720 Training loss: 0.7811 0.2116 sec/batch\n",
      "Epoch 13/20  Iteration 22095/35720 Training loss: 0.7811 0.2143 sec/batch\n",
      "Epoch 13/20  Iteration 22096/35720 Training loss: 0.7811 0.2215 sec/batch\n",
      "Epoch 13/20  Iteration 22097/35720 Training loss: 0.7811 0.2214 sec/batch\n",
      "Epoch 13/20  Iteration 22098/35720 Training loss: 0.7811 0.2227 sec/batch\n",
      "Epoch 13/20  Iteration 22099/35720 Training loss: 0.7812 0.2189 sec/batch\n",
      "Epoch 13/20  Iteration 22100/35720 Training loss: 0.7812 0.2203 sec/batch\n",
      "Epoch 13/20  Iteration 22101/35720 Training loss: 0.7811 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 22102/35720 Training loss: 0.7811 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 22103/35720 Training loss: 0.7809 0.2141 sec/batch\n",
      "Epoch 13/20  Iteration 22104/35720 Training loss: 0.7809 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 22105/35720 Training loss: 0.7809 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 22106/35720 Training loss: 0.7808 0.2273 sec/batch\n",
      "Epoch 13/20  Iteration 22107/35720 Training loss: 0.7808 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 22108/35720 Training loss: 0.7808 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22109/35720 Training loss: 0.7808 0.2176 sec/batch\n",
      "Epoch 13/20  Iteration 22110/35720 Training loss: 0.7807 0.2185 sec/batch\n",
      "Epoch 13/20  Iteration 22111/35720 Training loss: 0.7808 0.2534 sec/batch\n",
      "Epoch 13/20  Iteration 22112/35720 Training loss: 0.7807 0.2765 sec/batch\n",
      "Epoch 13/20  Iteration 22113/35720 Training loss: 0.7807 0.2156 sec/batch\n",
      "Epoch 13/20  Iteration 22114/35720 Training loss: 0.7806 0.2175 sec/batch\n",
      "Epoch 13/20  Iteration 22115/35720 Training loss: 0.7806 0.2169 sec/batch\n",
      "Epoch 13/20  Iteration 22116/35720 Training loss: 0.7806 0.2055 sec/batch\n",
      "Epoch 13/20  Iteration 22117/35720 Training loss: 0.7806 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22118/35720 Training loss: 0.7806 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22119/35720 Training loss: 0.7805 0.2120 sec/batch\n",
      "Epoch 13/20  Iteration 22120/35720 Training loss: 0.7805 0.2298 sec/batch\n",
      "Epoch 13/20  Iteration 22121/35720 Training loss: 0.7805 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 22122/35720 Training loss: 0.7805 0.2070 sec/batch\n",
      "Epoch 13/20  Iteration 22123/35720 Training loss: 0.7805 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 22124/35720 Training loss: 0.7806 0.2141 sec/batch\n",
      "Epoch 13/20  Iteration 22125/35720 Training loss: 0.7806 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22126/35720 Training loss: 0.7806 0.2238 sec/batch\n",
      "Epoch 13/20  Iteration 22127/35720 Training loss: 0.7806 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 22128/35720 Training loss: 0.7806 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 22129/35720 Training loss: 0.7806 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22130/35720 Training loss: 0.7806 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 22131/35720 Training loss: 0.7806 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 22132/35720 Training loss: 0.7805 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 22133/35720 Training loss: 0.7804 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 22134/35720 Training loss: 0.7805 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 22135/35720 Training loss: 0.7805 0.2284 sec/batch\n",
      "Epoch 13/20  Iteration 22136/35720 Training loss: 0.7805 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 22137/35720 Training loss: 0.7805 0.2258 sec/batch\n",
      "Epoch 13/20  Iteration 22138/35720 Training loss: 0.7805 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 22139/35720 Training loss: 0.7805 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 22140/35720 Training loss: 0.7806 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 22141/35720 Training loss: 0.7806 0.2236 sec/batch\n",
      "Epoch 13/20  Iteration 22142/35720 Training loss: 0.7807 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 22143/35720 Training loss: 0.7808 0.2275 sec/batch\n",
      "Epoch 13/20  Iteration 22144/35720 Training loss: 0.7808 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22145/35720 Training loss: 0.7808 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 22146/35720 Training loss: 0.7808 0.2271 sec/batch\n",
      "Epoch 13/20  Iteration 22147/35720 Training loss: 0.7809 0.2263 sec/batch\n",
      "Epoch 13/20  Iteration 22148/35720 Training loss: 0.7809 0.2302 sec/batch\n",
      "Epoch 13/20  Iteration 22149/35720 Training loss: 0.7810 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22150/35720 Training loss: 0.7810 0.2057 sec/batch\n",
      "Epoch 13/20  Iteration 22151/35720 Training loss: 0.7809 0.2171 sec/batch\n",
      "Epoch 13/20  Iteration 22152/35720 Training loss: 0.7810 0.2253 sec/batch\n",
      "Epoch 13/20  Iteration 22153/35720 Training loss: 0.7810 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 22154/35720 Training loss: 0.7811 0.2127 sec/batch\n",
      "Epoch 13/20  Iteration 22155/35720 Training loss: 0.7811 0.2296 sec/batch\n",
      "Epoch 13/20  Iteration 22156/35720 Training loss: 0.7810 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 22157/35720 Training loss: 0.7810 0.2249 sec/batch\n",
      "Epoch 13/20  Iteration 22158/35720 Training loss: 0.7810 0.2084 sec/batch\n",
      "Epoch 13/20  Iteration 22159/35720 Training loss: 0.7812 0.2328 sec/batch\n",
      "Epoch 13/20  Iteration 22160/35720 Training loss: 0.7812 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22161/35720 Training loss: 0.7813 0.2202 sec/batch\n",
      "Epoch 13/20  Iteration 22162/35720 Training loss: 0.7813 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 22163/35720 Training loss: 0.7812 0.2303 sec/batch\n",
      "Epoch 13/20  Iteration 22164/35720 Training loss: 0.7812 0.2271 sec/batch\n",
      "Epoch 13/20  Iteration 22165/35720 Training loss: 0.7812 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 22166/35720 Training loss: 0.7812 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22167/35720 Training loss: 0.7812 0.2176 sec/batch\n",
      "Epoch 13/20  Iteration 22168/35720 Training loss: 0.7812 0.2166 sec/batch\n",
      "Epoch 13/20  Iteration 22169/35720 Training loss: 0.7813 0.2203 sec/batch\n",
      "Epoch 13/20  Iteration 22170/35720 Training loss: 0.7812 0.2353 sec/batch\n",
      "Epoch 13/20  Iteration 22171/35720 Training loss: 0.7812 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 22172/35720 Training loss: 0.7813 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 22173/35720 Training loss: 0.7813 0.2121 sec/batch\n",
      "Epoch 13/20  Iteration 22174/35720 Training loss: 0.7813 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 22175/35720 Training loss: 0.7812 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22176/35720 Training loss: 0.7812 0.2226 sec/batch\n",
      "Epoch 13/20  Iteration 22177/35720 Training loss: 0.7812 0.2157 sec/batch\n",
      "Epoch 13/20  Iteration 22178/35720 Training loss: 0.7812 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22179/35720 Training loss: 0.7811 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 22180/35720 Training loss: 0.7811 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 22181/35720 Training loss: 0.7810 0.2177 sec/batch\n",
      "Epoch 13/20  Iteration 22182/35720 Training loss: 0.7810 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 22183/35720 Training loss: 0.7810 0.2183 sec/batch\n",
      "Epoch 13/20  Iteration 22184/35720 Training loss: 0.7811 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22185/35720 Training loss: 0.7810 0.2165 sec/batch\n",
      "Epoch 13/20  Iteration 22186/35720 Training loss: 0.7809 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22187/35720 Training loss: 0.7810 0.2169 sec/batch\n",
      "Epoch 13/20  Iteration 22188/35720 Training loss: 0.7809 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 22189/35720 Training loss: 0.7809 0.2143 sec/batch\n",
      "Epoch 13/20  Iteration 22190/35720 Training loss: 0.7809 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 22191/35720 Training loss: 0.7809 0.2187 sec/batch\n",
      "Epoch 13/20  Iteration 22192/35720 Training loss: 0.7810 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 22193/35720 Training loss: 0.7810 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 22194/35720 Training loss: 0.7810 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 22195/35720 Training loss: 0.7810 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 22196/35720 Training loss: 0.7809 0.2186 sec/batch\n",
      "Epoch 13/20  Iteration 22197/35720 Training loss: 0.7809 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 22198/35720 Training loss: 0.7810 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22199/35720 Training loss: 0.7810 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 22200/35720 Training loss: 0.7810 0.2078 sec/batch\n",
      "Validation loss: 1.4725 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 22201/35720 Training loss: 0.7815 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22202/35720 Training loss: 0.7816 0.2203 sec/batch\n",
      "Epoch 13/20  Iteration 22203/35720 Training loss: 0.7816 0.2163 sec/batch\n",
      "Epoch 13/20  Iteration 22204/35720 Training loss: 0.7817 0.2118 sec/batch\n",
      "Epoch 13/20  Iteration 22205/35720 Training loss: 0.7817 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 22206/35720 Training loss: 0.7817 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22207/35720 Training loss: 0.7816 0.2187 sec/batch\n",
      "Epoch 13/20  Iteration 22208/35720 Training loss: 0.7815 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 22209/35720 Training loss: 0.7815 0.2283 sec/batch\n",
      "Epoch 13/20  Iteration 22210/35720 Training loss: 0.7815 0.2324 sec/batch\n",
      "Epoch 13/20  Iteration 22211/35720 Training loss: 0.7815 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 22212/35720 Training loss: 0.7815 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 22213/35720 Training loss: 0.7815 0.2193 sec/batch\n",
      "Epoch 13/20  Iteration 22214/35720 Training loss: 0.7815 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 22215/35720 Training loss: 0.7815 0.2165 sec/batch\n",
      "Epoch 13/20  Iteration 22216/35720 Training loss: 0.7815 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 22217/35720 Training loss: 0.7814 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 22218/35720 Training loss: 0.7814 0.2239 sec/batch\n",
      "Epoch 13/20  Iteration 22219/35720 Training loss: 0.7815 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22220/35720 Training loss: 0.7814 0.2192 sec/batch\n",
      "Epoch 13/20  Iteration 22221/35720 Training loss: 0.7815 0.2243 sec/batch\n",
      "Epoch 13/20  Iteration 22222/35720 Training loss: 0.7815 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 22223/35720 Training loss: 0.7816 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22224/35720 Training loss: 0.7816 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22225/35720 Training loss: 0.7816 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22226/35720 Training loss: 0.7815 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 22227/35720 Training loss: 0.7815 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 22228/35720 Training loss: 0.7815 0.2888 sec/batch\n",
      "Epoch 13/20  Iteration 22229/35720 Training loss: 0.7815 0.2152 sec/batch\n",
      "Epoch 13/20  Iteration 22230/35720 Training loss: 0.7815 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22231/35720 Training loss: 0.7815 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 22232/35720 Training loss: 0.7816 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 22233/35720 Training loss: 0.7816 0.2103 sec/batch\n",
      "Epoch 13/20  Iteration 22234/35720 Training loss: 0.7816 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 22235/35720 Training loss: 0.7816 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 22236/35720 Training loss: 0.7817 0.2174 sec/batch\n",
      "Epoch 13/20  Iteration 22237/35720 Training loss: 0.7817 0.2120 sec/batch\n",
      "Epoch 13/20  Iteration 22238/35720 Training loss: 0.7817 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 22239/35720 Training loss: 0.7817 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22240/35720 Training loss: 0.7817 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 22241/35720 Training loss: 0.7817 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22242/35720 Training loss: 0.7818 0.2179 sec/batch\n",
      "Epoch 13/20  Iteration 22243/35720 Training loss: 0.7818 0.2231 sec/batch\n",
      "Epoch 13/20  Iteration 22244/35720 Training loss: 0.7818 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22245/35720 Training loss: 0.7818 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22246/35720 Training loss: 0.7818 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 22247/35720 Training loss: 0.7819 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 22248/35720 Training loss: 0.7819 0.2182 sec/batch\n",
      "Epoch 13/20  Iteration 22249/35720 Training loss: 0.7819 0.2174 sec/batch\n",
      "Epoch 13/20  Iteration 22250/35720 Training loss: 0.7819 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22251/35720 Training loss: 0.7819 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22252/35720 Training loss: 0.7819 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 22253/35720 Training loss: 0.7819 0.2258 sec/batch\n",
      "Epoch 13/20  Iteration 22254/35720 Training loss: 0.7818 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22255/35720 Training loss: 0.7818 0.2179 sec/batch\n",
      "Epoch 13/20  Iteration 22256/35720 Training loss: 0.7817 0.2118 sec/batch\n",
      "Epoch 13/20  Iteration 22257/35720 Training loss: 0.7816 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 22258/35720 Training loss: 0.7816 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 22259/35720 Training loss: 0.7816 0.2234 sec/batch\n",
      "Epoch 13/20  Iteration 22260/35720 Training loss: 0.7815 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 22261/35720 Training loss: 0.7815 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 22262/35720 Training loss: 0.7814 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 22263/35720 Training loss: 0.7814 0.2196 sec/batch\n",
      "Epoch 13/20  Iteration 22264/35720 Training loss: 0.7814 0.2257 sec/batch\n",
      "Epoch 13/20  Iteration 22265/35720 Training loss: 0.7814 0.2281 sec/batch\n",
      "Epoch 13/20  Iteration 22266/35720 Training loss: 0.7815 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22267/35720 Training loss: 0.7814 0.2231 sec/batch\n",
      "Epoch 13/20  Iteration 22268/35720 Training loss: 0.7813 0.2222 sec/batch\n",
      "Epoch 13/20  Iteration 22269/35720 Training loss: 0.7813 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22270/35720 Training loss: 0.7813 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 22271/35720 Training loss: 0.7814 0.2261 sec/batch\n",
      "Epoch 13/20  Iteration 22272/35720 Training loss: 0.7814 0.2194 sec/batch\n",
      "Epoch 13/20  Iteration 22273/35720 Training loss: 0.7814 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 22274/35720 Training loss: 0.7814 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 22275/35720 Training loss: 0.7814 0.2261 sec/batch\n",
      "Epoch 13/20  Iteration 22276/35720 Training loss: 0.7814 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 22277/35720 Training loss: 0.7814 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 22278/35720 Training loss: 0.7814 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22279/35720 Training loss: 0.7813 0.2264 sec/batch\n",
      "Epoch 13/20  Iteration 22280/35720 Training loss: 0.7814 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 22281/35720 Training loss: 0.7813 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 22282/35720 Training loss: 0.7813 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 22283/35720 Training loss: 0.7813 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 22284/35720 Training loss: 0.7813 0.2237 sec/batch\n",
      "Epoch 13/20  Iteration 22285/35720 Training loss: 0.7812 0.2290 sec/batch\n",
      "Epoch 13/20  Iteration 22286/35720 Training loss: 0.7812 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 22287/35720 Training loss: 0.7812 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 22288/35720 Training loss: 0.7812 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 22289/35720 Training loss: 0.7811 0.2161 sec/batch\n",
      "Epoch 13/20  Iteration 22290/35720 Training loss: 0.7811 0.2272 sec/batch\n",
      "Epoch 13/20  Iteration 22291/35720 Training loss: 0.7810 0.2261 sec/batch\n",
      "Epoch 13/20  Iteration 22292/35720 Training loss: 0.7810 0.2245 sec/batch\n",
      "Epoch 13/20  Iteration 22293/35720 Training loss: 0.7810 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 22294/35720 Training loss: 0.7810 0.2064 sec/batch\n",
      "Epoch 13/20  Iteration 22295/35720 Training loss: 0.7810 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 22296/35720 Training loss: 0.7809 0.2261 sec/batch\n",
      "Epoch 13/20  Iteration 22297/35720 Training loss: 0.7809 0.2210 sec/batch\n",
      "Epoch 13/20  Iteration 22298/35720 Training loss: 0.7809 0.2154 sec/batch\n",
      "Epoch 13/20  Iteration 22299/35720 Training loss: 0.7810 0.2055 sec/batch\n",
      "Epoch 13/20  Iteration 22300/35720 Training loss: 0.7809 0.2148 sec/batch\n",
      "Epoch 13/20  Iteration 22301/35720 Training loss: 0.7809 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22302/35720 Training loss: 0.7809 0.2161 sec/batch\n",
      "Epoch 13/20  Iteration 22303/35720 Training loss: 0.7809 0.2379 sec/batch\n",
      "Epoch 13/20  Iteration 22304/35720 Training loss: 0.7808 0.2141 sec/batch\n",
      "Epoch 13/20  Iteration 22305/35720 Training loss: 0.7808 0.2058 sec/batch\n",
      "Epoch 13/20  Iteration 22306/35720 Training loss: 0.7808 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22307/35720 Training loss: 0.7808 0.2153 sec/batch\n",
      "Epoch 13/20  Iteration 22308/35720 Training loss: 0.7807 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 22309/35720 Training loss: 0.7807 0.2054 sec/batch\n",
      "Epoch 13/20  Iteration 22310/35720 Training loss: 0.7807 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22311/35720 Training loss: 0.7807 0.2177 sec/batch\n",
      "Epoch 13/20  Iteration 22312/35720 Training loss: 0.7806 0.2197 sec/batch\n",
      "Epoch 13/20  Iteration 22313/35720 Training loss: 0.7806 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22314/35720 Training loss: 0.7805 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 22315/35720 Training loss: 0.7806 0.2215 sec/batch\n",
      "Epoch 13/20  Iteration 22316/35720 Training loss: 0.7806 0.2185 sec/batch\n",
      "Epoch 13/20  Iteration 22317/35720 Training loss: 0.7806 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 22318/35720 Training loss: 0.7805 0.2253 sec/batch\n",
      "Epoch 13/20  Iteration 22319/35720 Training loss: 0.7805 0.2224 sec/batch\n",
      "Epoch 13/20  Iteration 22320/35720 Training loss: 0.7805 0.2291 sec/batch\n",
      "Epoch 13/20  Iteration 22321/35720 Training loss: 0.7804 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 22322/35720 Training loss: 0.7803 0.2222 sec/batch\n",
      "Epoch 13/20  Iteration 22323/35720 Training loss: 0.7803 0.2337 sec/batch\n",
      "Epoch 13/20  Iteration 22324/35720 Training loss: 0.7802 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22325/35720 Training loss: 0.7801 0.2174 sec/batch\n",
      "Epoch 13/20  Iteration 22326/35720 Training loss: 0.7801 0.2184 sec/batch\n",
      "Epoch 13/20  Iteration 22327/35720 Training loss: 0.7801 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22328/35720 Training loss: 0.7800 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22329/35720 Training loss: 0.7799 0.2296 sec/batch\n",
      "Epoch 13/20  Iteration 22330/35720 Training loss: 0.7798 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22331/35720 Training loss: 0.7797 0.2290 sec/batch\n",
      "Epoch 13/20  Iteration 22332/35720 Training loss: 0.7797 0.2236 sec/batch\n",
      "Epoch 13/20  Iteration 22333/35720 Training loss: 0.7796 0.2139 sec/batch\n",
      "Epoch 13/20  Iteration 22334/35720 Training loss: 0.7796 0.2139 sec/batch\n",
      "Epoch 13/20  Iteration 22335/35720 Training loss: 0.7795 0.2271 sec/batch\n",
      "Epoch 13/20  Iteration 22336/35720 Training loss: 0.7794 0.2696 sec/batch\n",
      "Epoch 13/20  Iteration 22337/35720 Training loss: 0.7794 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 22338/35720 Training loss: 0.7794 0.2116 sec/batch\n",
      "Epoch 13/20  Iteration 22339/35720 Training loss: 0.7793 0.2270 sec/batch\n",
      "Epoch 13/20  Iteration 22340/35720 Training loss: 0.7793 0.2379 sec/batch\n",
      "Epoch 13/20  Iteration 22341/35720 Training loss: 0.7793 0.2212 sec/batch\n",
      "Epoch 13/20  Iteration 22342/35720 Training loss: 0.7792 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22343/35720 Training loss: 0.7792 0.2057 sec/batch\n",
      "Epoch 13/20  Iteration 22344/35720 Training loss: 0.7792 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 22345/35720 Training loss: 0.7792 0.2153 sec/batch\n",
      "Epoch 13/20  Iteration 22346/35720 Training loss: 0.7792 0.2145 sec/batch\n",
      "Epoch 13/20  Iteration 22347/35720 Training loss: 0.7793 0.2203 sec/batch\n",
      "Epoch 13/20  Iteration 22348/35720 Training loss: 0.7793 0.2118 sec/batch\n",
      "Epoch 13/20  Iteration 22349/35720 Training loss: 0.7793 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 22350/35720 Training loss: 0.7793 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22351/35720 Training loss: 0.7792 0.2138 sec/batch\n",
      "Epoch 13/20  Iteration 22352/35720 Training loss: 0.7793 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22353/35720 Training loss: 0.7792 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 22354/35720 Training loss: 0.7792 0.2123 sec/batch\n",
      "Epoch 13/20  Iteration 22355/35720 Training loss: 0.7792 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 22356/35720 Training loss: 0.7792 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22357/35720 Training loss: 0.7792 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22358/35720 Training loss: 0.7792 0.2260 sec/batch\n",
      "Epoch 13/20  Iteration 22359/35720 Training loss: 0.7792 0.2077 sec/batch\n",
      "Epoch 13/20  Iteration 22360/35720 Training loss: 0.7791 0.2079 sec/batch\n",
      "Epoch 13/20  Iteration 22361/35720 Training loss: 0.7792 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22362/35720 Training loss: 0.7792 0.2203 sec/batch\n",
      "Epoch 13/20  Iteration 22363/35720 Training loss: 0.7792 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22364/35720 Training loss: 0.7792 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 22365/35720 Training loss: 0.7792 0.2110 sec/batch\n",
      "Epoch 13/20  Iteration 22366/35720 Training loss: 0.7792 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22367/35720 Training loss: 0.7791 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 22368/35720 Training loss: 0.7790 0.2179 sec/batch\n",
      "Epoch 13/20  Iteration 22369/35720 Training loss: 0.7790 0.2186 sec/batch\n",
      "Epoch 13/20  Iteration 22370/35720 Training loss: 0.7789 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 22371/35720 Training loss: 0.7789 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22372/35720 Training loss: 0.7788 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22373/35720 Training loss: 0.7788 0.2376 sec/batch\n",
      "Epoch 13/20  Iteration 22374/35720 Training loss: 0.7788 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22375/35720 Training loss: 0.7787 0.2210 sec/batch\n",
      "Epoch 13/20  Iteration 22376/35720 Training loss: 0.7788 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22377/35720 Training loss: 0.7787 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22378/35720 Training loss: 0.7787 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 22379/35720 Training loss: 0.7787 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 22380/35720 Training loss: 0.7786 0.2175 sec/batch\n",
      "Epoch 13/20  Iteration 22381/35720 Training loss: 0.7786 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22382/35720 Training loss: 0.7786 0.2161 sec/batch\n",
      "Epoch 13/20  Iteration 22383/35720 Training loss: 0.7785 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22384/35720 Training loss: 0.7785 0.2282 sec/batch\n",
      "Epoch 13/20  Iteration 22385/35720 Training loss: 0.7784 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22386/35720 Training loss: 0.7784 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22387/35720 Training loss: 0.7784 0.2064 sec/batch\n",
      "Epoch 13/20  Iteration 22388/35720 Training loss: 0.7784 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22389/35720 Training loss: 0.7783 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22390/35720 Training loss: 0.7783 0.2329 sec/batch\n",
      "Epoch 13/20  Iteration 22391/35720 Training loss: 0.7783 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22392/35720 Training loss: 0.7782 0.2243 sec/batch\n",
      "Epoch 13/20  Iteration 22393/35720 Training loss: 0.7781 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22394/35720 Training loss: 0.7781 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 22395/35720 Training loss: 0.7781 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22396/35720 Training loss: 0.7781 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 22397/35720 Training loss: 0.7780 0.2082 sec/batch\n",
      "Epoch 13/20  Iteration 22398/35720 Training loss: 0.7780 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 22399/35720 Training loss: 0.7780 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 22400/35720 Training loss: 0.7779 0.2188 sec/batch\n",
      "Validation loss: 1.49739 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 22401/35720 Training loss: 0.7784 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 22402/35720 Training loss: 0.7785 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 22403/35720 Training loss: 0.7785 0.2153 sec/batch\n",
      "Epoch 13/20  Iteration 22404/35720 Training loss: 0.7784 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 22405/35720 Training loss: 0.7784 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22406/35720 Training loss: 0.7783 0.2241 sec/batch\n",
      "Epoch 13/20  Iteration 22407/35720 Training loss: 0.7782 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 22408/35720 Training loss: 0.7782 0.2192 sec/batch\n",
      "Epoch 13/20  Iteration 22409/35720 Training loss: 0.7782 0.2252 sec/batch\n",
      "Epoch 13/20  Iteration 22410/35720 Training loss: 0.7782 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 22411/35720 Training loss: 0.7782 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22412/35720 Training loss: 0.7781 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 22413/35720 Training loss: 0.7781 0.2346 sec/batch\n",
      "Epoch 13/20  Iteration 22414/35720 Training loss: 0.7780 0.2139 sec/batch\n",
      "Epoch 13/20  Iteration 22415/35720 Training loss: 0.7780 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 22416/35720 Training loss: 0.7779 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 22417/35720 Training loss: 0.7779 0.2183 sec/batch\n",
      "Epoch 13/20  Iteration 22418/35720 Training loss: 0.7779 0.2081 sec/batch\n",
      "Epoch 13/20  Iteration 22419/35720 Training loss: 0.7779 0.2239 sec/batch\n",
      "Epoch 13/20  Iteration 22420/35720 Training loss: 0.7778 0.2211 sec/batch\n",
      "Epoch 13/20  Iteration 22421/35720 Training loss: 0.7777 0.2156 sec/batch\n",
      "Epoch 13/20  Iteration 22422/35720 Training loss: 0.7777 0.2175 sec/batch\n",
      "Epoch 13/20  Iteration 22423/35720 Training loss: 0.7777 0.2153 sec/batch\n",
      "Epoch 13/20  Iteration 22424/35720 Training loss: 0.7777 0.2241 sec/batch\n",
      "Epoch 13/20  Iteration 22425/35720 Training loss: 0.7777 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 22426/35720 Training loss: 0.7777 0.2178 sec/batch\n",
      "Epoch 13/20  Iteration 22427/35720 Training loss: 0.7778 0.2179 sec/batch\n",
      "Epoch 13/20  Iteration 22428/35720 Training loss: 0.7778 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 22429/35720 Training loss: 0.7778 0.2123 sec/batch\n",
      "Epoch 13/20  Iteration 22430/35720 Training loss: 0.7779 0.2264 sec/batch\n",
      "Epoch 13/20  Iteration 22431/35720 Training loss: 0.7778 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22432/35720 Training loss: 0.7778 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22433/35720 Training loss: 0.7777 0.2215 sec/batch\n",
      "Epoch 13/20  Iteration 22434/35720 Training loss: 0.7777 0.2369 sec/batch\n",
      "Epoch 13/20  Iteration 22435/35720 Training loss: 0.7776 0.2182 sec/batch\n",
      "Epoch 13/20  Iteration 22436/35720 Training loss: 0.7775 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 22437/35720 Training loss: 0.7774 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 22438/35720 Training loss: 0.7774 0.2107 sec/batch\n",
      "Epoch 13/20  Iteration 22439/35720 Training loss: 0.7774 0.2176 sec/batch\n",
      "Epoch 13/20  Iteration 22440/35720 Training loss: 0.7773 0.2172 sec/batch\n",
      "Epoch 13/20  Iteration 22441/35720 Training loss: 0.7772 0.2365 sec/batch\n",
      "Epoch 13/20  Iteration 22442/35720 Training loss: 0.7772 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 22443/35720 Training loss: 0.7772 0.2140 sec/batch\n",
      "Epoch 13/20  Iteration 22444/35720 Training loss: 0.7772 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 22445/35720 Training loss: 0.7771 0.2256 sec/batch\n",
      "Epoch 13/20  Iteration 22446/35720 Training loss: 0.7771 0.2278 sec/batch\n",
      "Epoch 13/20  Iteration 22447/35720 Training loss: 0.7771 0.2262 sec/batch\n",
      "Epoch 13/20  Iteration 22448/35720 Training loss: 0.7772 0.2276 sec/batch\n",
      "Epoch 13/20  Iteration 22449/35720 Training loss: 0.7771 0.2279 sec/batch\n",
      "Epoch 13/20  Iteration 22450/35720 Training loss: 0.7771 0.2251 sec/batch\n",
      "Epoch 13/20  Iteration 22451/35720 Training loss: 0.7771 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22452/35720 Training loss: 0.7770 0.2172 sec/batch\n",
      "Epoch 13/20  Iteration 22453/35720 Training loss: 0.7769 0.2109 sec/batch\n",
      "Epoch 13/20  Iteration 22454/35720 Training loss: 0.7770 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 22455/35720 Training loss: 0.7771 0.2315 sec/batch\n",
      "Epoch 13/20  Iteration 22456/35720 Training loss: 0.7771 0.2084 sec/batch\n",
      "Epoch 13/20  Iteration 22457/35720 Training loss: 0.7771 0.2305 sec/batch\n",
      "Epoch 13/20  Iteration 22458/35720 Training loss: 0.7771 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 22459/35720 Training loss: 0.7770 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 22460/35720 Training loss: 0.7770 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 22461/35720 Training loss: 0.7769 0.2241 sec/batch\n",
      "Epoch 13/20  Iteration 22462/35720 Training loss: 0.7769 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 22463/35720 Training loss: 0.7769 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 22464/35720 Training loss: 0.7769 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22465/35720 Training loss: 0.7769 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 22466/35720 Training loss: 0.7769 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 22467/35720 Training loss: 0.7770 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 22468/35720 Training loss: 0.7770 0.2183 sec/batch\n",
      "Epoch 13/20  Iteration 22469/35720 Training loss: 0.7770 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 22470/35720 Training loss: 0.7770 0.2103 sec/batch\n",
      "Epoch 13/20  Iteration 22471/35720 Training loss: 0.7770 0.2080 sec/batch\n",
      "Epoch 13/20  Iteration 22472/35720 Training loss: 0.7770 0.2216 sec/batch\n",
      "Epoch 13/20  Iteration 22473/35720 Training loss: 0.7770 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22474/35720 Training loss: 0.7770 0.2255 sec/batch\n",
      "Epoch 13/20  Iteration 22475/35720 Training loss: 0.7770 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 22476/35720 Training loss: 0.7770 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22477/35720 Training loss: 0.7770 0.2103 sec/batch\n",
      "Epoch 13/20  Iteration 22478/35720 Training loss: 0.7770 0.2223 sec/batch\n",
      "Epoch 13/20  Iteration 22479/35720 Training loss: 0.7770 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22480/35720 Training loss: 0.7770 0.2155 sec/batch\n",
      "Epoch 13/20  Iteration 22481/35720 Training loss: 0.7770 0.2286 sec/batch\n",
      "Epoch 13/20  Iteration 22482/35720 Training loss: 0.7770 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22483/35720 Training loss: 0.7770 0.2237 sec/batch\n",
      "Epoch 13/20  Iteration 22484/35720 Training loss: 0.7770 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 22485/35720 Training loss: 0.7771 0.2181 sec/batch\n",
      "Epoch 13/20  Iteration 22486/35720 Training loss: 0.7771 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22487/35720 Training loss: 0.7771 0.2166 sec/batch\n",
      "Epoch 13/20  Iteration 22488/35720 Training loss: 0.7771 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 22489/35720 Training loss: 0.7772 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 22490/35720 Training loss: 0.7772 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22491/35720 Training loss: 0.7771 0.2221 sec/batch\n",
      "Epoch 13/20  Iteration 22492/35720 Training loss: 0.7770 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 22493/35720 Training loss: 0.7771 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 22494/35720 Training loss: 0.7771 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22495/35720 Training loss: 0.7772 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 22496/35720 Training loss: 0.7771 0.2309 sec/batch\n",
      "Epoch 13/20  Iteration 22497/35720 Training loss: 0.7771 0.2107 sec/batch\n",
      "Epoch 13/20  Iteration 22498/35720 Training loss: 0.7771 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22499/35720 Training loss: 0.7771 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22500/35720 Training loss: 0.7771 0.2206 sec/batch\n",
      "Epoch 13/20  Iteration 22501/35720 Training loss: 0.7771 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22502/35720 Training loss: 0.7771 0.2187 sec/batch\n",
      "Epoch 13/20  Iteration 22503/35720 Training loss: 0.7771 0.2118 sec/batch\n",
      "Epoch 13/20  Iteration 22504/35720 Training loss: 0.7771 0.2270 sec/batch\n",
      "Epoch 13/20  Iteration 22505/35720 Training loss: 0.7771 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22506/35720 Training loss: 0.7771 0.2397 sec/batch\n",
      "Epoch 13/20  Iteration 22507/35720 Training loss: 0.7771 0.2135 sec/batch\n",
      "Epoch 13/20  Iteration 22508/35720 Training loss: 0.7771 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 22509/35720 Training loss: 0.7770 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 22510/35720 Training loss: 0.7770 0.2134 sec/batch\n",
      "Epoch 13/20  Iteration 22511/35720 Training loss: 0.7770 0.2156 sec/batch\n",
      "Epoch 13/20  Iteration 22512/35720 Training loss: 0.7770 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22513/35720 Training loss: 0.7770 0.2435 sec/batch\n",
      "Epoch 13/20  Iteration 22514/35720 Training loss: 0.7769 0.2110 sec/batch\n",
      "Epoch 13/20  Iteration 22515/35720 Training loss: 0.7769 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22516/35720 Training loss: 0.7769 0.2084 sec/batch\n",
      "Epoch 13/20  Iteration 22517/35720 Training loss: 0.7769 0.2254 sec/batch\n",
      "Epoch 13/20  Iteration 22518/35720 Training loss: 0.7769 0.2123 sec/batch\n",
      "Epoch 13/20  Iteration 22519/35720 Training loss: 0.7769 0.2135 sec/batch\n",
      "Epoch 13/20  Iteration 22520/35720 Training loss: 0.7770 0.2259 sec/batch\n",
      "Epoch 13/20  Iteration 22521/35720 Training loss: 0.7770 0.2141 sec/batch\n",
      "Epoch 13/20  Iteration 22522/35720 Training loss: 0.7770 0.2202 sec/batch\n",
      "Epoch 13/20  Iteration 22523/35720 Training loss: 0.7770 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 22524/35720 Training loss: 0.7770 0.2290 sec/batch\n",
      "Epoch 13/20  Iteration 22525/35720 Training loss: 0.7769 0.2134 sec/batch\n",
      "Epoch 13/20  Iteration 22526/35720 Training loss: 0.7769 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 22527/35720 Training loss: 0.7769 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 22528/35720 Training loss: 0.7769 0.2163 sec/batch\n",
      "Epoch 13/20  Iteration 22529/35720 Training loss: 0.7768 0.2192 sec/batch\n",
      "Epoch 13/20  Iteration 22530/35720 Training loss: 0.7768 0.2227 sec/batch\n",
      "Epoch 13/20  Iteration 22531/35720 Training loss: 0.7769 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22532/35720 Training loss: 0.7769 0.2210 sec/batch\n",
      "Epoch 13/20  Iteration 22533/35720 Training loss: 0.7769 0.2193 sec/batch\n",
      "Epoch 13/20  Iteration 22534/35720 Training loss: 0.7769 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 22535/35720 Training loss: 0.7770 0.2139 sec/batch\n",
      "Epoch 13/20  Iteration 22536/35720 Training loss: 0.7770 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22537/35720 Training loss: 0.7770 0.2216 sec/batch\n",
      "Epoch 13/20  Iteration 22538/35720 Training loss: 0.7769 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22539/35720 Training loss: 0.7769 0.2183 sec/batch\n",
      "Epoch 13/20  Iteration 22540/35720 Training loss: 0.7769 0.2189 sec/batch\n",
      "Epoch 13/20  Iteration 22541/35720 Training loss: 0.7769 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 22542/35720 Training loss: 0.7769 0.2112 sec/batch\n",
      "Epoch 13/20  Iteration 22543/35720 Training loss: 0.7769 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 22544/35720 Training loss: 0.7768 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22545/35720 Training loss: 0.7768 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 22546/35720 Training loss: 0.7768 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 22547/35720 Training loss: 0.7768 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 22548/35720 Training loss: 0.7768 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22549/35720 Training loss: 0.7767 0.2140 sec/batch\n",
      "Epoch 13/20  Iteration 22550/35720 Training loss: 0.7767 0.2206 sec/batch\n",
      "Epoch 13/20  Iteration 22551/35720 Training loss: 0.7767 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 22552/35720 Training loss: 0.7767 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 22553/35720 Training loss: 0.7767 0.2183 sec/batch\n",
      "Epoch 13/20  Iteration 22554/35720 Training loss: 0.7768 0.2174 sec/batch\n",
      "Epoch 13/20  Iteration 22555/35720 Training loss: 0.7768 0.2297 sec/batch\n",
      "Epoch 13/20  Iteration 22556/35720 Training loss: 0.7768 0.2277 sec/batch\n",
      "Epoch 13/20  Iteration 22557/35720 Training loss: 0.7767 0.2691 sec/batch\n",
      "Epoch 13/20  Iteration 22558/35720 Training loss: 0.7767 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 22559/35720 Training loss: 0.7767 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 22560/35720 Training loss: 0.7767 0.2157 sec/batch\n",
      "Epoch 13/20  Iteration 22561/35720 Training loss: 0.7767 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22562/35720 Training loss: 0.7767 0.2377 sec/batch\n",
      "Epoch 13/20  Iteration 22563/35720 Training loss: 0.7766 0.2274 sec/batch\n",
      "Epoch 13/20  Iteration 22564/35720 Training loss: 0.7766 0.2172 sec/batch\n",
      "Epoch 13/20  Iteration 22565/35720 Training loss: 0.7765 0.2079 sec/batch\n",
      "Epoch 13/20  Iteration 22566/35720 Training loss: 0.7765 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 22567/35720 Training loss: 0.7765 0.2207 sec/batch\n",
      "Epoch 13/20  Iteration 22568/35720 Training loss: 0.7765 0.2169 sec/batch\n",
      "Epoch 13/20  Iteration 22569/35720 Training loss: 0.7764 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 22570/35720 Training loss: 0.7764 0.2057 sec/batch\n",
      "Epoch 13/20  Iteration 22571/35720 Training loss: 0.7764 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 22572/35720 Training loss: 0.7764 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 22573/35720 Training loss: 0.7764 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 22574/35720 Training loss: 0.7764 0.2361 sec/batch\n",
      "Epoch 13/20  Iteration 22575/35720 Training loss: 0.7763 0.2121 sec/batch\n",
      "Epoch 13/20  Iteration 22576/35720 Training loss: 0.7763 0.2174 sec/batch\n",
      "Epoch 13/20  Iteration 22577/35720 Training loss: 0.7763 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 22578/35720 Training loss: 0.7762 0.2114 sec/batch\n",
      "Epoch 13/20  Iteration 22579/35720 Training loss: 0.7762 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 22580/35720 Training loss: 0.7761 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22581/35720 Training loss: 0.7761 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22582/35720 Training loss: 0.7761 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22583/35720 Training loss: 0.7762 0.2207 sec/batch\n",
      "Epoch 13/20  Iteration 22584/35720 Training loss: 0.7762 0.2177 sec/batch\n",
      "Epoch 13/20  Iteration 22585/35720 Training loss: 0.7762 0.2252 sec/batch\n",
      "Epoch 13/20  Iteration 22586/35720 Training loss: 0.7762 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 22587/35720 Training loss: 0.7762 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 22588/35720 Training loss: 0.7761 0.2183 sec/batch\n",
      "Epoch 13/20  Iteration 22589/35720 Training loss: 0.7762 0.2302 sec/batch\n",
      "Epoch 13/20  Iteration 22590/35720 Training loss: 0.7761 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22591/35720 Training loss: 0.7761 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22592/35720 Training loss: 0.7761 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22593/35720 Training loss: 0.7760 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 22594/35720 Training loss: 0.7760 0.2144 sec/batch\n",
      "Epoch 13/20  Iteration 22595/35720 Training loss: 0.7761 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22596/35720 Training loss: 0.7761 0.2064 sec/batch\n",
      "Epoch 13/20  Iteration 22597/35720 Training loss: 0.7761 0.2166 sec/batch\n",
      "Epoch 13/20  Iteration 22598/35720 Training loss: 0.7760 0.2207 sec/batch\n",
      "Epoch 13/20  Iteration 22599/35720 Training loss: 0.7760 0.2084 sec/batch\n",
      "Epoch 13/20  Iteration 22600/35720 Training loss: 0.7760 0.2149 sec/batch\n",
      "Validation loss: 1.48857 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 22601/35720 Training loss: 0.7763 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22602/35720 Training loss: 0.7763 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 22603/35720 Training loss: 0.7763 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 22604/35720 Training loss: 0.7763 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 22605/35720 Training loss: 0.7763 0.2145 sec/batch\n",
      "Epoch 13/20  Iteration 22606/35720 Training loss: 0.7763 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 22607/35720 Training loss: 0.7763 0.2266 sec/batch\n",
      "Epoch 13/20  Iteration 22608/35720 Training loss: 0.7763 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 22609/35720 Training loss: 0.7763 0.2186 sec/batch\n",
      "Epoch 13/20  Iteration 22610/35720 Training loss: 0.7763 0.2184 sec/batch\n",
      "Epoch 13/20  Iteration 22611/35720 Training loss: 0.7763 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22612/35720 Training loss: 0.7763 0.2154 sec/batch\n",
      "Epoch 13/20  Iteration 22613/35720 Training loss: 0.7764 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22614/35720 Training loss: 0.7763 0.2110 sec/batch\n",
      "Epoch 13/20  Iteration 22615/35720 Training loss: 0.7763 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22616/35720 Training loss: 0.7762 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 22617/35720 Training loss: 0.7762 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22618/35720 Training loss: 0.7762 0.2264 sec/batch\n",
      "Epoch 13/20  Iteration 22619/35720 Training loss: 0.7762 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22620/35720 Training loss: 0.7762 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 22621/35720 Training loss: 0.7761 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22622/35720 Training loss: 0.7761 0.2244 sec/batch\n",
      "Epoch 13/20  Iteration 22623/35720 Training loss: 0.7762 0.2238 sec/batch\n",
      "Epoch 13/20  Iteration 22624/35720 Training loss: 0.7762 0.2235 sec/batch\n",
      "Epoch 13/20  Iteration 22625/35720 Training loss: 0.7762 0.2054 sec/batch\n",
      "Epoch 13/20  Iteration 22626/35720 Training loss: 0.7762 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22627/35720 Training loss: 0.7762 0.2165 sec/batch\n",
      "Epoch 13/20  Iteration 22628/35720 Training loss: 0.7762 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22629/35720 Training loss: 0.7762 0.2171 sec/batch\n",
      "Epoch 13/20  Iteration 22630/35720 Training loss: 0.7762 0.2115 sec/batch\n",
      "Epoch 13/20  Iteration 22631/35720 Training loss: 0.7762 0.2200 sec/batch\n",
      "Epoch 13/20  Iteration 22632/35720 Training loss: 0.7762 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 22633/35720 Training loss: 0.7762 0.2258 sec/batch\n",
      "Epoch 13/20  Iteration 22634/35720 Training loss: 0.7761 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 22635/35720 Training loss: 0.7761 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 22636/35720 Training loss: 0.7761 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22637/35720 Training loss: 0.7760 0.2218 sec/batch\n",
      "Epoch 13/20  Iteration 22638/35720 Training loss: 0.7760 0.2144 sec/batch\n",
      "Epoch 13/20  Iteration 22639/35720 Training loss: 0.7760 0.2082 sec/batch\n",
      "Epoch 13/20  Iteration 22640/35720 Training loss: 0.7760 0.2208 sec/batch\n",
      "Epoch 13/20  Iteration 22641/35720 Training loss: 0.7760 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22642/35720 Training loss: 0.7760 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22643/35720 Training loss: 0.7759 0.2161 sec/batch\n",
      "Epoch 13/20  Iteration 22644/35720 Training loss: 0.7759 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 22645/35720 Training loss: 0.7759 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22646/35720 Training loss: 0.7759 0.2155 sec/batch\n",
      "Epoch 13/20  Iteration 22647/35720 Training loss: 0.7759 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 22648/35720 Training loss: 0.7759 0.2064 sec/batch\n",
      "Epoch 13/20  Iteration 22649/35720 Training loss: 0.7759 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 22650/35720 Training loss: 0.7759 0.2166 sec/batch\n",
      "Epoch 13/20  Iteration 22651/35720 Training loss: 0.7759 0.2249 sec/batch\n",
      "Epoch 13/20  Iteration 22652/35720 Training loss: 0.7758 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 22653/35720 Training loss: 0.7758 0.2134 sec/batch\n",
      "Epoch 13/20  Iteration 22654/35720 Training loss: 0.7758 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 22655/35720 Training loss: 0.7758 0.2251 sec/batch\n",
      "Epoch 13/20  Iteration 22656/35720 Training loss: 0.7757 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22657/35720 Training loss: 0.7757 0.2296 sec/batch\n",
      "Epoch 13/20  Iteration 22658/35720 Training loss: 0.7757 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 22659/35720 Training loss: 0.7757 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22660/35720 Training loss: 0.7757 0.2188 sec/batch\n",
      "Epoch 13/20  Iteration 22661/35720 Training loss: 0.7757 0.2252 sec/batch\n",
      "Epoch 13/20  Iteration 22662/35720 Training loss: 0.7757 0.2080 sec/batch\n",
      "Epoch 13/20  Iteration 22663/35720 Training loss: 0.7757 0.2246 sec/batch\n",
      "Epoch 13/20  Iteration 22664/35720 Training loss: 0.7756 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22665/35720 Training loss: 0.7756 0.2212 sec/batch\n",
      "Epoch 13/20  Iteration 22666/35720 Training loss: 0.7756 0.2196 sec/batch\n",
      "Epoch 13/20  Iteration 22667/35720 Training loss: 0.7756 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22668/35720 Training loss: 0.7756 0.2342 sec/batch\n",
      "Epoch 13/20  Iteration 22669/35720 Training loss: 0.7755 0.2192 sec/batch\n",
      "Epoch 13/20  Iteration 22670/35720 Training loss: 0.7755 0.2146 sec/batch\n",
      "Epoch 13/20  Iteration 22671/35720 Training loss: 0.7754 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 22672/35720 Training loss: 0.7754 0.2282 sec/batch\n",
      "Epoch 13/20  Iteration 22673/35720 Training loss: 0.7753 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 22674/35720 Training loss: 0.7753 0.2217 sec/batch\n",
      "Epoch 13/20  Iteration 22675/35720 Training loss: 0.7753 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 22676/35720 Training loss: 0.7752 0.2179 sec/batch\n",
      "Epoch 13/20  Iteration 22677/35720 Training loss: 0.7752 0.2053 sec/batch\n",
      "Epoch 13/20  Iteration 22678/35720 Training loss: 0.7752 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 22679/35720 Training loss: 0.7751 0.2181 sec/batch\n",
      "Epoch 13/20  Iteration 22680/35720 Training loss: 0.7751 0.2302 sec/batch\n",
      "Epoch 13/20  Iteration 22681/35720 Training loss: 0.7751 0.2077 sec/batch\n",
      "Epoch 13/20  Iteration 22682/35720 Training loss: 0.7751 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22683/35720 Training loss: 0.7750 0.2134 sec/batch\n",
      "Epoch 13/20  Iteration 22684/35720 Training loss: 0.7750 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22685/35720 Training loss: 0.7750 0.2193 sec/batch\n",
      "Epoch 13/20  Iteration 22686/35720 Training loss: 0.7750 0.2064 sec/batch\n",
      "Epoch 13/20  Iteration 22687/35720 Training loss: 0.7750 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22688/35720 Training loss: 0.7750 0.2262 sec/batch\n",
      "Epoch 13/20  Iteration 22689/35720 Training loss: 0.7749 0.2273 sec/batch\n",
      "Epoch 13/20  Iteration 22690/35720 Training loss: 0.7749 0.2116 sec/batch\n",
      "Epoch 13/20  Iteration 22691/35720 Training loss: 0.7749 0.2233 sec/batch\n",
      "Epoch 13/20  Iteration 22692/35720 Training loss: 0.7749 0.2107 sec/batch\n",
      "Epoch 13/20  Iteration 22693/35720 Training loss: 0.7749 0.2151 sec/batch\n",
      "Epoch 13/20  Iteration 22694/35720 Training loss: 0.7748 0.2194 sec/batch\n",
      "Epoch 13/20  Iteration 22695/35720 Training loss: 0.7748 0.2259 sec/batch\n",
      "Epoch 13/20  Iteration 22696/35720 Training loss: 0.7747 0.2211 sec/batch\n",
      "Epoch 13/20  Iteration 22697/35720 Training loss: 0.7746 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 22698/35720 Training loss: 0.7746 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22699/35720 Training loss: 0.7746 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22700/35720 Training loss: 0.7746 0.2191 sec/batch\n",
      "Epoch 13/20  Iteration 22701/35720 Training loss: 0.7746 0.2181 sec/batch\n",
      "Epoch 13/20  Iteration 22702/35720 Training loss: 0.7746 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 22703/35720 Training loss: 0.7745 0.2320 sec/batch\n",
      "Epoch 13/20  Iteration 22704/35720 Training loss: 0.7745 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22705/35720 Training loss: 0.7745 0.2174 sec/batch\n",
      "Epoch 13/20  Iteration 22706/35720 Training loss: 0.7745 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 22707/35720 Training loss: 0.7744 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22708/35720 Training loss: 0.7744 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22709/35720 Training loss: 0.7743 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22710/35720 Training loss: 0.7743 0.2151 sec/batch\n",
      "Epoch 13/20  Iteration 22711/35720 Training loss: 0.7742 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 22712/35720 Training loss: 0.7742 0.2172 sec/batch\n",
      "Epoch 13/20  Iteration 22713/35720 Training loss: 0.7742 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22714/35720 Training loss: 0.7742 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22715/35720 Training loss: 0.7741 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 22716/35720 Training loss: 0.7740 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22717/35720 Training loss: 0.7740 0.2286 sec/batch\n",
      "Epoch 13/20  Iteration 22718/35720 Training loss: 0.7740 0.2135 sec/batch\n",
      "Epoch 13/20  Iteration 22719/35720 Training loss: 0.7739 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 22720/35720 Training loss: 0.7739 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22721/35720 Training loss: 0.7739 0.2177 sec/batch\n",
      "Epoch 13/20  Iteration 22722/35720 Training loss: 0.7739 0.2216 sec/batch\n",
      "Epoch 13/20  Iteration 22723/35720 Training loss: 0.7739 0.2237 sec/batch\n",
      "Epoch 13/20  Iteration 22724/35720 Training loss: 0.7738 0.2180 sec/batch\n",
      "Epoch 13/20  Iteration 22725/35720 Training loss: 0.7738 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 22726/35720 Training loss: 0.7738 0.2195 sec/batch\n",
      "Epoch 13/20  Iteration 22727/35720 Training loss: 0.7738 0.2197 sec/batch\n",
      "Epoch 13/20  Iteration 22728/35720 Training loss: 0.7738 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 22729/35720 Training loss: 0.7738 0.2135 sec/batch\n",
      "Epoch 13/20  Iteration 22730/35720 Training loss: 0.7738 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 22731/35720 Training loss: 0.7737 0.2058 sec/batch\n",
      "Epoch 13/20  Iteration 22732/35720 Training loss: 0.7737 0.2134 sec/batch\n",
      "Epoch 13/20  Iteration 22733/35720 Training loss: 0.7736 0.2175 sec/batch\n",
      "Epoch 13/20  Iteration 22734/35720 Training loss: 0.7736 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 22735/35720 Training loss: 0.7736 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22736/35720 Training loss: 0.7736 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 22737/35720 Training loss: 0.7736 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 22738/35720 Training loss: 0.7735 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 22739/35720 Training loss: 0.7735 0.2327 sec/batch\n",
      "Epoch 13/20  Iteration 22740/35720 Training loss: 0.7735 0.2206 sec/batch\n",
      "Epoch 13/20  Iteration 22741/35720 Training loss: 0.7734 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 22742/35720 Training loss: 0.7734 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22743/35720 Training loss: 0.7734 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22744/35720 Training loss: 0.7734 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 22745/35720 Training loss: 0.7734 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 22746/35720 Training loss: 0.7734 0.2123 sec/batch\n",
      "Epoch 13/20  Iteration 22747/35720 Training loss: 0.7733 0.2236 sec/batch\n",
      "Epoch 13/20  Iteration 22748/35720 Training loss: 0.7733 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 22749/35720 Training loss: 0.7733 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22750/35720 Training loss: 0.7732 0.2155 sec/batch\n",
      "Epoch 13/20  Iteration 22751/35720 Training loss: 0.7732 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22752/35720 Training loss: 0.7732 0.2258 sec/batch\n",
      "Epoch 13/20  Iteration 22753/35720 Training loss: 0.7732 0.2070 sec/batch\n",
      "Epoch 13/20  Iteration 22754/35720 Training loss: 0.7732 0.2058 sec/batch\n",
      "Epoch 13/20  Iteration 22755/35720 Training loss: 0.7732 0.2081 sec/batch\n",
      "Epoch 13/20  Iteration 22756/35720 Training loss: 0.7732 0.2324 sec/batch\n",
      "Epoch 13/20  Iteration 22757/35720 Training loss: 0.7732 0.2243 sec/batch\n",
      "Epoch 13/20  Iteration 22758/35720 Training loss: 0.7732 0.2057 sec/batch\n",
      "Epoch 13/20  Iteration 22759/35720 Training loss: 0.7732 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22760/35720 Training loss: 0.7732 0.2157 sec/batch\n",
      "Epoch 13/20  Iteration 22761/35720 Training loss: 0.7732 0.2287 sec/batch\n",
      "Epoch 13/20  Iteration 22762/35720 Training loss: 0.7732 0.2287 sec/batch\n",
      "Epoch 13/20  Iteration 22763/35720 Training loss: 0.7732 0.2323 sec/batch\n",
      "Epoch 13/20  Iteration 22764/35720 Training loss: 0.7731 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22765/35720 Training loss: 0.7732 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22766/35720 Training loss: 0.7731 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 22767/35720 Training loss: 0.7731 0.2178 sec/batch\n",
      "Epoch 13/20  Iteration 22768/35720 Training loss: 0.7731 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 22769/35720 Training loss: 0.7732 0.2199 sec/batch\n",
      "Epoch 13/20  Iteration 22770/35720 Training loss: 0.7732 0.2048 sec/batch\n",
      "Epoch 13/20  Iteration 22771/35720 Training loss: 0.7731 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 22772/35720 Training loss: 0.7732 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 22773/35720 Training loss: 0.7732 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 22774/35720 Training loss: 0.7732 0.2202 sec/batch\n",
      "Epoch 13/20  Iteration 22775/35720 Training loss: 0.7731 0.2166 sec/batch\n",
      "Epoch 13/20  Iteration 22776/35720 Training loss: 0.7731 0.2390 sec/batch\n",
      "Epoch 13/20  Iteration 22777/35720 Training loss: 0.7731 0.2964 sec/batch\n",
      "Epoch 13/20  Iteration 22778/35720 Training loss: 0.7731 0.2277 sec/batch\n",
      "Epoch 13/20  Iteration 22779/35720 Training loss: 0.7731 0.2250 sec/batch\n",
      "Epoch 13/20  Iteration 22780/35720 Training loss: 0.7731 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22781/35720 Training loss: 0.7731 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 22782/35720 Training loss: 0.7731 0.2075 sec/batch\n",
      "Epoch 13/20  Iteration 22783/35720 Training loss: 0.7731 0.2300 sec/batch\n",
      "Epoch 13/20  Iteration 22784/35720 Training loss: 0.7731 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 22785/35720 Training loss: 0.7732 0.2212 sec/batch\n",
      "Epoch 13/20  Iteration 22786/35720 Training loss: 0.7731 0.2136 sec/batch\n",
      "Epoch 13/20  Iteration 22787/35720 Training loss: 0.7731 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 22788/35720 Training loss: 0.7731 0.2253 sec/batch\n",
      "Epoch 13/20  Iteration 22789/35720 Training loss: 0.7731 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 22790/35720 Training loss: 0.7730 0.2296 sec/batch\n",
      "Epoch 13/20  Iteration 22791/35720 Training loss: 0.7730 0.2179 sec/batch\n",
      "Epoch 13/20  Iteration 22792/35720 Training loss: 0.7730 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 22793/35720 Training loss: 0.7730 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 22794/35720 Training loss: 0.7729 0.2118 sec/batch\n",
      "Epoch 13/20  Iteration 22795/35720 Training loss: 0.7729 0.2116 sec/batch\n",
      "Epoch 13/20  Iteration 22796/35720 Training loss: 0.7729 0.2149 sec/batch\n",
      "Epoch 13/20  Iteration 22797/35720 Training loss: 0.7728 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 22798/35720 Training loss: 0.7728 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22799/35720 Training loss: 0.7728 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 22800/35720 Training loss: 0.7728 0.2107 sec/batch\n",
      "Validation loss: 1.49036 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 22801/35720 Training loss: 0.7731 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22802/35720 Training loss: 0.7731 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 22803/35720 Training loss: 0.7731 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22804/35720 Training loss: 0.7731 0.2194 sec/batch\n",
      "Epoch 13/20  Iteration 22805/35720 Training loss: 0.7731 0.2082 sec/batch\n",
      "Epoch 13/20  Iteration 22806/35720 Training loss: 0.7731 0.2118 sec/batch\n",
      "Epoch 13/20  Iteration 22807/35720 Training loss: 0.7730 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22808/35720 Training loss: 0.7731 0.2077 sec/batch\n",
      "Epoch 13/20  Iteration 22809/35720 Training loss: 0.7730 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 22810/35720 Training loss: 0.7730 0.2120 sec/batch\n",
      "Epoch 13/20  Iteration 22811/35720 Training loss: 0.7731 0.2324 sec/batch\n",
      "Epoch 13/20  Iteration 22812/35720 Training loss: 0.7731 0.2211 sec/batch\n",
      "Epoch 13/20  Iteration 22813/35720 Training loss: 0.7731 0.2058 sec/batch\n",
      "Epoch 13/20  Iteration 22814/35720 Training loss: 0.7731 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 22815/35720 Training loss: 0.7731 0.2127 sec/batch\n",
      "Epoch 13/20  Iteration 22816/35720 Training loss: 0.7730 0.2182 sec/batch\n",
      "Epoch 13/20  Iteration 22817/35720 Training loss: 0.7731 0.2166 sec/batch\n",
      "Epoch 13/20  Iteration 22818/35720 Training loss: 0.7731 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22819/35720 Training loss: 0.7731 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 22820/35720 Training loss: 0.7731 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22821/35720 Training loss: 0.7731 0.2221 sec/batch\n",
      "Epoch 13/20  Iteration 22822/35720 Training loss: 0.7731 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22823/35720 Training loss: 0.7730 0.2182 sec/batch\n",
      "Epoch 13/20  Iteration 22824/35720 Training loss: 0.7730 0.2049 sec/batch\n",
      "Epoch 13/20  Iteration 22825/35720 Training loss: 0.7730 0.2053 sec/batch\n",
      "Epoch 13/20  Iteration 22826/35720 Training loss: 0.7729 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 22827/35720 Training loss: 0.7729 0.2275 sec/batch\n",
      "Epoch 13/20  Iteration 22828/35720 Training loss: 0.7729 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 22829/35720 Training loss: 0.7728 0.2156 sec/batch\n",
      "Epoch 13/20  Iteration 22830/35720 Training loss: 0.7728 0.2201 sec/batch\n",
      "Epoch 13/20  Iteration 22831/35720 Training loss: 0.7727 0.2238 sec/batch\n",
      "Epoch 13/20  Iteration 22832/35720 Training loss: 0.7727 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22833/35720 Training loss: 0.7727 0.2184 sec/batch\n",
      "Epoch 13/20  Iteration 22834/35720 Training loss: 0.7727 0.2147 sec/batch\n",
      "Epoch 13/20  Iteration 22835/35720 Training loss: 0.7727 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 22836/35720 Training loss: 0.7727 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 22837/35720 Training loss: 0.7726 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 22838/35720 Training loss: 0.7726 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 22839/35720 Training loss: 0.7726 0.2158 sec/batch\n",
      "Epoch 13/20  Iteration 22840/35720 Training loss: 0.7726 0.2116 sec/batch\n",
      "Epoch 13/20  Iteration 22841/35720 Training loss: 0.7727 0.2115 sec/batch\n",
      "Epoch 13/20  Iteration 22842/35720 Training loss: 0.7727 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 22843/35720 Training loss: 0.7727 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 22844/35720 Training loss: 0.7727 0.2287 sec/batch\n",
      "Epoch 13/20  Iteration 22845/35720 Training loss: 0.7726 0.2125 sec/batch\n",
      "Epoch 13/20  Iteration 22846/35720 Training loss: 0.7726 0.2134 sec/batch\n",
      "Epoch 13/20  Iteration 22847/35720 Training loss: 0.7726 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22848/35720 Training loss: 0.7726 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 22849/35720 Training loss: 0.7726 0.2070 sec/batch\n",
      "Epoch 13/20  Iteration 22850/35720 Training loss: 0.7726 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 22851/35720 Training loss: 0.7727 0.2183 sec/batch\n",
      "Epoch 13/20  Iteration 22852/35720 Training loss: 0.7727 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22853/35720 Training loss: 0.7727 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 22854/35720 Training loss: 0.7726 0.2163 sec/batch\n",
      "Epoch 13/20  Iteration 22855/35720 Training loss: 0.7726 0.2169 sec/batch\n",
      "Epoch 13/20  Iteration 22856/35720 Training loss: 0.7726 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 22857/35720 Training loss: 0.7726 0.2230 sec/batch\n",
      "Epoch 13/20  Iteration 22858/35720 Training loss: 0.7726 0.2056 sec/batch\n",
      "Epoch 13/20  Iteration 22859/35720 Training loss: 0.7726 0.2110 sec/batch\n",
      "Epoch 13/20  Iteration 22860/35720 Training loss: 0.7727 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 22861/35720 Training loss: 0.7727 0.2200 sec/batch\n",
      "Epoch 13/20  Iteration 22862/35720 Training loss: 0.7726 0.2180 sec/batch\n",
      "Epoch 13/20  Iteration 22863/35720 Training loss: 0.7726 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 22864/35720 Training loss: 0.7726 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22865/35720 Training loss: 0.7725 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22866/35720 Training loss: 0.7725 0.2146 sec/batch\n",
      "Epoch 13/20  Iteration 22867/35720 Training loss: 0.7726 0.2244 sec/batch\n",
      "Epoch 13/20  Iteration 22868/35720 Training loss: 0.7726 0.2210 sec/batch\n",
      "Epoch 13/20  Iteration 22869/35720 Training loss: 0.7725 0.2166 sec/batch\n",
      "Epoch 13/20  Iteration 22870/35720 Training loss: 0.7726 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22871/35720 Training loss: 0.7726 0.2088 sec/batch\n",
      "Epoch 13/20  Iteration 22872/35720 Training loss: 0.7726 0.2161 sec/batch\n",
      "Epoch 13/20  Iteration 22873/35720 Training loss: 0.7726 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 22874/35720 Training loss: 0.7726 0.2173 sec/batch\n",
      "Epoch 13/20  Iteration 22875/35720 Training loss: 0.7726 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 22876/35720 Training loss: 0.7726 0.2181 sec/batch\n",
      "Epoch 13/20  Iteration 22877/35720 Training loss: 0.7726 0.2177 sec/batch\n",
      "Epoch 13/20  Iteration 22878/35720 Training loss: 0.7726 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22879/35720 Training loss: 0.7726 0.2062 sec/batch\n",
      "Epoch 13/20  Iteration 22880/35720 Training loss: 0.7725 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 22881/35720 Training loss: 0.7726 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 22882/35720 Training loss: 0.7725 0.2083 sec/batch\n",
      "Epoch 13/20  Iteration 22883/35720 Training loss: 0.7725 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 22884/35720 Training loss: 0.7725 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 22885/35720 Training loss: 0.7725 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 22886/35720 Training loss: 0.7725 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22887/35720 Training loss: 0.7725 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 22888/35720 Training loss: 0.7725 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 22889/35720 Training loss: 0.7725 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 22890/35720 Training loss: 0.7725 0.2231 sec/batch\n",
      "Epoch 13/20  Iteration 22891/35720 Training loss: 0.7726 0.2223 sec/batch\n",
      "Epoch 13/20  Iteration 22892/35720 Training loss: 0.7726 0.2154 sec/batch\n",
      "Epoch 13/20  Iteration 22893/35720 Training loss: 0.7726 0.2262 sec/batch\n",
      "Epoch 13/20  Iteration 22894/35720 Training loss: 0.7726 0.2178 sec/batch\n",
      "Epoch 13/20  Iteration 22895/35720 Training loss: 0.7726 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 22896/35720 Training loss: 0.7726 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 22897/35720 Training loss: 0.7726 0.2194 sec/batch\n",
      "Epoch 13/20  Iteration 22898/35720 Training loss: 0.7725 0.2248 sec/batch\n",
      "Epoch 13/20  Iteration 22899/35720 Training loss: 0.7725 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 22900/35720 Training loss: 0.7726 0.2163 sec/batch\n",
      "Epoch 13/20  Iteration 22901/35720 Training loss: 0.7725 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 22902/35720 Training loss: 0.7725 0.2173 sec/batch\n",
      "Epoch 13/20  Iteration 22903/35720 Training loss: 0.7725 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 22904/35720 Training loss: 0.7725 0.2189 sec/batch\n",
      "Epoch 13/20  Iteration 22905/35720 Training loss: 0.7725 0.2187 sec/batch\n",
      "Epoch 13/20  Iteration 22906/35720 Training loss: 0.7724 0.2077 sec/batch\n",
      "Epoch 13/20  Iteration 22907/35720 Training loss: 0.7724 0.2152 sec/batch\n",
      "Epoch 13/20  Iteration 22908/35720 Training loss: 0.7723 0.2055 sec/batch\n",
      "Epoch 13/20  Iteration 22909/35720 Training loss: 0.7722 0.2110 sec/batch\n",
      "Epoch 13/20  Iteration 22910/35720 Training loss: 0.7722 0.2220 sec/batch\n",
      "Epoch 13/20  Iteration 22911/35720 Training loss: 0.7722 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 22912/35720 Training loss: 0.7721 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 22913/35720 Training loss: 0.7721 0.2280 sec/batch\n",
      "Epoch 13/20  Iteration 22914/35720 Training loss: 0.7721 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 22915/35720 Training loss: 0.7721 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22916/35720 Training loss: 0.7720 0.2127 sec/batch\n",
      "Epoch 13/20  Iteration 22917/35720 Training loss: 0.7720 0.2189 sec/batch\n",
      "Epoch 13/20  Iteration 22918/35720 Training loss: 0.7720 0.2230 sec/batch\n",
      "Epoch 13/20  Iteration 22919/35720 Training loss: 0.7720 0.2234 sec/batch\n",
      "Epoch 13/20  Iteration 22920/35720 Training loss: 0.7719 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22921/35720 Training loss: 0.7719 0.2077 sec/batch\n",
      "Epoch 13/20  Iteration 22922/35720 Training loss: 0.7719 0.2391 sec/batch\n",
      "Epoch 13/20  Iteration 22923/35720 Training loss: 0.7719 0.2163 sec/batch\n",
      "Epoch 13/20  Iteration 22924/35720 Training loss: 0.7718 0.2206 sec/batch\n",
      "Epoch 13/20  Iteration 22925/35720 Training loss: 0.7719 0.2123 sec/batch\n",
      "Epoch 13/20  Iteration 22926/35720 Training loss: 0.7719 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 22927/35720 Training loss: 0.7719 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 22928/35720 Training loss: 0.7718 0.2288 sec/batch\n",
      "Epoch 13/20  Iteration 22929/35720 Training loss: 0.7718 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 22930/35720 Training loss: 0.7718 0.2308 sec/batch\n",
      "Epoch 13/20  Iteration 22931/35720 Training loss: 0.7718 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22932/35720 Training loss: 0.7717 0.2079 sec/batch\n",
      "Epoch 13/20  Iteration 22933/35720 Training loss: 0.7717 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 22934/35720 Training loss: 0.7717 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 22935/35720 Training loss: 0.7717 0.2213 sec/batch\n",
      "Epoch 13/20  Iteration 22936/35720 Training loss: 0.7717 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 22937/35720 Training loss: 0.7717 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 22938/35720 Training loss: 0.7717 0.2214 sec/batch\n",
      "Epoch 13/20  Iteration 22939/35720 Training loss: 0.7717 0.2189 sec/batch\n",
      "Epoch 13/20  Iteration 22940/35720 Training loss: 0.7716 0.2211 sec/batch\n",
      "Epoch 13/20  Iteration 22941/35720 Training loss: 0.7716 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 22942/35720 Training loss: 0.7716 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 22943/35720 Training loss: 0.7716 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22944/35720 Training loss: 0.7716 0.2277 sec/batch\n",
      "Epoch 13/20  Iteration 22945/35720 Training loss: 0.7716 0.2208 sec/batch\n",
      "Epoch 13/20  Iteration 22946/35720 Training loss: 0.7716 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 22947/35720 Training loss: 0.7716 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 22948/35720 Training loss: 0.7716 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 22949/35720 Training loss: 0.7716 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 22950/35720 Training loss: 0.7715 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 22951/35720 Training loss: 0.7716 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 22952/35720 Training loss: 0.7716 0.2192 sec/batch\n",
      "Epoch 13/20  Iteration 22953/35720 Training loss: 0.7716 0.2449 sec/batch\n",
      "Epoch 13/20  Iteration 22954/35720 Training loss: 0.7716 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 22955/35720 Training loss: 0.7716 0.2174 sec/batch\n",
      "Epoch 13/20  Iteration 22956/35720 Training loss: 0.7717 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22957/35720 Training loss: 0.7717 0.2295 sec/batch\n",
      "Epoch 13/20  Iteration 22958/35720 Training loss: 0.7716 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 22959/35720 Training loss: 0.7717 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22960/35720 Training loss: 0.7717 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 22961/35720 Training loss: 0.7717 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 22962/35720 Training loss: 0.7717 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 22963/35720 Training loss: 0.7717 0.2184 sec/batch\n",
      "Epoch 13/20  Iteration 22964/35720 Training loss: 0.7717 0.2107 sec/batch\n",
      "Epoch 13/20  Iteration 22965/35720 Training loss: 0.7717 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 22966/35720 Training loss: 0.7717 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 22967/35720 Training loss: 0.7717 0.2222 sec/batch\n",
      "Epoch 13/20  Iteration 22968/35720 Training loss: 0.7717 0.2169 sec/batch\n",
      "Epoch 13/20  Iteration 22969/35720 Training loss: 0.7717 0.2046 sec/batch\n",
      "Epoch 13/20  Iteration 22970/35720 Training loss: 0.7716 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 22971/35720 Training loss: 0.7716 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22972/35720 Training loss: 0.7716 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 22973/35720 Training loss: 0.7715 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 22974/35720 Training loss: 0.7716 0.2212 sec/batch\n",
      "Epoch 13/20  Iteration 22975/35720 Training loss: 0.7715 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 22976/35720 Training loss: 0.7715 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 22977/35720 Training loss: 0.7715 0.2079 sec/batch\n",
      "Epoch 13/20  Iteration 22978/35720 Training loss: 0.7714 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 22979/35720 Training loss: 0.7714 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 22980/35720 Training loss: 0.7714 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 22981/35720 Training loss: 0.7714 0.2065 sec/batch\n",
      "Epoch 13/20  Iteration 22982/35720 Training loss: 0.7714 0.2080 sec/batch\n",
      "Epoch 13/20  Iteration 22983/35720 Training loss: 0.7714 0.2099 sec/batch\n",
      "Epoch 13/20  Iteration 22984/35720 Training loss: 0.7714 0.2157 sec/batch\n",
      "Epoch 13/20  Iteration 22985/35720 Training loss: 0.7714 0.2232 sec/batch\n",
      "Epoch 13/20  Iteration 22986/35720 Training loss: 0.7714 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 22987/35720 Training loss: 0.7713 0.2102 sec/batch\n",
      "Epoch 13/20  Iteration 22988/35720 Training loss: 0.7713 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 22989/35720 Training loss: 0.7713 0.2172 sec/batch\n",
      "Epoch 13/20  Iteration 22990/35720 Training loss: 0.7712 0.2237 sec/batch\n",
      "Epoch 13/20  Iteration 22991/35720 Training loss: 0.7712 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 22992/35720 Training loss: 0.7711 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 22993/35720 Training loss: 0.7711 0.2049 sec/batch\n",
      "Epoch 13/20  Iteration 22994/35720 Training loss: 0.7711 0.2281 sec/batch\n",
      "Epoch 13/20  Iteration 22995/35720 Training loss: 0.7711 0.2220 sec/batch\n",
      "Epoch 13/20  Iteration 22996/35720 Training loss: 0.7710 0.2223 sec/batch\n",
      "Epoch 13/20  Iteration 22997/35720 Training loss: 0.7710 0.2120 sec/batch\n",
      "Epoch 13/20  Iteration 22998/35720 Training loss: 0.7710 0.2952 sec/batch\n",
      "Epoch 13/20  Iteration 22999/35720 Training loss: 0.7709 0.2282 sec/batch\n",
      "Epoch 13/20  Iteration 23000/35720 Training loss: 0.7710 0.2156 sec/batch\n",
      "Validation loss: 1.50313 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 23001/35720 Training loss: 0.7712 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 23002/35720 Training loss: 0.7712 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 23003/35720 Training loss: 0.7711 0.2123 sec/batch\n",
      "Epoch 13/20  Iteration 23004/35720 Training loss: 0.7711 0.2143 sec/batch\n",
      "Epoch 13/20  Iteration 23005/35720 Training loss: 0.7710 0.2172 sec/batch\n",
      "Epoch 13/20  Iteration 23006/35720 Training loss: 0.7710 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 23007/35720 Training loss: 0.7710 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 23008/35720 Training loss: 0.7710 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 23009/35720 Training loss: 0.7709 0.2251 sec/batch\n",
      "Epoch 13/20  Iteration 23010/35720 Training loss: 0.7710 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 23011/35720 Training loss: 0.7710 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 23012/35720 Training loss: 0.7709 0.2271 sec/batch\n",
      "Epoch 13/20  Iteration 23013/35720 Training loss: 0.7710 0.2223 sec/batch\n",
      "Epoch 13/20  Iteration 23014/35720 Training loss: 0.7709 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 23015/35720 Training loss: 0.7709 0.2211 sec/batch\n",
      "Epoch 13/20  Iteration 23016/35720 Training loss: 0.7708 0.2235 sec/batch\n",
      "Epoch 13/20  Iteration 23017/35720 Training loss: 0.7709 0.2107 sec/batch\n",
      "Epoch 13/20  Iteration 23018/35720 Training loss: 0.7709 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 23019/35720 Training loss: 0.7708 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 23020/35720 Training loss: 0.7708 0.2146 sec/batch\n",
      "Epoch 13/20  Iteration 23021/35720 Training loss: 0.7707 0.2173 sec/batch\n",
      "Epoch 13/20  Iteration 23022/35720 Training loss: 0.7707 0.2164 sec/batch\n",
      "Epoch 13/20  Iteration 23023/35720 Training loss: 0.7707 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 23024/35720 Training loss: 0.7707 0.2077 sec/batch\n",
      "Epoch 13/20  Iteration 23025/35720 Training loss: 0.7707 0.2079 sec/batch\n",
      "Epoch 13/20  Iteration 23026/35720 Training loss: 0.7707 0.2117 sec/batch\n",
      "Epoch 13/20  Iteration 23027/35720 Training loss: 0.7706 0.2186 sec/batch\n",
      "Epoch 13/20  Iteration 23028/35720 Training loss: 0.7705 0.2111 sec/batch\n",
      "Epoch 13/20  Iteration 23029/35720 Training loss: 0.7705 0.2234 sec/batch\n",
      "Epoch 13/20  Iteration 23030/35720 Training loss: 0.7705 0.2144 sec/batch\n",
      "Epoch 13/20  Iteration 23031/35720 Training loss: 0.7705 0.2133 sec/batch\n",
      "Epoch 13/20  Iteration 23032/35720 Training loss: 0.7705 0.2346 sec/batch\n",
      "Epoch 13/20  Iteration 23033/35720 Training loss: 0.7704 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 23034/35720 Training loss: 0.7704 0.2200 sec/batch\n",
      "Epoch 13/20  Iteration 23035/35720 Training loss: 0.7704 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 23036/35720 Training loss: 0.7704 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 23037/35720 Training loss: 0.7703 0.2246 sec/batch\n",
      "Epoch 13/20  Iteration 23038/35720 Training loss: 0.7703 0.2150 sec/batch\n",
      "Epoch 13/20  Iteration 23039/35720 Training loss: 0.7703 0.2158 sec/batch\n",
      "Epoch 13/20  Iteration 23040/35720 Training loss: 0.7703 0.2254 sec/batch\n",
      "Epoch 13/20  Iteration 23041/35720 Training loss: 0.7702 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 23042/35720 Training loss: 0.7702 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 23043/35720 Training loss: 0.7702 0.2178 sec/batch\n",
      "Epoch 13/20  Iteration 23044/35720 Training loss: 0.7701 0.2317 sec/batch\n",
      "Epoch 13/20  Iteration 23045/35720 Training loss: 0.7701 0.2327 sec/batch\n",
      "Epoch 13/20  Iteration 23046/35720 Training loss: 0.7701 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 23047/35720 Training loss: 0.7701 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 23048/35720 Training loss: 0.7701 0.2158 sec/batch\n",
      "Epoch 13/20  Iteration 23049/35720 Training loss: 0.7701 0.2236 sec/batch\n",
      "Epoch 13/20  Iteration 23050/35720 Training loss: 0.7701 0.2210 sec/batch\n",
      "Epoch 13/20  Iteration 23051/35720 Training loss: 0.7701 0.2252 sec/batch\n",
      "Epoch 13/20  Iteration 23052/35720 Training loss: 0.7701 0.2091 sec/batch\n",
      "Epoch 13/20  Iteration 23053/35720 Training loss: 0.7700 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 23054/35720 Training loss: 0.7700 0.2161 sec/batch\n",
      "Epoch 13/20  Iteration 23055/35720 Training loss: 0.7700 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 23056/35720 Training loss: 0.7700 0.2251 sec/batch\n",
      "Epoch 13/20  Iteration 23057/35720 Training loss: 0.7700 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 23058/35720 Training loss: 0.7700 0.2127 sec/batch\n",
      "Epoch 13/20  Iteration 23059/35720 Training loss: 0.7700 0.2076 sec/batch\n",
      "Epoch 13/20  Iteration 23060/35720 Training loss: 0.7700 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 23061/35720 Training loss: 0.7699 0.2120 sec/batch\n",
      "Epoch 13/20  Iteration 23062/35720 Training loss: 0.7699 0.2122 sec/batch\n",
      "Epoch 13/20  Iteration 23063/35720 Training loss: 0.7699 0.2175 sec/batch\n",
      "Epoch 13/20  Iteration 23064/35720 Training loss: 0.7698 0.2186 sec/batch\n",
      "Epoch 13/20  Iteration 23065/35720 Training loss: 0.7698 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 23066/35720 Training loss: 0.7699 0.2238 sec/batch\n",
      "Epoch 13/20  Iteration 23067/35720 Training loss: 0.7698 0.2229 sec/batch\n",
      "Epoch 13/20  Iteration 23068/35720 Training loss: 0.7699 0.2101 sec/batch\n",
      "Epoch 13/20  Iteration 23069/35720 Training loss: 0.7698 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 23070/35720 Training loss: 0.7698 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 23071/35720 Training loss: 0.7698 0.2179 sec/batch\n",
      "Epoch 13/20  Iteration 23072/35720 Training loss: 0.7698 0.2287 sec/batch\n",
      "Epoch 13/20  Iteration 23073/35720 Training loss: 0.7698 0.2276 sec/batch\n",
      "Epoch 13/20  Iteration 23074/35720 Training loss: 0.7698 0.2220 sec/batch\n",
      "Epoch 13/20  Iteration 23075/35720 Training loss: 0.7697 0.2097 sec/batch\n",
      "Epoch 13/20  Iteration 23076/35720 Training loss: 0.7697 0.2126 sec/batch\n",
      "Epoch 13/20  Iteration 23077/35720 Training loss: 0.7697 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 23078/35720 Training loss: 0.7697 0.2169 sec/batch\n",
      "Epoch 13/20  Iteration 23079/35720 Training loss: 0.7698 0.2198 sec/batch\n",
      "Epoch 13/20  Iteration 23080/35720 Training loss: 0.7698 0.2173 sec/batch\n",
      "Epoch 13/20  Iteration 23081/35720 Training loss: 0.7698 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 23082/35720 Training loss: 0.7698 0.2144 sec/batch\n",
      "Epoch 13/20  Iteration 23083/35720 Training loss: 0.7698 0.2145 sec/batch\n",
      "Epoch 13/20  Iteration 23084/35720 Training loss: 0.7698 0.2205 sec/batch\n",
      "Epoch 13/20  Iteration 23085/35720 Training loss: 0.7698 0.2185 sec/batch\n",
      "Epoch 13/20  Iteration 23086/35720 Training loss: 0.7698 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 23087/35720 Training loss: 0.7698 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 23088/35720 Training loss: 0.7698 0.2129 sec/batch\n",
      "Epoch 13/20  Iteration 23089/35720 Training loss: 0.7698 0.2162 sec/batch\n",
      "Epoch 13/20  Iteration 23090/35720 Training loss: 0.7698 0.2274 sec/batch\n",
      "Epoch 13/20  Iteration 23091/35720 Training loss: 0.7698 0.2120 sec/batch\n",
      "Epoch 13/20  Iteration 23092/35720 Training loss: 0.7698 0.2199 sec/batch\n",
      "Epoch 13/20  Iteration 23093/35720 Training loss: 0.7698 0.2230 sec/batch\n",
      "Epoch 13/20  Iteration 23094/35720 Training loss: 0.7698 0.2131 sec/batch\n",
      "Epoch 13/20  Iteration 23095/35720 Training loss: 0.7698 0.2222 sec/batch\n",
      "Epoch 13/20  Iteration 23096/35720 Training loss: 0.7697 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 23097/35720 Training loss: 0.7698 0.2061 sec/batch\n",
      "Epoch 13/20  Iteration 23098/35720 Training loss: 0.7698 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 23099/35720 Training loss: 0.7698 0.2238 sec/batch\n",
      "Epoch 13/20  Iteration 23100/35720 Training loss: 0.7698 0.2146 sec/batch\n",
      "Epoch 13/20  Iteration 23101/35720 Training loss: 0.7698 0.2178 sec/batch\n",
      "Epoch 13/20  Iteration 23102/35720 Training loss: 0.7698 0.2067 sec/batch\n",
      "Epoch 13/20  Iteration 23103/35720 Training loss: 0.7698 0.2106 sec/batch\n",
      "Epoch 13/20  Iteration 23104/35720 Training loss: 0.7698 0.2176 sec/batch\n",
      "Epoch 13/20  Iteration 23105/35720 Training loss: 0.7698 0.2317 sec/batch\n",
      "Epoch 13/20  Iteration 23106/35720 Training loss: 0.7699 0.2121 sec/batch\n",
      "Epoch 13/20  Iteration 23107/35720 Training loss: 0.7698 0.2063 sec/batch\n",
      "Epoch 13/20  Iteration 23108/35720 Training loss: 0.7698 0.2056 sec/batch\n",
      "Epoch 13/20  Iteration 23109/35720 Training loss: 0.7698 0.2135 sec/batch\n",
      "Epoch 13/20  Iteration 23110/35720 Training loss: 0.7698 0.2214 sec/batch\n",
      "Epoch 13/20  Iteration 23111/35720 Training loss: 0.7698 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 23112/35720 Training loss: 0.7698 0.2374 sec/batch\n",
      "Epoch 13/20  Iteration 23113/35720 Training loss: 0.7697 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 23114/35720 Training loss: 0.7697 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 23115/35720 Training loss: 0.7697 0.2271 sec/batch\n",
      "Epoch 13/20  Iteration 23116/35720 Training loss: 0.7697 0.2283 sec/batch\n",
      "Epoch 13/20  Iteration 23117/35720 Training loss: 0.7697 0.2266 sec/batch\n",
      "Epoch 13/20  Iteration 23118/35720 Training loss: 0.7697 0.2072 sec/batch\n",
      "Epoch 13/20  Iteration 23119/35720 Training loss: 0.7697 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 23120/35720 Training loss: 0.7697 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 23121/35720 Training loss: 0.7697 0.2159 sec/batch\n",
      "Epoch 13/20  Iteration 23122/35720 Training loss: 0.7697 0.2883 sec/batch\n",
      "Epoch 13/20  Iteration 23123/35720 Training loss: 0.7697 0.2222 sec/batch\n",
      "Epoch 13/20  Iteration 23124/35720 Training loss: 0.7696 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 23125/35720 Training loss: 0.7696 0.2095 sec/batch\n",
      "Epoch 13/20  Iteration 23126/35720 Training loss: 0.7696 0.2137 sec/batch\n",
      "Epoch 13/20  Iteration 23127/35720 Training loss: 0.7696 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 23128/35720 Training loss: 0.7696 0.2219 sec/batch\n",
      "Epoch 13/20  Iteration 23129/35720 Training loss: 0.7696 0.2154 sec/batch\n",
      "Epoch 13/20  Iteration 23130/35720 Training loss: 0.7696 0.2070 sec/batch\n",
      "Epoch 13/20  Iteration 23131/35720 Training loss: 0.7696 0.2092 sec/batch\n",
      "Epoch 13/20  Iteration 23132/35720 Training loss: 0.7696 0.2073 sec/batch\n",
      "Epoch 13/20  Iteration 23133/35720 Training loss: 0.7696 0.2093 sec/batch\n",
      "Epoch 13/20  Iteration 23134/35720 Training loss: 0.7695 0.2124 sec/batch\n",
      "Epoch 13/20  Iteration 23135/35720 Training loss: 0.7695 0.2054 sec/batch\n",
      "Epoch 13/20  Iteration 23136/35720 Training loss: 0.7695 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 23137/35720 Training loss: 0.7695 0.2118 sec/batch\n",
      "Epoch 13/20  Iteration 23138/35720 Training loss: 0.7694 0.2181 sec/batch\n",
      "Epoch 13/20  Iteration 23139/35720 Training loss: 0.7694 0.2313 sec/batch\n",
      "Epoch 13/20  Iteration 23140/35720 Training loss: 0.7694 0.2168 sec/batch\n",
      "Epoch 13/20  Iteration 23141/35720 Training loss: 0.7694 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 23142/35720 Training loss: 0.7694 0.2226 sec/batch\n",
      "Epoch 13/20  Iteration 23143/35720 Training loss: 0.7695 0.2266 sec/batch\n",
      "Epoch 13/20  Iteration 23144/35720 Training loss: 0.7695 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 23145/35720 Training loss: 0.7695 0.2334 sec/batch\n",
      "Epoch 13/20  Iteration 23146/35720 Training loss: 0.7695 0.2104 sec/batch\n",
      "Epoch 13/20  Iteration 23147/35720 Training loss: 0.7695 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 23148/35720 Training loss: 0.7695 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 23149/35720 Training loss: 0.7695 0.2175 sec/batch\n",
      "Epoch 13/20  Iteration 23150/35720 Training loss: 0.7695 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 23151/35720 Training loss: 0.7694 0.2225 sec/batch\n",
      "Epoch 13/20  Iteration 23152/35720 Training loss: 0.7695 0.2127 sec/batch\n",
      "Epoch 13/20  Iteration 23153/35720 Training loss: 0.7695 0.2321 sec/batch\n",
      "Epoch 13/20  Iteration 23154/35720 Training loss: 0.7694 0.2211 sec/batch\n",
      "Epoch 13/20  Iteration 23155/35720 Training loss: 0.7694 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 23156/35720 Training loss: 0.7694 0.2188 sec/batch\n",
      "Epoch 13/20  Iteration 23157/35720 Training loss: 0.7694 0.2059 sec/batch\n",
      "Epoch 13/20  Iteration 23158/35720 Training loss: 0.7694 0.2098 sec/batch\n",
      "Epoch 13/20  Iteration 23159/35720 Training loss: 0.7694 0.2094 sec/batch\n",
      "Epoch 13/20  Iteration 23160/35720 Training loss: 0.7694 0.2195 sec/batch\n",
      "Epoch 13/20  Iteration 23161/35720 Training loss: 0.7694 0.2197 sec/batch\n",
      "Epoch 13/20  Iteration 23162/35720 Training loss: 0.7694 0.2160 sec/batch\n",
      "Epoch 13/20  Iteration 23163/35720 Training loss: 0.7694 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 23164/35720 Training loss: 0.7694 0.2069 sec/batch\n",
      "Epoch 13/20  Iteration 23165/35720 Training loss: 0.7694 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 23166/35720 Training loss: 0.7694 0.2256 sec/batch\n",
      "Epoch 13/20  Iteration 23167/35720 Training loss: 0.7694 0.2096 sec/batch\n",
      "Epoch 13/20  Iteration 23168/35720 Training loss: 0.7694 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 23169/35720 Training loss: 0.7695 0.2086 sec/batch\n",
      "Epoch 13/20  Iteration 23170/35720 Training loss: 0.7694 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 23171/35720 Training loss: 0.7695 0.2193 sec/batch\n",
      "Epoch 13/20  Iteration 23172/35720 Training loss: 0.7694 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 23173/35720 Training loss: 0.7694 0.2055 sec/batch\n",
      "Epoch 13/20  Iteration 23174/35720 Training loss: 0.7694 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 23175/35720 Training loss: 0.7694 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 23176/35720 Training loss: 0.7694 0.2347 sec/batch\n",
      "Epoch 13/20  Iteration 23177/35720 Training loss: 0.7694 0.2170 sec/batch\n",
      "Epoch 13/20  Iteration 23178/35720 Training loss: 0.7695 0.2089 sec/batch\n",
      "Epoch 13/20  Iteration 23179/35720 Training loss: 0.7695 0.2113 sec/batch\n",
      "Epoch 13/20  Iteration 23180/35720 Training loss: 0.7695 0.2060 sec/batch\n",
      "Epoch 13/20  Iteration 23181/35720 Training loss: 0.7695 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 23182/35720 Training loss: 0.7695 0.2145 sec/batch\n",
      "Epoch 13/20  Iteration 23183/35720 Training loss: 0.7695 0.2200 sec/batch\n",
      "Epoch 13/20  Iteration 23184/35720 Training loss: 0.7695 0.2142 sec/batch\n",
      "Epoch 13/20  Iteration 23185/35720 Training loss: 0.7695 0.2172 sec/batch\n",
      "Epoch 13/20  Iteration 23186/35720 Training loss: 0.7695 0.2100 sec/batch\n",
      "Epoch 13/20  Iteration 23187/35720 Training loss: 0.7695 0.2085 sec/batch\n",
      "Epoch 13/20  Iteration 23188/35720 Training loss: 0.7695 0.2158 sec/batch\n",
      "Epoch 13/20  Iteration 23189/35720 Training loss: 0.7695 0.2188 sec/batch\n",
      "Epoch 13/20  Iteration 23190/35720 Training loss: 0.7695 0.2167 sec/batch\n",
      "Epoch 13/20  Iteration 23191/35720 Training loss: 0.7695 0.2188 sec/batch\n",
      "Epoch 13/20  Iteration 23192/35720 Training loss: 0.7695 0.2051 sec/batch\n",
      "Epoch 13/20  Iteration 23193/35720 Training loss: 0.7695 0.2108 sec/batch\n",
      "Epoch 13/20  Iteration 23194/35720 Training loss: 0.7695 0.2220 sec/batch\n",
      "Epoch 13/20  Iteration 23195/35720 Training loss: 0.7695 0.2090 sec/batch\n",
      "Epoch 13/20  Iteration 23196/35720 Training loss: 0.7694 0.2216 sec/batch\n",
      "Epoch 13/20  Iteration 23197/35720 Training loss: 0.7694 0.2066 sec/batch\n",
      "Epoch 13/20  Iteration 23198/35720 Training loss: 0.7694 0.2132 sec/batch\n",
      "Epoch 13/20  Iteration 23199/35720 Training loss: 0.7694 0.2119 sec/batch\n",
      "Epoch 13/20  Iteration 23200/35720 Training loss: 0.7695 0.2074 sec/batch\n",
      "Validation loss: 1.49139 Saving checkpoint!\n",
      "Epoch 13/20  Iteration 23201/35720 Training loss: 0.7696 0.2082 sec/batch\n",
      "Epoch 13/20  Iteration 23202/35720 Training loss: 0.7696 0.2087 sec/batch\n",
      "Epoch 13/20  Iteration 23203/35720 Training loss: 0.7696 0.2157 sec/batch\n",
      "Epoch 13/20  Iteration 23204/35720 Training loss: 0.7696 0.2191 sec/batch\n",
      "Epoch 13/20  Iteration 23205/35720 Training loss: 0.7696 0.2130 sec/batch\n",
      "Epoch 13/20  Iteration 23206/35720 Training loss: 0.7696 0.2241 sec/batch\n",
      "Epoch 13/20  Iteration 23207/35720 Training loss: 0.7696 0.2120 sec/batch\n",
      "Epoch 13/20  Iteration 23208/35720 Training loss: 0.7695 0.2071 sec/batch\n",
      "Epoch 13/20  Iteration 23209/35720 Training loss: 0.7696 0.2314 sec/batch\n",
      "Epoch 13/20  Iteration 23210/35720 Training loss: 0.7695 0.2190 sec/batch\n",
      "Epoch 13/20  Iteration 23211/35720 Training loss: 0.7695 0.2217 sec/batch\n",
      "Epoch 13/20  Iteration 23212/35720 Training loss: 0.7695 0.2058 sec/batch\n",
      "Epoch 13/20  Iteration 23213/35720 Training loss: 0.7694 0.2068 sec/batch\n",
      "Epoch 13/20  Iteration 23214/35720 Training loss: 0.7694 0.2105 sec/batch\n",
      "Epoch 13/20  Iteration 23215/35720 Training loss: 0.7694 0.2295 sec/batch\n",
      "Epoch 13/20  Iteration 23216/35720 Training loss: 0.7694 0.2249 sec/batch\n",
      "Epoch 13/20  Iteration 23217/35720 Training loss: 0.7694 0.2228 sec/batch\n",
      "Epoch 13/20  Iteration 23218/35720 Training loss: 0.7694 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 23219/35720 Training loss: 0.7906 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23220/35720 Training loss: 0.7931 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 23221/35720 Training loss: 0.7832 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 23222/35720 Training loss: 0.7778 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 23223/35720 Training loss: 0.7841 0.2166 sec/batch\n",
      "Epoch 14/20  Iteration 23224/35720 Training loss: 0.7742 0.2151 sec/batch\n",
      "Epoch 14/20  Iteration 23225/35720 Training loss: 0.7773 0.2125 sec/batch\n",
      "Epoch 14/20  Iteration 23226/35720 Training loss: 0.7706 0.2154 sec/batch\n",
      "Epoch 14/20  Iteration 23227/35720 Training loss: 0.7647 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 23228/35720 Training loss: 0.7677 0.2208 sec/batch\n",
      "Epoch 14/20  Iteration 23229/35720 Training loss: 0.7681 0.2385 sec/batch\n",
      "Epoch 14/20  Iteration 23230/35720 Training loss: 0.7624 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 23231/35720 Training loss: 0.7641 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 23232/35720 Training loss: 0.7669 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 23233/35720 Training loss: 0.7673 0.2233 sec/batch\n",
      "Epoch 14/20  Iteration 23234/35720 Training loss: 0.7658 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 23235/35720 Training loss: 0.7658 0.2128 sec/batch\n",
      "Epoch 14/20  Iteration 23236/35720 Training loss: 0.7624 0.2119 sec/batch\n",
      "Epoch 14/20  Iteration 23237/35720 Training loss: 0.7606 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 23238/35720 Training loss: 0.7607 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 23239/35720 Training loss: 0.7617 0.2175 sec/batch\n",
      "Epoch 14/20  Iteration 23240/35720 Training loss: 0.7604 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 23241/35720 Training loss: 0.7596 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 23242/35720 Training loss: 0.7621 0.2081 sec/batch\n",
      "Epoch 14/20  Iteration 23243/35720 Training loss: 0.7637 0.2209 sec/batch\n",
      "Epoch 14/20  Iteration 23244/35720 Training loss: 0.7633 0.2107 sec/batch\n",
      "Epoch 14/20  Iteration 23245/35720 Training loss: 0.7657 0.2176 sec/batch\n",
      "Epoch 14/20  Iteration 23246/35720 Training loss: 0.7661 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 23247/35720 Training loss: 0.7646 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 23248/35720 Training loss: 0.7653 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 23249/35720 Training loss: 0.7672 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 23250/35720 Training loss: 0.7658 0.2235 sec/batch\n",
      "Epoch 14/20  Iteration 23251/35720 Training loss: 0.7677 0.2210 sec/batch\n",
      "Epoch 14/20  Iteration 23252/35720 Training loss: 0.7710 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 23253/35720 Training loss: 0.7735 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 23254/35720 Training loss: 0.7735 0.2316 sec/batch\n",
      "Epoch 14/20  Iteration 23255/35720 Training loss: 0.7735 0.2128 sec/batch\n",
      "Epoch 14/20  Iteration 23256/35720 Training loss: 0.7730 0.2240 sec/batch\n",
      "Epoch 14/20  Iteration 23257/35720 Training loss: 0.7722 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 23258/35720 Training loss: 0.7739 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 23259/35720 Training loss: 0.7741 0.2142 sec/batch\n",
      "Epoch 14/20  Iteration 23260/35720 Training loss: 0.7733 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 23261/35720 Training loss: 0.7716 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 23262/35720 Training loss: 0.7710 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 23263/35720 Training loss: 0.7705 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23264/35720 Training loss: 0.7704 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 23265/35720 Training loss: 0.7698 0.2172 sec/batch\n",
      "Epoch 14/20  Iteration 23266/35720 Training loss: 0.7691 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 23267/35720 Training loss: 0.7683 0.2142 sec/batch\n",
      "Epoch 14/20  Iteration 23268/35720 Training loss: 0.7676 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 23269/35720 Training loss: 0.7681 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 23270/35720 Training loss: 0.7681 0.2125 sec/batch\n",
      "Epoch 14/20  Iteration 23271/35720 Training loss: 0.7682 0.2254 sec/batch\n",
      "Epoch 14/20  Iteration 23272/35720 Training loss: 0.7664 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 23273/35720 Training loss: 0.7656 0.2237 sec/batch\n",
      "Epoch 14/20  Iteration 23274/35720 Training loss: 0.7651 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 23275/35720 Training loss: 0.7648 0.2078 sec/batch\n",
      "Epoch 14/20  Iteration 23276/35720 Training loss: 0.7643 0.2077 sec/batch\n",
      "Epoch 14/20  Iteration 23277/35720 Training loss: 0.7633 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 23278/35720 Training loss: 0.7622 0.2303 sec/batch\n",
      "Epoch 14/20  Iteration 23279/35720 Training loss: 0.7619 0.2269 sec/batch\n",
      "Epoch 14/20  Iteration 23280/35720 Training loss: 0.7603 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 23281/35720 Training loss: 0.7612 0.2101 sec/batch\n",
      "Epoch 14/20  Iteration 23282/35720 Training loss: 0.7611 0.2222 sec/batch\n",
      "Epoch 14/20  Iteration 23283/35720 Training loss: 0.7616 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 23284/35720 Training loss: 0.7619 0.2154 sec/batch\n",
      "Epoch 14/20  Iteration 23285/35720 Training loss: 0.7614 0.2103 sec/batch\n",
      "Epoch 14/20  Iteration 23286/35720 Training loss: 0.7608 0.2206 sec/batch\n",
      "Epoch 14/20  Iteration 23287/35720 Training loss: 0.7615 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 23288/35720 Training loss: 0.7613 0.2176 sec/batch\n",
      "Epoch 14/20  Iteration 23289/35720 Training loss: 0.7616 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 23290/35720 Training loss: 0.7622 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23291/35720 Training loss: 0.7623 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 23292/35720 Training loss: 0.7622 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23293/35720 Training loss: 0.7617 0.2220 sec/batch\n",
      "Epoch 14/20  Iteration 23294/35720 Training loss: 0.7616 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 23295/35720 Training loss: 0.7611 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 23296/35720 Training loss: 0.7616 0.2166 sec/batch\n",
      "Epoch 14/20  Iteration 23297/35720 Training loss: 0.7615 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 23298/35720 Training loss: 0.7622 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23299/35720 Training loss: 0.7627 0.2278 sec/batch\n",
      "Epoch 14/20  Iteration 23300/35720 Training loss: 0.7623 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 23301/35720 Training loss: 0.7623 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 23302/35720 Training loss: 0.7624 0.2175 sec/batch\n",
      "Epoch 14/20  Iteration 23303/35720 Training loss: 0.7624 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 23304/35720 Training loss: 0.7622 0.2112 sec/batch\n",
      "Epoch 14/20  Iteration 23305/35720 Training loss: 0.7621 0.2153 sec/batch\n",
      "Epoch 14/20  Iteration 23306/35720 Training loss: 0.7618 0.2276 sec/batch\n",
      "Epoch 14/20  Iteration 23307/35720 Training loss: 0.7608 0.2099 sec/batch\n",
      "Epoch 14/20  Iteration 23308/35720 Training loss: 0.7603 0.2142 sec/batch\n",
      "Epoch 14/20  Iteration 23309/35720 Training loss: 0.7604 0.2151 sec/batch\n",
      "Epoch 14/20  Iteration 23310/35720 Training loss: 0.7602 0.2216 sec/batch\n",
      "Epoch 14/20  Iteration 23311/35720 Training loss: 0.7606 0.2134 sec/batch\n",
      "Epoch 14/20  Iteration 23312/35720 Training loss: 0.7610 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 23313/35720 Training loss: 0.7612 0.2058 sec/batch\n",
      "Epoch 14/20  Iteration 23314/35720 Training loss: 0.7608 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 23315/35720 Training loss: 0.7611 0.2197 sec/batch\n",
      "Epoch 14/20  Iteration 23316/35720 Training loss: 0.7610 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 23317/35720 Training loss: 0.7610 0.2113 sec/batch\n",
      "Epoch 14/20  Iteration 23318/35720 Training loss: 0.7609 0.2239 sec/batch\n",
      "Epoch 14/20  Iteration 23319/35720 Training loss: 0.7604 0.2156 sec/batch\n",
      "Epoch 14/20  Iteration 23320/35720 Training loss: 0.7605 0.2135 sec/batch\n",
      "Epoch 14/20  Iteration 23321/35720 Training loss: 0.7601 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 23322/35720 Training loss: 0.7597 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 23323/35720 Training loss: 0.7596 0.2210 sec/batch\n",
      "Epoch 14/20  Iteration 23324/35720 Training loss: 0.7593 0.2191 sec/batch\n",
      "Epoch 14/20  Iteration 23325/35720 Training loss: 0.7593 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 23326/35720 Training loss: 0.7594 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 23327/35720 Training loss: 0.7598 0.2261 sec/batch\n",
      "Epoch 14/20  Iteration 23328/35720 Training loss: 0.7595 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 23329/35720 Training loss: 0.7597 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 23330/35720 Training loss: 0.7600 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 23331/35720 Training loss: 0.7599 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 23332/35720 Training loss: 0.7602 0.2221 sec/batch\n",
      "Epoch 14/20  Iteration 23333/35720 Training loss: 0.7600 0.2240 sec/batch\n",
      "Epoch 14/20  Iteration 23334/35720 Training loss: 0.7600 0.2285 sec/batch\n",
      "Epoch 14/20  Iteration 23335/35720 Training loss: 0.7599 0.2227 sec/batch\n",
      "Epoch 14/20  Iteration 23336/35720 Training loss: 0.7602 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23337/35720 Training loss: 0.7600 0.2213 sec/batch\n",
      "Epoch 14/20  Iteration 23338/35720 Training loss: 0.7607 0.2355 sec/batch\n",
      "Epoch 14/20  Iteration 23339/35720 Training loss: 0.7609 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 23340/35720 Training loss: 0.7603 0.2213 sec/batch\n",
      "Epoch 14/20  Iteration 23341/35720 Training loss: 0.7604 0.2208 sec/batch\n",
      "Epoch 14/20  Iteration 23342/35720 Training loss: 0.7610 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 23343/35720 Training loss: 0.7606 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 23344/35720 Training loss: 0.7606 0.2273 sec/batch\n",
      "Epoch 14/20  Iteration 23345/35720 Training loss: 0.7607 0.2283 sec/batch\n",
      "Epoch 14/20  Iteration 23346/35720 Training loss: 0.7606 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 23347/35720 Training loss: 0.7602 0.2214 sec/batch\n",
      "Epoch 14/20  Iteration 23348/35720 Training loss: 0.7603 0.2152 sec/batch\n",
      "Epoch 14/20  Iteration 23349/35720 Training loss: 0.7600 0.2489 sec/batch\n",
      "Epoch 14/20  Iteration 23350/35720 Training loss: 0.7597 0.2200 sec/batch\n",
      "Epoch 14/20  Iteration 23351/35720 Training loss: 0.7597 0.2260 sec/batch\n",
      "Epoch 14/20  Iteration 23352/35720 Training loss: 0.7597 0.2178 sec/batch\n",
      "Epoch 14/20  Iteration 23353/35720 Training loss: 0.7596 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 23354/35720 Training loss: 0.7596 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 23355/35720 Training loss: 0.7600 0.2192 sec/batch\n",
      "Epoch 14/20  Iteration 23356/35720 Training loss: 0.7602 0.2240 sec/batch\n",
      "Epoch 14/20  Iteration 23357/35720 Training loss: 0.7601 0.2073 sec/batch\n",
      "Epoch 14/20  Iteration 23358/35720 Training loss: 0.7603 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 23359/35720 Training loss: 0.7602 0.2242 sec/batch\n",
      "Epoch 14/20  Iteration 23360/35720 Training loss: 0.7598 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 23361/35720 Training loss: 0.7593 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 23362/35720 Training loss: 0.7588 0.2192 sec/batch\n",
      "Epoch 14/20  Iteration 23363/35720 Training loss: 0.7590 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 23364/35720 Training loss: 0.7594 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 23365/35720 Training loss: 0.7592 0.2081 sec/batch\n",
      "Epoch 14/20  Iteration 23366/35720 Training loss: 0.7592 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 23367/35720 Training loss: 0.7593 0.2122 sec/batch\n",
      "Epoch 14/20  Iteration 23368/35720 Training loss: 0.7586 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23369/35720 Training loss: 0.7586 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 23370/35720 Training loss: 0.7586 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 23371/35720 Training loss: 0.7588 0.2299 sec/batch\n",
      "Epoch 14/20  Iteration 23372/35720 Training loss: 0.7591 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23373/35720 Training loss: 0.7592 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 23374/35720 Training loss: 0.7595 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23375/35720 Training loss: 0.7596 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 23376/35720 Training loss: 0.7599 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 23377/35720 Training loss: 0.7595 0.2234 sec/batch\n",
      "Epoch 14/20  Iteration 23378/35720 Training loss: 0.7595 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 23379/35720 Training loss: 0.7593 0.2284 sec/batch\n",
      "Epoch 14/20  Iteration 23380/35720 Training loss: 0.7593 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 23381/35720 Training loss: 0.7594 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 23382/35720 Training loss: 0.7595 0.2319 sec/batch\n",
      "Epoch 14/20  Iteration 23383/35720 Training loss: 0.7595 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 23384/35720 Training loss: 0.7595 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 23385/35720 Training loss: 0.7596 0.2310 sec/batch\n",
      "Epoch 14/20  Iteration 23386/35720 Training loss: 0.7598 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23387/35720 Training loss: 0.7603 0.2073 sec/batch\n",
      "Epoch 14/20  Iteration 23388/35720 Training loss: 0.7605 0.2122 sec/batch\n",
      "Epoch 14/20  Iteration 23389/35720 Training loss: 0.7609 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 23390/35720 Training loss: 0.7612 0.2300 sec/batch\n",
      "Epoch 14/20  Iteration 23391/35720 Training loss: 0.7615 0.2124 sec/batch\n",
      "Epoch 14/20  Iteration 23392/35720 Training loss: 0.7619 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 23393/35720 Training loss: 0.7621 0.2409 sec/batch\n",
      "Epoch 14/20  Iteration 23394/35720 Training loss: 0.7622 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 23395/35720 Training loss: 0.7624 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 23396/35720 Training loss: 0.7623 0.2056 sec/batch\n",
      "Epoch 14/20  Iteration 23397/35720 Training loss: 0.7620 0.2235 sec/batch\n",
      "Epoch 14/20  Iteration 23398/35720 Training loss: 0.7618 0.2191 sec/batch\n",
      "Epoch 14/20  Iteration 23399/35720 Training loss: 0.7617 0.2165 sec/batch\n",
      "Epoch 14/20  Iteration 23400/35720 Training loss: 0.7619 0.2154 sec/batch\n",
      "Validation loss: 1.50415 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 23401/35720 Training loss: 0.7645 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 23402/35720 Training loss: 0.7648 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 23403/35720 Training loss: 0.7647 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 23404/35720 Training loss: 0.7647 0.2224 sec/batch\n",
      "Epoch 14/20  Iteration 23405/35720 Training loss: 0.7645 0.2103 sec/batch\n",
      "Epoch 14/20  Iteration 23406/35720 Training loss: 0.7646 0.2172 sec/batch\n",
      "Epoch 14/20  Iteration 23407/35720 Training loss: 0.7648 0.2220 sec/batch\n",
      "Epoch 14/20  Iteration 23408/35720 Training loss: 0.7648 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 23409/35720 Training loss: 0.7649 0.2213 sec/batch\n",
      "Epoch 14/20  Iteration 23410/35720 Training loss: 0.7654 0.2249 sec/batch\n",
      "Epoch 14/20  Iteration 23411/35720 Training loss: 0.7656 0.2253 sec/batch\n",
      "Epoch 14/20  Iteration 23412/35720 Training loss: 0.7658 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 23413/35720 Training loss: 0.7659 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 23414/35720 Training loss: 0.7661 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 23415/35720 Training loss: 0.7659 0.2113 sec/batch\n",
      "Epoch 14/20  Iteration 23416/35720 Training loss: 0.7658 0.2377 sec/batch\n",
      "Epoch 14/20  Iteration 23417/35720 Training loss: 0.7659 0.2236 sec/batch\n",
      "Epoch 14/20  Iteration 23418/35720 Training loss: 0.7659 0.2107 sec/batch\n",
      "Epoch 14/20  Iteration 23419/35720 Training loss: 0.7658 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 23420/35720 Training loss: 0.7659 0.2200 sec/batch\n",
      "Epoch 14/20  Iteration 23421/35720 Training loss: 0.7660 0.2053 sec/batch\n",
      "Epoch 14/20  Iteration 23422/35720 Training loss: 0.7658 0.2191 sec/batch\n",
      "Epoch 14/20  Iteration 23423/35720 Training loss: 0.7658 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 23424/35720 Training loss: 0.7657 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 23425/35720 Training loss: 0.7660 0.2112 sec/batch\n",
      "Epoch 14/20  Iteration 23426/35720 Training loss: 0.7663 0.2272 sec/batch\n",
      "Epoch 14/20  Iteration 23427/35720 Training loss: 0.7667 0.2209 sec/batch\n",
      "Epoch 14/20  Iteration 23428/35720 Training loss: 0.7667 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 23429/35720 Training loss: 0.7669 0.2210 sec/batch\n",
      "Epoch 14/20  Iteration 23430/35720 Training loss: 0.7669 0.2248 sec/batch\n",
      "Epoch 14/20  Iteration 23431/35720 Training loss: 0.7668 0.2251 sec/batch\n",
      "Epoch 14/20  Iteration 23432/35720 Training loss: 0.7667 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 23433/35720 Training loss: 0.7667 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 23434/35720 Training loss: 0.7666 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 23435/35720 Training loss: 0.7664 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 23436/35720 Training loss: 0.7662 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 23437/35720 Training loss: 0.7663 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 23438/35720 Training loss: 0.7663 0.2191 sec/batch\n",
      "Epoch 14/20  Iteration 23439/35720 Training loss: 0.7664 0.2264 sec/batch\n",
      "Epoch 14/20  Iteration 23440/35720 Training loss: 0.7664 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23441/35720 Training loss: 0.7667 0.2206 sec/batch\n",
      "Epoch 14/20  Iteration 23442/35720 Training loss: 0.7668 0.2190 sec/batch\n",
      "Epoch 14/20  Iteration 23443/35720 Training loss: 0.7669 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23444/35720 Training loss: 0.7668 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 23445/35720 Training loss: 0.7666 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 23446/35720 Training loss: 0.7666 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23447/35720 Training loss: 0.7662 0.2196 sec/batch\n",
      "Epoch 14/20  Iteration 23448/35720 Training loss: 0.7662 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 23449/35720 Training loss: 0.7665 0.2241 sec/batch\n",
      "Epoch 14/20  Iteration 23450/35720 Training loss: 0.7664 0.2179 sec/batch\n",
      "Epoch 14/20  Iteration 23451/35720 Training loss: 0.7664 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 23452/35720 Training loss: 0.7664 0.2073 sec/batch\n",
      "Epoch 14/20  Iteration 23453/35720 Training loss: 0.7663 0.2135 sec/batch\n",
      "Epoch 14/20  Iteration 23454/35720 Training loss: 0.7665 0.2190 sec/batch\n",
      "Epoch 14/20  Iteration 23455/35720 Training loss: 0.7666 0.2151 sec/batch\n",
      "Epoch 14/20  Iteration 23456/35720 Training loss: 0.7664 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 23457/35720 Training loss: 0.7663 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 23458/35720 Training loss: 0.7662 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 23459/35720 Training loss: 0.7660 0.2283 sec/batch\n",
      "Epoch 14/20  Iteration 23460/35720 Training loss: 0.7660 0.2218 sec/batch\n",
      "Epoch 14/20  Iteration 23461/35720 Training loss: 0.7659 0.2365 sec/batch\n",
      "Epoch 14/20  Iteration 23462/35720 Training loss: 0.7658 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 23463/35720 Training loss: 0.7654 0.2179 sec/batch\n",
      "Epoch 14/20  Iteration 23464/35720 Training loss: 0.7655 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 23465/35720 Training loss: 0.7654 0.2206 sec/batch\n",
      "Epoch 14/20  Iteration 23466/35720 Training loss: 0.7654 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 23467/35720 Training loss: 0.7652 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 23468/35720 Training loss: 0.7650 0.2108 sec/batch\n",
      "Epoch 14/20  Iteration 23469/35720 Training loss: 0.7650 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 23470/35720 Training loss: 0.7649 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 23471/35720 Training loss: 0.7646 0.2142 sec/batch\n",
      "Epoch 14/20  Iteration 23472/35720 Training loss: 0.7647 0.2197 sec/batch\n",
      "Epoch 14/20  Iteration 23473/35720 Training loss: 0.7649 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23474/35720 Training loss: 0.7649 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 23475/35720 Training loss: 0.7649 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 23476/35720 Training loss: 0.7648 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 23477/35720 Training loss: 0.7651 0.2103 sec/batch\n",
      "Epoch 14/20  Iteration 23478/35720 Training loss: 0.7652 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 23479/35720 Training loss: 0.7651 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 23480/35720 Training loss: 0.7649 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23481/35720 Training loss: 0.7649 0.2213 sec/batch\n",
      "Epoch 14/20  Iteration 23482/35720 Training loss: 0.7650 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 23483/35720 Training loss: 0.7649 0.2201 sec/batch\n",
      "Epoch 14/20  Iteration 23484/35720 Training loss: 0.7651 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23485/35720 Training loss: 0.7651 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 23486/35720 Training loss: 0.7651 0.2235 sec/batch\n",
      "Epoch 14/20  Iteration 23487/35720 Training loss: 0.7650 0.2185 sec/batch\n",
      "Epoch 14/20  Iteration 23488/35720 Training loss: 0.7646 0.2259 sec/batch\n",
      "Epoch 14/20  Iteration 23489/35720 Training loss: 0.7644 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 23490/35720 Training loss: 0.7644 0.2217 sec/batch\n",
      "Epoch 14/20  Iteration 23491/35720 Training loss: 0.7644 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 23492/35720 Training loss: 0.7644 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 23493/35720 Training loss: 0.7643 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 23494/35720 Training loss: 0.7642 0.2151 sec/batch\n",
      "Epoch 14/20  Iteration 23495/35720 Training loss: 0.7639 0.2108 sec/batch\n",
      "Epoch 14/20  Iteration 23496/35720 Training loss: 0.7638 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 23497/35720 Training loss: 0.7636 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 23498/35720 Training loss: 0.7636 0.2283 sec/batch\n",
      "Epoch 14/20  Iteration 23499/35720 Training loss: 0.7634 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 23500/35720 Training loss: 0.7632 0.2342 sec/batch\n",
      "Epoch 14/20  Iteration 23501/35720 Training loss: 0.7629 0.2118 sec/batch\n",
      "Epoch 14/20  Iteration 23502/35720 Training loss: 0.7628 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23503/35720 Training loss: 0.7630 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 23504/35720 Training loss: 0.7629 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 23505/35720 Training loss: 0.7626 0.2389 sec/batch\n",
      "Epoch 14/20  Iteration 23506/35720 Training loss: 0.7626 0.2180 sec/batch\n",
      "Epoch 14/20  Iteration 23507/35720 Training loss: 0.7626 0.2245 sec/batch\n",
      "Epoch 14/20  Iteration 23508/35720 Training loss: 0.7627 0.2125 sec/batch\n",
      "Epoch 14/20  Iteration 23509/35720 Training loss: 0.7626 0.2163 sec/batch\n",
      "Epoch 14/20  Iteration 23510/35720 Training loss: 0.7627 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 23511/35720 Training loss: 0.7626 0.2280 sec/batch\n",
      "Epoch 14/20  Iteration 23512/35720 Training loss: 0.7626 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 23513/35720 Training loss: 0.7629 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 23514/35720 Training loss: 0.7629 0.2190 sec/batch\n",
      "Epoch 14/20  Iteration 23515/35720 Training loss: 0.7629 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 23516/35720 Training loss: 0.7630 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 23517/35720 Training loss: 0.7628 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 23518/35720 Training loss: 0.7628 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 23519/35720 Training loss: 0.7629 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23520/35720 Training loss: 0.7628 0.2188 sec/batch\n",
      "Epoch 14/20  Iteration 23521/35720 Training loss: 0.7629 0.2175 sec/batch\n",
      "Epoch 14/20  Iteration 23522/35720 Training loss: 0.7629 0.2231 sec/batch\n",
      "Epoch 14/20  Iteration 23523/35720 Training loss: 0.7629 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23524/35720 Training loss: 0.7628 0.2071 sec/batch\n",
      "Epoch 14/20  Iteration 23525/35720 Training loss: 0.7628 0.2134 sec/batch\n",
      "Epoch 14/20  Iteration 23526/35720 Training loss: 0.7629 0.2273 sec/batch\n",
      "Epoch 14/20  Iteration 23527/35720 Training loss: 0.7628 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 23528/35720 Training loss: 0.7628 0.2340 sec/batch\n",
      "Epoch 14/20  Iteration 23529/35720 Training loss: 0.7628 0.2217 sec/batch\n",
      "Epoch 14/20  Iteration 23530/35720 Training loss: 0.7628 0.2135 sec/batch\n",
      "Epoch 14/20  Iteration 23531/35720 Training loss: 0.7628 0.2149 sec/batch\n",
      "Epoch 14/20  Iteration 23532/35720 Training loss: 0.7627 0.2156 sec/batch\n",
      "Epoch 14/20  Iteration 23533/35720 Training loss: 0.7626 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 23534/35720 Training loss: 0.7626 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 23535/35720 Training loss: 0.7624 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 23536/35720 Training loss: 0.7624 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23537/35720 Training loss: 0.7625 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 23538/35720 Training loss: 0.7624 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 23539/35720 Training loss: 0.7626 0.2118 sec/batch\n",
      "Epoch 14/20  Iteration 23540/35720 Training loss: 0.7625 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 23541/35720 Training loss: 0.7626 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 23542/35720 Training loss: 0.7627 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 23543/35720 Training loss: 0.7626 0.2231 sec/batch\n",
      "Epoch 14/20  Iteration 23544/35720 Training loss: 0.7627 0.2247 sec/batch\n",
      "Epoch 14/20  Iteration 23545/35720 Training loss: 0.7627 0.2200 sec/batch\n",
      "Epoch 14/20  Iteration 23546/35720 Training loss: 0.7627 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23547/35720 Training loss: 0.7626 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 23548/35720 Training loss: 0.7626 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 23549/35720 Training loss: 0.7627 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 23550/35720 Training loss: 0.7627 0.2312 sec/batch\n",
      "Epoch 14/20  Iteration 23551/35720 Training loss: 0.7627 0.2246 sec/batch\n",
      "Epoch 14/20  Iteration 23552/35720 Training loss: 0.7627 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 23553/35720 Training loss: 0.7627 0.2371 sec/batch\n",
      "Epoch 14/20  Iteration 23554/35720 Training loss: 0.7625 0.2305 sec/batch\n",
      "Epoch 14/20  Iteration 23555/35720 Training loss: 0.7625 0.2217 sec/batch\n",
      "Epoch 14/20  Iteration 23556/35720 Training loss: 0.7623 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 23557/35720 Training loss: 0.7623 0.2153 sec/batch\n",
      "Epoch 14/20  Iteration 23558/35720 Training loss: 0.7623 0.2290 sec/batch\n",
      "Epoch 14/20  Iteration 23559/35720 Training loss: 0.7622 0.2752 sec/batch\n",
      "Epoch 14/20  Iteration 23560/35720 Training loss: 0.7622 0.2249 sec/batch\n",
      "Epoch 14/20  Iteration 23561/35720 Training loss: 0.7623 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23562/35720 Training loss: 0.7623 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 23563/35720 Training loss: 0.7621 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 23564/35720 Training loss: 0.7622 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 23565/35720 Training loss: 0.7623 0.2212 sec/batch\n",
      "Epoch 14/20  Iteration 23566/35720 Training loss: 0.7624 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 23567/35720 Training loss: 0.7624 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23568/35720 Training loss: 0.7625 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 23569/35720 Training loss: 0.7625 0.2227 sec/batch\n",
      "Epoch 14/20  Iteration 23570/35720 Training loss: 0.7624 0.2163 sec/batch\n",
      "Epoch 14/20  Iteration 23571/35720 Training loss: 0.7624 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 23572/35720 Training loss: 0.7624 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 23573/35720 Training loss: 0.7624 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 23574/35720 Training loss: 0.7625 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 23575/35720 Training loss: 0.7626 0.2346 sec/batch\n",
      "Epoch 14/20  Iteration 23576/35720 Training loss: 0.7627 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 23577/35720 Training loss: 0.7627 0.2183 sec/batch\n",
      "Epoch 14/20  Iteration 23578/35720 Training loss: 0.7627 0.2079 sec/batch\n",
      "Epoch 14/20  Iteration 23579/35720 Training loss: 0.7627 0.2124 sec/batch\n",
      "Epoch 14/20  Iteration 23580/35720 Training loss: 0.7627 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 23581/35720 Training loss: 0.7627 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23582/35720 Training loss: 0.7628 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 23583/35720 Training loss: 0.7627 0.2212 sec/batch\n",
      "Epoch 14/20  Iteration 23584/35720 Training loss: 0.7627 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 23585/35720 Training loss: 0.7627 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 23586/35720 Training loss: 0.7627 0.2283 sec/batch\n",
      "Epoch 14/20  Iteration 23587/35720 Training loss: 0.7627 0.2136 sec/batch\n",
      "Epoch 14/20  Iteration 23588/35720 Training loss: 0.7625 0.2175 sec/batch\n",
      "Epoch 14/20  Iteration 23589/35720 Training loss: 0.7625 0.2073 sec/batch\n",
      "Epoch 14/20  Iteration 23590/35720 Training loss: 0.7624 0.2099 sec/batch\n",
      "Epoch 14/20  Iteration 23591/35720 Training loss: 0.7625 0.2099 sec/batch\n",
      "Epoch 14/20  Iteration 23592/35720 Training loss: 0.7624 0.2333 sec/batch\n",
      "Epoch 14/20  Iteration 23593/35720 Training loss: 0.7625 0.2078 sec/batch\n",
      "Epoch 14/20  Iteration 23594/35720 Training loss: 0.7625 0.2183 sec/batch\n",
      "Epoch 14/20  Iteration 23595/35720 Training loss: 0.7626 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 23596/35720 Training loss: 0.7626 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 23597/35720 Training loss: 0.7625 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 23598/35720 Training loss: 0.7623 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 23599/35720 Training loss: 0.7623 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 23600/35720 Training loss: 0.7622 0.2123 sec/batch\n",
      "Validation loss: 1.50386 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 23601/35720 Training loss: 0.7632 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 23602/35720 Training loss: 0.7632 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 23603/35720 Training loss: 0.7633 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 23604/35720 Training loss: 0.7632 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 23605/35720 Training loss: 0.7632 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23606/35720 Training loss: 0.7633 0.2075 sec/batch\n",
      "Epoch 14/20  Iteration 23607/35720 Training loss: 0.7633 0.2723 sec/batch\n",
      "Epoch 14/20  Iteration 23608/35720 Training loss: 0.7632 0.2486 sec/batch\n",
      "Epoch 14/20  Iteration 23609/35720 Training loss: 0.7632 0.2773 sec/batch\n",
      "Epoch 14/20  Iteration 23610/35720 Training loss: 0.7631 0.2236 sec/batch\n",
      "Epoch 14/20  Iteration 23611/35720 Training loss: 0.7632 0.2073 sec/batch\n",
      "Epoch 14/20  Iteration 23612/35720 Training loss: 0.7630 0.2137 sec/batch\n",
      "Epoch 14/20  Iteration 23613/35720 Training loss: 0.7631 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 23614/35720 Training loss: 0.7630 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 23615/35720 Training loss: 0.7630 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 23616/35720 Training loss: 0.7628 0.2214 sec/batch\n",
      "Epoch 14/20  Iteration 23617/35720 Training loss: 0.7628 0.2069 sec/batch\n",
      "Epoch 14/20  Iteration 23618/35720 Training loss: 0.7627 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 23619/35720 Training loss: 0.7627 0.2185 sec/batch\n",
      "Epoch 14/20  Iteration 23620/35720 Training loss: 0.7628 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23621/35720 Training loss: 0.7628 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 23622/35720 Training loss: 0.7628 0.2215 sec/batch\n",
      "Epoch 14/20  Iteration 23623/35720 Training loss: 0.7626 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23624/35720 Training loss: 0.7625 0.2103 sec/batch\n",
      "Epoch 14/20  Iteration 23625/35720 Training loss: 0.7625 0.2182 sec/batch\n",
      "Epoch 14/20  Iteration 23626/35720 Training loss: 0.7626 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 23627/35720 Training loss: 0.7624 0.2133 sec/batch\n",
      "Epoch 14/20  Iteration 23628/35720 Training loss: 0.7623 0.2242 sec/batch\n",
      "Epoch 14/20  Iteration 23629/35720 Training loss: 0.7623 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23630/35720 Training loss: 0.7622 0.2177 sec/batch\n",
      "Epoch 14/20  Iteration 23631/35720 Training loss: 0.7621 0.2302 sec/batch\n",
      "Epoch 14/20  Iteration 23632/35720 Training loss: 0.7621 0.2255 sec/batch\n",
      "Epoch 14/20  Iteration 23633/35720 Training loss: 0.7621 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 23634/35720 Training loss: 0.7620 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23635/35720 Training loss: 0.7620 0.2201 sec/batch\n",
      "Epoch 14/20  Iteration 23636/35720 Training loss: 0.7618 0.2233 sec/batch\n",
      "Epoch 14/20  Iteration 23637/35720 Training loss: 0.7618 0.2163 sec/batch\n",
      "Epoch 14/20  Iteration 23638/35720 Training loss: 0.7618 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 23639/35720 Training loss: 0.7618 0.2081 sec/batch\n",
      "Epoch 14/20  Iteration 23640/35720 Training loss: 0.7618 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 23641/35720 Training loss: 0.7620 0.2293 sec/batch\n",
      "Epoch 14/20  Iteration 23642/35720 Training loss: 0.7620 0.2121 sec/batch\n",
      "Epoch 14/20  Iteration 23643/35720 Training loss: 0.7620 0.2304 sec/batch\n",
      "Epoch 14/20  Iteration 23644/35720 Training loss: 0.7619 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 23645/35720 Training loss: 0.7618 0.2269 sec/batch\n",
      "Epoch 14/20  Iteration 23646/35720 Training loss: 0.7619 0.2113 sec/batch\n",
      "Epoch 14/20  Iteration 23647/35720 Training loss: 0.7618 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 23648/35720 Training loss: 0.7618 0.2334 sec/batch\n",
      "Epoch 14/20  Iteration 23649/35720 Training loss: 0.7620 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 23650/35720 Training loss: 0.7619 0.2107 sec/batch\n",
      "Epoch 14/20  Iteration 23651/35720 Training loss: 0.7621 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 23652/35720 Training loss: 0.7621 0.2172 sec/batch\n",
      "Epoch 14/20  Iteration 23653/35720 Training loss: 0.7622 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 23654/35720 Training loss: 0.7621 0.2218 sec/batch\n",
      "Epoch 14/20  Iteration 23655/35720 Training loss: 0.7622 0.2198 sec/batch\n",
      "Epoch 14/20  Iteration 23656/35720 Training loss: 0.7624 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 23657/35720 Training loss: 0.7624 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 23658/35720 Training loss: 0.7624 0.2278 sec/batch\n",
      "Epoch 14/20  Iteration 23659/35720 Training loss: 0.7625 0.2305 sec/batch\n",
      "Epoch 14/20  Iteration 23660/35720 Training loss: 0.7627 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 23661/35720 Training loss: 0.7627 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 23662/35720 Training loss: 0.7627 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 23663/35720 Training loss: 0.7627 0.2265 sec/batch\n",
      "Epoch 14/20  Iteration 23664/35720 Training loss: 0.7629 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 23665/35720 Training loss: 0.7630 0.2177 sec/batch\n",
      "Epoch 14/20  Iteration 23666/35720 Training loss: 0.7632 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 23667/35720 Training loss: 0.7633 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23668/35720 Training loss: 0.7633 0.2253 sec/batch\n",
      "Epoch 14/20  Iteration 23669/35720 Training loss: 0.7632 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 23670/35720 Training loss: 0.7631 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 23671/35720 Training loss: 0.7632 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23672/35720 Training loss: 0.7633 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 23673/35720 Training loss: 0.7635 0.2120 sec/batch\n",
      "Epoch 14/20  Iteration 23674/35720 Training loss: 0.7637 0.2223 sec/batch\n",
      "Epoch 14/20  Iteration 23675/35720 Training loss: 0.7638 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 23676/35720 Training loss: 0.7639 0.2395 sec/batch\n",
      "Epoch 14/20  Iteration 23677/35720 Training loss: 0.7638 0.2289 sec/batch\n",
      "Epoch 14/20  Iteration 23678/35720 Training loss: 0.7638 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 23679/35720 Training loss: 0.7638 0.2124 sec/batch\n",
      "Epoch 14/20  Iteration 23680/35720 Training loss: 0.7639 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 23681/35720 Training loss: 0.7640 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 23682/35720 Training loss: 0.7640 0.2152 sec/batch\n",
      "Epoch 14/20  Iteration 23683/35720 Training loss: 0.7640 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 23684/35720 Training loss: 0.7639 0.2099 sec/batch\n",
      "Epoch 14/20  Iteration 23685/35720 Training loss: 0.7639 0.2180 sec/batch\n",
      "Epoch 14/20  Iteration 23686/35720 Training loss: 0.7639 0.2282 sec/batch\n",
      "Epoch 14/20  Iteration 23687/35720 Training loss: 0.7639 0.2219 sec/batch\n",
      "Epoch 14/20  Iteration 23688/35720 Training loss: 0.7638 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 23689/35720 Training loss: 0.7638 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 23690/35720 Training loss: 0.7638 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 23691/35720 Training loss: 0.7639 0.2228 sec/batch\n",
      "Epoch 14/20  Iteration 23692/35720 Training loss: 0.7638 0.2210 sec/batch\n",
      "Epoch 14/20  Iteration 23693/35720 Training loss: 0.7639 0.2176 sec/batch\n",
      "Epoch 14/20  Iteration 23694/35720 Training loss: 0.7639 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 23695/35720 Training loss: 0.7639 0.2069 sec/batch\n",
      "Epoch 14/20  Iteration 23696/35720 Training loss: 0.7639 0.2209 sec/batch\n",
      "Epoch 14/20  Iteration 23697/35720 Training loss: 0.7639 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 23698/35720 Training loss: 0.7638 0.2315 sec/batch\n",
      "Epoch 14/20  Iteration 23699/35720 Training loss: 0.7637 0.2210 sec/batch\n",
      "Epoch 14/20  Iteration 23700/35720 Training loss: 0.7636 0.2204 sec/batch\n",
      "Epoch 14/20  Iteration 23701/35720 Training loss: 0.7636 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 23702/35720 Training loss: 0.7636 0.2273 sec/batch\n",
      "Epoch 14/20  Iteration 23703/35720 Training loss: 0.7636 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 23704/35720 Training loss: 0.7635 0.2297 sec/batch\n",
      "Epoch 14/20  Iteration 23705/35720 Training loss: 0.7635 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 23706/35720 Training loss: 0.7634 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 23707/35720 Training loss: 0.7634 0.2175 sec/batch\n",
      "Epoch 14/20  Iteration 23708/35720 Training loss: 0.7633 0.2149 sec/batch\n",
      "Epoch 14/20  Iteration 23709/35720 Training loss: 0.7632 0.2468 sec/batch\n",
      "Epoch 14/20  Iteration 23710/35720 Training loss: 0.7634 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23711/35720 Training loss: 0.7634 0.2195 sec/batch\n",
      "Epoch 14/20  Iteration 23712/35720 Training loss: 0.7633 0.2191 sec/batch\n",
      "Epoch 14/20  Iteration 23713/35720 Training loss: 0.7632 0.2058 sec/batch\n",
      "Epoch 14/20  Iteration 23714/35720 Training loss: 0.7631 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 23715/35720 Training loss: 0.7632 0.2249 sec/batch\n",
      "Epoch 14/20  Iteration 23716/35720 Training loss: 0.7632 0.2076 sec/batch\n",
      "Epoch 14/20  Iteration 23717/35720 Training loss: 0.7632 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23718/35720 Training loss: 0.7632 0.2160 sec/batch\n",
      "Epoch 14/20  Iteration 23719/35720 Training loss: 0.7631 0.2222 sec/batch\n",
      "Epoch 14/20  Iteration 23720/35720 Training loss: 0.7630 0.2300 sec/batch\n",
      "Epoch 14/20  Iteration 23721/35720 Training loss: 0.7629 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 23722/35720 Training loss: 0.7628 0.2112 sec/batch\n",
      "Epoch 14/20  Iteration 23723/35720 Training loss: 0.7628 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 23724/35720 Training loss: 0.7629 0.2154 sec/batch\n",
      "Epoch 14/20  Iteration 23725/35720 Training loss: 0.7628 0.2128 sec/batch\n",
      "Epoch 14/20  Iteration 23726/35720 Training loss: 0.7628 0.2237 sec/batch\n",
      "Epoch 14/20  Iteration 23727/35720 Training loss: 0.7629 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 23728/35720 Training loss: 0.7629 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 23729/35720 Training loss: 0.7628 0.2189 sec/batch\n",
      "Epoch 14/20  Iteration 23730/35720 Training loss: 0.7629 0.2311 sec/batch\n",
      "Epoch 14/20  Iteration 23731/35720 Training loss: 0.7628 0.2255 sec/batch\n",
      "Epoch 14/20  Iteration 23732/35720 Training loss: 0.7628 0.2053 sec/batch\n",
      "Epoch 14/20  Iteration 23733/35720 Training loss: 0.7629 0.2120 sec/batch\n",
      "Epoch 14/20  Iteration 23734/35720 Training loss: 0.7629 0.2200 sec/batch\n",
      "Epoch 14/20  Iteration 23735/35720 Training loss: 0.7629 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 23736/35720 Training loss: 0.7629 0.2229 sec/batch\n",
      "Epoch 14/20  Iteration 23737/35720 Training loss: 0.7630 0.2155 sec/batch\n",
      "Epoch 14/20  Iteration 23738/35720 Training loss: 0.7631 0.2120 sec/batch\n",
      "Epoch 14/20  Iteration 23739/35720 Training loss: 0.7630 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23740/35720 Training loss: 0.7630 0.2122 sec/batch\n",
      "Epoch 14/20  Iteration 23741/35720 Training loss: 0.7630 0.2299 sec/batch\n",
      "Epoch 14/20  Iteration 23742/35720 Training loss: 0.7629 0.2244 sec/batch\n",
      "Epoch 14/20  Iteration 23743/35720 Training loss: 0.7629 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 23744/35720 Training loss: 0.7629 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 23745/35720 Training loss: 0.7629 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 23746/35720 Training loss: 0.7629 0.2195 sec/batch\n",
      "Epoch 14/20  Iteration 23747/35720 Training loss: 0.7630 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 23748/35720 Training loss: 0.7629 0.2118 sec/batch\n",
      "Epoch 14/20  Iteration 23749/35720 Training loss: 0.7630 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 23750/35720 Training loss: 0.7629 0.2118 sec/batch\n",
      "Epoch 14/20  Iteration 23751/35720 Training loss: 0.7629 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 23752/35720 Training loss: 0.7629 0.2490 sec/batch\n",
      "Epoch 14/20  Iteration 23753/35720 Training loss: 0.7628 0.2251 sec/batch\n",
      "Epoch 14/20  Iteration 23754/35720 Training loss: 0.7627 0.2235 sec/batch\n",
      "Epoch 14/20  Iteration 23755/35720 Training loss: 0.7626 0.2069 sec/batch\n",
      "Epoch 14/20  Iteration 23756/35720 Training loss: 0.7625 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23757/35720 Training loss: 0.7625 0.2248 sec/batch\n",
      "Epoch 14/20  Iteration 23758/35720 Training loss: 0.7625 0.2128 sec/batch\n",
      "Epoch 14/20  Iteration 23759/35720 Training loss: 0.7625 0.2238 sec/batch\n",
      "Epoch 14/20  Iteration 23760/35720 Training loss: 0.7624 0.2073 sec/batch\n",
      "Epoch 14/20  Iteration 23761/35720 Training loss: 0.7623 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 23762/35720 Training loss: 0.7622 0.2172 sec/batch\n",
      "Epoch 14/20  Iteration 23763/35720 Training loss: 0.7623 0.2166 sec/batch\n",
      "Epoch 14/20  Iteration 23764/35720 Training loss: 0.7623 0.2495 sec/batch\n",
      "Epoch 14/20  Iteration 23765/35720 Training loss: 0.7623 0.2232 sec/batch\n",
      "Epoch 14/20  Iteration 23766/35720 Training loss: 0.7624 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 23767/35720 Training loss: 0.7623 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 23768/35720 Training loss: 0.7624 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 23769/35720 Training loss: 0.7623 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 23770/35720 Training loss: 0.7622 0.2251 sec/batch\n",
      "Epoch 14/20  Iteration 23771/35720 Training loss: 0.7622 0.2221 sec/batch\n",
      "Epoch 14/20  Iteration 23772/35720 Training loss: 0.7622 0.2077 sec/batch\n",
      "Epoch 14/20  Iteration 23773/35720 Training loss: 0.7623 0.2284 sec/batch\n",
      "Epoch 14/20  Iteration 23774/35720 Training loss: 0.7624 0.2238 sec/batch\n",
      "Epoch 14/20  Iteration 23775/35720 Training loss: 0.7623 0.2406 sec/batch\n",
      "Epoch 14/20  Iteration 23776/35720 Training loss: 0.7624 0.2295 sec/batch\n",
      "Epoch 14/20  Iteration 23777/35720 Training loss: 0.7624 0.2198 sec/batch\n",
      "Epoch 14/20  Iteration 23778/35720 Training loss: 0.7623 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 23779/35720 Training loss: 0.7624 0.2303 sec/batch\n",
      "Epoch 14/20  Iteration 23780/35720 Training loss: 0.7624 0.2291 sec/batch\n",
      "Epoch 14/20  Iteration 23781/35720 Training loss: 0.7623 0.2345 sec/batch\n",
      "Epoch 14/20  Iteration 23782/35720 Training loss: 0.7624 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 23783/35720 Training loss: 0.7623 0.2208 sec/batch\n",
      "Epoch 14/20  Iteration 23784/35720 Training loss: 0.7623 0.2183 sec/batch\n",
      "Epoch 14/20  Iteration 23785/35720 Training loss: 0.7623 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 23786/35720 Training loss: 0.7622 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 23787/35720 Training loss: 0.7622 0.2226 sec/batch\n",
      "Epoch 14/20  Iteration 23788/35720 Training loss: 0.7621 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 23789/35720 Training loss: 0.7621 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 23790/35720 Training loss: 0.7621 0.2233 sec/batch\n",
      "Epoch 14/20  Iteration 23791/35720 Training loss: 0.7622 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 23792/35720 Training loss: 0.7623 0.2198 sec/batch\n",
      "Epoch 14/20  Iteration 23793/35720 Training loss: 0.7623 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 23794/35720 Training loss: 0.7623 0.2118 sec/batch\n",
      "Epoch 14/20  Iteration 23795/35720 Training loss: 0.7624 0.2135 sec/batch\n",
      "Epoch 14/20  Iteration 23796/35720 Training loss: 0.7624 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 23797/35720 Training loss: 0.7624 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 23798/35720 Training loss: 0.7624 0.2112 sec/batch\n",
      "Epoch 14/20  Iteration 23799/35720 Training loss: 0.7625 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 23800/35720 Training loss: 0.7624 0.2092 sec/batch\n",
      "Validation loss: 1.50307 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 23801/35720 Training loss: 0.7630 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 23802/35720 Training loss: 0.7630 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 23803/35720 Training loss: 0.7629 0.2118 sec/batch\n",
      "Epoch 14/20  Iteration 23804/35720 Training loss: 0.7628 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 23805/35720 Training loss: 0.7627 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23806/35720 Training loss: 0.7627 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 23807/35720 Training loss: 0.7626 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 23808/35720 Training loss: 0.7625 0.2413 sec/batch\n",
      "Epoch 14/20  Iteration 23809/35720 Training loss: 0.7626 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 23810/35720 Training loss: 0.7626 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23811/35720 Training loss: 0.7625 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 23812/35720 Training loss: 0.7625 0.2313 sec/batch\n",
      "Epoch 14/20  Iteration 23813/35720 Training loss: 0.7625 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 23814/35720 Training loss: 0.7624 0.2222 sec/batch\n",
      "Epoch 14/20  Iteration 23815/35720 Training loss: 0.7623 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 23816/35720 Training loss: 0.7623 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 23817/35720 Training loss: 0.7622 0.2233 sec/batch\n",
      "Epoch 14/20  Iteration 23818/35720 Training loss: 0.7621 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 23819/35720 Training loss: 0.7620 0.2246 sec/batch\n",
      "Epoch 14/20  Iteration 23820/35720 Training loss: 0.7619 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 23821/35720 Training loss: 0.7618 0.2112 sec/batch\n",
      "Epoch 14/20  Iteration 23822/35720 Training loss: 0.7618 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 23823/35720 Training loss: 0.7618 0.2164 sec/batch\n",
      "Epoch 14/20  Iteration 23824/35720 Training loss: 0.7619 0.2107 sec/batch\n",
      "Epoch 14/20  Iteration 23825/35720 Training loss: 0.7618 0.2252 sec/batch\n",
      "Epoch 14/20  Iteration 23826/35720 Training loss: 0.7618 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 23827/35720 Training loss: 0.7619 0.2644 sec/batch\n",
      "Epoch 14/20  Iteration 23828/35720 Training loss: 0.7617 0.2873 sec/batch\n",
      "Epoch 14/20  Iteration 23829/35720 Training loss: 0.7617 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 23830/35720 Training loss: 0.7616 0.2155 sec/batch\n",
      "Epoch 14/20  Iteration 23831/35720 Training loss: 0.7616 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23832/35720 Training loss: 0.7615 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 23833/35720 Training loss: 0.7615 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23834/35720 Training loss: 0.7614 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 23835/35720 Training loss: 0.7614 0.2227 sec/batch\n",
      "Epoch 14/20  Iteration 23836/35720 Training loss: 0.7613 0.2226 sec/batch\n",
      "Epoch 14/20  Iteration 23837/35720 Training loss: 0.7612 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 23838/35720 Training loss: 0.7611 0.2124 sec/batch\n",
      "Epoch 14/20  Iteration 23839/35720 Training loss: 0.7611 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 23840/35720 Training loss: 0.7612 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 23841/35720 Training loss: 0.7611 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 23842/35720 Training loss: 0.7611 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 23843/35720 Training loss: 0.7610 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 23844/35720 Training loss: 0.7610 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 23845/35720 Training loss: 0.7610 0.2277 sec/batch\n",
      "Epoch 14/20  Iteration 23846/35720 Training loss: 0.7609 0.2297 sec/batch\n",
      "Epoch 14/20  Iteration 23847/35720 Training loss: 0.7610 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 23848/35720 Training loss: 0.7609 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 23849/35720 Training loss: 0.7610 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 23850/35720 Training loss: 0.7608 0.2226 sec/batch\n",
      "Epoch 14/20  Iteration 23851/35720 Training loss: 0.7608 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 23852/35720 Training loss: 0.7607 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 23853/35720 Training loss: 0.7607 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 23854/35720 Training loss: 0.7607 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23855/35720 Training loss: 0.7607 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 23856/35720 Training loss: 0.7606 0.2305 sec/batch\n",
      "Epoch 14/20  Iteration 23857/35720 Training loss: 0.7606 0.2195 sec/batch\n",
      "Epoch 14/20  Iteration 23858/35720 Training loss: 0.7606 0.2252 sec/batch\n",
      "Epoch 14/20  Iteration 23859/35720 Training loss: 0.7607 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 23860/35720 Training loss: 0.7608 0.2165 sec/batch\n",
      "Epoch 14/20  Iteration 23861/35720 Training loss: 0.7607 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 23862/35720 Training loss: 0.7607 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 23863/35720 Training loss: 0.7606 0.2384 sec/batch\n",
      "Epoch 14/20  Iteration 23864/35720 Training loss: 0.7607 0.2154 sec/batch\n",
      "Epoch 14/20  Iteration 23865/35720 Training loss: 0.7606 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 23866/35720 Training loss: 0.7605 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 23867/35720 Training loss: 0.7604 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 23868/35720 Training loss: 0.7604 0.2201 sec/batch\n",
      "Epoch 14/20  Iteration 23869/35720 Training loss: 0.7603 0.2149 sec/batch\n",
      "Epoch 14/20  Iteration 23870/35720 Training loss: 0.7603 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 23871/35720 Training loss: 0.7603 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 23872/35720 Training loss: 0.7604 0.2249 sec/batch\n",
      "Epoch 14/20  Iteration 23873/35720 Training loss: 0.7603 0.2486 sec/batch\n",
      "Epoch 14/20  Iteration 23874/35720 Training loss: 0.7604 0.2188 sec/batch\n",
      "Epoch 14/20  Iteration 23875/35720 Training loss: 0.7604 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 23876/35720 Training loss: 0.7605 0.2142 sec/batch\n",
      "Epoch 14/20  Iteration 23877/35720 Training loss: 0.7605 0.2110 sec/batch\n",
      "Epoch 14/20  Iteration 23878/35720 Training loss: 0.7605 0.2165 sec/batch\n",
      "Epoch 14/20  Iteration 23879/35720 Training loss: 0.7605 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 23880/35720 Training loss: 0.7606 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 23881/35720 Training loss: 0.7606 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 23882/35720 Training loss: 0.7606 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 23883/35720 Training loss: 0.7607 0.2253 sec/batch\n",
      "Epoch 14/20  Iteration 23884/35720 Training loss: 0.7607 0.2153 sec/batch\n",
      "Epoch 14/20  Iteration 23885/35720 Training loss: 0.7608 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 23886/35720 Training loss: 0.7607 0.2192 sec/batch\n",
      "Epoch 14/20  Iteration 23887/35720 Training loss: 0.7607 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 23888/35720 Training loss: 0.7607 0.2150 sec/batch\n",
      "Epoch 14/20  Iteration 23889/35720 Training loss: 0.7605 0.2265 sec/batch\n",
      "Epoch 14/20  Iteration 23890/35720 Training loss: 0.7605 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 23891/35720 Training loss: 0.7605 0.2326 sec/batch\n",
      "Epoch 14/20  Iteration 23892/35720 Training loss: 0.7604 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 23893/35720 Training loss: 0.7604 0.2247 sec/batch\n",
      "Epoch 14/20  Iteration 23894/35720 Training loss: 0.7604 0.2331 sec/batch\n",
      "Epoch 14/20  Iteration 23895/35720 Training loss: 0.7604 0.2153 sec/batch\n",
      "Epoch 14/20  Iteration 23896/35720 Training loss: 0.7604 0.2287 sec/batch\n",
      "Epoch 14/20  Iteration 23897/35720 Training loss: 0.7605 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 23898/35720 Training loss: 0.7604 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 23899/35720 Training loss: 0.7604 0.2133 sec/batch\n",
      "Epoch 14/20  Iteration 23900/35720 Training loss: 0.7604 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 23901/35720 Training loss: 0.7604 0.2204 sec/batch\n",
      "Epoch 14/20  Iteration 23902/35720 Training loss: 0.7603 0.2275 sec/batch\n",
      "Epoch 14/20  Iteration 23903/35720 Training loss: 0.7603 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 23904/35720 Training loss: 0.7603 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 23905/35720 Training loss: 0.7603 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 23906/35720 Training loss: 0.7603 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 23907/35720 Training loss: 0.7602 0.2298 sec/batch\n",
      "Epoch 14/20  Iteration 23908/35720 Training loss: 0.7602 0.2195 sec/batch\n",
      "Epoch 14/20  Iteration 23909/35720 Training loss: 0.7603 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 23910/35720 Training loss: 0.7604 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 23911/35720 Training loss: 0.7605 0.2237 sec/batch\n",
      "Epoch 14/20  Iteration 23912/35720 Training loss: 0.7605 0.2133 sec/batch\n",
      "Epoch 14/20  Iteration 23913/35720 Training loss: 0.7605 0.2163 sec/batch\n",
      "Epoch 14/20  Iteration 23914/35720 Training loss: 0.7605 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 23915/35720 Training loss: 0.7605 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 23916/35720 Training loss: 0.7604 0.2211 sec/batch\n",
      "Epoch 14/20  Iteration 23917/35720 Training loss: 0.7604 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 23918/35720 Training loss: 0.7603 0.2071 sec/batch\n",
      "Epoch 14/20  Iteration 23919/35720 Training loss: 0.7603 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 23920/35720 Training loss: 0.7603 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 23921/35720 Training loss: 0.7602 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 23922/35720 Training loss: 0.7602 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 23923/35720 Training loss: 0.7602 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23924/35720 Training loss: 0.7602 0.2173 sec/batch\n",
      "Epoch 14/20  Iteration 23925/35720 Training loss: 0.7603 0.2298 sec/batch\n",
      "Epoch 14/20  Iteration 23926/35720 Training loss: 0.7603 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 23927/35720 Training loss: 0.7604 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 23928/35720 Training loss: 0.7605 0.2260 sec/batch\n",
      "Epoch 14/20  Iteration 23929/35720 Training loss: 0.7605 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 23930/35720 Training loss: 0.7605 0.2297 sec/batch\n",
      "Epoch 14/20  Iteration 23931/35720 Training loss: 0.7605 0.2189 sec/batch\n",
      "Epoch 14/20  Iteration 23932/35720 Training loss: 0.7605 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 23933/35720 Training loss: 0.7605 0.2150 sec/batch\n",
      "Epoch 14/20  Iteration 23934/35720 Training loss: 0.7605 0.2145 sec/batch\n",
      "Epoch 14/20  Iteration 23935/35720 Training loss: 0.7607 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 23936/35720 Training loss: 0.7607 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 23937/35720 Training loss: 0.7607 0.2105 sec/batch\n",
      "Epoch 14/20  Iteration 23938/35720 Training loss: 0.7607 0.2195 sec/batch\n",
      "Epoch 14/20  Iteration 23939/35720 Training loss: 0.7607 0.2320 sec/batch\n",
      "Epoch 14/20  Iteration 23940/35720 Training loss: 0.7608 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 23941/35720 Training loss: 0.7608 0.2150 sec/batch\n",
      "Epoch 14/20  Iteration 23942/35720 Training loss: 0.7608 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 23943/35720 Training loss: 0.7608 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23944/35720 Training loss: 0.7607 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 23945/35720 Training loss: 0.7609 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 23946/35720 Training loss: 0.7609 0.2166 sec/batch\n",
      "Epoch 14/20  Iteration 23947/35720 Training loss: 0.7610 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 23948/35720 Training loss: 0.7610 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 23949/35720 Training loss: 0.7609 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23950/35720 Training loss: 0.7609 0.2164 sec/batch\n",
      "Epoch 14/20  Iteration 23951/35720 Training loss: 0.7609 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 23952/35720 Training loss: 0.7609 0.2276 sec/batch\n",
      "Epoch 14/20  Iteration 23953/35720 Training loss: 0.7609 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23954/35720 Training loss: 0.7609 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 23955/35720 Training loss: 0.7609 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23956/35720 Training loss: 0.7609 0.2260 sec/batch\n",
      "Epoch 14/20  Iteration 23957/35720 Training loss: 0.7609 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 23958/35720 Training loss: 0.7609 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 23959/35720 Training loss: 0.7610 0.2071 sec/batch\n",
      "Epoch 14/20  Iteration 23960/35720 Training loss: 0.7610 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 23961/35720 Training loss: 0.7609 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 23962/35720 Training loss: 0.7609 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 23963/35720 Training loss: 0.7609 0.2402 sec/batch\n",
      "Epoch 14/20  Iteration 23964/35720 Training loss: 0.7609 0.2217 sec/batch\n",
      "Epoch 14/20  Iteration 23965/35720 Training loss: 0.7609 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 23966/35720 Training loss: 0.7608 0.2135 sec/batch\n",
      "Epoch 14/20  Iteration 23967/35720 Training loss: 0.7608 0.2267 sec/batch\n",
      "Epoch 14/20  Iteration 23968/35720 Training loss: 0.7608 0.2229 sec/batch\n",
      "Epoch 14/20  Iteration 23969/35720 Training loss: 0.7608 0.2298 sec/batch\n",
      "Epoch 14/20  Iteration 23970/35720 Training loss: 0.7609 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 23971/35720 Training loss: 0.7608 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 23972/35720 Training loss: 0.7608 0.2364 sec/batch\n",
      "Epoch 14/20  Iteration 23973/35720 Training loss: 0.7607 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 23974/35720 Training loss: 0.7607 0.2218 sec/batch\n",
      "Epoch 14/20  Iteration 23975/35720 Training loss: 0.7606 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 23976/35720 Training loss: 0.7607 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 23977/35720 Training loss: 0.7607 0.2139 sec/batch\n",
      "Epoch 14/20  Iteration 23978/35720 Training loss: 0.7607 0.2258 sec/batch\n",
      "Epoch 14/20  Iteration 23979/35720 Training loss: 0.7607 0.2276 sec/batch\n",
      "Epoch 14/20  Iteration 23980/35720 Training loss: 0.7607 0.2354 sec/batch\n",
      "Epoch 14/20  Iteration 23981/35720 Training loss: 0.7607 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 23982/35720 Training loss: 0.7606 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 23983/35720 Training loss: 0.7606 0.2120 sec/batch\n",
      "Epoch 14/20  Iteration 23984/35720 Training loss: 0.7607 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 23985/35720 Training loss: 0.7607 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 23986/35720 Training loss: 0.7607 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 23987/35720 Training loss: 0.7608 0.2191 sec/batch\n",
      "Epoch 14/20  Iteration 23988/35720 Training loss: 0.7609 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 23989/35720 Training loss: 0.7609 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 23990/35720 Training loss: 0.7610 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23991/35720 Training loss: 0.7610 0.2221 sec/batch\n",
      "Epoch 14/20  Iteration 23992/35720 Training loss: 0.7610 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 23993/35720 Training loss: 0.7609 0.2051 sec/batch\n",
      "Epoch 14/20  Iteration 23994/35720 Training loss: 0.7608 0.2306 sec/batch\n",
      "Epoch 14/20  Iteration 23995/35720 Training loss: 0.7608 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 23996/35720 Training loss: 0.7608 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 23997/35720 Training loss: 0.7608 0.2054 sec/batch\n",
      "Epoch 14/20  Iteration 23998/35720 Training loss: 0.7608 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 23999/35720 Training loss: 0.7607 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 24000/35720 Training loss: 0.7608 0.2140 sec/batch\n",
      "Validation loss: 1.50276 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 24001/35720 Training loss: 0.7612 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24002/35720 Training loss: 0.7612 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 24003/35720 Training loss: 0.7611 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 24004/35720 Training loss: 0.7611 0.2195 sec/batch\n",
      "Epoch 14/20  Iteration 24005/35720 Training loss: 0.7612 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 24006/35720 Training loss: 0.7611 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 24007/35720 Training loss: 0.7612 0.2079 sec/batch\n",
      "Epoch 14/20  Iteration 24008/35720 Training loss: 0.7612 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 24009/35720 Training loss: 0.7612 0.2196 sec/batch\n",
      "Epoch 14/20  Iteration 24010/35720 Training loss: 0.7612 0.2204 sec/batch\n",
      "Epoch 14/20  Iteration 24011/35720 Training loss: 0.7612 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 24012/35720 Training loss: 0.7611 0.2165 sec/batch\n",
      "Epoch 14/20  Iteration 24013/35720 Training loss: 0.7611 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 24014/35720 Training loss: 0.7612 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 24015/35720 Training loss: 0.7611 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 24016/35720 Training loss: 0.7611 0.2222 sec/batch\n",
      "Epoch 14/20  Iteration 24017/35720 Training loss: 0.7611 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 24018/35720 Training loss: 0.7611 0.2166 sec/batch\n",
      "Epoch 14/20  Iteration 24019/35720 Training loss: 0.7611 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 24020/35720 Training loss: 0.7611 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24021/35720 Training loss: 0.7611 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 24022/35720 Training loss: 0.7612 0.2318 sec/batch\n",
      "Epoch 14/20  Iteration 24023/35720 Training loss: 0.7612 0.2275 sec/batch\n",
      "Epoch 14/20  Iteration 24024/35720 Training loss: 0.7612 0.2251 sec/batch\n",
      "Epoch 14/20  Iteration 24025/35720 Training loss: 0.7612 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24026/35720 Training loss: 0.7612 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 24027/35720 Training loss: 0.7612 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 24028/35720 Training loss: 0.7613 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 24029/35720 Training loss: 0.7613 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 24030/35720 Training loss: 0.7613 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 24031/35720 Training loss: 0.7613 0.2071 sec/batch\n",
      "Epoch 14/20  Iteration 24032/35720 Training loss: 0.7613 0.2123 sec/batch\n",
      "Epoch 14/20  Iteration 24033/35720 Training loss: 0.7614 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 24034/35720 Training loss: 0.7614 0.2175 sec/batch\n",
      "Epoch 14/20  Iteration 24035/35720 Training loss: 0.7614 0.2150 sec/batch\n",
      "Epoch 14/20  Iteration 24036/35720 Training loss: 0.7614 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24037/35720 Training loss: 0.7614 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24038/35720 Training loss: 0.7614 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 24039/35720 Training loss: 0.7614 0.2188 sec/batch\n",
      "Epoch 14/20  Iteration 24040/35720 Training loss: 0.7613 0.2259 sec/batch\n",
      "Epoch 14/20  Iteration 24041/35720 Training loss: 0.7612 0.2196 sec/batch\n",
      "Epoch 14/20  Iteration 24042/35720 Training loss: 0.7612 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 24043/35720 Training loss: 0.7611 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 24044/35720 Training loss: 0.7611 0.2268 sec/batch\n",
      "Epoch 14/20  Iteration 24045/35720 Training loss: 0.7610 0.2182 sec/batch\n",
      "Epoch 14/20  Iteration 24046/35720 Training loss: 0.7610 0.2164 sec/batch\n",
      "Epoch 14/20  Iteration 24047/35720 Training loss: 0.7610 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 24048/35720 Training loss: 0.7609 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 24049/35720 Training loss: 0.7609 0.2163 sec/batch\n",
      "Epoch 14/20  Iteration 24050/35720 Training loss: 0.7609 0.2185 sec/batch\n",
      "Epoch 14/20  Iteration 24051/35720 Training loss: 0.7610 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 24052/35720 Training loss: 0.7610 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 24053/35720 Training loss: 0.7610 0.2155 sec/batch\n",
      "Epoch 14/20  Iteration 24054/35720 Training loss: 0.7609 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 24055/35720 Training loss: 0.7609 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 24056/35720 Training loss: 0.7609 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 24057/35720 Training loss: 0.7609 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24058/35720 Training loss: 0.7609 0.2056 sec/batch\n",
      "Epoch 14/20  Iteration 24059/35720 Training loss: 0.7610 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 24060/35720 Training loss: 0.7610 0.2145 sec/batch\n",
      "Epoch 14/20  Iteration 24061/35720 Training loss: 0.7610 0.2253 sec/batch\n",
      "Epoch 14/20  Iteration 24062/35720 Training loss: 0.7610 0.2177 sec/batch\n",
      "Epoch 14/20  Iteration 24063/35720 Training loss: 0.7610 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 24064/35720 Training loss: 0.7610 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24065/35720 Training loss: 0.7610 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 24066/35720 Training loss: 0.7610 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 24067/35720 Training loss: 0.7610 0.2139 sec/batch\n",
      "Epoch 14/20  Iteration 24068/35720 Training loss: 0.7610 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 24069/35720 Training loss: 0.7610 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 24070/35720 Training loss: 0.7609 0.2053 sec/batch\n",
      "Epoch 14/20  Iteration 24071/35720 Training loss: 0.7609 0.2190 sec/batch\n",
      "Epoch 14/20  Iteration 24072/35720 Training loss: 0.7609 0.2218 sec/batch\n",
      "Epoch 14/20  Iteration 24073/35720 Training loss: 0.7609 0.2227 sec/batch\n",
      "Epoch 14/20  Iteration 24074/35720 Training loss: 0.7609 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24075/35720 Training loss: 0.7608 0.2110 sec/batch\n",
      "Epoch 14/20  Iteration 24076/35720 Training loss: 0.7609 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 24077/35720 Training loss: 0.7608 0.2119 sec/batch\n",
      "Epoch 14/20  Iteration 24078/35720 Training loss: 0.7608 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 24079/35720 Training loss: 0.7608 0.2279 sec/batch\n",
      "Epoch 14/20  Iteration 24080/35720 Training loss: 0.7607 0.2081 sec/batch\n",
      "Epoch 14/20  Iteration 24081/35720 Training loss: 0.7608 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24082/35720 Training loss: 0.7607 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24083/35720 Training loss: 0.7607 0.2266 sec/batch\n",
      "Epoch 14/20  Iteration 24084/35720 Training loss: 0.7607 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 24085/35720 Training loss: 0.7608 0.2328 sec/batch\n",
      "Epoch 14/20  Iteration 24086/35720 Training loss: 0.7607 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24087/35720 Training loss: 0.7607 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24088/35720 Training loss: 0.7606 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 24089/35720 Training loss: 0.7606 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 24090/35720 Training loss: 0.7606 0.2188 sec/batch\n",
      "Epoch 14/20  Iteration 24091/35720 Training loss: 0.7605 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24092/35720 Training loss: 0.7605 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24093/35720 Training loss: 0.7605 0.2152 sec/batch\n",
      "Epoch 14/20  Iteration 24094/35720 Training loss: 0.7604 0.2321 sec/batch\n",
      "Epoch 14/20  Iteration 24095/35720 Training loss: 0.7604 0.2264 sec/batch\n",
      "Epoch 14/20  Iteration 24096/35720 Training loss: 0.7604 0.2287 sec/batch\n",
      "Epoch 14/20  Iteration 24097/35720 Training loss: 0.7604 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24098/35720 Training loss: 0.7604 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 24099/35720 Training loss: 0.7603 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 24100/35720 Training loss: 0.7603 0.2333 sec/batch\n",
      "Epoch 14/20  Iteration 24101/35720 Training loss: 0.7603 0.2316 sec/batch\n",
      "Epoch 14/20  Iteration 24102/35720 Training loss: 0.7603 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 24103/35720 Training loss: 0.7603 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 24104/35720 Training loss: 0.7602 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24105/35720 Training loss: 0.7602 0.2105 sec/batch\n",
      "Epoch 14/20  Iteration 24106/35720 Training loss: 0.7602 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24107/35720 Training loss: 0.7602 0.2190 sec/batch\n",
      "Epoch 14/20  Iteration 24108/35720 Training loss: 0.7601 0.2195 sec/batch\n",
      "Epoch 14/20  Iteration 24109/35720 Training loss: 0.7601 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 24110/35720 Training loss: 0.7600 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 24111/35720 Training loss: 0.7600 0.2290 sec/batch\n",
      "Epoch 14/20  Iteration 24112/35720 Training loss: 0.7599 0.2154 sec/batch\n",
      "Epoch 14/20  Iteration 24113/35720 Training loss: 0.7599 0.2139 sec/batch\n",
      "Epoch 14/20  Iteration 24114/35720 Training loss: 0.7599 0.2134 sec/batch\n",
      "Epoch 14/20  Iteration 24115/35720 Training loss: 0.7598 0.2305 sec/batch\n",
      "Epoch 14/20  Iteration 24116/35720 Training loss: 0.7597 0.2207 sec/batch\n",
      "Epoch 14/20  Iteration 24117/35720 Training loss: 0.7596 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 24118/35720 Training loss: 0.7596 0.2201 sec/batch\n",
      "Epoch 14/20  Iteration 24119/35720 Training loss: 0.7595 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 24120/35720 Training loss: 0.7594 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 24121/35720 Training loss: 0.7594 0.2180 sec/batch\n",
      "Epoch 14/20  Iteration 24122/35720 Training loss: 0.7593 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24123/35720 Training loss: 0.7593 0.2124 sec/batch\n",
      "Epoch 14/20  Iteration 24124/35720 Training loss: 0.7593 0.2056 sec/batch\n",
      "Epoch 14/20  Iteration 24125/35720 Training loss: 0.7592 0.2259 sec/batch\n",
      "Epoch 14/20  Iteration 24126/35720 Training loss: 0.7593 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 24127/35720 Training loss: 0.7593 0.2274 sec/batch\n",
      "Epoch 14/20  Iteration 24128/35720 Training loss: 0.7592 0.2263 sec/batch\n",
      "Epoch 14/20  Iteration 24129/35720 Training loss: 0.7592 0.2263 sec/batch\n",
      "Epoch 14/20  Iteration 24130/35720 Training loss: 0.7592 0.2173 sec/batch\n",
      "Epoch 14/20  Iteration 24131/35720 Training loss: 0.7592 0.2226 sec/batch\n",
      "Epoch 14/20  Iteration 24132/35720 Training loss: 0.7591 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24133/35720 Training loss: 0.7592 0.2217 sec/batch\n",
      "Epoch 14/20  Iteration 24134/35720 Training loss: 0.7592 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 24135/35720 Training loss: 0.7592 0.2076 sec/batch\n",
      "Epoch 14/20  Iteration 24136/35720 Training loss: 0.7592 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 24137/35720 Training loss: 0.7592 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 24138/35720 Training loss: 0.7592 0.2218 sec/batch\n",
      "Epoch 14/20  Iteration 24139/35720 Training loss: 0.7592 0.2232 sec/batch\n",
      "Epoch 14/20  Iteration 24140/35720 Training loss: 0.7591 0.2273 sec/batch\n",
      "Epoch 14/20  Iteration 24141/35720 Training loss: 0.7591 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 24142/35720 Training loss: 0.7591 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24143/35720 Training loss: 0.7591 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 24144/35720 Training loss: 0.7591 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 24145/35720 Training loss: 0.7591 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24146/35720 Training loss: 0.7590 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 24147/35720 Training loss: 0.7591 0.2228 sec/batch\n",
      "Epoch 14/20  Iteration 24148/35720 Training loss: 0.7591 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 24149/35720 Training loss: 0.7591 0.2135 sec/batch\n",
      "Epoch 14/20  Iteration 24150/35720 Training loss: 0.7592 0.2113 sec/batch\n",
      "Epoch 14/20  Iteration 24151/35720 Training loss: 0.7591 0.2270 sec/batch\n",
      "Epoch 14/20  Iteration 24152/35720 Training loss: 0.7591 0.2108 sec/batch\n",
      "Epoch 14/20  Iteration 24153/35720 Training loss: 0.7590 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 24154/35720 Training loss: 0.7590 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 24155/35720 Training loss: 0.7589 0.2297 sec/batch\n",
      "Epoch 14/20  Iteration 24156/35720 Training loss: 0.7589 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24157/35720 Training loss: 0.7589 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 24158/35720 Training loss: 0.7588 0.2081 sec/batch\n",
      "Epoch 14/20  Iteration 24159/35720 Training loss: 0.7588 0.2175 sec/batch\n",
      "Epoch 14/20  Iteration 24160/35720 Training loss: 0.7587 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 24161/35720 Training loss: 0.7587 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 24162/35720 Training loss: 0.7587 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 24163/35720 Training loss: 0.7587 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 24164/35720 Training loss: 0.7587 0.2101 sec/batch\n",
      "Epoch 14/20  Iteration 24165/35720 Training loss: 0.7587 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 24166/35720 Training loss: 0.7587 0.2163 sec/batch\n",
      "Epoch 14/20  Iteration 24167/35720 Training loss: 0.7587 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 24168/35720 Training loss: 0.7586 0.2205 sec/batch\n",
      "Epoch 14/20  Iteration 24169/35720 Training loss: 0.7586 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 24170/35720 Training loss: 0.7585 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 24171/35720 Training loss: 0.7584 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 24172/35720 Training loss: 0.7584 0.2259 sec/batch\n",
      "Epoch 14/20  Iteration 24173/35720 Training loss: 0.7584 0.2258 sec/batch\n",
      "Epoch 14/20  Iteration 24174/35720 Training loss: 0.7584 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 24175/35720 Training loss: 0.7584 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 24176/35720 Training loss: 0.7584 0.2078 sec/batch\n",
      "Epoch 14/20  Iteration 24177/35720 Training loss: 0.7583 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 24178/35720 Training loss: 0.7583 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 24179/35720 Training loss: 0.7582 0.2151 sec/batch\n",
      "Epoch 14/20  Iteration 24180/35720 Training loss: 0.7582 0.2076 sec/batch\n",
      "Epoch 14/20  Iteration 24181/35720 Training loss: 0.7581 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 24182/35720 Training loss: 0.7581 0.2124 sec/batch\n",
      "Epoch 14/20  Iteration 24183/35720 Training loss: 0.7581 0.2270 sec/batch\n",
      "Epoch 14/20  Iteration 24184/35720 Training loss: 0.7581 0.2136 sec/batch\n",
      "Epoch 14/20  Iteration 24185/35720 Training loss: 0.7581 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 24186/35720 Training loss: 0.7580 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 24187/35720 Training loss: 0.7581 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 24188/35720 Training loss: 0.7583 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 24189/35720 Training loss: 0.7582 0.2440 sec/batch\n",
      "Epoch 14/20  Iteration 24190/35720 Training loss: 0.7581 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 24191/35720 Training loss: 0.7581 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 24192/35720 Training loss: 0.7580 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24193/35720 Training loss: 0.7579 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24194/35720 Training loss: 0.7579 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 24195/35720 Training loss: 0.7579 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24196/35720 Training loss: 0.7579 0.2206 sec/batch\n",
      "Epoch 14/20  Iteration 24197/35720 Training loss: 0.7578 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 24198/35720 Training loss: 0.7578 0.2055 sec/batch\n",
      "Epoch 14/20  Iteration 24199/35720 Training loss: 0.7578 0.2236 sec/batch\n",
      "Epoch 14/20  Iteration 24200/35720 Training loss: 0.7577 0.2235 sec/batch\n",
      "Validation loss: 1.52191 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 24201/35720 Training loss: 0.7581 0.2105 sec/batch\n",
      "Epoch 14/20  Iteration 24202/35720 Training loss: 0.7580 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 24203/35720 Training loss: 0.7580 0.2153 sec/batch\n",
      "Epoch 14/20  Iteration 24204/35720 Training loss: 0.7580 0.2182 sec/batch\n",
      "Epoch 14/20  Iteration 24205/35720 Training loss: 0.7579 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 24206/35720 Training loss: 0.7579 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 24207/35720 Training loss: 0.7578 0.2055 sec/batch\n",
      "Epoch 14/20  Iteration 24208/35720 Training loss: 0.7578 0.2180 sec/batch\n",
      "Epoch 14/20  Iteration 24209/35720 Training loss: 0.7577 0.2280 sec/batch\n",
      "Epoch 14/20  Iteration 24210/35720 Training loss: 0.7578 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 24211/35720 Training loss: 0.7578 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24212/35720 Training loss: 0.7577 0.2142 sec/batch\n",
      "Epoch 14/20  Iteration 24213/35720 Training loss: 0.7578 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 24214/35720 Training loss: 0.7578 0.2165 sec/batch\n",
      "Epoch 14/20  Iteration 24215/35720 Training loss: 0.7578 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 24216/35720 Training loss: 0.7578 0.2212 sec/batch\n",
      "Epoch 14/20  Iteration 24217/35720 Training loss: 0.7578 0.2152 sec/batch\n",
      "Epoch 14/20  Iteration 24218/35720 Training loss: 0.7577 0.2075 sec/batch\n",
      "Epoch 14/20  Iteration 24219/35720 Training loss: 0.7577 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 24220/35720 Training loss: 0.7576 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24221/35720 Training loss: 0.7575 0.2312 sec/batch\n",
      "Epoch 14/20  Iteration 24222/35720 Training loss: 0.7575 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24223/35720 Training loss: 0.7574 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 24224/35720 Training loss: 0.7574 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 24225/35720 Training loss: 0.7573 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24226/35720 Training loss: 0.7573 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24227/35720 Training loss: 0.7572 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 24228/35720 Training loss: 0.7571 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24229/35720 Training loss: 0.7571 0.2203 sec/batch\n",
      "Epoch 14/20  Iteration 24230/35720 Training loss: 0.7571 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 24231/35720 Training loss: 0.7571 0.2179 sec/batch\n",
      "Epoch 14/20  Iteration 24232/35720 Training loss: 0.7571 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 24233/35720 Training loss: 0.7570 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 24234/35720 Training loss: 0.7571 0.2145 sec/batch\n",
      "Epoch 14/20  Iteration 24235/35720 Training loss: 0.7571 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 24236/35720 Training loss: 0.7570 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 24237/35720 Training loss: 0.7570 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 24238/35720 Training loss: 0.7569 0.2178 sec/batch\n",
      "Epoch 14/20  Iteration 24239/35720 Training loss: 0.7569 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24240/35720 Training loss: 0.7570 0.2200 sec/batch\n",
      "Epoch 14/20  Iteration 24241/35720 Training loss: 0.7570 0.2212 sec/batch\n",
      "Epoch 14/20  Iteration 24242/35720 Training loss: 0.7570 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24243/35720 Training loss: 0.7570 0.2176 sec/batch\n",
      "Epoch 14/20  Iteration 24244/35720 Training loss: 0.7570 0.2235 sec/batch\n",
      "Epoch 14/20  Iteration 24245/35720 Training loss: 0.7570 0.2226 sec/batch\n",
      "Epoch 14/20  Iteration 24246/35720 Training loss: 0.7570 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 24247/35720 Training loss: 0.7569 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 24248/35720 Training loss: 0.7569 0.2192 sec/batch\n",
      "Epoch 14/20  Iteration 24249/35720 Training loss: 0.7569 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 24250/35720 Training loss: 0.7569 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 24251/35720 Training loss: 0.7569 0.2164 sec/batch\n",
      "Epoch 14/20  Iteration 24252/35720 Training loss: 0.7569 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24253/35720 Training loss: 0.7570 0.2202 sec/batch\n",
      "Epoch 14/20  Iteration 24254/35720 Training loss: 0.7570 0.2118 sec/batch\n",
      "Epoch 14/20  Iteration 24255/35720 Training loss: 0.7570 0.2234 sec/batch\n",
      "Epoch 14/20  Iteration 24256/35720 Training loss: 0.7571 0.2291 sec/batch\n",
      "Epoch 14/20  Iteration 24257/35720 Training loss: 0.7570 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 24258/35720 Training loss: 0.7570 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 24259/35720 Training loss: 0.7571 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 24260/35720 Training loss: 0.7570 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24261/35720 Training loss: 0.7570 0.2248 sec/batch\n",
      "Epoch 14/20  Iteration 24262/35720 Training loss: 0.7570 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 24263/35720 Training loss: 0.7570 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 24264/35720 Training loss: 0.7570 0.2099 sec/batch\n",
      "Epoch 14/20  Iteration 24265/35720 Training loss: 0.7570 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 24266/35720 Training loss: 0.7570 0.2153 sec/batch\n",
      "Epoch 14/20  Iteration 24267/35720 Training loss: 0.7570 0.2149 sec/batch\n",
      "Epoch 14/20  Iteration 24268/35720 Training loss: 0.7570 0.2279 sec/batch\n",
      "Epoch 14/20  Iteration 24269/35720 Training loss: 0.7570 0.2055 sec/batch\n",
      "Epoch 14/20  Iteration 24270/35720 Training loss: 0.7570 0.2122 sec/batch\n",
      "Epoch 14/20  Iteration 24271/35720 Training loss: 0.7571 0.2160 sec/batch\n",
      "Epoch 14/20  Iteration 24272/35720 Training loss: 0.7571 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24273/35720 Training loss: 0.7571 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 24274/35720 Training loss: 0.7571 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 24275/35720 Training loss: 0.7571 0.2417 sec/batch\n",
      "Epoch 14/20  Iteration 24276/35720 Training loss: 0.7571 0.3284 sec/batch\n",
      "Epoch 14/20  Iteration 24277/35720 Training loss: 0.7571 0.2116 sec/batch\n",
      "Epoch 14/20  Iteration 24278/35720 Training loss: 0.7570 0.2160 sec/batch\n",
      "Epoch 14/20  Iteration 24279/35720 Training loss: 0.7571 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 24280/35720 Training loss: 0.7571 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 24281/35720 Training loss: 0.7572 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 24282/35720 Training loss: 0.7572 0.2262 sec/batch\n",
      "Epoch 14/20  Iteration 24283/35720 Training loss: 0.7571 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 24284/35720 Training loss: 0.7571 0.2314 sec/batch\n",
      "Epoch 14/20  Iteration 24285/35720 Training loss: 0.7571 0.2101 sec/batch\n",
      "Epoch 14/20  Iteration 24286/35720 Training loss: 0.7571 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24287/35720 Training loss: 0.7571 0.2121 sec/batch\n",
      "Epoch 14/20  Iteration 24288/35720 Training loss: 0.7571 0.2229 sec/batch\n",
      "Epoch 14/20  Iteration 24289/35720 Training loss: 0.7571 0.2267 sec/batch\n",
      "Epoch 14/20  Iteration 24290/35720 Training loss: 0.7571 0.2232 sec/batch\n",
      "Epoch 14/20  Iteration 24291/35720 Training loss: 0.7571 0.2215 sec/batch\n",
      "Epoch 14/20  Iteration 24292/35720 Training loss: 0.7571 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24293/35720 Training loss: 0.7571 0.2278 sec/batch\n",
      "Epoch 14/20  Iteration 24294/35720 Training loss: 0.7571 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 24295/35720 Training loss: 0.7570 0.2200 sec/batch\n",
      "Epoch 14/20  Iteration 24296/35720 Training loss: 0.7570 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 24297/35720 Training loss: 0.7570 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 24298/35720 Training loss: 0.7570 0.2079 sec/batch\n",
      "Epoch 14/20  Iteration 24299/35720 Training loss: 0.7570 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24300/35720 Training loss: 0.7569 0.2371 sec/batch\n",
      "Epoch 14/20  Iteration 24301/35720 Training loss: 0.7569 0.2123 sec/batch\n",
      "Epoch 14/20  Iteration 24302/35720 Training loss: 0.7569 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 24303/35720 Training loss: 0.7569 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 24304/35720 Training loss: 0.7569 0.2179 sec/batch\n",
      "Epoch 14/20  Iteration 24305/35720 Training loss: 0.7569 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 24306/35720 Training loss: 0.7569 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 24307/35720 Training loss: 0.7570 0.2194 sec/batch\n",
      "Epoch 14/20  Iteration 24308/35720 Training loss: 0.7570 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 24309/35720 Training loss: 0.7570 0.2160 sec/batch\n",
      "Epoch 14/20  Iteration 24310/35720 Training loss: 0.7570 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24311/35720 Training loss: 0.7569 0.2192 sec/batch\n",
      "Epoch 14/20  Iteration 24312/35720 Training loss: 0.7569 0.2103 sec/batch\n",
      "Epoch 14/20  Iteration 24313/35720 Training loss: 0.7569 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 24314/35720 Training loss: 0.7569 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 24315/35720 Training loss: 0.7569 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 24316/35720 Training loss: 0.7568 0.2229 sec/batch\n",
      "Epoch 14/20  Iteration 24317/35720 Training loss: 0.7569 0.2272 sec/batch\n",
      "Epoch 14/20  Iteration 24318/35720 Training loss: 0.7569 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 24319/35720 Training loss: 0.7569 0.2120 sec/batch\n",
      "Epoch 14/20  Iteration 24320/35720 Training loss: 0.7569 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24321/35720 Training loss: 0.7570 0.2226 sec/batch\n",
      "Epoch 14/20  Iteration 24322/35720 Training loss: 0.7570 0.2251 sec/batch\n",
      "Epoch 14/20  Iteration 24323/35720 Training loss: 0.7570 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 24324/35720 Training loss: 0.7569 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 24325/35720 Training loss: 0.7569 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 24326/35720 Training loss: 0.7569 0.2216 sec/batch\n",
      "Epoch 14/20  Iteration 24327/35720 Training loss: 0.7569 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24328/35720 Training loss: 0.7570 0.2290 sec/batch\n",
      "Epoch 14/20  Iteration 24329/35720 Training loss: 0.7569 0.2137 sec/batch\n",
      "Epoch 14/20  Iteration 24330/35720 Training loss: 0.7568 0.2230 sec/batch\n",
      "Epoch 14/20  Iteration 24331/35720 Training loss: 0.7568 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 24332/35720 Training loss: 0.7568 0.2277 sec/batch\n",
      "Epoch 14/20  Iteration 24333/35720 Training loss: 0.7568 0.2224 sec/batch\n",
      "Epoch 14/20  Iteration 24334/35720 Training loss: 0.7568 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24335/35720 Training loss: 0.7568 0.2112 sec/batch\n",
      "Epoch 14/20  Iteration 24336/35720 Training loss: 0.7567 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24337/35720 Training loss: 0.7567 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 24338/35720 Training loss: 0.7568 0.2128 sec/batch\n",
      "Epoch 14/20  Iteration 24339/35720 Training loss: 0.7568 0.2175 sec/batch\n",
      "Epoch 14/20  Iteration 24340/35720 Training loss: 0.7568 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 24341/35720 Training loss: 0.7568 0.2136 sec/batch\n",
      "Epoch 14/20  Iteration 24342/35720 Training loss: 0.7568 0.2110 sec/batch\n",
      "Epoch 14/20  Iteration 24343/35720 Training loss: 0.7568 0.2183 sec/batch\n",
      "Epoch 14/20  Iteration 24344/35720 Training loss: 0.7568 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24345/35720 Training loss: 0.7568 0.2180 sec/batch\n",
      "Epoch 14/20  Iteration 24346/35720 Training loss: 0.7568 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 24347/35720 Training loss: 0.7567 0.2055 sec/batch\n",
      "Epoch 14/20  Iteration 24348/35720 Training loss: 0.7567 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 24349/35720 Training loss: 0.7567 0.2179 sec/batch\n",
      "Epoch 14/20  Iteration 24350/35720 Training loss: 0.7566 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 24351/35720 Training loss: 0.7565 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24352/35720 Training loss: 0.7566 0.2176 sec/batch\n",
      "Epoch 14/20  Iteration 24353/35720 Training loss: 0.7565 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 24354/35720 Training loss: 0.7565 0.2219 sec/batch\n",
      "Epoch 14/20  Iteration 24355/35720 Training loss: 0.7564 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 24356/35720 Training loss: 0.7564 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 24357/35720 Training loss: 0.7564 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24358/35720 Training loss: 0.7564 0.2075 sec/batch\n",
      "Epoch 14/20  Iteration 24359/35720 Training loss: 0.7564 0.2220 sec/batch\n",
      "Epoch 14/20  Iteration 24360/35720 Training loss: 0.7563 0.2192 sec/batch\n",
      "Epoch 14/20  Iteration 24361/35720 Training loss: 0.7563 0.2209 sec/batch\n",
      "Epoch 14/20  Iteration 24362/35720 Training loss: 0.7563 0.2197 sec/batch\n",
      "Epoch 14/20  Iteration 24363/35720 Training loss: 0.7562 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 24364/35720 Training loss: 0.7562 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24365/35720 Training loss: 0.7562 0.2213 sec/batch\n",
      "Epoch 14/20  Iteration 24366/35720 Training loss: 0.7561 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 24367/35720 Training loss: 0.7561 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 24368/35720 Training loss: 0.7561 0.2243 sec/batch\n",
      "Epoch 14/20  Iteration 24369/35720 Training loss: 0.7561 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24370/35720 Training loss: 0.7561 0.2233 sec/batch\n",
      "Epoch 14/20  Iteration 24371/35720 Training loss: 0.7561 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 24372/35720 Training loss: 0.7561 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 24373/35720 Training loss: 0.7560 0.2280 sec/batch\n",
      "Epoch 14/20  Iteration 24374/35720 Training loss: 0.7560 0.2058 sec/batch\n",
      "Epoch 14/20  Iteration 24375/35720 Training loss: 0.7560 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 24376/35720 Training loss: 0.7560 0.2400 sec/batch\n",
      "Epoch 14/20  Iteration 24377/35720 Training loss: 0.7560 0.2164 sec/batch\n",
      "Epoch 14/20  Iteration 24378/35720 Training loss: 0.7560 0.2294 sec/batch\n",
      "Epoch 14/20  Iteration 24379/35720 Training loss: 0.7559 0.2125 sec/batch\n",
      "Epoch 14/20  Iteration 24380/35720 Training loss: 0.7559 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 24381/35720 Training loss: 0.7560 0.2107 sec/batch\n",
      "Epoch 14/20  Iteration 24382/35720 Training loss: 0.7560 0.2300 sec/batch\n",
      "Epoch 14/20  Iteration 24383/35720 Training loss: 0.7559 0.2352 sec/batch\n",
      "Epoch 14/20  Iteration 24384/35720 Training loss: 0.7559 0.2240 sec/batch\n",
      "Epoch 14/20  Iteration 24385/35720 Training loss: 0.7559 0.2201 sec/batch\n",
      "Epoch 14/20  Iteration 24386/35720 Training loss: 0.7559 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24387/35720 Training loss: 0.7559 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 24388/35720 Training loss: 0.7559 0.2220 sec/batch\n",
      "Epoch 14/20  Iteration 24389/35720 Training loss: 0.7559 0.2269 sec/batch\n",
      "Epoch 14/20  Iteration 24390/35720 Training loss: 0.7560 0.2182 sec/batch\n",
      "Epoch 14/20  Iteration 24391/35720 Training loss: 0.7559 0.2112 sec/batch\n",
      "Epoch 14/20  Iteration 24392/35720 Training loss: 0.7559 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 24393/35720 Training loss: 0.7559 0.2269 sec/batch\n",
      "Epoch 14/20  Iteration 24394/35720 Training loss: 0.7559 0.2209 sec/batch\n",
      "Epoch 14/20  Iteration 24395/35720 Training loss: 0.7560 0.2197 sec/batch\n",
      "Epoch 14/20  Iteration 24396/35720 Training loss: 0.7560 0.2103 sec/batch\n",
      "Epoch 14/20  Iteration 24397/35720 Training loss: 0.7560 0.2204 sec/batch\n",
      "Epoch 14/20  Iteration 24398/35720 Training loss: 0.7559 0.2165 sec/batch\n",
      "Epoch 14/20  Iteration 24399/35720 Training loss: 0.7560 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 24400/35720 Training loss: 0.7560 0.2311 sec/batch\n",
      "Validation loss: 1.51371 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 24401/35720 Training loss: 0.7562 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24402/35720 Training loss: 0.7562 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 24403/35720 Training loss: 0.7562 0.2172 sec/batch\n",
      "Epoch 14/20  Iteration 24404/35720 Training loss: 0.7561 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 24405/35720 Training loss: 0.7561 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 24406/35720 Training loss: 0.7562 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24407/35720 Training loss: 0.7561 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24408/35720 Training loss: 0.7561 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24409/35720 Training loss: 0.7562 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24410/35720 Training loss: 0.7562 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 24411/35720 Training loss: 0.7562 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 24412/35720 Training loss: 0.7562 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24413/35720 Training loss: 0.7562 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 24414/35720 Training loss: 0.7562 0.2160 sec/batch\n",
      "Epoch 14/20  Iteration 24415/35720 Training loss: 0.7562 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 24416/35720 Training loss: 0.7563 0.2182 sec/batch\n",
      "Epoch 14/20  Iteration 24417/35720 Training loss: 0.7562 0.2172 sec/batch\n",
      "Epoch 14/20  Iteration 24418/35720 Training loss: 0.7562 0.2058 sec/batch\n",
      "Epoch 14/20  Iteration 24419/35720 Training loss: 0.7562 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 24420/35720 Training loss: 0.7561 0.2218 sec/batch\n",
      "Epoch 14/20  Iteration 24421/35720 Training loss: 0.7561 0.2189 sec/batch\n",
      "Epoch 14/20  Iteration 24422/35720 Training loss: 0.7560 0.2054 sec/batch\n",
      "Epoch 14/20  Iteration 24423/35720 Training loss: 0.7560 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 24424/35720 Training loss: 0.7560 0.2201 sec/batch\n",
      "Epoch 14/20  Iteration 24425/35720 Training loss: 0.7560 0.2264 sec/batch\n",
      "Epoch 14/20  Iteration 24426/35720 Training loss: 0.7560 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 24427/35720 Training loss: 0.7559 0.2283 sec/batch\n",
      "Epoch 14/20  Iteration 24428/35720 Training loss: 0.7559 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 24429/35720 Training loss: 0.7559 0.2189 sec/batch\n",
      "Epoch 14/20  Iteration 24430/35720 Training loss: 0.7559 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 24431/35720 Training loss: 0.7558 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 24432/35720 Training loss: 0.7559 0.2258 sec/batch\n",
      "Epoch 14/20  Iteration 24433/35720 Training loss: 0.7558 0.2266 sec/batch\n",
      "Epoch 14/20  Iteration 24434/35720 Training loss: 0.7558 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 24435/35720 Training loss: 0.7558 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 24436/35720 Training loss: 0.7558 0.2198 sec/batch\n",
      "Epoch 14/20  Iteration 24437/35720 Training loss: 0.7557 0.2137 sec/batch\n",
      "Epoch 14/20  Iteration 24438/35720 Training loss: 0.7557 0.2212 sec/batch\n",
      "Epoch 14/20  Iteration 24439/35720 Training loss: 0.7557 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 24440/35720 Training loss: 0.7557 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 24441/35720 Training loss: 0.7557 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24442/35720 Training loss: 0.7556 0.2209 sec/batch\n",
      "Epoch 14/20  Iteration 24443/35720 Training loss: 0.7556 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 24444/35720 Training loss: 0.7556 0.2198 sec/batch\n",
      "Epoch 14/20  Iteration 24445/35720 Training loss: 0.7556 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24446/35720 Training loss: 0.7556 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 24447/35720 Training loss: 0.7556 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 24448/35720 Training loss: 0.7556 0.2298 sec/batch\n",
      "Epoch 14/20  Iteration 24449/35720 Training loss: 0.7556 0.2054 sec/batch\n",
      "Epoch 14/20  Iteration 24450/35720 Training loss: 0.7556 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 24451/35720 Training loss: 0.7556 0.2054 sec/batch\n",
      "Epoch 14/20  Iteration 24452/35720 Training loss: 0.7556 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 24453/35720 Training loss: 0.7556 0.2347 sec/batch\n",
      "Epoch 14/20  Iteration 24454/35720 Training loss: 0.7555 0.2188 sec/batch\n",
      "Epoch 14/20  Iteration 24455/35720 Training loss: 0.7555 0.2243 sec/batch\n",
      "Epoch 14/20  Iteration 24456/35720 Training loss: 0.7554 0.2125 sec/batch\n",
      "Epoch 14/20  Iteration 24457/35720 Training loss: 0.7553 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 24458/35720 Training loss: 0.7553 0.2103 sec/batch\n",
      "Epoch 14/20  Iteration 24459/35720 Training loss: 0.7552 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24460/35720 Training loss: 0.7552 0.2341 sec/batch\n",
      "Epoch 14/20  Iteration 24461/35720 Training loss: 0.7552 0.2076 sec/batch\n",
      "Epoch 14/20  Iteration 24462/35720 Training loss: 0.7552 0.2099 sec/batch\n",
      "Epoch 14/20  Iteration 24463/35720 Training loss: 0.7551 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24464/35720 Training loss: 0.7551 0.2177 sec/batch\n",
      "Epoch 14/20  Iteration 24465/35720 Training loss: 0.7551 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 24466/35720 Training loss: 0.7551 0.2180 sec/batch\n",
      "Epoch 14/20  Iteration 24467/35720 Training loss: 0.7550 0.2075 sec/batch\n",
      "Epoch 14/20  Iteration 24468/35720 Training loss: 0.7550 0.2231 sec/batch\n",
      "Epoch 14/20  Iteration 24469/35720 Training loss: 0.7549 0.2197 sec/batch\n",
      "Epoch 14/20  Iteration 24470/35720 Training loss: 0.7549 0.2268 sec/batch\n",
      "Epoch 14/20  Iteration 24471/35720 Training loss: 0.7549 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 24472/35720 Training loss: 0.7549 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 24473/35720 Training loss: 0.7549 0.2116 sec/batch\n",
      "Epoch 14/20  Iteration 24474/35720 Training loss: 0.7549 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24475/35720 Training loss: 0.7548 0.2205 sec/batch\n",
      "Epoch 14/20  Iteration 24476/35720 Training loss: 0.7548 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 24477/35720 Training loss: 0.7548 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 24478/35720 Training loss: 0.7548 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24479/35720 Training loss: 0.7547 0.2123 sec/batch\n",
      "Epoch 14/20  Iteration 24480/35720 Training loss: 0.7547 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24481/35720 Training loss: 0.7547 0.2239 sec/batch\n",
      "Epoch 14/20  Iteration 24482/35720 Training loss: 0.7546 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 24483/35720 Training loss: 0.7546 0.2208 sec/batch\n",
      "Epoch 14/20  Iteration 24484/35720 Training loss: 0.7545 0.2053 sec/batch\n",
      "Epoch 14/20  Iteration 24485/35720 Training loss: 0.7545 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 24486/35720 Training loss: 0.7545 0.2251 sec/batch\n",
      "Epoch 14/20  Iteration 24487/35720 Training loss: 0.7545 0.2300 sec/batch\n",
      "Epoch 14/20  Iteration 24488/35720 Training loss: 0.7545 0.2304 sec/batch\n",
      "Epoch 14/20  Iteration 24489/35720 Training loss: 0.7545 0.2082 sec/batch\n",
      "Epoch 14/20  Iteration 24490/35720 Training loss: 0.7545 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 24491/35720 Training loss: 0.7544 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 24492/35720 Training loss: 0.7544 0.2238 sec/batch\n",
      "Epoch 14/20  Iteration 24493/35720 Training loss: 0.7544 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 24494/35720 Training loss: 0.7544 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 24495/35720 Training loss: 0.7543 0.2256 sec/batch\n",
      "Epoch 14/20  Iteration 24496/35720 Training loss: 0.7543 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 24497/35720 Training loss: 0.7542 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 24498/35720 Training loss: 0.7541 0.2587 sec/batch\n",
      "Epoch 14/20  Iteration 24499/35720 Training loss: 0.7542 0.2458 sec/batch\n",
      "Epoch 14/20  Iteration 24500/35720 Training loss: 0.7541 0.2264 sec/batch\n",
      "Epoch 14/20  Iteration 24501/35720 Training loss: 0.7541 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 24502/35720 Training loss: 0.7541 0.2391 sec/batch\n",
      "Epoch 14/20  Iteration 24503/35720 Training loss: 0.7540 0.2244 sec/batch\n",
      "Epoch 14/20  Iteration 24504/35720 Training loss: 0.7540 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 24505/35720 Training loss: 0.7539 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 24506/35720 Training loss: 0.7539 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 24507/35720 Training loss: 0.7539 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 24508/35720 Training loss: 0.7539 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 24509/35720 Training loss: 0.7538 0.2208 sec/batch\n",
      "Epoch 14/20  Iteration 24510/35720 Training loss: 0.7538 0.2145 sec/batch\n",
      "Epoch 14/20  Iteration 24511/35720 Training loss: 0.7538 0.2099 sec/batch\n",
      "Epoch 14/20  Iteration 24512/35720 Training loss: 0.7538 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24513/35720 Training loss: 0.7538 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24514/35720 Training loss: 0.7537 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 24515/35720 Training loss: 0.7538 0.2142 sec/batch\n",
      "Epoch 14/20  Iteration 24516/35720 Training loss: 0.7537 0.2249 sec/batch\n",
      "Epoch 14/20  Iteration 24517/35720 Training loss: 0.7537 0.2228 sec/batch\n",
      "Epoch 14/20  Iteration 24518/35720 Training loss: 0.7536 0.2220 sec/batch\n",
      "Epoch 14/20  Iteration 24519/35720 Training loss: 0.7536 0.2281 sec/batch\n",
      "Epoch 14/20  Iteration 24520/35720 Training loss: 0.7536 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 24521/35720 Training loss: 0.7535 0.2244 sec/batch\n",
      "Epoch 14/20  Iteration 24522/35720 Training loss: 0.7535 0.2055 sec/batch\n",
      "Epoch 14/20  Iteration 24523/35720 Training loss: 0.7535 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 24524/35720 Training loss: 0.7535 0.2179 sec/batch\n",
      "Epoch 14/20  Iteration 24525/35720 Training loss: 0.7534 0.2214 sec/batch\n",
      "Epoch 14/20  Iteration 24526/35720 Training loss: 0.7534 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 24527/35720 Training loss: 0.7534 0.2350 sec/batch\n",
      "Epoch 14/20  Iteration 24528/35720 Training loss: 0.7533 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 24529/35720 Training loss: 0.7533 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24530/35720 Training loss: 0.7533 0.2200 sec/batch\n",
      "Epoch 14/20  Iteration 24531/35720 Training loss: 0.7533 0.2113 sec/batch\n",
      "Epoch 14/20  Iteration 24532/35720 Training loss: 0.7533 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24533/35720 Training loss: 0.7532 0.2211 sec/batch\n",
      "Epoch 14/20  Iteration 24534/35720 Training loss: 0.7532 0.2069 sec/batch\n",
      "Epoch 14/20  Iteration 24535/35720 Training loss: 0.7532 0.2197 sec/batch\n",
      "Epoch 14/20  Iteration 24536/35720 Training loss: 0.7532 0.2248 sec/batch\n",
      "Epoch 14/20  Iteration 24537/35720 Training loss: 0.7532 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24538/35720 Training loss: 0.7531 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 24539/35720 Training loss: 0.7531 0.2271 sec/batch\n",
      "Epoch 14/20  Iteration 24540/35720 Training loss: 0.7531 0.2176 sec/batch\n",
      "Epoch 14/20  Iteration 24541/35720 Training loss: 0.7531 0.2284 sec/batch\n",
      "Epoch 14/20  Iteration 24542/35720 Training loss: 0.7531 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 24543/35720 Training loss: 0.7531 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 24544/35720 Training loss: 0.7531 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 24545/35720 Training loss: 0.7531 0.2120 sec/batch\n",
      "Epoch 14/20  Iteration 24546/35720 Training loss: 0.7531 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24547/35720 Training loss: 0.7531 0.2219 sec/batch\n",
      "Epoch 14/20  Iteration 24548/35720 Training loss: 0.7531 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24549/35720 Training loss: 0.7531 0.2214 sec/batch\n",
      "Epoch 14/20  Iteration 24550/35720 Training loss: 0.7531 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 24551/35720 Training loss: 0.7531 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 24552/35720 Training loss: 0.7531 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 24553/35720 Training loss: 0.7531 0.2107 sec/batch\n",
      "Epoch 14/20  Iteration 24554/35720 Training loss: 0.7531 0.2121 sec/batch\n",
      "Epoch 14/20  Iteration 24555/35720 Training loss: 0.7532 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 24556/35720 Training loss: 0.7532 0.2114 sec/batch\n",
      "Epoch 14/20  Iteration 24557/35720 Training loss: 0.7531 0.2318 sec/batch\n",
      "Epoch 14/20  Iteration 24558/35720 Training loss: 0.7531 0.2333 sec/batch\n",
      "Epoch 14/20  Iteration 24559/35720 Training loss: 0.7531 0.2218 sec/batch\n",
      "Epoch 14/20  Iteration 24560/35720 Training loss: 0.7531 0.2330 sec/batch\n",
      "Epoch 14/20  Iteration 24561/35720 Training loss: 0.7531 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24562/35720 Training loss: 0.7530 0.2078 sec/batch\n",
      "Epoch 14/20  Iteration 24563/35720 Training loss: 0.7530 0.2075 sec/batch\n",
      "Epoch 14/20  Iteration 24564/35720 Training loss: 0.7530 0.2188 sec/batch\n",
      "Epoch 14/20  Iteration 24565/35720 Training loss: 0.7530 0.2153 sec/batch\n",
      "Epoch 14/20  Iteration 24566/35720 Training loss: 0.7530 0.2253 sec/batch\n",
      "Epoch 14/20  Iteration 24567/35720 Training loss: 0.7530 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24568/35720 Training loss: 0.7530 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 24569/35720 Training loss: 0.7530 0.2315 sec/batch\n",
      "Epoch 14/20  Iteration 24570/35720 Training loss: 0.7530 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 24571/35720 Training loss: 0.7531 0.2296 sec/batch\n",
      "Epoch 14/20  Iteration 24572/35720 Training loss: 0.7530 0.2156 sec/batch\n",
      "Epoch 14/20  Iteration 24573/35720 Training loss: 0.7530 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24574/35720 Training loss: 0.7529 0.2215 sec/batch\n",
      "Epoch 14/20  Iteration 24575/35720 Training loss: 0.7529 0.2108 sec/batch\n",
      "Epoch 14/20  Iteration 24576/35720 Training loss: 0.7529 0.2344 sec/batch\n",
      "Epoch 14/20  Iteration 24577/35720 Training loss: 0.7529 0.2108 sec/batch\n",
      "Epoch 14/20  Iteration 24578/35720 Training loss: 0.7528 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24579/35720 Training loss: 0.7528 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 24580/35720 Training loss: 0.7528 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24581/35720 Training loss: 0.7527 0.2222 sec/batch\n",
      "Epoch 14/20  Iteration 24582/35720 Training loss: 0.7527 0.2149 sec/batch\n",
      "Epoch 14/20  Iteration 24583/35720 Training loss: 0.7527 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24584/35720 Training loss: 0.7527 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 24585/35720 Training loss: 0.7527 0.2110 sec/batch\n",
      "Epoch 14/20  Iteration 24586/35720 Training loss: 0.7526 0.2313 sec/batch\n",
      "Epoch 14/20  Iteration 24587/35720 Training loss: 0.7527 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 24588/35720 Training loss: 0.7527 0.2210 sec/batch\n",
      "Epoch 14/20  Iteration 24589/35720 Training loss: 0.7527 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 24590/35720 Training loss: 0.7527 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 24591/35720 Training loss: 0.7527 0.2343 sec/batch\n",
      "Epoch 14/20  Iteration 24592/35720 Training loss: 0.7527 0.2126 sec/batch\n",
      "Epoch 14/20  Iteration 24593/35720 Training loss: 0.7527 0.2308 sec/batch\n",
      "Epoch 14/20  Iteration 24594/35720 Training loss: 0.7527 0.2145 sec/batch\n",
      "Epoch 14/20  Iteration 24595/35720 Training loss: 0.7527 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 24596/35720 Training loss: 0.7527 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24597/35720 Training loss: 0.7527 0.2257 sec/batch\n",
      "Epoch 14/20  Iteration 24598/35720 Training loss: 0.7527 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 24599/35720 Training loss: 0.7527 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 24600/35720 Training loss: 0.7527 0.2193 sec/batch\n",
      "Validation loss: 1.52341 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 24601/35720 Training loss: 0.7530 0.2105 sec/batch\n",
      "Epoch 14/20  Iteration 24602/35720 Training loss: 0.7529 0.2125 sec/batch\n",
      "Epoch 14/20  Iteration 24603/35720 Training loss: 0.7530 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 24604/35720 Training loss: 0.7530 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24605/35720 Training loss: 0.7530 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24606/35720 Training loss: 0.7530 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 24607/35720 Training loss: 0.7530 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 24608/35720 Training loss: 0.7530 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 24609/35720 Training loss: 0.7530 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 24610/35720 Training loss: 0.7529 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 24611/35720 Training loss: 0.7529 0.2358 sec/batch\n",
      "Epoch 14/20  Iteration 24612/35720 Training loss: 0.7528 0.2245 sec/batch\n",
      "Epoch 14/20  Iteration 24613/35720 Training loss: 0.7528 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 24614/35720 Training loss: 0.7528 0.2248 sec/batch\n",
      "Epoch 14/20  Iteration 24615/35720 Training loss: 0.7527 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 24616/35720 Training loss: 0.7527 0.2164 sec/batch\n",
      "Epoch 14/20  Iteration 24617/35720 Training loss: 0.7527 0.2254 sec/batch\n",
      "Epoch 14/20  Iteration 24618/35720 Training loss: 0.7526 0.2323 sec/batch\n",
      "Epoch 14/20  Iteration 24619/35720 Training loss: 0.7526 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 24620/35720 Training loss: 0.7526 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 24621/35720 Training loss: 0.7526 0.2216 sec/batch\n",
      "Epoch 14/20  Iteration 24622/35720 Training loss: 0.7526 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 24623/35720 Training loss: 0.7526 0.2124 sec/batch\n",
      "Epoch 14/20  Iteration 24624/35720 Training loss: 0.7525 0.2275 sec/batch\n",
      "Epoch 14/20  Iteration 24625/35720 Training loss: 0.7525 0.2268 sec/batch\n",
      "Epoch 14/20  Iteration 24626/35720 Training loss: 0.7526 0.2232 sec/batch\n",
      "Epoch 14/20  Iteration 24627/35720 Training loss: 0.7526 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 24628/35720 Training loss: 0.7526 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24629/35720 Training loss: 0.7526 0.2150 sec/batch\n",
      "Epoch 14/20  Iteration 24630/35720 Training loss: 0.7526 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 24631/35720 Training loss: 0.7526 0.2173 sec/batch\n",
      "Epoch 14/20  Iteration 24632/35720 Training loss: 0.7526 0.2125 sec/batch\n",
      "Epoch 14/20  Iteration 24633/35720 Training loss: 0.7526 0.2110 sec/batch\n",
      "Epoch 14/20  Iteration 24634/35720 Training loss: 0.7525 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 24635/35720 Training loss: 0.7525 0.2137 sec/batch\n",
      "Epoch 14/20  Iteration 24636/35720 Training loss: 0.7525 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 24637/35720 Training loss: 0.7526 0.2260 sec/batch\n",
      "Epoch 14/20  Iteration 24638/35720 Training loss: 0.7526 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 24639/35720 Training loss: 0.7526 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 24640/35720 Training loss: 0.7526 0.2058 sec/batch\n",
      "Epoch 14/20  Iteration 24641/35720 Training loss: 0.7525 0.2110 sec/batch\n",
      "Epoch 14/20  Iteration 24642/35720 Training loss: 0.7525 0.2246 sec/batch\n",
      "Epoch 14/20  Iteration 24643/35720 Training loss: 0.7526 0.2081 sec/batch\n",
      "Epoch 14/20  Iteration 24644/35720 Training loss: 0.7526 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 24645/35720 Training loss: 0.7526 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 24646/35720 Training loss: 0.7526 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 24647/35720 Training loss: 0.7526 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 24648/35720 Training loss: 0.7526 0.2259 sec/batch\n",
      "Epoch 14/20  Iteration 24649/35720 Training loss: 0.7526 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 24650/35720 Training loss: 0.7525 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 24651/35720 Training loss: 0.7525 0.2364 sec/batch\n",
      "Epoch 14/20  Iteration 24652/35720 Training loss: 0.7525 0.2136 sec/batch\n",
      "Epoch 14/20  Iteration 24653/35720 Training loss: 0.7525 0.2119 sec/batch\n",
      "Epoch 14/20  Iteration 24654/35720 Training loss: 0.7525 0.2243 sec/batch\n",
      "Epoch 14/20  Iteration 24655/35720 Training loss: 0.7525 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24656/35720 Training loss: 0.7525 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24657/35720 Training loss: 0.7525 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 24658/35720 Training loss: 0.7525 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 24659/35720 Training loss: 0.7525 0.2171 sec/batch\n",
      "Epoch 14/20  Iteration 24660/35720 Training loss: 0.7525 0.2119 sec/batch\n",
      "Epoch 14/20  Iteration 24661/35720 Training loss: 0.7525 0.2076 sec/batch\n",
      "Epoch 14/20  Iteration 24662/35720 Training loss: 0.7525 0.2271 sec/batch\n",
      "Epoch 14/20  Iteration 24663/35720 Training loss: 0.7525 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24664/35720 Training loss: 0.7525 0.2072 sec/batch\n",
      "Epoch 14/20  Iteration 24665/35720 Training loss: 0.7525 0.2081 sec/batch\n",
      "Epoch 14/20  Iteration 24666/35720 Training loss: 0.7525 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24667/35720 Training loss: 0.7525 0.2121 sec/batch\n",
      "Epoch 14/20  Iteration 24668/35720 Training loss: 0.7525 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24669/35720 Training loss: 0.7524 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 24670/35720 Training loss: 0.7524 0.2357 sec/batch\n",
      "Epoch 14/20  Iteration 24671/35720 Training loss: 0.7524 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 24672/35720 Training loss: 0.7524 0.2113 sec/batch\n",
      "Epoch 14/20  Iteration 24673/35720 Training loss: 0.7524 0.2152 sec/batch\n",
      "Epoch 14/20  Iteration 24674/35720 Training loss: 0.7524 0.2221 sec/batch\n",
      "Epoch 14/20  Iteration 24675/35720 Training loss: 0.7525 0.2293 sec/batch\n",
      "Epoch 14/20  Iteration 24676/35720 Training loss: 0.7525 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 24677/35720 Training loss: 0.7525 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 24678/35720 Training loss: 0.7525 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 24679/35720 Training loss: 0.7525 0.2211 sec/batch\n",
      "Epoch 14/20  Iteration 24680/35720 Training loss: 0.7526 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24681/35720 Training loss: 0.7526 0.2240 sec/batch\n",
      "Epoch 14/20  Iteration 24682/35720 Training loss: 0.7526 0.2112 sec/batch\n",
      "Epoch 14/20  Iteration 24683/35720 Training loss: 0.7525 0.2185 sec/batch\n",
      "Epoch 14/20  Iteration 24684/35720 Training loss: 0.7525 0.2194 sec/batch\n",
      "Epoch 14/20  Iteration 24685/35720 Training loss: 0.7525 0.2273 sec/batch\n",
      "Epoch 14/20  Iteration 24686/35720 Training loss: 0.7525 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 24687/35720 Training loss: 0.7525 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 24688/35720 Training loss: 0.7525 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24689/35720 Training loss: 0.7525 0.2138 sec/batch\n",
      "Epoch 14/20  Iteration 24690/35720 Training loss: 0.7524 0.2295 sec/batch\n",
      "Epoch 14/20  Iteration 24691/35720 Training loss: 0.7524 0.2270 sec/batch\n",
      "Epoch 14/20  Iteration 24692/35720 Training loss: 0.7524 0.2330 sec/batch\n",
      "Epoch 14/20  Iteration 24693/35720 Training loss: 0.7523 0.2134 sec/batch\n",
      "Epoch 14/20  Iteration 24694/35720 Training loss: 0.7522 0.2070 sec/batch\n",
      "Epoch 14/20  Iteration 24695/35720 Training loss: 0.7522 0.2176 sec/batch\n",
      "Epoch 14/20  Iteration 24696/35720 Training loss: 0.7522 0.2263 sec/batch\n",
      "Epoch 14/20  Iteration 24697/35720 Training loss: 0.7521 0.2152 sec/batch\n",
      "Epoch 14/20  Iteration 24698/35720 Training loss: 0.7521 0.2245 sec/batch\n",
      "Epoch 14/20  Iteration 24699/35720 Training loss: 0.7521 0.2311 sec/batch\n",
      "Epoch 14/20  Iteration 24700/35720 Training loss: 0.7521 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 24701/35720 Training loss: 0.7520 0.2152 sec/batch\n",
      "Epoch 14/20  Iteration 24702/35720 Training loss: 0.7520 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24703/35720 Training loss: 0.7520 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24704/35720 Training loss: 0.7520 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 24705/35720 Training loss: 0.7519 0.2085 sec/batch\n",
      "Epoch 14/20  Iteration 24706/35720 Training loss: 0.7519 0.2109 sec/batch\n",
      "Epoch 14/20  Iteration 24707/35720 Training loss: 0.7519 0.2156 sec/batch\n",
      "Epoch 14/20  Iteration 24708/35720 Training loss: 0.7519 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 24709/35720 Training loss: 0.7519 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 24710/35720 Training loss: 0.7518 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 24711/35720 Training loss: 0.7518 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 24712/35720 Training loss: 0.7518 0.2132 sec/batch\n",
      "Epoch 14/20  Iteration 24713/35720 Training loss: 0.7518 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 24714/35720 Training loss: 0.7518 0.2393 sec/batch\n",
      "Epoch 14/20  Iteration 24715/35720 Training loss: 0.7518 0.2231 sec/batch\n",
      "Epoch 14/20  Iteration 24716/35720 Training loss: 0.7517 0.2058 sec/batch\n",
      "Epoch 14/20  Iteration 24717/35720 Training loss: 0.7517 0.2150 sec/batch\n",
      "Epoch 14/20  Iteration 24718/35720 Training loss: 0.7517 0.2254 sec/batch\n",
      "Epoch 14/20  Iteration 24719/35720 Training loss: 0.7516 0.2080 sec/batch\n",
      "Epoch 14/20  Iteration 24720/35720 Training loss: 0.7516 0.2225 sec/batch\n",
      "Epoch 14/20  Iteration 24721/35720 Training loss: 0.7516 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 24722/35720 Training loss: 0.7516 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 24723/35720 Training loss: 0.7516 0.2224 sec/batch\n",
      "Epoch 14/20  Iteration 24724/35720 Training loss: 0.7516 0.2658 sec/batch\n",
      "Epoch 14/20  Iteration 24725/35720 Training loss: 0.7516 0.2341 sec/batch\n",
      "Epoch 14/20  Iteration 24726/35720 Training loss: 0.7516 0.2071 sec/batch\n",
      "Epoch 14/20  Iteration 24727/35720 Training loss: 0.7516 0.2116 sec/batch\n",
      "Epoch 14/20  Iteration 24728/35720 Training loss: 0.7515 0.2275 sec/batch\n",
      "Epoch 14/20  Iteration 24729/35720 Training loss: 0.7515 0.2226 sec/batch\n",
      "Epoch 14/20  Iteration 24730/35720 Training loss: 0.7515 0.2089 sec/batch\n",
      "Epoch 14/20  Iteration 24731/35720 Training loss: 0.7515 0.2279 sec/batch\n",
      "Epoch 14/20  Iteration 24732/35720 Training loss: 0.7515 0.2123 sec/batch\n",
      "Epoch 14/20  Iteration 24733/35720 Training loss: 0.7515 0.2115 sec/batch\n",
      "Epoch 14/20  Iteration 24734/35720 Training loss: 0.7515 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24735/35720 Training loss: 0.7515 0.2116 sec/batch\n",
      "Epoch 14/20  Iteration 24736/35720 Training loss: 0.7515 0.2139 sec/batch\n",
      "Epoch 14/20  Iteration 24737/35720 Training loss: 0.7515 0.2207 sec/batch\n",
      "Epoch 14/20  Iteration 24738/35720 Training loss: 0.7515 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 24739/35720 Training loss: 0.7515 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 24740/35720 Training loss: 0.7515 0.2243 sec/batch\n",
      "Epoch 14/20  Iteration 24741/35720 Training loss: 0.7515 0.2165 sec/batch\n",
      "Epoch 14/20  Iteration 24742/35720 Training loss: 0.7516 0.2200 sec/batch\n",
      "Epoch 14/20  Iteration 24743/35720 Training loss: 0.7516 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 24744/35720 Training loss: 0.7516 0.2117 sec/batch\n",
      "Epoch 14/20  Iteration 24745/35720 Training loss: 0.7516 0.2139 sec/batch\n",
      "Epoch 14/20  Iteration 24746/35720 Training loss: 0.7516 0.2345 sec/batch\n",
      "Epoch 14/20  Iteration 24747/35720 Training loss: 0.7516 0.2297 sec/batch\n",
      "Epoch 14/20  Iteration 24748/35720 Training loss: 0.7516 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24749/35720 Training loss: 0.7516 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 24750/35720 Training loss: 0.7516 0.2141 sec/batch\n",
      "Epoch 14/20  Iteration 24751/35720 Training loss: 0.7516 0.2189 sec/batch\n",
      "Epoch 14/20  Iteration 24752/35720 Training loss: 0.7516 0.2208 sec/batch\n",
      "Epoch 14/20  Iteration 24753/35720 Training loss: 0.7516 0.2260 sec/batch\n",
      "Epoch 14/20  Iteration 24754/35720 Training loss: 0.7516 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 24755/35720 Training loss: 0.7515 0.2073 sec/batch\n",
      "Epoch 14/20  Iteration 24756/35720 Training loss: 0.7515 0.2087 sec/batch\n",
      "Epoch 14/20  Iteration 24757/35720 Training loss: 0.7515 0.2265 sec/batch\n",
      "Epoch 14/20  Iteration 24758/35720 Training loss: 0.7515 0.2244 sec/batch\n",
      "Epoch 14/20  Iteration 24759/35720 Training loss: 0.7514 0.2116 sec/batch\n",
      "Epoch 14/20  Iteration 24760/35720 Training loss: 0.7515 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 24761/35720 Training loss: 0.7514 0.2078 sec/batch\n",
      "Epoch 14/20  Iteration 24762/35720 Training loss: 0.7514 0.2230 sec/batch\n",
      "Epoch 14/20  Iteration 24763/35720 Training loss: 0.7514 0.2110 sec/batch\n",
      "Epoch 14/20  Iteration 24764/35720 Training loss: 0.7513 0.2221 sec/batch\n",
      "Epoch 14/20  Iteration 24765/35720 Training loss: 0.7513 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24766/35720 Training loss: 0.7514 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 24767/35720 Training loss: 0.7514 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 24768/35720 Training loss: 0.7513 0.2270 sec/batch\n",
      "Epoch 14/20  Iteration 24769/35720 Training loss: 0.7513 0.2266 sec/batch\n",
      "Epoch 14/20  Iteration 24770/35720 Training loss: 0.7513 0.2233 sec/batch\n",
      "Epoch 14/20  Iteration 24771/35720 Training loss: 0.7513 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 24772/35720 Training loss: 0.7512 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24773/35720 Training loss: 0.7512 0.2107 sec/batch\n",
      "Epoch 14/20  Iteration 24774/35720 Training loss: 0.7512 0.2123 sec/batch\n",
      "Epoch 14/20  Iteration 24775/35720 Training loss: 0.7512 0.2221 sec/batch\n",
      "Epoch 14/20  Iteration 24776/35720 Training loss: 0.7511 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 24777/35720 Training loss: 0.7511 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24778/35720 Training loss: 0.7510 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24779/35720 Training loss: 0.7510 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 24780/35720 Training loss: 0.7510 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24781/35720 Training loss: 0.7509 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 24782/35720 Training loss: 0.7509 0.2139 sec/batch\n",
      "Epoch 14/20  Iteration 24783/35720 Training loss: 0.7509 0.2096 sec/batch\n",
      "Epoch 14/20  Iteration 24784/35720 Training loss: 0.7509 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 24785/35720 Training loss: 0.7508 0.2227 sec/batch\n",
      "Epoch 14/20  Iteration 24786/35720 Training loss: 0.7509 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24787/35720 Training loss: 0.7509 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24788/35720 Training loss: 0.7508 0.2135 sec/batch\n",
      "Epoch 14/20  Iteration 24789/35720 Training loss: 0.7508 0.2149 sec/batch\n",
      "Epoch 14/20  Iteration 24790/35720 Training loss: 0.7508 0.2270 sec/batch\n",
      "Epoch 14/20  Iteration 24791/35720 Training loss: 0.7508 0.2154 sec/batch\n",
      "Epoch 14/20  Iteration 24792/35720 Training loss: 0.7507 0.2204 sec/batch\n",
      "Epoch 14/20  Iteration 24793/35720 Training loss: 0.7507 0.2306 sec/batch\n",
      "Epoch 14/20  Iteration 24794/35720 Training loss: 0.7507 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24795/35720 Training loss: 0.7507 0.2099 sec/batch\n",
      "Epoch 14/20  Iteration 24796/35720 Training loss: 0.7507 0.2205 sec/batch\n",
      "Epoch 14/20  Iteration 24797/35720 Training loss: 0.7507 0.2181 sec/batch\n",
      "Epoch 14/20  Iteration 24798/35720 Training loss: 0.7506 0.2185 sec/batch\n",
      "Epoch 14/20  Iteration 24799/35720 Training loss: 0.7507 0.2231 sec/batch\n",
      "Epoch 14/20  Iteration 24800/35720 Training loss: 0.7506 0.2137 sec/batch\n",
      "Validation loss: 1.52682 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 24801/35720 Training loss: 0.7508 0.2121 sec/batch\n",
      "Epoch 14/20  Iteration 24802/35720 Training loss: 0.7508 0.2158 sec/batch\n",
      "Epoch 14/20  Iteration 24803/35720 Training loss: 0.7508 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 24804/35720 Training loss: 0.7508 0.2121 sec/batch\n",
      "Epoch 14/20  Iteration 24805/35720 Training loss: 0.7507 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 24806/35720 Training loss: 0.7507 0.2279 sec/batch\n",
      "Epoch 14/20  Iteration 24807/35720 Training loss: 0.7507 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 24808/35720 Training loss: 0.7506 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 24809/35720 Training loss: 0.7506 0.2203 sec/batch\n",
      "Epoch 14/20  Iteration 24810/35720 Training loss: 0.7506 0.2235 sec/batch\n",
      "Epoch 14/20  Iteration 24811/35720 Training loss: 0.7506 0.2134 sec/batch\n",
      "Epoch 14/20  Iteration 24812/35720 Training loss: 0.7505 0.2217 sec/batch\n",
      "Epoch 14/20  Iteration 24813/35720 Training loss: 0.7505 0.2172 sec/batch\n",
      "Epoch 14/20  Iteration 24814/35720 Training loss: 0.7504 0.2113 sec/batch\n",
      "Epoch 14/20  Iteration 24815/35720 Training loss: 0.7504 0.2162 sec/batch\n",
      "Epoch 14/20  Iteration 24816/35720 Training loss: 0.7504 0.2282 sec/batch\n",
      "Epoch 14/20  Iteration 24817/35720 Training loss: 0.7503 0.2430 sec/batch\n",
      "Epoch 14/20  Iteration 24818/35720 Training loss: 0.7504 0.2309 sec/batch\n",
      "Epoch 14/20  Iteration 24819/35720 Training loss: 0.7503 0.2307 sec/batch\n",
      "Epoch 14/20  Iteration 24820/35720 Training loss: 0.7503 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 24821/35720 Training loss: 0.7502 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 24822/35720 Training loss: 0.7502 0.2204 sec/batch\n",
      "Epoch 14/20  Iteration 24823/35720 Training loss: 0.7502 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 24824/35720 Training loss: 0.7502 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 24825/35720 Training loss: 0.7502 0.2180 sec/batch\n",
      "Epoch 14/20  Iteration 24826/35720 Training loss: 0.7501 0.2081 sec/batch\n",
      "Epoch 14/20  Iteration 24827/35720 Training loss: 0.7501 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 24828/35720 Training loss: 0.7501 0.2243 sec/batch\n",
      "Epoch 14/20  Iteration 24829/35720 Training loss: 0.7500 0.2160 sec/batch\n",
      "Epoch 14/20  Iteration 24830/35720 Training loss: 0.7500 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 24831/35720 Training loss: 0.7500 0.2101 sec/batch\n",
      "Epoch 14/20  Iteration 24832/35720 Training loss: 0.7500 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 24833/35720 Training loss: 0.7500 0.2240 sec/batch\n",
      "Epoch 14/20  Iteration 24834/35720 Training loss: 0.7500 0.2272 sec/batch\n",
      "Epoch 14/20  Iteration 24835/35720 Training loss: 0.7500 0.2155 sec/batch\n",
      "Epoch 14/20  Iteration 24836/35720 Training loss: 0.7499 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 24837/35720 Training loss: 0.7499 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 24838/35720 Training loss: 0.7499 0.2282 sec/batch\n",
      "Epoch 14/20  Iteration 24839/35720 Training loss: 0.7499 0.2169 sec/batch\n",
      "Epoch 14/20  Iteration 24840/35720 Training loss: 0.7499 0.2209 sec/batch\n",
      "Epoch 14/20  Iteration 24841/35720 Training loss: 0.7498 0.2272 sec/batch\n",
      "Epoch 14/20  Iteration 24842/35720 Training loss: 0.7498 0.2074 sec/batch\n",
      "Epoch 14/20  Iteration 24843/35720 Training loss: 0.7498 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 24844/35720 Training loss: 0.7498 0.2328 sec/batch\n",
      "Epoch 14/20  Iteration 24845/35720 Training loss: 0.7498 0.2094 sec/batch\n",
      "Epoch 14/20  Iteration 24846/35720 Training loss: 0.7498 0.2255 sec/batch\n",
      "Epoch 14/20  Iteration 24847/35720 Training loss: 0.7498 0.2121 sec/batch\n",
      "Epoch 14/20  Iteration 24848/35720 Training loss: 0.7498 0.2052 sec/batch\n",
      "Epoch 14/20  Iteration 24849/35720 Training loss: 0.7497 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 24850/35720 Training loss: 0.7497 0.2268 sec/batch\n",
      "Epoch 14/20  Iteration 24851/35720 Training loss: 0.7497 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 24852/35720 Training loss: 0.7497 0.2231 sec/batch\n",
      "Epoch 14/20  Iteration 24853/35720 Training loss: 0.7497 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24854/35720 Training loss: 0.7497 0.2086 sec/batch\n",
      "Epoch 14/20  Iteration 24855/35720 Training loss: 0.7497 0.2224 sec/batch\n",
      "Epoch 14/20  Iteration 24856/35720 Training loss: 0.7496 0.2238 sec/batch\n",
      "Epoch 14/20  Iteration 24857/35720 Training loss: 0.7496 0.2199 sec/batch\n",
      "Epoch 14/20  Iteration 24858/35720 Training loss: 0.7496 0.2056 sec/batch\n",
      "Epoch 14/20  Iteration 24859/35720 Training loss: 0.7496 0.2061 sec/batch\n",
      "Epoch 14/20  Iteration 24860/35720 Training loss: 0.7496 0.2149 sec/batch\n",
      "Epoch 14/20  Iteration 24861/35720 Training loss: 0.7496 0.2260 sec/batch\n",
      "Epoch 14/20  Iteration 24862/35720 Training loss: 0.7496 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 24863/35720 Training loss: 0.7496 0.2104 sec/batch\n",
      "Epoch 14/20  Iteration 24864/35720 Training loss: 0.7496 0.2241 sec/batch\n",
      "Epoch 14/20  Iteration 24865/35720 Training loss: 0.7496 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24866/35720 Training loss: 0.7496 0.2196 sec/batch\n",
      "Epoch 14/20  Iteration 24867/35720 Training loss: 0.7496 0.2108 sec/batch\n",
      "Epoch 14/20  Iteration 24868/35720 Training loss: 0.7496 0.2057 sec/batch\n",
      "Epoch 14/20  Iteration 24869/35720 Training loss: 0.7496 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 24870/35720 Training loss: 0.7496 0.2268 sec/batch\n",
      "Epoch 14/20  Iteration 24871/35720 Training loss: 0.7496 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 24872/35720 Training loss: 0.7496 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 24873/35720 Training loss: 0.7496 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 24874/35720 Training loss: 0.7496 0.2148 sec/batch\n",
      "Epoch 14/20  Iteration 24875/35720 Training loss: 0.7497 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 24876/35720 Training loss: 0.7497 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 24877/35720 Training loss: 0.7496 0.2097 sec/batch\n",
      "Epoch 14/20  Iteration 24878/35720 Training loss: 0.7496 0.2227 sec/batch\n",
      "Epoch 14/20  Iteration 24879/35720 Training loss: 0.7496 0.2091 sec/batch\n",
      "Epoch 14/20  Iteration 24880/35720 Training loss: 0.7496 0.2294 sec/batch\n",
      "Epoch 14/20  Iteration 24881/35720 Training loss: 0.7496 0.2077 sec/batch\n",
      "Epoch 14/20  Iteration 24882/35720 Training loss: 0.7496 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 24883/35720 Training loss: 0.7496 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 24884/35720 Training loss: 0.7496 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24885/35720 Training loss: 0.7496 0.2219 sec/batch\n",
      "Epoch 14/20  Iteration 24886/35720 Training loss: 0.7497 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24887/35720 Training loss: 0.7497 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24888/35720 Training loss: 0.7497 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24889/35720 Training loss: 0.7497 0.2242 sec/batch\n",
      "Epoch 14/20  Iteration 24890/35720 Training loss: 0.7497 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24891/35720 Training loss: 0.7497 0.2163 sec/batch\n",
      "Epoch 14/20  Iteration 24892/35720 Training loss: 0.7497 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 24893/35720 Training loss: 0.7497 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24894/35720 Training loss: 0.7497 0.2246 sec/batch\n",
      "Epoch 14/20  Iteration 24895/35720 Training loss: 0.7497 0.2157 sec/batch\n",
      "Epoch 14/20  Iteration 24896/35720 Training loss: 0.7496 0.2131 sec/batch\n",
      "Epoch 14/20  Iteration 24897/35720 Training loss: 0.7496 0.2120 sec/batch\n",
      "Epoch 14/20  Iteration 24898/35720 Training loss: 0.7496 0.2054 sec/batch\n",
      "Epoch 14/20  Iteration 24899/35720 Training loss: 0.7496 0.2101 sec/batch\n",
      "Epoch 14/20  Iteration 24900/35720 Training loss: 0.7495 0.2168 sec/batch\n",
      "Epoch 14/20  Iteration 24901/35720 Training loss: 0.7495 0.2130 sec/batch\n",
      "Epoch 14/20  Iteration 24902/35720 Training loss: 0.7495 0.2392 sec/batch\n",
      "Epoch 14/20  Iteration 24903/35720 Training loss: 0.7495 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24904/35720 Training loss: 0.7495 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24905/35720 Training loss: 0.7495 0.2170 sec/batch\n",
      "Epoch 14/20  Iteration 24906/35720 Training loss: 0.7495 0.2208 sec/batch\n",
      "Epoch 14/20  Iteration 24907/35720 Training loss: 0.7495 0.2161 sec/batch\n",
      "Epoch 14/20  Iteration 24908/35720 Training loss: 0.7495 0.2100 sec/batch\n",
      "Epoch 14/20  Iteration 24909/35720 Training loss: 0.7494 0.2068 sec/batch\n",
      "Epoch 14/20  Iteration 24910/35720 Training loss: 0.7494 0.2146 sec/batch\n",
      "Epoch 14/20  Iteration 24911/35720 Training loss: 0.7494 0.2188 sec/batch\n",
      "Epoch 14/20  Iteration 24912/35720 Training loss: 0.7494 0.2111 sec/batch\n",
      "Epoch 14/20  Iteration 24913/35720 Training loss: 0.7494 0.2225 sec/batch\n",
      "Epoch 14/20  Iteration 24914/35720 Training loss: 0.7493 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 24915/35720 Training loss: 0.7493 0.2439 sec/batch\n",
      "Epoch 14/20  Iteration 24916/35720 Training loss: 0.7493 0.2090 sec/batch\n",
      "Epoch 14/20  Iteration 24917/35720 Training loss: 0.7493 0.2251 sec/batch\n",
      "Epoch 14/20  Iteration 24918/35720 Training loss: 0.7494 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24919/35720 Training loss: 0.7494 0.2204 sec/batch\n",
      "Epoch 14/20  Iteration 24920/35720 Training loss: 0.7493 0.2110 sec/batch\n",
      "Epoch 14/20  Iteration 24921/35720 Training loss: 0.7493 0.2128 sec/batch\n",
      "Epoch 14/20  Iteration 24922/35720 Training loss: 0.7493 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 24923/35720 Training loss: 0.7493 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24924/35720 Training loss: 0.7493 0.2166 sec/batch\n",
      "Epoch 14/20  Iteration 24925/35720 Training loss: 0.7492 0.2113 sec/batch\n",
      "Epoch 14/20  Iteration 24926/35720 Training loss: 0.7492 0.2059 sec/batch\n",
      "Epoch 14/20  Iteration 24927/35720 Training loss: 0.7492 0.2140 sec/batch\n",
      "Epoch 14/20  Iteration 24928/35720 Training loss: 0.7493 0.2238 sec/batch\n",
      "Epoch 14/20  Iteration 24929/35720 Training loss: 0.7493 0.2201 sec/batch\n",
      "Epoch 14/20  Iteration 24930/35720 Training loss: 0.7493 0.2164 sec/batch\n",
      "Epoch 14/20  Iteration 24931/35720 Training loss: 0.7494 0.2067 sec/batch\n",
      "Epoch 14/20  Iteration 24932/35720 Training loss: 0.7493 0.2084 sec/batch\n",
      "Epoch 14/20  Iteration 24933/35720 Training loss: 0.7493 0.2127 sec/batch\n",
      "Epoch 14/20  Iteration 24934/35720 Training loss: 0.7493 0.2174 sec/batch\n",
      "Epoch 14/20  Iteration 24935/35720 Training loss: 0.7493 0.2204 sec/batch\n",
      "Epoch 14/20  Iteration 24936/35720 Training loss: 0.7493 0.2123 sec/batch\n",
      "Epoch 14/20  Iteration 24937/35720 Training loss: 0.7493 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24938/35720 Training loss: 0.7493 0.2092 sec/batch\n",
      "Epoch 14/20  Iteration 24939/35720 Training loss: 0.7493 0.2192 sec/batch\n",
      "Epoch 14/20  Iteration 24940/35720 Training loss: 0.7492 0.2101 sec/batch\n",
      "Epoch 14/20  Iteration 24941/35720 Training loss: 0.7492 0.2257 sec/batch\n",
      "Epoch 14/20  Iteration 24942/35720 Training loss: 0.7492 0.2149 sec/batch\n",
      "Epoch 14/20  Iteration 24943/35720 Training loss: 0.7492 0.2167 sec/batch\n",
      "Epoch 14/20  Iteration 24944/35720 Training loss: 0.7492 0.2187 sec/batch\n",
      "Epoch 14/20  Iteration 24945/35720 Training loss: 0.7492 0.2206 sec/batch\n",
      "Epoch 14/20  Iteration 24946/35720 Training loss: 0.7492 0.2186 sec/batch\n",
      "Epoch 14/20  Iteration 24947/35720 Training loss: 0.7492 0.2144 sec/batch\n",
      "Epoch 14/20  Iteration 24948/35720 Training loss: 0.7492 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 24949/35720 Training loss: 0.7492 0.2136 sec/batch\n",
      "Epoch 14/20  Iteration 24950/35720 Training loss: 0.7492 0.2225 sec/batch\n",
      "Epoch 14/20  Iteration 24951/35720 Training loss: 0.7492 0.2193 sec/batch\n",
      "Epoch 14/20  Iteration 24952/35720 Training loss: 0.7492 0.2250 sec/batch\n",
      "Epoch 14/20  Iteration 24953/35720 Training loss: 0.7492 0.2147 sec/batch\n",
      "Epoch 14/20  Iteration 24954/35720 Training loss: 0.7492 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 24955/35720 Training loss: 0.7493 0.2116 sec/batch\n",
      "Epoch 14/20  Iteration 24956/35720 Training loss: 0.7492 0.2182 sec/batch\n",
      "Epoch 14/20  Iteration 24957/35720 Training loss: 0.7493 0.2088 sec/batch\n",
      "Epoch 14/20  Iteration 24958/35720 Training loss: 0.7492 0.2256 sec/batch\n",
      "Epoch 14/20  Iteration 24959/35720 Training loss: 0.7492 0.2064 sec/batch\n",
      "Epoch 14/20  Iteration 24960/35720 Training loss: 0.7492 0.2119 sec/batch\n",
      "Epoch 14/20  Iteration 24961/35720 Training loss: 0.7492 0.2309 sec/batch\n",
      "Epoch 14/20  Iteration 24962/35720 Training loss: 0.7492 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24963/35720 Training loss: 0.7493 0.2318 sec/batch\n",
      "Epoch 14/20  Iteration 24964/35720 Training loss: 0.7493 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 24965/35720 Training loss: 0.7493 0.2083 sec/batch\n",
      "Epoch 14/20  Iteration 24966/35720 Training loss: 0.7493 0.2102 sec/batch\n",
      "Epoch 14/20  Iteration 24967/35720 Training loss: 0.7493 0.2221 sec/batch\n",
      "Epoch 14/20  Iteration 24968/35720 Training loss: 0.7493 0.2143 sec/batch\n",
      "Epoch 14/20  Iteration 24969/35720 Training loss: 0.7493 0.2202 sec/batch\n",
      "Epoch 14/20  Iteration 24970/35720 Training loss: 0.7493 0.2184 sec/batch\n",
      "Epoch 14/20  Iteration 24971/35720 Training loss: 0.7493 0.2066 sec/batch\n",
      "Epoch 14/20  Iteration 24972/35720 Training loss: 0.7493 0.2202 sec/batch\n",
      "Epoch 14/20  Iteration 24973/35720 Training loss: 0.7493 0.2176 sec/batch\n",
      "Epoch 14/20  Iteration 24974/35720 Training loss: 0.7493 0.2198 sec/batch\n",
      "Epoch 14/20  Iteration 24975/35720 Training loss: 0.7493 0.2065 sec/batch\n",
      "Epoch 14/20  Iteration 24976/35720 Training loss: 0.7493 0.2145 sec/batch\n",
      "Epoch 14/20  Iteration 24977/35720 Training loss: 0.7493 0.2210 sec/batch\n",
      "Epoch 14/20  Iteration 24978/35720 Training loss: 0.7493 0.2165 sec/batch\n",
      "Epoch 14/20  Iteration 24979/35720 Training loss: 0.7493 0.2098 sec/batch\n",
      "Epoch 14/20  Iteration 24980/35720 Training loss: 0.7493 0.2142 sec/batch\n",
      "Epoch 14/20  Iteration 24981/35720 Training loss: 0.7493 0.2106 sec/batch\n",
      "Epoch 14/20  Iteration 24982/35720 Training loss: 0.7493 0.2179 sec/batch\n",
      "Epoch 14/20  Iteration 24983/35720 Training loss: 0.7493 0.2116 sec/batch\n",
      "Epoch 14/20  Iteration 24984/35720 Training loss: 0.7492 0.2227 sec/batch\n",
      "Epoch 14/20  Iteration 24985/35720 Training loss: 0.7493 0.2322 sec/batch\n",
      "Epoch 14/20  Iteration 24986/35720 Training loss: 0.7493 0.2330 sec/batch\n",
      "Epoch 14/20  Iteration 24987/35720 Training loss: 0.7493 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 24988/35720 Training loss: 0.7493 0.2121 sec/batch\n",
      "Epoch 14/20  Iteration 24989/35720 Training loss: 0.7493 0.2266 sec/batch\n",
      "Epoch 14/20  Iteration 24990/35720 Training loss: 0.7493 0.2093 sec/batch\n",
      "Epoch 14/20  Iteration 24991/35720 Training loss: 0.7492 0.2159 sec/batch\n",
      "Epoch 14/20  Iteration 24992/35720 Training loss: 0.7492 0.2101 sec/batch\n",
      "Epoch 14/20  Iteration 24993/35720 Training loss: 0.7492 0.2050 sec/batch\n",
      "Epoch 14/20  Iteration 24994/35720 Training loss: 0.7492 0.2095 sec/batch\n",
      "Epoch 14/20  Iteration 24995/35720 Training loss: 0.7492 0.2133 sec/batch\n",
      "Epoch 14/20  Iteration 24996/35720 Training loss: 0.7492 0.2129 sec/batch\n",
      "Epoch 14/20  Iteration 24997/35720 Training loss: 0.7492 0.2424 sec/batch\n",
      "Epoch 14/20  Iteration 24998/35720 Training loss: 0.7491 0.2062 sec/batch\n",
      "Epoch 14/20  Iteration 24999/35720 Training loss: 0.7491 0.2202 sec/batch\n",
      "Epoch 14/20  Iteration 25000/35720 Training loss: 0.7491 0.2075 sec/batch\n",
      "Validation loss: 1.52388 Saving checkpoint!\n",
      "Epoch 14/20  Iteration 25001/35720 Training loss: 0.7493 0.2105 sec/batch\n",
      "Epoch 14/20  Iteration 25002/35720 Training loss: 0.7492 0.2060 sec/batch\n",
      "Epoch 14/20  Iteration 25003/35720 Training loss: 0.7492 0.2063 sec/batch\n",
      "Epoch 14/20  Iteration 25004/35720 Training loss: 0.7492 0.2218 sec/batch\n",
      "Epoch 15/20  Iteration 25005/35720 Training loss: 0.7588 0.2328 sec/batch\n",
      "Epoch 15/20  Iteration 25006/35720 Training loss: 0.7628 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 25007/35720 Training loss: 0.7562 0.2277 sec/batch\n",
      "Epoch 15/20  Iteration 25008/35720 Training loss: 0.7582 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 25009/35720 Training loss: 0.7647 0.2302 sec/batch\n",
      "Epoch 15/20  Iteration 25010/35720 Training loss: 0.7547 0.2187 sec/batch\n",
      "Epoch 15/20  Iteration 25011/35720 Training loss: 0.7551 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25012/35720 Training loss: 0.7480 0.2443 sec/batch\n",
      "Epoch 15/20  Iteration 25013/35720 Training loss: 0.7431 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 25014/35720 Training loss: 0.7444 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25015/35720 Training loss: 0.7426 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 25016/35720 Training loss: 0.7375 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 25017/35720 Training loss: 0.7372 0.2185 sec/batch\n",
      "Epoch 15/20  Iteration 25018/35720 Training loss: 0.7410 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 25019/35720 Training loss: 0.7419 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25020/35720 Training loss: 0.7420 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25021/35720 Training loss: 0.7411 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 25022/35720 Training loss: 0.7381 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 25023/35720 Training loss: 0.7371 0.2157 sec/batch\n",
      "Epoch 15/20  Iteration 25024/35720 Training loss: 0.7376 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25025/35720 Training loss: 0.7387 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 25026/35720 Training loss: 0.7369 0.2138 sec/batch\n",
      "Epoch 15/20  Iteration 25027/35720 Training loss: 0.7355 0.2082 sec/batch\n",
      "Epoch 15/20  Iteration 25028/35720 Training loss: 0.7378 0.2229 sec/batch\n",
      "Epoch 15/20  Iteration 25029/35720 Training loss: 0.7385 0.2194 sec/batch\n",
      "Epoch 15/20  Iteration 25030/35720 Training loss: 0.7379 0.2109 sec/batch\n",
      "Epoch 15/20  Iteration 25031/35720 Training loss: 0.7402 0.2208 sec/batch\n",
      "Epoch 15/20  Iteration 25032/35720 Training loss: 0.7410 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25033/35720 Training loss: 0.7400 0.2288 sec/batch\n",
      "Epoch 15/20  Iteration 25034/35720 Training loss: 0.7411 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25035/35720 Training loss: 0.7429 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 25036/35720 Training loss: 0.7419 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 25037/35720 Training loss: 0.7442 0.2200 sec/batch\n",
      "Epoch 15/20  Iteration 25038/35720 Training loss: 0.7466 0.2230 sec/batch\n",
      "Epoch 15/20  Iteration 25039/35720 Training loss: 0.7485 0.2139 sec/batch\n",
      "Epoch 15/20  Iteration 25040/35720 Training loss: 0.7488 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 25041/35720 Training loss: 0.7491 0.2223 sec/batch\n",
      "Epoch 15/20  Iteration 25042/35720 Training loss: 0.7486 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 25043/35720 Training loss: 0.7474 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25044/35720 Training loss: 0.7484 0.2157 sec/batch\n",
      "Epoch 15/20  Iteration 25045/35720 Training loss: 0.7478 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 25046/35720 Training loss: 0.7467 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 25047/35720 Training loss: 0.7456 0.2076 sec/batch\n",
      "Epoch 15/20  Iteration 25048/35720 Training loss: 0.7451 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 25049/35720 Training loss: 0.7452 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 25050/35720 Training loss: 0.7450 0.2217 sec/batch\n",
      "Epoch 15/20  Iteration 25051/35720 Training loss: 0.7443 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25052/35720 Training loss: 0.7437 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 25053/35720 Training loss: 0.7430 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25054/35720 Training loss: 0.7425 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25055/35720 Training loss: 0.7430 0.2201 sec/batch\n",
      "Epoch 15/20  Iteration 25056/35720 Training loss: 0.7432 0.2209 sec/batch\n",
      "Epoch 15/20  Iteration 25057/35720 Training loss: 0.7436 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 25058/35720 Training loss: 0.7420 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25059/35720 Training loss: 0.7411 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25060/35720 Training loss: 0.7407 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 25061/35720 Training loss: 0.7402 0.2243 sec/batch\n",
      "Epoch 15/20  Iteration 25062/35720 Training loss: 0.7393 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25063/35720 Training loss: 0.7387 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 25064/35720 Training loss: 0.7379 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 25065/35720 Training loss: 0.7377 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25066/35720 Training loss: 0.7363 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 25067/35720 Training loss: 0.7371 0.2197 sec/batch\n",
      "Epoch 15/20  Iteration 25068/35720 Training loss: 0.7371 0.2197 sec/batch\n",
      "Epoch 15/20  Iteration 25069/35720 Training loss: 0.7379 0.2234 sec/batch\n",
      "Epoch 15/20  Iteration 25070/35720 Training loss: 0.7381 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25071/35720 Training loss: 0.7378 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 25072/35720 Training loss: 0.7371 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 25073/35720 Training loss: 0.7377 0.2234 sec/batch\n",
      "Epoch 15/20  Iteration 25074/35720 Training loss: 0.7377 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 25075/35720 Training loss: 0.7382 0.2242 sec/batch\n",
      "Epoch 15/20  Iteration 25076/35720 Training loss: 0.7388 0.2057 sec/batch\n",
      "Epoch 15/20  Iteration 25077/35720 Training loss: 0.7387 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25078/35720 Training loss: 0.7387 0.2135 sec/batch\n",
      "Epoch 15/20  Iteration 25079/35720 Training loss: 0.7380 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25080/35720 Training loss: 0.7378 0.2194 sec/batch\n",
      "Epoch 15/20  Iteration 25081/35720 Training loss: 0.7374 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 25082/35720 Training loss: 0.7382 0.2189 sec/batch\n",
      "Epoch 15/20  Iteration 25083/35720 Training loss: 0.7381 0.2193 sec/batch\n",
      "Epoch 15/20  Iteration 25084/35720 Training loss: 0.7389 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 25085/35720 Training loss: 0.7394 0.2109 sec/batch\n",
      "Epoch 15/20  Iteration 25086/35720 Training loss: 0.7394 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 25087/35720 Training loss: 0.7395 0.2119 sec/batch\n",
      "Epoch 15/20  Iteration 25088/35720 Training loss: 0.7396 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25089/35720 Training loss: 0.7394 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 25090/35720 Training loss: 0.7395 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 25091/35720 Training loss: 0.7395 0.2283 sec/batch\n",
      "Epoch 15/20  Iteration 25092/35720 Training loss: 0.7393 0.2146 sec/batch\n",
      "Epoch 15/20  Iteration 25093/35720 Training loss: 0.7384 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 25094/35720 Training loss: 0.7379 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25095/35720 Training loss: 0.7380 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25096/35720 Training loss: 0.7377 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 25097/35720 Training loss: 0.7381 0.2150 sec/batch\n",
      "Epoch 15/20  Iteration 25098/35720 Training loss: 0.7386 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 25099/35720 Training loss: 0.7385 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 25100/35720 Training loss: 0.7380 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25101/35720 Training loss: 0.7381 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25102/35720 Training loss: 0.7382 0.2178 sec/batch\n",
      "Epoch 15/20  Iteration 25103/35720 Training loss: 0.7380 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25104/35720 Training loss: 0.7378 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25105/35720 Training loss: 0.7373 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25106/35720 Training loss: 0.7374 0.2207 sec/batch\n",
      "Epoch 15/20  Iteration 25107/35720 Training loss: 0.7369 0.2242 sec/batch\n",
      "Epoch 15/20  Iteration 25108/35720 Training loss: 0.7365 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 25109/35720 Training loss: 0.7364 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25110/35720 Training loss: 0.7361 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 25111/35720 Training loss: 0.7362 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25112/35720 Training loss: 0.7364 0.2234 sec/batch\n",
      "Epoch 15/20  Iteration 25113/35720 Training loss: 0.7369 0.2298 sec/batch\n",
      "Epoch 15/20  Iteration 25114/35720 Training loss: 0.7366 0.2077 sec/batch\n",
      "Epoch 15/20  Iteration 25115/35720 Training loss: 0.7368 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 25116/35720 Training loss: 0.7369 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 25117/35720 Training loss: 0.7369 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 25118/35720 Training loss: 0.7370 0.2155 sec/batch\n",
      "Epoch 15/20  Iteration 25119/35720 Training loss: 0.7369 0.2162 sec/batch\n",
      "Epoch 15/20  Iteration 25120/35720 Training loss: 0.7369 0.2126 sec/batch\n",
      "Epoch 15/20  Iteration 25121/35720 Training loss: 0.7369 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25122/35720 Training loss: 0.7370 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25123/35720 Training loss: 0.7369 0.2171 sec/batch\n",
      "Epoch 15/20  Iteration 25124/35720 Training loss: 0.7376 0.2275 sec/batch\n",
      "Epoch 15/20  Iteration 25125/35720 Training loss: 0.7376 0.2226 sec/batch\n",
      "Epoch 15/20  Iteration 25126/35720 Training loss: 0.7370 0.2207 sec/batch\n",
      "Epoch 15/20  Iteration 25127/35720 Training loss: 0.7369 0.2134 sec/batch\n",
      "Epoch 15/20  Iteration 25128/35720 Training loss: 0.7376 0.2228 sec/batch\n",
      "Epoch 15/20  Iteration 25129/35720 Training loss: 0.7372 0.2138 sec/batch\n",
      "Epoch 15/20  Iteration 25130/35720 Training loss: 0.7374 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 25131/35720 Training loss: 0.7375 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25132/35720 Training loss: 0.7373 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25133/35720 Training loss: 0.7368 0.2198 sec/batch\n",
      "Epoch 15/20  Iteration 25134/35720 Training loss: 0.7370 0.2309 sec/batch\n",
      "Epoch 15/20  Iteration 25135/35720 Training loss: 0.7369 0.2246 sec/batch\n",
      "Epoch 15/20  Iteration 25136/35720 Training loss: 0.7364 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 25137/35720 Training loss: 0.7365 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 25138/35720 Training loss: 0.7366 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 25139/35720 Training loss: 0.7363 0.2221 sec/batch\n",
      "Epoch 15/20  Iteration 25140/35720 Training loss: 0.7361 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 25141/35720 Training loss: 0.7367 0.2140 sec/batch\n",
      "Epoch 15/20  Iteration 25142/35720 Training loss: 0.7369 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25143/35720 Training loss: 0.7370 0.2108 sec/batch\n",
      "Epoch 15/20  Iteration 25144/35720 Training loss: 0.7371 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 25145/35720 Training loss: 0.7368 0.2358 sec/batch\n",
      "Epoch 15/20  Iteration 25146/35720 Training loss: 0.7365 0.2216 sec/batch\n",
      "Epoch 15/20  Iteration 25147/35720 Training loss: 0.7362 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 25148/35720 Training loss: 0.7356 0.2183 sec/batch\n",
      "Epoch 15/20  Iteration 25149/35720 Training loss: 0.7357 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 25150/35720 Training loss: 0.7360 0.2148 sec/batch\n",
      "Epoch 15/20  Iteration 25151/35720 Training loss: 0.7357 0.2104 sec/batch\n",
      "Epoch 15/20  Iteration 25152/35720 Training loss: 0.7355 0.2186 sec/batch\n",
      "Epoch 15/20  Iteration 25153/35720 Training loss: 0.7357 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25154/35720 Training loss: 0.7351 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 25155/35720 Training loss: 0.7350 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25156/35720 Training loss: 0.7349 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 25157/35720 Training loss: 0.7349 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25158/35720 Training loss: 0.7353 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 25159/35720 Training loss: 0.7354 0.2135 sec/batch\n",
      "Epoch 15/20  Iteration 25160/35720 Training loss: 0.7356 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25161/35720 Training loss: 0.7358 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25162/35720 Training loss: 0.7361 0.2226 sec/batch\n",
      "Epoch 15/20  Iteration 25163/35720 Training loss: 0.7358 0.2188 sec/batch\n",
      "Epoch 15/20  Iteration 25164/35720 Training loss: 0.7357 0.2106 sec/batch\n",
      "Epoch 15/20  Iteration 25165/35720 Training loss: 0.7355 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 25166/35720 Training loss: 0.7356 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25167/35720 Training loss: 0.7358 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25168/35720 Training loss: 0.7357 0.2313 sec/batch\n",
      "Epoch 15/20  Iteration 25169/35720 Training loss: 0.7359 0.2437 sec/batch\n",
      "Epoch 15/20  Iteration 25170/35720 Training loss: 0.7361 0.2294 sec/batch\n",
      "Epoch 15/20  Iteration 25171/35720 Training loss: 0.7362 0.2060 sec/batch\n",
      "Epoch 15/20  Iteration 25172/35720 Training loss: 0.7366 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 25173/35720 Training loss: 0.7372 0.2187 sec/batch\n",
      "Epoch 15/20  Iteration 25174/35720 Training loss: 0.7374 0.2253 sec/batch\n",
      "Epoch 15/20  Iteration 25175/35720 Training loss: 0.7378 0.2233 sec/batch\n",
      "Epoch 15/20  Iteration 25176/35720 Training loss: 0.7382 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25177/35720 Training loss: 0.7384 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25178/35720 Training loss: 0.7388 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 25179/35720 Training loss: 0.7391 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 25180/35720 Training loss: 0.7391 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 25181/35720 Training loss: 0.7392 0.2081 sec/batch\n",
      "Epoch 15/20  Iteration 25182/35720 Training loss: 0.7391 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 25183/35720 Training loss: 0.7389 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25184/35720 Training loss: 0.7386 0.2200 sec/batch\n",
      "Epoch 15/20  Iteration 25185/35720 Training loss: 0.7389 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 25186/35720 Training loss: 0.7390 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 25187/35720 Training loss: 0.7391 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25188/35720 Training loss: 0.7391 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 25189/35720 Training loss: 0.7390 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 25190/35720 Training loss: 0.7389 0.2215 sec/batch\n",
      "Epoch 15/20  Iteration 25191/35720 Training loss: 0.7388 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 25192/35720 Training loss: 0.7389 0.2208 sec/batch\n",
      "Epoch 15/20  Iteration 25193/35720 Training loss: 0.7391 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25194/35720 Training loss: 0.7390 0.2099 sec/batch\n",
      "Epoch 15/20  Iteration 25195/35720 Training loss: 0.7392 0.2141 sec/batch\n",
      "Epoch 15/20  Iteration 25196/35720 Training loss: 0.7395 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 25197/35720 Training loss: 0.7397 0.2211 sec/batch\n",
      "Epoch 15/20  Iteration 25198/35720 Training loss: 0.7400 0.2185 sec/batch\n",
      "Epoch 15/20  Iteration 25199/35720 Training loss: 0.7401 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 25200/35720 Training loss: 0.7403 0.2081 sec/batch\n",
      "Validation loss: 1.5364 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 25201/35720 Training loss: 0.7416 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 25202/35720 Training loss: 0.7418 0.2201 sec/batch\n",
      "Epoch 15/20  Iteration 25203/35720 Training loss: 0.7419 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 25204/35720 Training loss: 0.7419 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 25205/35720 Training loss: 0.7418 0.2300 sec/batch\n",
      "Epoch 15/20  Iteration 25206/35720 Training loss: 0.7417 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 25207/35720 Training loss: 0.7420 0.2281 sec/batch\n",
      "Epoch 15/20  Iteration 25208/35720 Training loss: 0.7419 0.2163 sec/batch\n",
      "Epoch 15/20  Iteration 25209/35720 Training loss: 0.7419 0.2213 sec/batch\n",
      "Epoch 15/20  Iteration 25210/35720 Training loss: 0.7419 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25211/35720 Training loss: 0.7422 0.2239 sec/batch\n",
      "Epoch 15/20  Iteration 25212/35720 Training loss: 0.7425 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 25213/35720 Training loss: 0.7428 0.2158 sec/batch\n",
      "Epoch 15/20  Iteration 25214/35720 Training loss: 0.7429 0.2112 sec/batch\n",
      "Epoch 15/20  Iteration 25215/35720 Training loss: 0.7431 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 25216/35720 Training loss: 0.7431 0.2271 sec/batch\n",
      "Epoch 15/20  Iteration 25217/35720 Training loss: 0.7430 0.2087 sec/batch\n",
      "Epoch 15/20  Iteration 25218/35720 Training loss: 0.7428 0.2229 sec/batch\n",
      "Epoch 15/20  Iteration 25219/35720 Training loss: 0.7428 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 25220/35720 Training loss: 0.7429 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 25221/35720 Training loss: 0.7427 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 25222/35720 Training loss: 0.7426 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 25223/35720 Training loss: 0.7428 0.2280 sec/batch\n",
      "Epoch 15/20  Iteration 25224/35720 Training loss: 0.7427 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 25225/35720 Training loss: 0.7428 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 25226/35720 Training loss: 0.7428 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 25227/35720 Training loss: 0.7432 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25228/35720 Training loss: 0.7433 0.2230 sec/batch\n",
      "Epoch 15/20  Iteration 25229/35720 Training loss: 0.7433 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25230/35720 Training loss: 0.7433 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 25231/35720 Training loss: 0.7431 0.2183 sec/batch\n",
      "Epoch 15/20  Iteration 25232/35720 Training loss: 0.7430 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 25233/35720 Training loss: 0.7427 0.2139 sec/batch\n",
      "Epoch 15/20  Iteration 25234/35720 Training loss: 0.7427 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 25235/35720 Training loss: 0.7431 0.2215 sec/batch\n",
      "Epoch 15/20  Iteration 25236/35720 Training loss: 0.7430 0.2060 sec/batch\n",
      "Epoch 15/20  Iteration 25237/35720 Training loss: 0.7431 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25238/35720 Training loss: 0.7430 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25239/35720 Training loss: 0.7431 0.2220 sec/batch\n",
      "Epoch 15/20  Iteration 25240/35720 Training loss: 0.7431 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 25241/35720 Training loss: 0.7433 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 25242/35720 Training loss: 0.7432 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 25243/35720 Training loss: 0.7431 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25244/35720 Training loss: 0.7432 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 25245/35720 Training loss: 0.7430 0.2225 sec/batch\n",
      "Epoch 15/20  Iteration 25246/35720 Training loss: 0.7429 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25247/35720 Training loss: 0.7428 0.2192 sec/batch\n",
      "Epoch 15/20  Iteration 25248/35720 Training loss: 0.7427 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 25249/35720 Training loss: 0.7424 0.2133 sec/batch\n",
      "Epoch 15/20  Iteration 25250/35720 Training loss: 0.7425 0.2082 sec/batch\n",
      "Epoch 15/20  Iteration 25251/35720 Training loss: 0.7425 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 25252/35720 Training loss: 0.7423 0.2316 sec/batch\n",
      "Epoch 15/20  Iteration 25253/35720 Training loss: 0.7422 0.2057 sec/batch\n",
      "Epoch 15/20  Iteration 25254/35720 Training loss: 0.7421 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 25255/35720 Training loss: 0.7421 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 25256/35720 Training loss: 0.7420 0.2206 sec/batch\n",
      "Epoch 15/20  Iteration 25257/35720 Training loss: 0.7418 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 25258/35720 Training loss: 0.7419 0.2227 sec/batch\n",
      "Epoch 15/20  Iteration 25259/35720 Training loss: 0.7421 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25260/35720 Training loss: 0.7421 0.2131 sec/batch\n",
      "Epoch 15/20  Iteration 25261/35720 Training loss: 0.7421 0.2191 sec/batch\n",
      "Epoch 15/20  Iteration 25262/35720 Training loss: 0.7421 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 25263/35720 Training loss: 0.7423 0.2398 sec/batch\n",
      "Epoch 15/20  Iteration 25264/35720 Training loss: 0.7424 0.2380 sec/batch\n",
      "Epoch 15/20  Iteration 25265/35720 Training loss: 0.7423 0.2236 sec/batch\n",
      "Epoch 15/20  Iteration 25266/35720 Training loss: 0.7422 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 25267/35720 Training loss: 0.7423 0.2240 sec/batch\n",
      "Epoch 15/20  Iteration 25268/35720 Training loss: 0.7423 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 25269/35720 Training loss: 0.7423 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 25270/35720 Training loss: 0.7425 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25271/35720 Training loss: 0.7425 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 25272/35720 Training loss: 0.7426 0.3094 sec/batch\n",
      "Epoch 15/20  Iteration 25273/35720 Training loss: 0.7425 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25274/35720 Training loss: 0.7422 0.2283 sec/batch\n",
      "Epoch 15/20  Iteration 25275/35720 Training loss: 0.7419 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 25276/35720 Training loss: 0.7419 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 25277/35720 Training loss: 0.7419 0.2270 sec/batch\n",
      "Epoch 15/20  Iteration 25278/35720 Training loss: 0.7420 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 25279/35720 Training loss: 0.7418 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 25280/35720 Training loss: 0.7418 0.2151 sec/batch\n",
      "Epoch 15/20  Iteration 25281/35720 Training loss: 0.7415 0.2502 sec/batch\n",
      "Epoch 15/20  Iteration 25282/35720 Training loss: 0.7414 0.2131 sec/batch\n",
      "Epoch 15/20  Iteration 25283/35720 Training loss: 0.7411 0.2337 sec/batch\n",
      "Epoch 15/20  Iteration 25284/35720 Training loss: 0.7411 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25285/35720 Training loss: 0.7411 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 25286/35720 Training loss: 0.7408 0.2104 sec/batch\n",
      "Epoch 15/20  Iteration 25287/35720 Training loss: 0.7406 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25288/35720 Training loss: 0.7404 0.2121 sec/batch\n",
      "Epoch 15/20  Iteration 25289/35720 Training loss: 0.7406 0.2325 sec/batch\n",
      "Epoch 15/20  Iteration 25290/35720 Training loss: 0.7405 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 25291/35720 Training loss: 0.7403 0.2240 sec/batch\n",
      "Epoch 15/20  Iteration 25292/35720 Training loss: 0.7404 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25293/35720 Training loss: 0.7405 0.2157 sec/batch\n",
      "Epoch 15/20  Iteration 25294/35720 Training loss: 0.7406 0.2342 sec/batch\n",
      "Epoch 15/20  Iteration 25295/35720 Training loss: 0.7405 0.2106 sec/batch\n",
      "Epoch 15/20  Iteration 25296/35720 Training loss: 0.7406 0.2241 sec/batch\n",
      "Epoch 15/20  Iteration 25297/35720 Training loss: 0.7405 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 25298/35720 Training loss: 0.7404 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25299/35720 Training loss: 0.7406 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 25300/35720 Training loss: 0.7406 0.2278 sec/batch\n",
      "Epoch 15/20  Iteration 25301/35720 Training loss: 0.7406 0.2215 sec/batch\n",
      "Epoch 15/20  Iteration 25302/35720 Training loss: 0.7407 0.2055 sec/batch\n",
      "Epoch 15/20  Iteration 25303/35720 Training loss: 0.7405 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 25304/35720 Training loss: 0.7406 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 25305/35720 Training loss: 0.7406 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 25306/35720 Training loss: 0.7404 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 25307/35720 Training loss: 0.7406 0.2243 sec/batch\n",
      "Epoch 15/20  Iteration 25308/35720 Training loss: 0.7407 0.2396 sec/batch\n",
      "Epoch 15/20  Iteration 25309/35720 Training loss: 0.7406 0.2058 sec/batch\n",
      "Epoch 15/20  Iteration 25310/35720 Training loss: 0.7405 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 25311/35720 Training loss: 0.7404 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25312/35720 Training loss: 0.7406 0.2226 sec/batch\n",
      "Epoch 15/20  Iteration 25313/35720 Training loss: 0.7404 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25314/35720 Training loss: 0.7404 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 25315/35720 Training loss: 0.7403 0.2242 sec/batch\n",
      "Epoch 15/20  Iteration 25316/35720 Training loss: 0.7403 0.2263 sec/batch\n",
      "Epoch 15/20  Iteration 25317/35720 Training loss: 0.7404 0.2133 sec/batch\n",
      "Epoch 15/20  Iteration 25318/35720 Training loss: 0.7403 0.2272 sec/batch\n",
      "Epoch 15/20  Iteration 25319/35720 Training loss: 0.7403 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25320/35720 Training loss: 0.7402 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 25321/35720 Training loss: 0.7402 0.2143 sec/batch\n",
      "Epoch 15/20  Iteration 25322/35720 Training loss: 0.7403 0.2183 sec/batch\n",
      "Epoch 15/20  Iteration 25323/35720 Training loss: 0.7405 0.2162 sec/batch\n",
      "Epoch 15/20  Iteration 25324/35720 Training loss: 0.7404 0.2108 sec/batch\n",
      "Epoch 15/20  Iteration 25325/35720 Training loss: 0.7405 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 25326/35720 Training loss: 0.7404 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25327/35720 Training loss: 0.7405 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 25328/35720 Training loss: 0.7405 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25329/35720 Training loss: 0.7404 0.2219 sec/batch\n",
      "Epoch 15/20  Iteration 25330/35720 Training loss: 0.7406 0.2177 sec/batch\n",
      "Epoch 15/20  Iteration 25331/35720 Training loss: 0.7406 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25332/35720 Training loss: 0.7405 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 25333/35720 Training loss: 0.7404 0.2226 sec/batch\n",
      "Epoch 15/20  Iteration 25334/35720 Training loss: 0.7404 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 25335/35720 Training loss: 0.7405 0.2262 sec/batch\n",
      "Epoch 15/20  Iteration 25336/35720 Training loss: 0.7406 0.2221 sec/batch\n",
      "Epoch 15/20  Iteration 25337/35720 Training loss: 0.7405 0.2081 sec/batch\n",
      "Epoch 15/20  Iteration 25338/35720 Training loss: 0.7404 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 25339/35720 Training loss: 0.7404 0.2198 sec/batch\n",
      "Epoch 15/20  Iteration 25340/35720 Training loss: 0.7402 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 25341/35720 Training loss: 0.7402 0.2186 sec/batch\n",
      "Epoch 15/20  Iteration 25342/35720 Training loss: 0.7401 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25343/35720 Training loss: 0.7401 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 25344/35720 Training loss: 0.7401 0.2304 sec/batch\n",
      "Epoch 15/20  Iteration 25345/35720 Training loss: 0.7400 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25346/35720 Training loss: 0.7399 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 25347/35720 Training loss: 0.7399 0.2221 sec/batch\n",
      "Epoch 15/20  Iteration 25348/35720 Training loss: 0.7400 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 25349/35720 Training loss: 0.7398 0.2087 sec/batch\n",
      "Epoch 15/20  Iteration 25350/35720 Training loss: 0.7399 0.2138 sec/batch\n",
      "Epoch 15/20  Iteration 25351/35720 Training loss: 0.7400 0.2386 sec/batch\n",
      "Epoch 15/20  Iteration 25352/35720 Training loss: 0.7400 0.2055 sec/batch\n",
      "Epoch 15/20  Iteration 25353/35720 Training loss: 0.7400 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 25354/35720 Training loss: 0.7400 0.2133 sec/batch\n",
      "Epoch 15/20  Iteration 25355/35720 Training loss: 0.7400 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 25356/35720 Training loss: 0.7398 0.2325 sec/batch\n",
      "Epoch 15/20  Iteration 25357/35720 Training loss: 0.7399 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 25358/35720 Training loss: 0.7399 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 25359/35720 Training loss: 0.7400 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 25360/35720 Training loss: 0.7400 0.2140 sec/batch\n",
      "Epoch 15/20  Iteration 25361/35720 Training loss: 0.7401 0.2184 sec/batch\n",
      "Epoch 15/20  Iteration 25362/35720 Training loss: 0.7401 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 25363/35720 Training loss: 0.7401 0.2145 sec/batch\n",
      "Epoch 15/20  Iteration 25364/35720 Training loss: 0.7401 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 25365/35720 Training loss: 0.7401 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25366/35720 Training loss: 0.7401 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 25367/35720 Training loss: 0.7401 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25368/35720 Training loss: 0.7401 0.2317 sec/batch\n",
      "Epoch 15/20  Iteration 25369/35720 Training loss: 0.7400 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 25370/35720 Training loss: 0.7401 0.2219 sec/batch\n",
      "Epoch 15/20  Iteration 25371/35720 Training loss: 0.7401 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25372/35720 Training loss: 0.7401 0.2165 sec/batch\n",
      "Epoch 15/20  Iteration 25373/35720 Training loss: 0.7400 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25374/35720 Training loss: 0.7399 0.2154 sec/batch\n",
      "Epoch 15/20  Iteration 25375/35720 Training loss: 0.7399 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25376/35720 Training loss: 0.7398 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25377/35720 Training loss: 0.7398 0.2221 sec/batch\n",
      "Epoch 15/20  Iteration 25378/35720 Training loss: 0.7398 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25379/35720 Training loss: 0.7397 0.2079 sec/batch\n",
      "Epoch 15/20  Iteration 25380/35720 Training loss: 0.7398 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 25381/35720 Training loss: 0.7399 0.2254 sec/batch\n",
      "Epoch 15/20  Iteration 25382/35720 Training loss: 0.7399 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25383/35720 Training loss: 0.7397 0.2158 sec/batch\n",
      "Epoch 15/20  Iteration 25384/35720 Training loss: 0.7396 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25385/35720 Training loss: 0.7396 0.2159 sec/batch\n",
      "Epoch 15/20  Iteration 25386/35720 Training loss: 0.7395 0.2108 sec/batch\n",
      "Epoch 15/20  Iteration 25387/35720 Training loss: 0.7396 0.2099 sec/batch\n",
      "Epoch 15/20  Iteration 25388/35720 Training loss: 0.7397 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25389/35720 Training loss: 0.7397 0.2053 sec/batch\n",
      "Epoch 15/20  Iteration 25390/35720 Training loss: 0.7396 0.2251 sec/batch\n",
      "Epoch 15/20  Iteration 25391/35720 Training loss: 0.7396 0.2283 sec/batch\n",
      "Epoch 15/20  Iteration 25392/35720 Training loss: 0.7398 0.2456 sec/batch\n",
      "Epoch 15/20  Iteration 25393/35720 Training loss: 0.7397 0.2200 sec/batch\n",
      "Epoch 15/20  Iteration 25394/35720 Training loss: 0.7397 0.2286 sec/batch\n",
      "Epoch 15/20  Iteration 25395/35720 Training loss: 0.7397 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25396/35720 Training loss: 0.7396 0.2183 sec/batch\n",
      "Epoch 15/20  Iteration 25397/35720 Training loss: 0.7397 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 25398/35720 Training loss: 0.7395 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 25399/35720 Training loss: 0.7396 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 25400/35720 Training loss: 0.7396 0.2165 sec/batch\n",
      "Validation loss: 1.54339 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 25401/35720 Training loss: 0.7406 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 25402/35720 Training loss: 0.7405 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25403/35720 Training loss: 0.7404 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 25404/35720 Training loss: 0.7403 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 25405/35720 Training loss: 0.7404 0.2281 sec/batch\n",
      "Epoch 15/20  Iteration 25406/35720 Training loss: 0.7405 0.2315 sec/batch\n",
      "Epoch 15/20  Iteration 25407/35720 Training loss: 0.7406 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25408/35720 Training loss: 0.7405 0.2104 sec/batch\n",
      "Epoch 15/20  Iteration 25409/35720 Training loss: 0.7404 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25410/35720 Training loss: 0.7404 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 25411/35720 Training loss: 0.7403 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 25412/35720 Training loss: 0.7404 0.2189 sec/batch\n",
      "Epoch 15/20  Iteration 25413/35720 Training loss: 0.7403 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25414/35720 Training loss: 0.7401 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25415/35720 Training loss: 0.7402 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25416/35720 Training loss: 0.7401 0.2212 sec/batch\n",
      "Epoch 15/20  Iteration 25417/35720 Training loss: 0.7400 0.2249 sec/batch\n",
      "Epoch 15/20  Iteration 25418/35720 Training loss: 0.7401 0.2222 sec/batch\n",
      "Epoch 15/20  Iteration 25419/35720 Training loss: 0.7400 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 25420/35720 Training loss: 0.7400 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 25421/35720 Training loss: 0.7399 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 25422/35720 Training loss: 0.7398 0.2144 sec/batch\n",
      "Epoch 15/20  Iteration 25423/35720 Training loss: 0.7397 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 25424/35720 Training loss: 0.7397 0.2074 sec/batch\n",
      "Epoch 15/20  Iteration 25425/35720 Training loss: 0.7397 0.2184 sec/batch\n",
      "Epoch 15/20  Iteration 25426/35720 Training loss: 0.7397 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 25427/35720 Training loss: 0.7398 0.2332 sec/batch\n",
      "Epoch 15/20  Iteration 25428/35720 Training loss: 0.7398 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25429/35720 Training loss: 0.7398 0.2231 sec/batch\n",
      "Epoch 15/20  Iteration 25430/35720 Training loss: 0.7397 0.2112 sec/batch\n",
      "Epoch 15/20  Iteration 25431/35720 Training loss: 0.7396 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25432/35720 Training loss: 0.7396 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25433/35720 Training loss: 0.7396 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 25434/35720 Training loss: 0.7397 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 25435/35720 Training loss: 0.7398 0.2148 sec/batch\n",
      "Epoch 15/20  Iteration 25436/35720 Training loss: 0.7398 0.2276 sec/batch\n",
      "Epoch 15/20  Iteration 25437/35720 Training loss: 0.7399 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 25438/35720 Training loss: 0.7400 0.2393 sec/batch\n",
      "Epoch 15/20  Iteration 25439/35720 Training loss: 0.7400 0.2211 sec/batch\n",
      "Epoch 15/20  Iteration 25440/35720 Training loss: 0.7400 0.2195 sec/batch\n",
      "Epoch 15/20  Iteration 25441/35720 Training loss: 0.7402 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 25442/35720 Training loss: 0.7403 0.2235 sec/batch\n",
      "Epoch 15/20  Iteration 25443/35720 Training loss: 0.7403 0.2329 sec/batch\n",
      "Epoch 15/20  Iteration 25444/35720 Training loss: 0.7403 0.2213 sec/batch\n",
      "Epoch 15/20  Iteration 25445/35720 Training loss: 0.7404 0.2146 sec/batch\n",
      "Epoch 15/20  Iteration 25446/35720 Training loss: 0.7405 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25447/35720 Training loss: 0.7405 0.2144 sec/batch\n",
      "Epoch 15/20  Iteration 25448/35720 Training loss: 0.7404 0.2251 sec/batch\n",
      "Epoch 15/20  Iteration 25449/35720 Training loss: 0.7404 0.2291 sec/batch\n",
      "Epoch 15/20  Iteration 25450/35720 Training loss: 0.7406 0.2108 sec/batch\n",
      "Epoch 15/20  Iteration 25451/35720 Training loss: 0.7408 0.2250 sec/batch\n",
      "Epoch 15/20  Iteration 25452/35720 Training loss: 0.7410 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25453/35720 Training loss: 0.7410 0.2081 sec/batch\n",
      "Epoch 15/20  Iteration 25454/35720 Training loss: 0.7410 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 25455/35720 Training loss: 0.7409 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25456/35720 Training loss: 0.7408 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25457/35720 Training loss: 0.7409 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 25458/35720 Training loss: 0.7410 0.2109 sec/batch\n",
      "Epoch 15/20  Iteration 25459/35720 Training loss: 0.7412 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25460/35720 Training loss: 0.7414 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 25461/35720 Training loss: 0.7416 0.2099 sec/batch\n",
      "Epoch 15/20  Iteration 25462/35720 Training loss: 0.7416 0.2261 sec/batch\n",
      "Epoch 15/20  Iteration 25463/35720 Training loss: 0.7414 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25464/35720 Training loss: 0.7414 0.2108 sec/batch\n",
      "Epoch 15/20  Iteration 25465/35720 Training loss: 0.7414 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 25466/35720 Training loss: 0.7416 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 25467/35720 Training loss: 0.7417 0.2183 sec/batch\n",
      "Epoch 15/20  Iteration 25468/35720 Training loss: 0.7416 0.2146 sec/batch\n",
      "Epoch 15/20  Iteration 25469/35720 Training loss: 0.7417 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25470/35720 Training loss: 0.7417 0.2133 sec/batch\n",
      "Epoch 15/20  Iteration 25471/35720 Training loss: 0.7417 0.2194 sec/batch\n",
      "Epoch 15/20  Iteration 25472/35720 Training loss: 0.7417 0.2128 sec/batch\n",
      "Epoch 15/20  Iteration 25473/35720 Training loss: 0.7417 0.2288 sec/batch\n",
      "Epoch 15/20  Iteration 25474/35720 Training loss: 0.7417 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 25475/35720 Training loss: 0.7417 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 25476/35720 Training loss: 0.7416 0.2145 sec/batch\n",
      "Epoch 15/20  Iteration 25477/35720 Training loss: 0.7417 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 25478/35720 Training loss: 0.7416 0.2236 sec/batch\n",
      "Epoch 15/20  Iteration 25479/35720 Training loss: 0.7417 0.2071 sec/batch\n",
      "Epoch 15/20  Iteration 25480/35720 Training loss: 0.7417 0.2214 sec/batch\n",
      "Epoch 15/20  Iteration 25481/35720 Training loss: 0.7417 0.2164 sec/batch\n",
      "Epoch 15/20  Iteration 25482/35720 Training loss: 0.7417 0.2187 sec/batch\n",
      "Epoch 15/20  Iteration 25483/35720 Training loss: 0.7417 0.2323 sec/batch\n",
      "Epoch 15/20  Iteration 25484/35720 Training loss: 0.7417 0.2268 sec/batch\n",
      "Epoch 15/20  Iteration 25485/35720 Training loss: 0.7416 0.2236 sec/batch\n",
      "Epoch 15/20  Iteration 25486/35720 Training loss: 0.7415 0.2075 sec/batch\n",
      "Epoch 15/20  Iteration 25487/35720 Training loss: 0.7416 0.2138 sec/batch\n",
      "Epoch 15/20  Iteration 25488/35720 Training loss: 0.7415 0.2314 sec/batch\n",
      "Epoch 15/20  Iteration 25489/35720 Training loss: 0.7416 0.2170 sec/batch\n",
      "Epoch 15/20  Iteration 25490/35720 Training loss: 0.7415 0.2151 sec/batch\n",
      "Epoch 15/20  Iteration 25491/35720 Training loss: 0.7414 0.2111 sec/batch\n",
      "Epoch 15/20  Iteration 25492/35720 Training loss: 0.7414 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 25493/35720 Training loss: 0.7413 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25494/35720 Training loss: 0.7412 0.2258 sec/batch\n",
      "Epoch 15/20  Iteration 25495/35720 Training loss: 0.7412 0.2188 sec/batch\n",
      "Epoch 15/20  Iteration 25496/35720 Training loss: 0.7413 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 25497/35720 Training loss: 0.7413 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25498/35720 Training loss: 0.7412 0.2081 sec/batch\n",
      "Epoch 15/20  Iteration 25499/35720 Training loss: 0.7412 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25500/35720 Training loss: 0.7410 0.2121 sec/batch\n",
      "Epoch 15/20  Iteration 25501/35720 Training loss: 0.7411 0.2131 sec/batch\n",
      "Epoch 15/20  Iteration 25502/35720 Training loss: 0.7412 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25503/35720 Training loss: 0.7411 0.2140 sec/batch\n",
      "Epoch 15/20  Iteration 25504/35720 Training loss: 0.7411 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 25505/35720 Training loss: 0.7410 0.2162 sec/batch\n",
      "Epoch 15/20  Iteration 25506/35720 Training loss: 0.7410 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 25507/35720 Training loss: 0.7408 0.2215 sec/batch\n",
      "Epoch 15/20  Iteration 25508/35720 Training loss: 0.7407 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 25509/35720 Training loss: 0.7407 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25510/35720 Training loss: 0.7407 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 25511/35720 Training loss: 0.7407 0.2155 sec/batch\n",
      "Epoch 15/20  Iteration 25512/35720 Training loss: 0.7406 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 25513/35720 Training loss: 0.7408 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 25514/35720 Training loss: 0.7407 0.2140 sec/batch\n",
      "Epoch 15/20  Iteration 25515/35720 Training loss: 0.7406 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 25516/35720 Training loss: 0.7406 0.2191 sec/batch\n",
      "Epoch 15/20  Iteration 25517/35720 Training loss: 0.7406 0.2111 sec/batch\n",
      "Epoch 15/20  Iteration 25518/35720 Training loss: 0.7406 0.2165 sec/batch\n",
      "Epoch 15/20  Iteration 25519/35720 Training loss: 0.7407 0.2054 sec/batch\n",
      "Epoch 15/20  Iteration 25520/35720 Training loss: 0.7407 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 25521/35720 Training loss: 0.7407 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 25522/35720 Training loss: 0.7407 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 25523/35720 Training loss: 0.7407 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 25524/35720 Training loss: 0.7408 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 25525/35720 Training loss: 0.7407 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25526/35720 Training loss: 0.7407 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 25527/35720 Training loss: 0.7406 0.2192 sec/batch\n",
      "Epoch 15/20  Iteration 25528/35720 Training loss: 0.7405 0.2225 sec/batch\n",
      "Epoch 15/20  Iteration 25529/35720 Training loss: 0.7405 0.2408 sec/batch\n",
      "Epoch 15/20  Iteration 25530/35720 Training loss: 0.7405 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25531/35720 Training loss: 0.7405 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 25532/35720 Training loss: 0.7405 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25533/35720 Training loss: 0.7406 0.2246 sec/batch\n",
      "Epoch 15/20  Iteration 25534/35720 Training loss: 0.7405 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25535/35720 Training loss: 0.7406 0.2209 sec/batch\n",
      "Epoch 15/20  Iteration 25536/35720 Training loss: 0.7406 0.2283 sec/batch\n",
      "Epoch 15/20  Iteration 25537/35720 Training loss: 0.7405 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 25538/35720 Training loss: 0.7406 0.2119 sec/batch\n",
      "Epoch 15/20  Iteration 25539/35720 Training loss: 0.7405 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 25540/35720 Training loss: 0.7405 0.2279 sec/batch\n",
      "Epoch 15/20  Iteration 25541/35720 Training loss: 0.7404 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25542/35720 Training loss: 0.7403 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25543/35720 Training loss: 0.7404 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 25544/35720 Training loss: 0.7403 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 25545/35720 Training loss: 0.7403 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25546/35720 Training loss: 0.7401 0.2194 sec/batch\n",
      "Epoch 15/20  Iteration 25547/35720 Training loss: 0.7401 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25548/35720 Training loss: 0.7400 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 25549/35720 Training loss: 0.7401 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 25550/35720 Training loss: 0.7402 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 25551/35720 Training loss: 0.7402 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25552/35720 Training loss: 0.7403 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 25553/35720 Training loss: 0.7402 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 25554/35720 Training loss: 0.7403 0.2134 sec/batch\n",
      "Epoch 15/20  Iteration 25555/35720 Training loss: 0.7402 0.2187 sec/batch\n",
      "Epoch 15/20  Iteration 25556/35720 Training loss: 0.7402 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 25557/35720 Training loss: 0.7402 0.2170 sec/batch\n",
      "Epoch 15/20  Iteration 25558/35720 Training loss: 0.7402 0.2109 sec/batch\n",
      "Epoch 15/20  Iteration 25559/35720 Training loss: 0.7402 0.2074 sec/batch\n",
      "Epoch 15/20  Iteration 25560/35720 Training loss: 0.7403 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 25561/35720 Training loss: 0.7402 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25562/35720 Training loss: 0.7403 0.2217 sec/batch\n",
      "Epoch 15/20  Iteration 25563/35720 Training loss: 0.7403 0.2248 sec/batch\n",
      "Epoch 15/20  Iteration 25564/35720 Training loss: 0.7403 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 25565/35720 Training loss: 0.7404 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 25566/35720 Training loss: 0.7403 0.2158 sec/batch\n",
      "Epoch 15/20  Iteration 25567/35720 Training loss: 0.7403 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25568/35720 Training loss: 0.7403 0.2221 sec/batch\n",
      "Epoch 15/20  Iteration 25569/35720 Training loss: 0.7402 0.2354 sec/batch\n",
      "Epoch 15/20  Iteration 25570/35720 Training loss: 0.7402 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 25571/35720 Training loss: 0.7402 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 25572/35720 Training loss: 0.7401 0.2144 sec/batch\n",
      "Epoch 15/20  Iteration 25573/35720 Training loss: 0.7401 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 25574/35720 Training loss: 0.7400 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 25575/35720 Training loss: 0.7400 0.2104 sec/batch\n",
      "Epoch 15/20  Iteration 25576/35720 Training loss: 0.7401 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 25577/35720 Training loss: 0.7402 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 25578/35720 Training loss: 0.7402 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 25579/35720 Training loss: 0.7402 0.2232 sec/batch\n",
      "Epoch 15/20  Iteration 25580/35720 Training loss: 0.7403 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25581/35720 Training loss: 0.7403 0.2054 sec/batch\n",
      "Epoch 15/20  Iteration 25582/35720 Training loss: 0.7403 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 25583/35720 Training loss: 0.7403 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 25584/35720 Training loss: 0.7403 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 25585/35720 Training loss: 0.7404 0.2291 sec/batch\n",
      "Epoch 15/20  Iteration 25586/35720 Training loss: 0.7404 0.2221 sec/batch\n",
      "Epoch 15/20  Iteration 25587/35720 Training loss: 0.7404 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 25588/35720 Training loss: 0.7403 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 25589/35720 Training loss: 0.7402 0.2256 sec/batch\n",
      "Epoch 15/20  Iteration 25590/35720 Training loss: 0.7402 0.2244 sec/batch\n",
      "Epoch 15/20  Iteration 25591/35720 Training loss: 0.7401 0.2197 sec/batch\n",
      "Epoch 15/20  Iteration 25592/35720 Training loss: 0.7401 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 25593/35720 Training loss: 0.7400 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 25594/35720 Training loss: 0.7399 0.2291 sec/batch\n",
      "Epoch 15/20  Iteration 25595/35720 Training loss: 0.7399 0.2255 sec/batch\n",
      "Epoch 15/20  Iteration 25596/35720 Training loss: 0.7399 0.2273 sec/batch\n",
      "Epoch 15/20  Iteration 25597/35720 Training loss: 0.7399 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 25598/35720 Training loss: 0.7399 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 25599/35720 Training loss: 0.7398 0.2228 sec/batch\n",
      "Epoch 15/20  Iteration 25600/35720 Training loss: 0.7398 0.2209 sec/batch\n",
      "Validation loss: 1.53793 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 25601/35720 Training loss: 0.7402 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25602/35720 Training loss: 0.7403 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 25603/35720 Training loss: 0.7402 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 25604/35720 Training loss: 0.7400 0.2288 sec/batch\n",
      "Epoch 15/20  Iteration 25605/35720 Training loss: 0.7400 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25606/35720 Training loss: 0.7399 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25607/35720 Training loss: 0.7398 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 25608/35720 Training loss: 0.7398 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25609/35720 Training loss: 0.7398 0.2302 sec/batch\n",
      "Epoch 15/20  Iteration 25610/35720 Training loss: 0.7399 0.2106 sec/batch\n",
      "Epoch 15/20  Iteration 25611/35720 Training loss: 0.7398 0.2246 sec/batch\n",
      "Epoch 15/20  Iteration 25612/35720 Training loss: 0.7398 0.2233 sec/batch\n",
      "Epoch 15/20  Iteration 25613/35720 Training loss: 0.7399 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 25614/35720 Training loss: 0.7398 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25615/35720 Training loss: 0.7398 0.2141 sec/batch\n",
      "Epoch 15/20  Iteration 25616/35720 Training loss: 0.7396 0.2119 sec/batch\n",
      "Epoch 15/20  Iteration 25617/35720 Training loss: 0.7396 0.2259 sec/batch\n",
      "Epoch 15/20  Iteration 25618/35720 Training loss: 0.7396 0.2099 sec/batch\n",
      "Epoch 15/20  Iteration 25619/35720 Training loss: 0.7396 0.2058 sec/batch\n",
      "Epoch 15/20  Iteration 25620/35720 Training loss: 0.7395 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25621/35720 Training loss: 0.7395 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 25622/35720 Training loss: 0.7394 0.2328 sec/batch\n",
      "Epoch 15/20  Iteration 25623/35720 Training loss: 0.7394 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 25624/35720 Training loss: 0.7393 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 25625/35720 Training loss: 0.7393 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 25626/35720 Training loss: 0.7394 0.2159 sec/batch\n",
      "Epoch 15/20  Iteration 25627/35720 Training loss: 0.7393 0.2226 sec/batch\n",
      "Epoch 15/20  Iteration 25628/35720 Training loss: 0.7393 0.2323 sec/batch\n",
      "Epoch 15/20  Iteration 25629/35720 Training loss: 0.7392 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 25630/35720 Training loss: 0.7392 0.2112 sec/batch\n",
      "Epoch 15/20  Iteration 25631/35720 Training loss: 0.7393 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 25632/35720 Training loss: 0.7391 0.2245 sec/batch\n",
      "Epoch 15/20  Iteration 25633/35720 Training loss: 0.7392 0.2154 sec/batch\n",
      "Epoch 15/20  Iteration 25634/35720 Training loss: 0.7391 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 25635/35720 Training loss: 0.7392 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 25636/35720 Training loss: 0.7391 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 25637/35720 Training loss: 0.7390 0.2240 sec/batch\n",
      "Epoch 15/20  Iteration 25638/35720 Training loss: 0.7390 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 25639/35720 Training loss: 0.7390 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25640/35720 Training loss: 0.7390 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 25641/35720 Training loss: 0.7390 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25642/35720 Training loss: 0.7389 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 25643/35720 Training loss: 0.7389 0.2213 sec/batch\n",
      "Epoch 15/20  Iteration 25644/35720 Training loss: 0.7389 0.2325 sec/batch\n",
      "Epoch 15/20  Iteration 25645/35720 Training loss: 0.7390 0.2060 sec/batch\n",
      "Epoch 15/20  Iteration 25646/35720 Training loss: 0.7390 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25647/35720 Training loss: 0.7390 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 25648/35720 Training loss: 0.7390 0.2314 sec/batch\n",
      "Epoch 15/20  Iteration 25649/35720 Training loss: 0.7390 0.2143 sec/batch\n",
      "Epoch 15/20  Iteration 25650/35720 Training loss: 0.7390 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 25651/35720 Training loss: 0.7389 0.2280 sec/batch\n",
      "Epoch 15/20  Iteration 25652/35720 Training loss: 0.7389 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 25653/35720 Training loss: 0.7388 0.2206 sec/batch\n",
      "Epoch 15/20  Iteration 25654/35720 Training loss: 0.7388 0.2193 sec/batch\n",
      "Epoch 15/20  Iteration 25655/35720 Training loss: 0.7387 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25656/35720 Training loss: 0.7387 0.2207 sec/batch\n",
      "Epoch 15/20  Iteration 25657/35720 Training loss: 0.7387 0.2076 sec/batch\n",
      "Epoch 15/20  Iteration 25658/35720 Training loss: 0.7388 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 25659/35720 Training loss: 0.7388 0.2138 sec/batch\n",
      "Epoch 15/20  Iteration 25660/35720 Training loss: 0.7388 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 25661/35720 Training loss: 0.7389 0.2260 sec/batch\n",
      "Epoch 15/20  Iteration 25662/35720 Training loss: 0.7389 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 25663/35720 Training loss: 0.7389 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25664/35720 Training loss: 0.7390 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25665/35720 Training loss: 0.7390 0.2332 sec/batch\n",
      "Epoch 15/20  Iteration 25666/35720 Training loss: 0.7391 0.2157 sec/batch\n",
      "Epoch 15/20  Iteration 25667/35720 Training loss: 0.7391 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25668/35720 Training loss: 0.7391 0.2121 sec/batch\n",
      "Epoch 15/20  Iteration 25669/35720 Training loss: 0.7391 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 25670/35720 Training loss: 0.7391 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 25671/35720 Training loss: 0.7392 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 25672/35720 Training loss: 0.7392 0.2197 sec/batch\n",
      "Epoch 15/20  Iteration 25673/35720 Training loss: 0.7391 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25674/35720 Training loss: 0.7392 0.2058 sec/batch\n",
      "Epoch 15/20  Iteration 25675/35720 Training loss: 0.7390 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 25676/35720 Training loss: 0.7390 0.2190 sec/batch\n",
      "Epoch 15/20  Iteration 25677/35720 Training loss: 0.7390 0.2300 sec/batch\n",
      "Epoch 15/20  Iteration 25678/35720 Training loss: 0.7389 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25679/35720 Training loss: 0.7390 0.2071 sec/batch\n",
      "Epoch 15/20  Iteration 25680/35720 Training loss: 0.7389 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25681/35720 Training loss: 0.7389 0.2126 sec/batch\n",
      "Epoch 15/20  Iteration 25682/35720 Training loss: 0.7390 0.2247 sec/batch\n",
      "Epoch 15/20  Iteration 25683/35720 Training loss: 0.7390 0.2239 sec/batch\n",
      "Epoch 15/20  Iteration 25684/35720 Training loss: 0.7389 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 25685/35720 Training loss: 0.7389 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25686/35720 Training loss: 0.7389 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25687/35720 Training loss: 0.7389 0.2306 sec/batch\n",
      "Epoch 15/20  Iteration 25688/35720 Training loss: 0.7389 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 25689/35720 Training loss: 0.7389 0.2109 sec/batch\n",
      "Epoch 15/20  Iteration 25690/35720 Training loss: 0.7388 0.2128 sec/batch\n",
      "Epoch 15/20  Iteration 25691/35720 Training loss: 0.7388 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 25692/35720 Training loss: 0.7388 0.2233 sec/batch\n",
      "Epoch 15/20  Iteration 25693/35720 Training loss: 0.7388 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 25694/35720 Training loss: 0.7387 0.2216 sec/batch\n",
      "Epoch 15/20  Iteration 25695/35720 Training loss: 0.7388 0.2199 sec/batch\n",
      "Epoch 15/20  Iteration 25696/35720 Training loss: 0.7389 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25697/35720 Training loss: 0.7390 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 25698/35720 Training loss: 0.7390 0.2252 sec/batch\n",
      "Epoch 15/20  Iteration 25699/35720 Training loss: 0.7390 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 25700/35720 Training loss: 0.7389 0.2216 sec/batch\n",
      "Epoch 15/20  Iteration 25701/35720 Training loss: 0.7390 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 25702/35720 Training loss: 0.7389 0.2058 sec/batch\n",
      "Epoch 15/20  Iteration 25703/35720 Training loss: 0.7389 0.2138 sec/batch\n",
      "Epoch 15/20  Iteration 25704/35720 Training loss: 0.7389 0.2225 sec/batch\n",
      "Epoch 15/20  Iteration 25705/35720 Training loss: 0.7388 0.2170 sec/batch\n",
      "Epoch 15/20  Iteration 25706/35720 Training loss: 0.7388 0.2190 sec/batch\n",
      "Epoch 15/20  Iteration 25707/35720 Training loss: 0.7388 0.2057 sec/batch\n",
      "Epoch 15/20  Iteration 25708/35720 Training loss: 0.7388 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 25709/35720 Training loss: 0.7388 0.2252 sec/batch\n",
      "Epoch 15/20  Iteration 25710/35720 Training loss: 0.7389 0.2213 sec/batch\n",
      "Epoch 15/20  Iteration 25711/35720 Training loss: 0.7389 0.2071 sec/batch\n",
      "Epoch 15/20  Iteration 25712/35720 Training loss: 0.7389 0.2076 sec/batch\n",
      "Epoch 15/20  Iteration 25713/35720 Training loss: 0.7389 0.2071 sec/batch\n",
      "Epoch 15/20  Iteration 25714/35720 Training loss: 0.7390 0.2189 sec/batch\n",
      "Epoch 15/20  Iteration 25715/35720 Training loss: 0.7390 0.2281 sec/batch\n",
      "Epoch 15/20  Iteration 25716/35720 Training loss: 0.7391 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 25717/35720 Training loss: 0.7391 0.2190 sec/batch\n",
      "Epoch 15/20  Iteration 25718/35720 Training loss: 0.7390 0.2193 sec/batch\n",
      "Epoch 15/20  Iteration 25719/35720 Training loss: 0.7391 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25720/35720 Training loss: 0.7391 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 25721/35720 Training loss: 0.7392 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 25722/35720 Training loss: 0.7393 0.2195 sec/batch\n",
      "Epoch 15/20  Iteration 25723/35720 Training loss: 0.7392 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 25724/35720 Training loss: 0.7393 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 25725/35720 Training loss: 0.7393 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25726/35720 Training loss: 0.7393 0.2283 sec/batch\n",
      "Epoch 15/20  Iteration 25727/35720 Training loss: 0.7394 0.2232 sec/batch\n",
      "Epoch 15/20  Iteration 25728/35720 Training loss: 0.7393 0.2223 sec/batch\n",
      "Epoch 15/20  Iteration 25729/35720 Training loss: 0.7393 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 25730/35720 Training loss: 0.7393 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 25731/35720 Training loss: 0.7394 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 25732/35720 Training loss: 0.7394 0.2141 sec/batch\n",
      "Epoch 15/20  Iteration 25733/35720 Training loss: 0.7395 0.2087 sec/batch\n",
      "Epoch 15/20  Iteration 25734/35720 Training loss: 0.7395 0.2225 sec/batch\n",
      "Epoch 15/20  Iteration 25735/35720 Training loss: 0.7395 0.2257 sec/batch\n",
      "Epoch 15/20  Iteration 25736/35720 Training loss: 0.7395 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 25737/35720 Training loss: 0.7395 0.2050 sec/batch\n",
      "Epoch 15/20  Iteration 25738/35720 Training loss: 0.7394 0.2149 sec/batch\n",
      "Epoch 15/20  Iteration 25739/35720 Training loss: 0.7394 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 25740/35720 Training loss: 0.7395 0.2051 sec/batch\n",
      "Epoch 15/20  Iteration 25741/35720 Training loss: 0.7395 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25742/35720 Training loss: 0.7395 0.2144 sec/batch\n",
      "Epoch 15/20  Iteration 25743/35720 Training loss: 0.7395 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25744/35720 Training loss: 0.7396 0.2163 sec/batch\n",
      "Epoch 15/20  Iteration 25745/35720 Training loss: 0.7396 0.2407 sec/batch\n",
      "Epoch 15/20  Iteration 25746/35720 Training loss: 0.7396 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 25747/35720 Training loss: 0.7395 0.2060 sec/batch\n",
      "Epoch 15/20  Iteration 25748/35720 Training loss: 0.7395 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 25749/35720 Training loss: 0.7395 0.2239 sec/batch\n",
      "Epoch 15/20  Iteration 25750/35720 Training loss: 0.7395 0.2188 sec/batch\n",
      "Epoch 15/20  Iteration 25751/35720 Training loss: 0.7395 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 25752/35720 Training loss: 0.7395 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25753/35720 Training loss: 0.7394 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 25754/35720 Training loss: 0.7394 0.2050 sec/batch\n",
      "Epoch 15/20  Iteration 25755/35720 Training loss: 0.7394 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 25756/35720 Training loss: 0.7395 0.2121 sec/batch\n",
      "Epoch 15/20  Iteration 25757/35720 Training loss: 0.7394 0.2240 sec/batch\n",
      "Epoch 15/20  Iteration 25758/35720 Training loss: 0.7394 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 25759/35720 Training loss: 0.7393 0.2099 sec/batch\n",
      "Epoch 15/20  Iteration 25760/35720 Training loss: 0.7393 0.2165 sec/batch\n",
      "Epoch 15/20  Iteration 25761/35720 Training loss: 0.7393 0.2177 sec/batch\n",
      "Epoch 15/20  Iteration 25762/35720 Training loss: 0.7393 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 25763/35720 Training loss: 0.7394 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25764/35720 Training loss: 0.7394 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 25765/35720 Training loss: 0.7394 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25766/35720 Training loss: 0.7395 0.2129 sec/batch\n",
      "Epoch 15/20  Iteration 25767/35720 Training loss: 0.7394 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 25768/35720 Training loss: 0.7393 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25769/35720 Training loss: 0.7394 0.2213 sec/batch\n",
      "Epoch 15/20  Iteration 25770/35720 Training loss: 0.7394 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 25771/35720 Training loss: 0.7394 0.2231 sec/batch\n",
      "Epoch 15/20  Iteration 25772/35720 Training loss: 0.7394 0.2087 sec/batch\n",
      "Epoch 15/20  Iteration 25773/35720 Training loss: 0.7395 0.2247 sec/batch\n",
      "Epoch 15/20  Iteration 25774/35720 Training loss: 0.7396 0.2177 sec/batch\n",
      "Epoch 15/20  Iteration 25775/35720 Training loss: 0.7396 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25776/35720 Training loss: 0.7397 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 25777/35720 Training loss: 0.7396 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 25778/35720 Training loss: 0.7396 0.2218 sec/batch\n",
      "Epoch 15/20  Iteration 25779/35720 Training loss: 0.7395 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 25780/35720 Training loss: 0.7395 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25781/35720 Training loss: 0.7394 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 25782/35720 Training loss: 0.7394 0.2148 sec/batch\n",
      "Epoch 15/20  Iteration 25783/35720 Training loss: 0.7395 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25784/35720 Training loss: 0.7395 0.2165 sec/batch\n",
      "Epoch 15/20  Iteration 25785/35720 Training loss: 0.7394 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25786/35720 Training loss: 0.7395 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 25787/35720 Training loss: 0.7396 0.2144 sec/batch\n",
      "Epoch 15/20  Iteration 25788/35720 Training loss: 0.7396 0.2183 sec/batch\n",
      "Epoch 15/20  Iteration 25789/35720 Training loss: 0.7395 0.2146 sec/batch\n",
      "Epoch 15/20  Iteration 25790/35720 Training loss: 0.7394 0.2154 sec/batch\n",
      "Epoch 15/20  Iteration 25791/35720 Training loss: 0.7395 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 25792/35720 Training loss: 0.7395 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25793/35720 Training loss: 0.7396 0.2162 sec/batch\n",
      "Epoch 15/20  Iteration 25794/35720 Training loss: 0.7396 0.2106 sec/batch\n",
      "Epoch 15/20  Iteration 25795/35720 Training loss: 0.7396 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25796/35720 Training loss: 0.7396 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 25797/35720 Training loss: 0.7396 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25798/35720 Training loss: 0.7395 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25799/35720 Training loss: 0.7395 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 25800/35720 Training loss: 0.7396 0.2169 sec/batch\n",
      "Validation loss: 1.54185 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 25801/35720 Training loss: 0.7400 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 25802/35720 Training loss: 0.7400 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25803/35720 Training loss: 0.7399 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 25804/35720 Training loss: 0.7400 0.2277 sec/batch\n",
      "Epoch 15/20  Iteration 25805/35720 Training loss: 0.7400 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25806/35720 Training loss: 0.7401 0.2322 sec/batch\n",
      "Epoch 15/20  Iteration 25807/35720 Training loss: 0.7401 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25808/35720 Training loss: 0.7402 0.2212 sec/batch\n",
      "Epoch 15/20  Iteration 25809/35720 Training loss: 0.7402 0.2185 sec/batch\n",
      "Epoch 15/20  Iteration 25810/35720 Training loss: 0.7402 0.2334 sec/batch\n",
      "Epoch 15/20  Iteration 25811/35720 Training loss: 0.7402 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 25812/35720 Training loss: 0.7402 0.2119 sec/batch\n",
      "Epoch 15/20  Iteration 25813/35720 Training loss: 0.7403 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25814/35720 Training loss: 0.7403 0.2235 sec/batch\n",
      "Epoch 15/20  Iteration 25815/35720 Training loss: 0.7402 0.2200 sec/batch\n",
      "Epoch 15/20  Iteration 25816/35720 Training loss: 0.7403 0.2240 sec/batch\n",
      "Epoch 15/20  Iteration 25817/35720 Training loss: 0.7403 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 25818/35720 Training loss: 0.7403 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25819/35720 Training loss: 0.7403 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 25820/35720 Training loss: 0.7403 0.2209 sec/batch\n",
      "Epoch 15/20  Iteration 25821/35720 Training loss: 0.7403 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 25822/35720 Training loss: 0.7404 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 25823/35720 Training loss: 0.7404 0.2204 sec/batch\n",
      "Epoch 15/20  Iteration 25824/35720 Training loss: 0.7404 0.2192 sec/batch\n",
      "Epoch 15/20  Iteration 25825/35720 Training loss: 0.7404 0.2131 sec/batch\n",
      "Epoch 15/20  Iteration 25826/35720 Training loss: 0.7404 0.2149 sec/batch\n",
      "Epoch 15/20  Iteration 25827/35720 Training loss: 0.7403 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 25828/35720 Training loss: 0.7403 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 25829/35720 Training loss: 0.7402 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 25830/35720 Training loss: 0.7402 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25831/35720 Training loss: 0.7402 0.2186 sec/batch\n",
      "Epoch 15/20  Iteration 25832/35720 Training loss: 0.7401 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 25833/35720 Training loss: 0.7401 0.2224 sec/batch\n",
      "Epoch 15/20  Iteration 25834/35720 Training loss: 0.7400 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 25835/35720 Training loss: 0.7400 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25836/35720 Training loss: 0.7401 0.2159 sec/batch\n",
      "Epoch 15/20  Iteration 25837/35720 Training loss: 0.7401 0.2283 sec/batch\n",
      "Epoch 15/20  Iteration 25838/35720 Training loss: 0.7401 0.2333 sec/batch\n",
      "Epoch 15/20  Iteration 25839/35720 Training loss: 0.7401 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 25840/35720 Training loss: 0.7401 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 25841/35720 Training loss: 0.7401 0.2229 sec/batch\n",
      "Epoch 15/20  Iteration 25842/35720 Training loss: 0.7401 0.2146 sec/batch\n",
      "Epoch 15/20  Iteration 25843/35720 Training loss: 0.7401 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 25844/35720 Training loss: 0.7401 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 25845/35720 Training loss: 0.7401 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 25846/35720 Training loss: 0.7401 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 25847/35720 Training loss: 0.7401 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 25848/35720 Training loss: 0.7401 0.2266 sec/batch\n",
      "Epoch 15/20  Iteration 25849/35720 Training loss: 0.7401 0.2304 sec/batch\n",
      "Epoch 15/20  Iteration 25850/35720 Training loss: 0.7401 0.2278 sec/batch\n",
      "Epoch 15/20  Iteration 25851/35720 Training loss: 0.7401 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 25852/35720 Training loss: 0.7401 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 25853/35720 Training loss: 0.7400 0.2112 sec/batch\n",
      "Epoch 15/20  Iteration 25854/35720 Training loss: 0.7401 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 25855/35720 Training loss: 0.7400 0.2201 sec/batch\n",
      "Epoch 15/20  Iteration 25856/35720 Training loss: 0.7400 0.2255 sec/batch\n",
      "Epoch 15/20  Iteration 25857/35720 Training loss: 0.7400 0.2099 sec/batch\n",
      "Epoch 15/20  Iteration 25858/35720 Training loss: 0.7399 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 25859/35720 Training loss: 0.7400 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 25860/35720 Training loss: 0.7399 0.2126 sec/batch\n",
      "Epoch 15/20  Iteration 25861/35720 Training loss: 0.7398 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25862/35720 Training loss: 0.7398 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25863/35720 Training loss: 0.7398 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 25864/35720 Training loss: 0.7398 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 25865/35720 Training loss: 0.7398 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 25866/35720 Training loss: 0.7397 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 25867/35720 Training loss: 0.7398 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 25868/35720 Training loss: 0.7397 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25869/35720 Training loss: 0.7396 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25870/35720 Training loss: 0.7396 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 25871/35720 Training loss: 0.7397 0.2307 sec/batch\n",
      "Epoch 15/20  Iteration 25872/35720 Training loss: 0.7397 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 25873/35720 Training loss: 0.7396 0.2163 sec/batch\n",
      "Epoch 15/20  Iteration 25874/35720 Training loss: 0.7396 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 25875/35720 Training loss: 0.7396 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 25876/35720 Training loss: 0.7396 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 25877/35720 Training loss: 0.7395 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 25878/35720 Training loss: 0.7395 0.2309 sec/batch\n",
      "Epoch 15/20  Iteration 25879/35720 Training loss: 0.7395 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 25880/35720 Training loss: 0.7395 0.2058 sec/batch\n",
      "Epoch 15/20  Iteration 25881/35720 Training loss: 0.7394 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25882/35720 Training loss: 0.7395 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 25883/35720 Training loss: 0.7395 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 25884/35720 Training loss: 0.7394 0.2131 sec/batch\n",
      "Epoch 15/20  Iteration 25885/35720 Training loss: 0.7394 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25886/35720 Training loss: 0.7393 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25887/35720 Training loss: 0.7393 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 25888/35720 Training loss: 0.7393 0.2220 sec/batch\n",
      "Epoch 15/20  Iteration 25889/35720 Training loss: 0.7393 0.2209 sec/batch\n",
      "Epoch 15/20  Iteration 25890/35720 Training loss: 0.7392 0.2260 sec/batch\n",
      "Epoch 15/20  Iteration 25891/35720 Training loss: 0.7392 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 25892/35720 Training loss: 0.7392 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 25893/35720 Training loss: 0.7392 0.2241 sec/batch\n",
      "Epoch 15/20  Iteration 25894/35720 Training loss: 0.7391 0.2077 sec/batch\n",
      "Epoch 15/20  Iteration 25895/35720 Training loss: 0.7390 0.2236 sec/batch\n",
      "Epoch 15/20  Iteration 25896/35720 Training loss: 0.7390 0.2075 sec/batch\n",
      "Epoch 15/20  Iteration 25897/35720 Training loss: 0.7390 0.2153 sec/batch\n",
      "Epoch 15/20  Iteration 25898/35720 Training loss: 0.7389 0.2207 sec/batch\n",
      "Epoch 15/20  Iteration 25899/35720 Training loss: 0.7389 0.2139 sec/batch\n",
      "Epoch 15/20  Iteration 25900/35720 Training loss: 0.7389 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 25901/35720 Training loss: 0.7388 0.2071 sec/batch\n",
      "Epoch 15/20  Iteration 25902/35720 Training loss: 0.7387 0.2049 sec/batch\n",
      "Epoch 15/20  Iteration 25903/35720 Training loss: 0.7386 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 25904/35720 Training loss: 0.7386 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 25905/35720 Training loss: 0.7386 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 25906/35720 Training loss: 0.7385 0.2150 sec/batch\n",
      "Epoch 15/20  Iteration 25907/35720 Training loss: 0.7385 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 25908/35720 Training loss: 0.7384 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 25909/35720 Training loss: 0.7384 0.2170 sec/batch\n",
      "Epoch 15/20  Iteration 25910/35720 Training loss: 0.7384 0.2255 sec/batch\n",
      "Epoch 15/20  Iteration 25911/35720 Training loss: 0.7384 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 25912/35720 Training loss: 0.7384 0.2302 sec/batch\n",
      "Epoch 15/20  Iteration 25913/35720 Training loss: 0.7384 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 25914/35720 Training loss: 0.7383 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 25915/35720 Training loss: 0.7383 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 25916/35720 Training loss: 0.7383 0.2276 sec/batch\n",
      "Epoch 15/20  Iteration 25917/35720 Training loss: 0.7383 0.2273 sec/batch\n",
      "Epoch 15/20  Iteration 25918/35720 Training loss: 0.7383 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25919/35720 Training loss: 0.7383 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 25920/35720 Training loss: 0.7384 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25921/35720 Training loss: 0.7384 0.2258 sec/batch\n",
      "Epoch 15/20  Iteration 25922/35720 Training loss: 0.7384 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 25923/35720 Training loss: 0.7383 0.2232 sec/batch\n",
      "Epoch 15/20  Iteration 25924/35720 Training loss: 0.7383 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25925/35720 Training loss: 0.7383 0.2057 sec/batch\n",
      "Epoch 15/20  Iteration 25926/35720 Training loss: 0.7382 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25927/35720 Training loss: 0.7382 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 25928/35720 Training loss: 0.7382 0.2220 sec/batch\n",
      "Epoch 15/20  Iteration 25929/35720 Training loss: 0.7382 0.2206 sec/batch\n",
      "Epoch 15/20  Iteration 25930/35720 Training loss: 0.7382 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 25931/35720 Training loss: 0.7383 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 25932/35720 Training loss: 0.7382 0.2300 sec/batch\n",
      "Epoch 15/20  Iteration 25933/35720 Training loss: 0.7382 0.2199 sec/batch\n",
      "Epoch 15/20  Iteration 25934/35720 Training loss: 0.7382 0.2294 sec/batch\n",
      "Epoch 15/20  Iteration 25935/35720 Training loss: 0.7383 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 25936/35720 Training loss: 0.7383 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25937/35720 Training loss: 0.7383 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 25938/35720 Training loss: 0.7383 0.2219 sec/batch\n",
      "Epoch 15/20  Iteration 25939/35720 Training loss: 0.7382 0.2133 sec/batch\n",
      "Epoch 15/20  Iteration 25940/35720 Training loss: 0.7381 0.2177 sec/batch\n",
      "Epoch 15/20  Iteration 25941/35720 Training loss: 0.7381 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 25942/35720 Training loss: 0.7380 0.2145 sec/batch\n",
      "Epoch 15/20  Iteration 25943/35720 Training loss: 0.7380 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 25944/35720 Training loss: 0.7379 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 25945/35720 Training loss: 0.7379 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 25946/35720 Training loss: 0.7378 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 25947/35720 Training loss: 0.7378 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 25948/35720 Training loss: 0.7379 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 25949/35720 Training loss: 0.7378 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 25950/35720 Training loss: 0.7378 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 25951/35720 Training loss: 0.7379 0.2204 sec/batch\n",
      "Epoch 15/20  Iteration 25952/35720 Training loss: 0.7378 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 25953/35720 Training loss: 0.7378 0.2049 sec/batch\n",
      "Epoch 15/20  Iteration 25954/35720 Training loss: 0.7378 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 25955/35720 Training loss: 0.7377 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 25956/35720 Training loss: 0.7377 0.2280 sec/batch\n",
      "Epoch 15/20  Iteration 25957/35720 Training loss: 0.7376 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 25958/35720 Training loss: 0.7377 0.2199 sec/batch\n",
      "Epoch 15/20  Iteration 25959/35720 Training loss: 0.7377 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 25960/35720 Training loss: 0.7377 0.2276 sec/batch\n",
      "Epoch 15/20  Iteration 25961/35720 Training loss: 0.7376 0.2082 sec/batch\n",
      "Epoch 15/20  Iteration 25962/35720 Training loss: 0.7376 0.2358 sec/batch\n",
      "Epoch 15/20  Iteration 25963/35720 Training loss: 0.7376 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25964/35720 Training loss: 0.7375 0.2190 sec/batch\n",
      "Epoch 15/20  Iteration 25965/35720 Training loss: 0.7375 0.2199 sec/batch\n",
      "Epoch 15/20  Iteration 25966/35720 Training loss: 0.7375 0.2227 sec/batch\n",
      "Epoch 15/20  Iteration 25967/35720 Training loss: 0.7375 0.2437 sec/batch\n",
      "Epoch 15/20  Iteration 25968/35720 Training loss: 0.7375 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 25969/35720 Training loss: 0.7375 0.2076 sec/batch\n",
      "Epoch 15/20  Iteration 25970/35720 Training loss: 0.7374 0.2213 sec/batch\n",
      "Epoch 15/20  Iteration 25971/35720 Training loss: 0.7374 0.2151 sec/batch\n",
      "Epoch 15/20  Iteration 25972/35720 Training loss: 0.7374 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 25973/35720 Training loss: 0.7375 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 25974/35720 Training loss: 0.7376 0.2111 sec/batch\n",
      "Epoch 15/20  Iteration 25975/35720 Training loss: 0.7375 0.2293 sec/batch\n",
      "Epoch 15/20  Iteration 25976/35720 Training loss: 0.7374 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 25977/35720 Training loss: 0.7374 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 25978/35720 Training loss: 0.7373 0.2256 sec/batch\n",
      "Epoch 15/20  Iteration 25979/35720 Training loss: 0.7373 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 25980/35720 Training loss: 0.7373 0.2112 sec/batch\n",
      "Epoch 15/20  Iteration 25981/35720 Training loss: 0.7372 0.2206 sec/batch\n",
      "Epoch 15/20  Iteration 25982/35720 Training loss: 0.7373 0.2262 sec/batch\n",
      "Epoch 15/20  Iteration 25983/35720 Training loss: 0.7372 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 25984/35720 Training loss: 0.7372 0.2270 sec/batch\n",
      "Epoch 15/20  Iteration 25985/35720 Training loss: 0.7372 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 25986/35720 Training loss: 0.7371 0.2055 sec/batch\n",
      "Epoch 15/20  Iteration 25987/35720 Training loss: 0.7370 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 25988/35720 Training loss: 0.7370 0.2254 sec/batch\n",
      "Epoch 15/20  Iteration 25989/35720 Training loss: 0.7370 0.2241 sec/batch\n",
      "Epoch 15/20  Iteration 25990/35720 Training loss: 0.7369 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 25991/35720 Training loss: 0.7369 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 25992/35720 Training loss: 0.7369 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 25993/35720 Training loss: 0.7368 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 25994/35720 Training loss: 0.7368 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 25995/35720 Training loss: 0.7367 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 25996/35720 Training loss: 0.7367 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 25997/35720 Training loss: 0.7367 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 25998/35720 Training loss: 0.7367 0.2273 sec/batch\n",
      "Epoch 15/20  Iteration 25999/35720 Training loss: 0.7368 0.2255 sec/batch\n",
      "Epoch 15/20  Iteration 26000/35720 Training loss: 0.7368 0.2215 sec/batch\n",
      "Validation loss: 1.54664 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 26001/35720 Training loss: 0.7372 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 26002/35720 Training loss: 0.7373 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 26003/35720 Training loss: 0.7372 0.2212 sec/batch\n",
      "Epoch 15/20  Iteration 26004/35720 Training loss: 0.7372 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 26005/35720 Training loss: 0.7371 0.2960 sec/batch\n",
      "Epoch 15/20  Iteration 26006/35720 Training loss: 0.7370 0.2429 sec/batch\n",
      "Epoch 15/20  Iteration 26007/35720 Training loss: 0.7369 0.2546 sec/batch\n",
      "Epoch 15/20  Iteration 26008/35720 Training loss: 0.7369 0.2185 sec/batch\n",
      "Epoch 15/20  Iteration 26009/35720 Training loss: 0.7368 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26010/35720 Training loss: 0.7368 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 26011/35720 Training loss: 0.7368 0.2215 sec/batch\n",
      "Epoch 15/20  Iteration 26012/35720 Training loss: 0.7367 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26013/35720 Training loss: 0.7367 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26014/35720 Training loss: 0.7366 0.2347 sec/batch\n",
      "Epoch 15/20  Iteration 26015/35720 Training loss: 0.7366 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 26016/35720 Training loss: 0.7366 0.2288 sec/batch\n",
      "Epoch 15/20  Iteration 26017/35720 Training loss: 0.7365 0.2159 sec/batch\n",
      "Epoch 15/20  Iteration 26018/35720 Training loss: 0.7365 0.2081 sec/batch\n",
      "Epoch 15/20  Iteration 26019/35720 Training loss: 0.7365 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 26020/35720 Training loss: 0.7365 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 26021/35720 Training loss: 0.7365 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 26022/35720 Training loss: 0.7365 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26023/35720 Training loss: 0.7364 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 26024/35720 Training loss: 0.7363 0.2138 sec/batch\n",
      "Epoch 15/20  Iteration 26025/35720 Training loss: 0.7363 0.2237 sec/batch\n",
      "Epoch 15/20  Iteration 26026/35720 Training loss: 0.7364 0.2158 sec/batch\n",
      "Epoch 15/20  Iteration 26027/35720 Training loss: 0.7365 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 26028/35720 Training loss: 0.7365 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 26029/35720 Training loss: 0.7365 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 26030/35720 Training loss: 0.7364 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 26031/35720 Training loss: 0.7364 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 26032/35720 Training loss: 0.7364 0.2087 sec/batch\n",
      "Epoch 15/20  Iteration 26033/35720 Training loss: 0.7363 0.2225 sec/batch\n",
      "Epoch 15/20  Iteration 26034/35720 Training loss: 0.7363 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 26035/35720 Training loss: 0.7363 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 26036/35720 Training loss: 0.7363 0.2158 sec/batch\n",
      "Epoch 15/20  Iteration 26037/35720 Training loss: 0.7363 0.2106 sec/batch\n",
      "Epoch 15/20  Iteration 26038/35720 Training loss: 0.7363 0.2129 sec/batch\n",
      "Epoch 15/20  Iteration 26039/35720 Training loss: 0.7363 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 26040/35720 Training loss: 0.7363 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 26041/35720 Training loss: 0.7364 0.2099 sec/batch\n",
      "Epoch 15/20  Iteration 26042/35720 Training loss: 0.7364 0.2211 sec/batch\n",
      "Epoch 15/20  Iteration 26043/35720 Training loss: 0.7364 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 26044/35720 Training loss: 0.7364 0.2071 sec/batch\n",
      "Epoch 15/20  Iteration 26045/35720 Training loss: 0.7364 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 26046/35720 Training loss: 0.7364 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 26047/35720 Training loss: 0.7364 0.2217 sec/batch\n",
      "Epoch 15/20  Iteration 26048/35720 Training loss: 0.7364 0.2277 sec/batch\n",
      "Epoch 15/20  Iteration 26049/35720 Training loss: 0.7364 0.2258 sec/batch\n",
      "Epoch 15/20  Iteration 26050/35720 Training loss: 0.7364 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 26051/35720 Training loss: 0.7364 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 26052/35720 Training loss: 0.7364 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 26053/35720 Training loss: 0.7364 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 26054/35720 Training loss: 0.7364 0.2133 sec/batch\n",
      "Epoch 15/20  Iteration 26055/35720 Training loss: 0.7364 0.2270 sec/batch\n",
      "Epoch 15/20  Iteration 26056/35720 Training loss: 0.7364 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 26057/35720 Training loss: 0.7365 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 26058/35720 Training loss: 0.7365 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 26059/35720 Training loss: 0.7365 0.2269 sec/batch\n",
      "Epoch 15/20  Iteration 26060/35720 Training loss: 0.7365 0.2224 sec/batch\n",
      "Epoch 15/20  Iteration 26061/35720 Training loss: 0.7365 0.2119 sec/batch\n",
      "Epoch 15/20  Iteration 26062/35720 Training loss: 0.7365 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 26063/35720 Training loss: 0.7365 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 26064/35720 Training loss: 0.7364 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 26065/35720 Training loss: 0.7365 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 26066/35720 Training loss: 0.7365 0.2151 sec/batch\n",
      "Epoch 15/20  Iteration 26067/35720 Training loss: 0.7366 0.2109 sec/batch\n",
      "Epoch 15/20  Iteration 26068/35720 Training loss: 0.7365 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 26069/35720 Training loss: 0.7365 0.2158 sec/batch\n",
      "Epoch 15/20  Iteration 26070/35720 Training loss: 0.7365 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 26071/35720 Training loss: 0.7365 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 26072/35720 Training loss: 0.7365 0.2257 sec/batch\n",
      "Epoch 15/20  Iteration 26073/35720 Training loss: 0.7365 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 26074/35720 Training loss: 0.7365 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 26075/35720 Training loss: 0.7365 0.2151 sec/batch\n",
      "Epoch 15/20  Iteration 26076/35720 Training loss: 0.7365 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 26077/35720 Training loss: 0.7365 0.2154 sec/batch\n",
      "Epoch 15/20  Iteration 26078/35720 Training loss: 0.7365 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 26079/35720 Training loss: 0.7365 0.2053 sec/batch\n",
      "Epoch 15/20  Iteration 26080/35720 Training loss: 0.7365 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 26081/35720 Training loss: 0.7364 0.2187 sec/batch\n",
      "Epoch 15/20  Iteration 26082/35720 Training loss: 0.7364 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 26083/35720 Training loss: 0.7364 0.2154 sec/batch\n",
      "Epoch 15/20  Iteration 26084/35720 Training loss: 0.7364 0.2106 sec/batch\n",
      "Epoch 15/20  Iteration 26085/35720 Training loss: 0.7364 0.2111 sec/batch\n",
      "Epoch 15/20  Iteration 26086/35720 Training loss: 0.7364 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 26087/35720 Training loss: 0.7364 0.2218 sec/batch\n",
      "Epoch 15/20  Iteration 26088/35720 Training loss: 0.7364 0.2275 sec/batch\n",
      "Epoch 15/20  Iteration 26089/35720 Training loss: 0.7364 0.2252 sec/batch\n",
      "Epoch 15/20  Iteration 26090/35720 Training loss: 0.7364 0.2060 sec/batch\n",
      "Epoch 15/20  Iteration 26091/35720 Training loss: 0.7364 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 26092/35720 Training loss: 0.7365 0.2233 sec/batch\n",
      "Epoch 15/20  Iteration 26093/35720 Training loss: 0.7365 0.2104 sec/batch\n",
      "Epoch 15/20  Iteration 26094/35720 Training loss: 0.7365 0.2252 sec/batch\n",
      "Epoch 15/20  Iteration 26095/35720 Training loss: 0.7365 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 26096/35720 Training loss: 0.7365 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 26097/35720 Training loss: 0.7364 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 26098/35720 Training loss: 0.7364 0.2236 sec/batch\n",
      "Epoch 15/20  Iteration 26099/35720 Training loss: 0.7364 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 26100/35720 Training loss: 0.7364 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 26101/35720 Training loss: 0.7363 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26102/35720 Training loss: 0.7363 0.2111 sec/batch\n",
      "Epoch 15/20  Iteration 26103/35720 Training loss: 0.7364 0.2157 sec/batch\n",
      "Epoch 15/20  Iteration 26104/35720 Training loss: 0.7363 0.2184 sec/batch\n",
      "Epoch 15/20  Iteration 26105/35720 Training loss: 0.7364 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 26106/35720 Training loss: 0.7364 0.2081 sec/batch\n",
      "Epoch 15/20  Iteration 26107/35720 Training loss: 0.7364 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 26108/35720 Training loss: 0.7364 0.2126 sec/batch\n",
      "Epoch 15/20  Iteration 26109/35720 Training loss: 0.7364 0.2262 sec/batch\n",
      "Epoch 15/20  Iteration 26110/35720 Training loss: 0.7364 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 26111/35720 Training loss: 0.7364 0.2162 sec/batch\n",
      "Epoch 15/20  Iteration 26112/35720 Training loss: 0.7364 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 26113/35720 Training loss: 0.7364 0.2126 sec/batch\n",
      "Epoch 15/20  Iteration 26114/35720 Training loss: 0.7364 0.2230 sec/batch\n",
      "Epoch 15/20  Iteration 26115/35720 Training loss: 0.7363 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 26116/35720 Training loss: 0.7363 0.2275 sec/batch\n",
      "Epoch 15/20  Iteration 26117/35720 Training loss: 0.7362 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 26118/35720 Training loss: 0.7362 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 26119/35720 Training loss: 0.7362 0.2155 sec/batch\n",
      "Epoch 15/20  Iteration 26120/35720 Training loss: 0.7362 0.2212 sec/batch\n",
      "Epoch 15/20  Iteration 26121/35720 Training loss: 0.7361 0.2246 sec/batch\n",
      "Epoch 15/20  Iteration 26122/35720 Training loss: 0.7361 0.2270 sec/batch\n",
      "Epoch 15/20  Iteration 26123/35720 Training loss: 0.7361 0.2109 sec/batch\n",
      "Epoch 15/20  Iteration 26124/35720 Training loss: 0.7361 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 26125/35720 Training loss: 0.7361 0.2153 sec/batch\n",
      "Epoch 15/20  Iteration 26126/35720 Training loss: 0.7361 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 26127/35720 Training loss: 0.7362 0.2188 sec/batch\n",
      "Epoch 15/20  Iteration 26128/35720 Training loss: 0.7362 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 26129/35720 Training loss: 0.7361 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 26130/35720 Training loss: 0.7361 0.2237 sec/batch\n",
      "Epoch 15/20  Iteration 26131/35720 Training loss: 0.7361 0.2258 sec/batch\n",
      "Epoch 15/20  Iteration 26132/35720 Training loss: 0.7361 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 26133/35720 Training loss: 0.7360 0.2337 sec/batch\n",
      "Epoch 15/20  Iteration 26134/35720 Training loss: 0.7360 0.2079 sec/batch\n",
      "Epoch 15/20  Iteration 26135/35720 Training loss: 0.7359 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 26136/35720 Training loss: 0.7359 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 26137/35720 Training loss: 0.7358 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 26138/35720 Training loss: 0.7358 0.2300 sec/batch\n",
      "Epoch 15/20  Iteration 26139/35720 Training loss: 0.7358 0.2189 sec/batch\n",
      "Epoch 15/20  Iteration 26140/35720 Training loss: 0.7358 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 26141/35720 Training loss: 0.7357 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 26142/35720 Training loss: 0.7357 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 26143/35720 Training loss: 0.7357 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 26144/35720 Training loss: 0.7357 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 26145/35720 Training loss: 0.7357 0.2112 sec/batch\n",
      "Epoch 15/20  Iteration 26146/35720 Training loss: 0.7356 0.2232 sec/batch\n",
      "Epoch 15/20  Iteration 26147/35720 Training loss: 0.7356 0.2163 sec/batch\n",
      "Epoch 15/20  Iteration 26148/35720 Training loss: 0.7356 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 26149/35720 Training loss: 0.7355 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 26150/35720 Training loss: 0.7355 0.2165 sec/batch\n",
      "Epoch 15/20  Iteration 26151/35720 Training loss: 0.7355 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 26152/35720 Training loss: 0.7355 0.2219 sec/batch\n",
      "Epoch 15/20  Iteration 26153/35720 Training loss: 0.7355 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 26154/35720 Training loss: 0.7355 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 26155/35720 Training loss: 0.7355 0.2309 sec/batch\n",
      "Epoch 15/20  Iteration 26156/35720 Training loss: 0.7355 0.2422 sec/batch\n",
      "Epoch 15/20  Iteration 26157/35720 Training loss: 0.7355 0.2067 sec/batch\n",
      "Epoch 15/20  Iteration 26158/35720 Training loss: 0.7355 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 26159/35720 Training loss: 0.7355 0.2262 sec/batch\n",
      "Epoch 15/20  Iteration 26160/35720 Training loss: 0.7355 0.2224 sec/batch\n",
      "Epoch 15/20  Iteration 26161/35720 Training loss: 0.7355 0.2149 sec/batch\n",
      "Epoch 15/20  Iteration 26162/35720 Training loss: 0.7355 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26163/35720 Training loss: 0.7355 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 26164/35720 Training loss: 0.7355 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 26165/35720 Training loss: 0.7354 0.2236 sec/batch\n",
      "Epoch 15/20  Iteration 26166/35720 Training loss: 0.7354 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 26167/35720 Training loss: 0.7355 0.2145 sec/batch\n",
      "Epoch 15/20  Iteration 26168/35720 Training loss: 0.7355 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 26169/35720 Training loss: 0.7355 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26170/35720 Training loss: 0.7354 0.2324 sec/batch\n",
      "Epoch 15/20  Iteration 26171/35720 Training loss: 0.7354 0.2262 sec/batch\n",
      "Epoch 15/20  Iteration 26172/35720 Training loss: 0.7354 0.2191 sec/batch\n",
      "Epoch 15/20  Iteration 26173/35720 Training loss: 0.7354 0.2071 sec/batch\n",
      "Epoch 15/20  Iteration 26174/35720 Training loss: 0.7355 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26175/35720 Training loss: 0.7355 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26176/35720 Training loss: 0.7355 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 26177/35720 Training loss: 0.7355 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 26178/35720 Training loss: 0.7354 0.2231 sec/batch\n",
      "Epoch 15/20  Iteration 26179/35720 Training loss: 0.7355 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 26180/35720 Training loss: 0.7355 0.2326 sec/batch\n",
      "Epoch 15/20  Iteration 26181/35720 Training loss: 0.7355 0.2058 sec/batch\n",
      "Epoch 15/20  Iteration 26182/35720 Training loss: 0.7355 0.2209 sec/batch\n",
      "Epoch 15/20  Iteration 26183/35720 Training loss: 0.7355 0.2236 sec/batch\n",
      "Epoch 15/20  Iteration 26184/35720 Training loss: 0.7355 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 26185/35720 Training loss: 0.7355 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 26186/35720 Training loss: 0.7355 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 26187/35720 Training loss: 0.7355 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 26188/35720 Training loss: 0.7355 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 26189/35720 Training loss: 0.7355 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 26190/35720 Training loss: 0.7354 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 26191/35720 Training loss: 0.7354 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 26192/35720 Training loss: 0.7354 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 26193/35720 Training loss: 0.7354 0.2221 sec/batch\n",
      "Epoch 15/20  Iteration 26194/35720 Training loss: 0.7354 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 26195/35720 Training loss: 0.7354 0.2204 sec/batch\n",
      "Epoch 15/20  Iteration 26196/35720 Training loss: 0.7355 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 26197/35720 Training loss: 0.7355 0.2081 sec/batch\n",
      "Epoch 15/20  Iteration 26198/35720 Training loss: 0.7354 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 26199/35720 Training loss: 0.7355 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 26200/35720 Training loss: 0.7355 0.2332 sec/batch\n",
      "Validation loss: 1.53721 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 26201/35720 Training loss: 0.7358 0.2087 sec/batch\n",
      "Epoch 15/20  Iteration 26202/35720 Training loss: 0.7358 0.2268 sec/batch\n",
      "Epoch 15/20  Iteration 26203/35720 Training loss: 0.7358 0.2242 sec/batch\n",
      "Epoch 15/20  Iteration 26204/35720 Training loss: 0.7358 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 26205/35720 Training loss: 0.7358 0.2244 sec/batch\n",
      "Epoch 15/20  Iteration 26206/35720 Training loss: 0.7357 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26207/35720 Training loss: 0.7357 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 26208/35720 Training loss: 0.7357 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 26209/35720 Training loss: 0.7356 0.2250 sec/batch\n",
      "Epoch 15/20  Iteration 26210/35720 Training loss: 0.7356 0.2301 sec/batch\n",
      "Epoch 15/20  Iteration 26211/35720 Training loss: 0.7356 0.2261 sec/batch\n",
      "Epoch 15/20  Iteration 26212/35720 Training loss: 0.7356 0.2060 sec/batch\n",
      "Epoch 15/20  Iteration 26213/35720 Training loss: 0.7356 0.2186 sec/batch\n",
      "Epoch 15/20  Iteration 26214/35720 Training loss: 0.7356 0.2271 sec/batch\n",
      "Epoch 15/20  Iteration 26215/35720 Training loss: 0.7356 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 26216/35720 Training loss: 0.7356 0.2218 sec/batch\n",
      "Epoch 15/20  Iteration 26217/35720 Training loss: 0.7355 0.2284 sec/batch\n",
      "Epoch 15/20  Iteration 26218/35720 Training loss: 0.7356 0.2079 sec/batch\n",
      "Epoch 15/20  Iteration 26219/35720 Training loss: 0.7356 0.2207 sec/batch\n",
      "Epoch 15/20  Iteration 26220/35720 Training loss: 0.7356 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 26221/35720 Training loss: 0.7356 0.2325 sec/batch\n",
      "Epoch 15/20  Iteration 26222/35720 Training loss: 0.7356 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 26223/35720 Training loss: 0.7356 0.2171 sec/batch\n",
      "Epoch 15/20  Iteration 26224/35720 Training loss: 0.7355 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 26225/35720 Training loss: 0.7355 0.2257 sec/batch\n",
      "Epoch 15/20  Iteration 26226/35720 Training loss: 0.7356 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 26227/35720 Training loss: 0.7355 0.2247 sec/batch\n",
      "Epoch 15/20  Iteration 26228/35720 Training loss: 0.7355 0.2653 sec/batch\n",
      "Epoch 15/20  Iteration 26229/35720 Training loss: 0.7355 0.3002 sec/batch\n",
      "Epoch 15/20  Iteration 26230/35720 Training loss: 0.7354 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 26231/35720 Training loss: 0.7354 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 26232/35720 Training loss: 0.7354 0.2153 sec/batch\n",
      "Epoch 15/20  Iteration 26233/35720 Training loss: 0.7355 0.2256 sec/batch\n",
      "Epoch 15/20  Iteration 26234/35720 Training loss: 0.7355 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26235/35720 Training loss: 0.7355 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 26236/35720 Training loss: 0.7355 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 26237/35720 Training loss: 0.7355 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 26238/35720 Training loss: 0.7355 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 26239/35720 Training loss: 0.7355 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 26240/35720 Training loss: 0.7354 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26241/35720 Training loss: 0.7354 0.2186 sec/batch\n",
      "Epoch 15/20  Iteration 26242/35720 Training loss: 0.7354 0.2214 sec/batch\n",
      "Epoch 15/20  Iteration 26243/35720 Training loss: 0.7353 0.2219 sec/batch\n",
      "Epoch 15/20  Iteration 26244/35720 Training loss: 0.7353 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 26245/35720 Training loss: 0.7352 0.2055 sec/batch\n",
      "Epoch 15/20  Iteration 26246/35720 Training loss: 0.7352 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 26247/35720 Training loss: 0.7352 0.2235 sec/batch\n",
      "Epoch 15/20  Iteration 26248/35720 Training loss: 0.7352 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 26249/35720 Training loss: 0.7351 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 26250/35720 Training loss: 0.7351 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 26251/35720 Training loss: 0.7351 0.2109 sec/batch\n",
      "Epoch 15/20  Iteration 26252/35720 Training loss: 0.7351 0.2171 sec/batch\n",
      "Epoch 15/20  Iteration 26253/35720 Training loss: 0.7350 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 26254/35720 Training loss: 0.7350 0.2185 sec/batch\n",
      "Epoch 15/20  Iteration 26255/35720 Training loss: 0.7350 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 26256/35720 Training loss: 0.7349 0.2075 sec/batch\n",
      "Epoch 15/20  Iteration 26257/35720 Training loss: 0.7349 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 26258/35720 Training loss: 0.7349 0.2185 sec/batch\n",
      "Epoch 15/20  Iteration 26259/35720 Training loss: 0.7350 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 26260/35720 Training loss: 0.7349 0.2163 sec/batch\n",
      "Epoch 15/20  Iteration 26261/35720 Training loss: 0.7349 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 26262/35720 Training loss: 0.7349 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 26263/35720 Training loss: 0.7349 0.2082 sec/batch\n",
      "Epoch 15/20  Iteration 26264/35720 Training loss: 0.7349 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 26265/35720 Training loss: 0.7348 0.2058 sec/batch\n",
      "Epoch 15/20  Iteration 26266/35720 Training loss: 0.7348 0.2155 sec/batch\n",
      "Epoch 15/20  Iteration 26267/35720 Training loss: 0.7347 0.2177 sec/batch\n",
      "Epoch 15/20  Iteration 26268/35720 Training loss: 0.7347 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 26269/35720 Training loss: 0.7346 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 26270/35720 Training loss: 0.7346 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 26271/35720 Training loss: 0.7346 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 26272/35720 Training loss: 0.7346 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 26273/35720 Training loss: 0.7346 0.2145 sec/batch\n",
      "Epoch 15/20  Iteration 26274/35720 Training loss: 0.7346 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 26275/35720 Training loss: 0.7346 0.2076 sec/batch\n",
      "Epoch 15/20  Iteration 26276/35720 Training loss: 0.7345 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26277/35720 Training loss: 0.7345 0.2129 sec/batch\n",
      "Epoch 15/20  Iteration 26278/35720 Training loss: 0.7345 0.2057 sec/batch\n",
      "Epoch 15/20  Iteration 26279/35720 Training loss: 0.7345 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 26280/35720 Training loss: 0.7345 0.2129 sec/batch\n",
      "Epoch 15/20  Iteration 26281/35720 Training loss: 0.7344 0.2162 sec/batch\n",
      "Epoch 15/20  Iteration 26282/35720 Training loss: 0.7344 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 26283/35720 Training loss: 0.7343 0.2155 sec/batch\n",
      "Epoch 15/20  Iteration 26284/35720 Training loss: 0.7343 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 26285/35720 Training loss: 0.7343 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26286/35720 Training loss: 0.7343 0.2133 sec/batch\n",
      "Epoch 15/20  Iteration 26287/35720 Training loss: 0.7343 0.2121 sec/batch\n",
      "Epoch 15/20  Iteration 26288/35720 Training loss: 0.7342 0.2264 sec/batch\n",
      "Epoch 15/20  Iteration 26289/35720 Training loss: 0.7342 0.2131 sec/batch\n",
      "Epoch 15/20  Iteration 26290/35720 Training loss: 0.7342 0.2242 sec/batch\n",
      "Epoch 15/20  Iteration 26291/35720 Training loss: 0.7341 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 26292/35720 Training loss: 0.7341 0.2171 sec/batch\n",
      "Epoch 15/20  Iteration 26293/35720 Training loss: 0.7341 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 26294/35720 Training loss: 0.7341 0.2246 sec/batch\n",
      "Epoch 15/20  Iteration 26295/35720 Training loss: 0.7340 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 26296/35720 Training loss: 0.7340 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 26297/35720 Training loss: 0.7340 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 26298/35720 Training loss: 0.7340 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 26299/35720 Training loss: 0.7339 0.2220 sec/batch\n",
      "Epoch 15/20  Iteration 26300/35720 Training loss: 0.7339 0.2140 sec/batch\n",
      "Epoch 15/20  Iteration 26301/35720 Training loss: 0.7339 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 26302/35720 Training loss: 0.7339 0.2104 sec/batch\n",
      "Epoch 15/20  Iteration 26303/35720 Training loss: 0.7338 0.2293 sec/batch\n",
      "Epoch 15/20  Iteration 26304/35720 Training loss: 0.7338 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 26305/35720 Training loss: 0.7338 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 26306/35720 Training loss: 0.7337 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 26307/35720 Training loss: 0.7337 0.2211 sec/batch\n",
      "Epoch 15/20  Iteration 26308/35720 Training loss: 0.7337 0.2053 sec/batch\n",
      "Epoch 15/20  Iteration 26309/35720 Training loss: 0.7337 0.2282 sec/batch\n",
      "Epoch 15/20  Iteration 26310/35720 Training loss: 0.7337 0.2121 sec/batch\n",
      "Epoch 15/20  Iteration 26311/35720 Training loss: 0.7336 0.2054 sec/batch\n",
      "Epoch 15/20  Iteration 26312/35720 Training loss: 0.7336 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 26313/35720 Training loss: 0.7336 0.2179 sec/batch\n",
      "Epoch 15/20  Iteration 26314/35720 Training loss: 0.7335 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 26315/35720 Training loss: 0.7335 0.2183 sec/batch\n",
      "Epoch 15/20  Iteration 26316/35720 Training loss: 0.7335 0.2220 sec/batch\n",
      "Epoch 15/20  Iteration 26317/35720 Training loss: 0.7335 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 26318/35720 Training loss: 0.7335 0.2200 sec/batch\n",
      "Epoch 15/20  Iteration 26319/35720 Training loss: 0.7334 0.2209 sec/batch\n",
      "Epoch 15/20  Iteration 26320/35720 Training loss: 0.7334 0.2248 sec/batch\n",
      "Epoch 15/20  Iteration 26321/35720 Training loss: 0.7334 0.2154 sec/batch\n",
      "Epoch 15/20  Iteration 26322/35720 Training loss: 0.7334 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 26323/35720 Training loss: 0.7333 0.2269 sec/batch\n",
      "Epoch 15/20  Iteration 26324/35720 Training loss: 0.7333 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 26325/35720 Training loss: 0.7333 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 26326/35720 Training loss: 0.7333 0.2163 sec/batch\n",
      "Epoch 15/20  Iteration 26327/35720 Training loss: 0.7333 0.2148 sec/batch\n",
      "Epoch 15/20  Iteration 26328/35720 Training loss: 0.7333 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26329/35720 Training loss: 0.7333 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 26330/35720 Training loss: 0.7333 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 26331/35720 Training loss: 0.7333 0.2184 sec/batch\n",
      "Epoch 15/20  Iteration 26332/35720 Training loss: 0.7333 0.2199 sec/batch\n",
      "Epoch 15/20  Iteration 26333/35720 Training loss: 0.7333 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 26334/35720 Training loss: 0.7333 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26335/35720 Training loss: 0.7333 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 26336/35720 Training loss: 0.7333 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 26337/35720 Training loss: 0.7333 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26338/35720 Training loss: 0.7333 0.2160 sec/batch\n",
      "Epoch 15/20  Iteration 26339/35720 Training loss: 0.7333 0.2201 sec/batch\n",
      "Epoch 15/20  Iteration 26340/35720 Training loss: 0.7333 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 26341/35720 Training loss: 0.7333 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 26342/35720 Training loss: 0.7333 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 26343/35720 Training loss: 0.7333 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 26344/35720 Training loss: 0.7333 0.2133 sec/batch\n",
      "Epoch 15/20  Iteration 26345/35720 Training loss: 0.7333 0.2134 sec/batch\n",
      "Epoch 15/20  Iteration 26346/35720 Training loss: 0.7333 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 26347/35720 Training loss: 0.7332 0.2194 sec/batch\n",
      "Epoch 15/20  Iteration 26348/35720 Training loss: 0.7332 0.2204 sec/batch\n",
      "Epoch 15/20  Iteration 26349/35720 Training loss: 0.7332 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 26350/35720 Training loss: 0.7332 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 26351/35720 Training loss: 0.7332 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26352/35720 Training loss: 0.7332 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 26353/35720 Training loss: 0.7332 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 26354/35720 Training loss: 0.7332 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 26355/35720 Training loss: 0.7332 0.2226 sec/batch\n",
      "Epoch 15/20  Iteration 26356/35720 Training loss: 0.7333 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 26357/35720 Training loss: 0.7333 0.2284 sec/batch\n",
      "Epoch 15/20  Iteration 26358/35720 Training loss: 0.7332 0.2236 sec/batch\n",
      "Epoch 15/20  Iteration 26359/35720 Training loss: 0.7332 0.2194 sec/batch\n",
      "Epoch 15/20  Iteration 26360/35720 Training loss: 0.7332 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 26361/35720 Training loss: 0.7331 0.2178 sec/batch\n",
      "Epoch 15/20  Iteration 26362/35720 Training loss: 0.7331 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 26363/35720 Training loss: 0.7331 0.2165 sec/batch\n",
      "Epoch 15/20  Iteration 26364/35720 Training loss: 0.7331 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26365/35720 Training loss: 0.7330 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 26366/35720 Training loss: 0.7330 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 26367/35720 Training loss: 0.7330 0.2128 sec/batch\n",
      "Epoch 15/20  Iteration 26368/35720 Training loss: 0.7330 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 26369/35720 Training loss: 0.7329 0.2192 sec/batch\n",
      "Epoch 15/20  Iteration 26370/35720 Training loss: 0.7329 0.2224 sec/batch\n",
      "Epoch 15/20  Iteration 26371/35720 Training loss: 0.7329 0.2214 sec/batch\n",
      "Epoch 15/20  Iteration 26372/35720 Training loss: 0.7329 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 26373/35720 Training loss: 0.7329 0.2229 sec/batch\n",
      "Epoch 15/20  Iteration 26374/35720 Training loss: 0.7330 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 26375/35720 Training loss: 0.7329 0.2145 sec/batch\n",
      "Epoch 15/20  Iteration 26376/35720 Training loss: 0.7330 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26377/35720 Training loss: 0.7330 0.2224 sec/batch\n",
      "Epoch 15/20  Iteration 26378/35720 Training loss: 0.7329 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 26379/35720 Training loss: 0.7329 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 26380/35720 Training loss: 0.7329 0.2153 sec/batch\n",
      "Epoch 15/20  Iteration 26381/35720 Training loss: 0.7329 0.2076 sec/batch\n",
      "Epoch 15/20  Iteration 26382/35720 Training loss: 0.7329 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 26383/35720 Training loss: 0.7329 0.2125 sec/batch\n",
      "Epoch 15/20  Iteration 26384/35720 Training loss: 0.7329 0.2260 sec/batch\n",
      "Epoch 15/20  Iteration 26385/35720 Training loss: 0.7329 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 26386/35720 Training loss: 0.7329 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 26387/35720 Training loss: 0.7329 0.2135 sec/batch\n",
      "Epoch 15/20  Iteration 26388/35720 Training loss: 0.7328 0.2134 sec/batch\n",
      "Epoch 15/20  Iteration 26389/35720 Training loss: 0.7329 0.2218 sec/batch\n",
      "Epoch 15/20  Iteration 26390/35720 Training loss: 0.7329 0.2148 sec/batch\n",
      "Epoch 15/20  Iteration 26391/35720 Training loss: 0.7329 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 26392/35720 Training loss: 0.7329 0.2277 sec/batch\n",
      "Epoch 15/20  Iteration 26393/35720 Training loss: 0.7329 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 26394/35720 Training loss: 0.7329 0.2148 sec/batch\n",
      "Epoch 15/20  Iteration 26395/35720 Training loss: 0.7328 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 26396/35720 Training loss: 0.7328 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 26397/35720 Training loss: 0.7327 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26398/35720 Training loss: 0.7327 0.2232 sec/batch\n",
      "Epoch 15/20  Iteration 26399/35720 Training loss: 0.7327 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 26400/35720 Training loss: 0.7327 0.2162 sec/batch\n",
      "Validation loss: 1.55487 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 26401/35720 Training loss: 0.7329 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26402/35720 Training loss: 0.7329 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26403/35720 Training loss: 0.7329 0.2243 sec/batch\n",
      "Epoch 15/20  Iteration 26404/35720 Training loss: 0.7329 0.2072 sec/batch\n",
      "Epoch 15/20  Iteration 26405/35720 Training loss: 0.7328 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 26406/35720 Training loss: 0.7328 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26407/35720 Training loss: 0.7328 0.2159 sec/batch\n",
      "Epoch 15/20  Iteration 26408/35720 Training loss: 0.7328 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 26409/35720 Training loss: 0.7328 0.2082 sec/batch\n",
      "Epoch 15/20  Iteration 26410/35720 Training loss: 0.7327 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 26411/35720 Training loss: 0.7327 0.2194 sec/batch\n",
      "Epoch 15/20  Iteration 26412/35720 Training loss: 0.7328 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 26413/35720 Training loss: 0.7328 0.2150 sec/batch\n",
      "Epoch 15/20  Iteration 26414/35720 Training loss: 0.7328 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 26415/35720 Training loss: 0.7328 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 26416/35720 Training loss: 0.7328 0.2269 sec/batch\n",
      "Epoch 15/20  Iteration 26417/35720 Training loss: 0.7327 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 26418/35720 Training loss: 0.7327 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 26419/35720 Training loss: 0.7327 0.2216 sec/batch\n",
      "Epoch 15/20  Iteration 26420/35720 Training loss: 0.7327 0.2081 sec/batch\n",
      "Epoch 15/20  Iteration 26421/35720 Training loss: 0.7327 0.2194 sec/batch\n",
      "Epoch 15/20  Iteration 26422/35720 Training loss: 0.7327 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 26423/35720 Training loss: 0.7327 0.2149 sec/batch\n",
      "Epoch 15/20  Iteration 26424/35720 Training loss: 0.7327 0.2229 sec/batch\n",
      "Epoch 15/20  Iteration 26425/35720 Training loss: 0.7327 0.2127 sec/batch\n",
      "Epoch 15/20  Iteration 26426/35720 Training loss: 0.7327 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 26427/35720 Training loss: 0.7327 0.2271 sec/batch\n",
      "Epoch 15/20  Iteration 26428/35720 Training loss: 0.7327 0.2070 sec/batch\n",
      "Epoch 15/20  Iteration 26429/35720 Training loss: 0.7327 0.2143 sec/batch\n",
      "Epoch 15/20  Iteration 26430/35720 Training loss: 0.7327 0.2195 sec/batch\n",
      "Epoch 15/20  Iteration 26431/35720 Training loss: 0.7327 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 26432/35720 Training loss: 0.7328 0.2247 sec/batch\n",
      "Epoch 15/20  Iteration 26433/35720 Training loss: 0.7328 0.2249 sec/batch\n",
      "Epoch 15/20  Iteration 26434/35720 Training loss: 0.7327 0.2050 sec/batch\n",
      "Epoch 15/20  Iteration 26435/35720 Training loss: 0.7327 0.2150 sec/batch\n",
      "Epoch 15/20  Iteration 26436/35720 Training loss: 0.7327 0.2158 sec/batch\n",
      "Epoch 15/20  Iteration 26437/35720 Training loss: 0.7327 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 26438/35720 Training loss: 0.7327 0.2280 sec/batch\n",
      "Epoch 15/20  Iteration 26439/35720 Training loss: 0.7327 0.2110 sec/batch\n",
      "Epoch 15/20  Iteration 26440/35720 Training loss: 0.7327 0.2074 sec/batch\n",
      "Epoch 15/20  Iteration 26441/35720 Training loss: 0.7326 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 26442/35720 Training loss: 0.7327 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 26443/35720 Training loss: 0.7327 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 26444/35720 Training loss: 0.7327 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 26445/35720 Training loss: 0.7327 0.2104 sec/batch\n",
      "Epoch 15/20  Iteration 26446/35720 Training loss: 0.7327 0.2115 sec/batch\n",
      "Epoch 15/20  Iteration 26447/35720 Training loss: 0.7327 0.2226 sec/batch\n",
      "Epoch 15/20  Iteration 26448/35720 Training loss: 0.7327 0.2233 sec/batch\n",
      "Epoch 15/20  Iteration 26449/35720 Training loss: 0.7327 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 26450/35720 Training loss: 0.7327 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 26451/35720 Training loss: 0.7327 0.2106 sec/batch\n",
      "Epoch 15/20  Iteration 26452/35720 Training loss: 0.7327 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26453/35720 Training loss: 0.7327 0.2274 sec/batch\n",
      "Epoch 15/20  Iteration 26454/35720 Training loss: 0.7326 0.2212 sec/batch\n",
      "Epoch 15/20  Iteration 26455/35720 Training loss: 0.7326 0.2220 sec/batch\n",
      "Epoch 15/20  Iteration 26456/35720 Training loss: 0.7326 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 26457/35720 Training loss: 0.7326 0.2112 sec/batch\n",
      "Epoch 15/20  Iteration 26458/35720 Training loss: 0.7326 0.2272 sec/batch\n",
      "Epoch 15/20  Iteration 26459/35720 Training loss: 0.7326 0.2177 sec/batch\n",
      "Epoch 15/20  Iteration 26460/35720 Training loss: 0.7326 0.2257 sec/batch\n",
      "Epoch 15/20  Iteration 26461/35720 Training loss: 0.7327 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 26462/35720 Training loss: 0.7327 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 26463/35720 Training loss: 0.7327 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 26464/35720 Training loss: 0.7327 0.2309 sec/batch\n",
      "Epoch 15/20  Iteration 26465/35720 Training loss: 0.7327 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 26466/35720 Training loss: 0.7328 0.2195 sec/batch\n",
      "Epoch 15/20  Iteration 26467/35720 Training loss: 0.7328 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 26468/35720 Training loss: 0.7327 0.2158 sec/batch\n",
      "Epoch 15/20  Iteration 26469/35720 Training loss: 0.7327 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 26470/35720 Training loss: 0.7327 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 26471/35720 Training loss: 0.7327 0.2224 sec/batch\n",
      "Epoch 15/20  Iteration 26472/35720 Training loss: 0.7327 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 26473/35720 Training loss: 0.7327 0.2072 sec/batch\n",
      "Epoch 15/20  Iteration 26474/35720 Training loss: 0.7327 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 26475/35720 Training loss: 0.7327 0.2204 sec/batch\n",
      "Epoch 15/20  Iteration 26476/35720 Training loss: 0.7326 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 26477/35720 Training loss: 0.7326 0.2275 sec/batch\n",
      "Epoch 15/20  Iteration 26478/35720 Training loss: 0.7326 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 26479/35720 Training loss: 0.7325 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26480/35720 Training loss: 0.7324 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 26481/35720 Training loss: 0.7324 0.2276 sec/batch\n",
      "Epoch 15/20  Iteration 26482/35720 Training loss: 0.7323 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 26483/35720 Training loss: 0.7323 0.2210 sec/batch\n",
      "Epoch 15/20  Iteration 26484/35720 Training loss: 0.7323 0.2332 sec/batch\n",
      "Epoch 15/20  Iteration 26485/35720 Training loss: 0.7323 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26486/35720 Training loss: 0.7323 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 26487/35720 Training loss: 0.7323 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 26488/35720 Training loss: 0.7322 0.2265 sec/batch\n",
      "Epoch 15/20  Iteration 26489/35720 Training loss: 0.7322 0.2078 sec/batch\n",
      "Epoch 15/20  Iteration 26490/35720 Training loss: 0.7322 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 26491/35720 Training loss: 0.7322 0.2170 sec/batch\n",
      "Epoch 15/20  Iteration 26492/35720 Training loss: 0.7322 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 26493/35720 Training loss: 0.7322 0.2393 sec/batch\n",
      "Epoch 15/20  Iteration 26494/35720 Training loss: 0.7322 0.2271 sec/batch\n",
      "Epoch 15/20  Iteration 26495/35720 Training loss: 0.7322 0.2128 sec/batch\n",
      "Epoch 15/20  Iteration 26496/35720 Training loss: 0.7321 0.2220 sec/batch\n",
      "Epoch 15/20  Iteration 26497/35720 Training loss: 0.7321 0.2280 sec/batch\n",
      "Epoch 15/20  Iteration 26498/35720 Training loss: 0.7321 0.2136 sec/batch\n",
      "Epoch 15/20  Iteration 26499/35720 Training loss: 0.7321 0.2175 sec/batch\n",
      "Epoch 15/20  Iteration 26500/35720 Training loss: 0.7321 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 26501/35720 Training loss: 0.7321 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 26502/35720 Training loss: 0.7321 0.2148 sec/batch\n",
      "Epoch 15/20  Iteration 26503/35720 Training loss: 0.7321 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 26504/35720 Training loss: 0.7320 0.2140 sec/batch\n",
      "Epoch 15/20  Iteration 26505/35720 Training loss: 0.7320 0.2121 sec/batch\n",
      "Epoch 15/20  Iteration 26506/35720 Training loss: 0.7320 0.2224 sec/batch\n",
      "Epoch 15/20  Iteration 26507/35720 Training loss: 0.7319 0.2164 sec/batch\n",
      "Epoch 15/20  Iteration 26508/35720 Training loss: 0.7320 0.2235 sec/batch\n",
      "Epoch 15/20  Iteration 26509/35720 Training loss: 0.7320 0.2234 sec/batch\n",
      "Epoch 15/20  Iteration 26510/35720 Training loss: 0.7320 0.2279 sec/batch\n",
      "Epoch 15/20  Iteration 26511/35720 Training loss: 0.7320 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26512/35720 Training loss: 0.7319 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 26513/35720 Training loss: 0.7320 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 26514/35720 Training loss: 0.7319 0.2190 sec/batch\n",
      "Epoch 15/20  Iteration 26515/35720 Training loss: 0.7320 0.2444 sec/batch\n",
      "Epoch 15/20  Iteration 26516/35720 Training loss: 0.7319 0.2126 sec/batch\n",
      "Epoch 15/20  Iteration 26517/35720 Training loss: 0.7319 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26518/35720 Training loss: 0.7319 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 26519/35720 Training loss: 0.7319 0.2345 sec/batch\n",
      "Epoch 15/20  Iteration 26520/35720 Training loss: 0.7319 0.2125 sec/batch\n",
      "Epoch 15/20  Iteration 26521/35720 Training loss: 0.7319 0.2391 sec/batch\n",
      "Epoch 15/20  Iteration 26522/35720 Training loss: 0.7319 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 26523/35720 Training loss: 0.7319 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 26524/35720 Training loss: 0.7319 0.2170 sec/batch\n",
      "Epoch 15/20  Iteration 26525/35720 Training loss: 0.7319 0.2277 sec/batch\n",
      "Epoch 15/20  Iteration 26526/35720 Training loss: 0.7318 0.2369 sec/batch\n",
      "Epoch 15/20  Iteration 26527/35720 Training loss: 0.7319 0.2064 sec/batch\n",
      "Epoch 15/20  Iteration 26528/35720 Training loss: 0.7319 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 26529/35720 Training loss: 0.7319 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 26530/35720 Training loss: 0.7319 0.2124 sec/batch\n",
      "Epoch 15/20  Iteration 26531/35720 Training loss: 0.7319 0.2073 sec/batch\n",
      "Epoch 15/20  Iteration 26532/35720 Training loss: 0.7319 0.2208 sec/batch\n",
      "Epoch 15/20  Iteration 26533/35720 Training loss: 0.7319 0.2223 sec/batch\n",
      "Epoch 15/20  Iteration 26534/35720 Training loss: 0.7319 0.2062 sec/batch\n",
      "Epoch 15/20  Iteration 26535/35720 Training loss: 0.7319 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 26536/35720 Training loss: 0.7319 0.2177 sec/batch\n",
      "Epoch 15/20  Iteration 26537/35720 Training loss: 0.7319 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26538/35720 Training loss: 0.7319 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 26539/35720 Training loss: 0.7319 0.2227 sec/batch\n",
      "Epoch 15/20  Iteration 26540/35720 Training loss: 0.7319 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26541/35720 Training loss: 0.7319 0.2150 sec/batch\n",
      "Epoch 15/20  Iteration 26542/35720 Training loss: 0.7319 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 26543/35720 Training loss: 0.7319 0.2371 sec/batch\n",
      "Epoch 15/20  Iteration 26544/35720 Training loss: 0.7318 0.2071 sec/batch\n",
      "Epoch 15/20  Iteration 26545/35720 Training loss: 0.7318 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26546/35720 Training loss: 0.7318 0.2199 sec/batch\n",
      "Epoch 15/20  Iteration 26547/35720 Training loss: 0.7317 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 26548/35720 Training loss: 0.7317 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 26549/35720 Training loss: 0.7317 0.2312 sec/batch\n",
      "Epoch 15/20  Iteration 26550/35720 Training loss: 0.7317 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 26551/35720 Training loss: 0.7317 0.2170 sec/batch\n",
      "Epoch 15/20  Iteration 26552/35720 Training loss: 0.7317 0.2249 sec/batch\n",
      "Epoch 15/20  Iteration 26553/35720 Training loss: 0.7317 0.2106 sec/batch\n",
      "Epoch 15/20  Iteration 26554/35720 Training loss: 0.7317 0.2244 sec/batch\n",
      "Epoch 15/20  Iteration 26555/35720 Training loss: 0.7317 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 26556/35720 Training loss: 0.7316 0.2195 sec/batch\n",
      "Epoch 15/20  Iteration 26557/35720 Training loss: 0.7316 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 26558/35720 Training loss: 0.7316 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 26559/35720 Training loss: 0.7315 0.2174 sec/batch\n",
      "Epoch 15/20  Iteration 26560/35720 Training loss: 0.7315 0.2154 sec/batch\n",
      "Epoch 15/20  Iteration 26561/35720 Training loss: 0.7315 0.2103 sec/batch\n",
      "Epoch 15/20  Iteration 26562/35720 Training loss: 0.7315 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 26563/35720 Training loss: 0.7314 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 26564/35720 Training loss: 0.7314 0.2111 sec/batch\n",
      "Epoch 15/20  Iteration 26565/35720 Training loss: 0.7313 0.2275 sec/batch\n",
      "Epoch 15/20  Iteration 26566/35720 Training loss: 0.7313 0.2121 sec/batch\n",
      "Epoch 15/20  Iteration 26567/35720 Training loss: 0.7313 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 26568/35720 Training loss: 0.7313 0.2192 sec/batch\n",
      "Epoch 15/20  Iteration 26569/35720 Training loss: 0.7313 0.2201 sec/batch\n",
      "Epoch 15/20  Iteration 26570/35720 Training loss: 0.7312 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26571/35720 Training loss: 0.7312 0.2153 sec/batch\n",
      "Epoch 15/20  Iteration 26572/35720 Training loss: 0.7312 0.2135 sec/batch\n",
      "Epoch 15/20  Iteration 26573/35720 Training loss: 0.7312 0.2196 sec/batch\n",
      "Epoch 15/20  Iteration 26574/35720 Training loss: 0.7312 0.2270 sec/batch\n",
      "Epoch 15/20  Iteration 26575/35720 Training loss: 0.7312 0.2108 sec/batch\n",
      "Epoch 15/20  Iteration 26576/35720 Training loss: 0.7312 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 26577/35720 Training loss: 0.7311 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26578/35720 Training loss: 0.7311 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26579/35720 Training loss: 0.7311 0.2178 sec/batch\n",
      "Epoch 15/20  Iteration 26580/35720 Training loss: 0.7311 0.2186 sec/batch\n",
      "Epoch 15/20  Iteration 26581/35720 Training loss: 0.7310 0.2139 sec/batch\n",
      "Epoch 15/20  Iteration 26582/35720 Training loss: 0.7310 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 26583/35720 Training loss: 0.7311 0.2273 sec/batch\n",
      "Epoch 15/20  Iteration 26584/35720 Training loss: 0.7310 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 26585/35720 Training loss: 0.7311 0.2093 sec/batch\n",
      "Epoch 15/20  Iteration 26586/35720 Training loss: 0.7310 0.2239 sec/batch\n",
      "Epoch 15/20  Iteration 26587/35720 Training loss: 0.7310 0.2079 sec/batch\n",
      "Epoch 15/20  Iteration 26588/35720 Training loss: 0.7310 0.2216 sec/batch\n",
      "Epoch 15/20  Iteration 26589/35720 Training loss: 0.7310 0.2229 sec/batch\n",
      "Epoch 15/20  Iteration 26590/35720 Training loss: 0.7310 0.2319 sec/batch\n",
      "Epoch 15/20  Iteration 26591/35720 Training loss: 0.7309 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 26592/35720 Training loss: 0.7309 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26593/35720 Training loss: 0.7308 0.2234 sec/batch\n",
      "Epoch 15/20  Iteration 26594/35720 Training loss: 0.7308 0.2061 sec/batch\n",
      "Epoch 15/20  Iteration 26595/35720 Training loss: 0.7308 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 26596/35720 Training loss: 0.7308 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26597/35720 Training loss: 0.7308 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 26598/35720 Training loss: 0.7308 0.2266 sec/batch\n",
      "Epoch 15/20  Iteration 26599/35720 Training loss: 0.7307 0.2431 sec/batch\n",
      "Epoch 15/20  Iteration 26600/35720 Training loss: 0.7307 0.2827 sec/batch\n",
      "Validation loss: 1.56189 Saving checkpoint!\n",
      "Epoch 15/20  Iteration 26601/35720 Training loss: 0.7309 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26602/35720 Training loss: 0.7309 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 26603/35720 Training loss: 0.7309 0.2163 sec/batch\n",
      "Epoch 15/20  Iteration 26604/35720 Training loss: 0.7309 0.2084 sec/batch\n",
      "Epoch 15/20  Iteration 26605/35720 Training loss: 0.7308 0.2076 sec/batch\n",
      "Epoch 15/20  Iteration 26606/35720 Training loss: 0.7308 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 26607/35720 Training loss: 0.7308 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 26608/35720 Training loss: 0.7308 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 26609/35720 Training loss: 0.7308 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 26610/35720 Training loss: 0.7308 0.2270 sec/batch\n",
      "Epoch 15/20  Iteration 26611/35720 Training loss: 0.7307 0.2231 sec/batch\n",
      "Epoch 15/20  Iteration 26612/35720 Training loss: 0.7307 0.2171 sec/batch\n",
      "Epoch 15/20  Iteration 26613/35720 Training loss: 0.7307 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 26614/35720 Training loss: 0.7307 0.2144 sec/batch\n",
      "Epoch 15/20  Iteration 26615/35720 Training loss: 0.7306 0.2143 sec/batch\n",
      "Epoch 15/20  Iteration 26616/35720 Training loss: 0.7306 0.2187 sec/batch\n",
      "Epoch 15/20  Iteration 26617/35720 Training loss: 0.7306 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26618/35720 Training loss: 0.7306 0.2232 sec/batch\n",
      "Epoch 15/20  Iteration 26619/35720 Training loss: 0.7306 0.2181 sec/batch\n",
      "Epoch 15/20  Iteration 26620/35720 Training loss: 0.7306 0.2134 sec/batch\n",
      "Epoch 15/20  Iteration 26621/35720 Training loss: 0.7306 0.2244 sec/batch\n",
      "Epoch 15/20  Iteration 26622/35720 Training loss: 0.7306 0.2085 sec/batch\n",
      "Epoch 15/20  Iteration 26623/35720 Training loss: 0.7305 0.2184 sec/batch\n",
      "Epoch 15/20  Iteration 26624/35720 Training loss: 0.7305 0.2258 sec/batch\n",
      "Epoch 15/20  Iteration 26625/35720 Training loss: 0.7305 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 26626/35720 Training loss: 0.7305 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 26627/35720 Training loss: 0.7305 0.2111 sec/batch\n",
      "Epoch 15/20  Iteration 26628/35720 Training loss: 0.7305 0.2126 sec/batch\n",
      "Epoch 15/20  Iteration 26629/35720 Training loss: 0.7305 0.2257 sec/batch\n",
      "Epoch 15/20  Iteration 26630/35720 Training loss: 0.7305 0.2128 sec/batch\n",
      "Epoch 15/20  Iteration 26631/35720 Training loss: 0.7305 0.2208 sec/batch\n",
      "Epoch 15/20  Iteration 26632/35720 Training loss: 0.7305 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 26633/35720 Training loss: 0.7304 0.2113 sec/batch\n",
      "Epoch 15/20  Iteration 26634/35720 Training loss: 0.7304 0.2132 sec/batch\n",
      "Epoch 15/20  Iteration 26635/35720 Training loss: 0.7304 0.2086 sec/batch\n",
      "Epoch 15/20  Iteration 26636/35720 Training loss: 0.7304 0.2178 sec/batch\n",
      "Epoch 15/20  Iteration 26637/35720 Training loss: 0.7304 0.2066 sec/batch\n",
      "Epoch 15/20  Iteration 26638/35720 Training loss: 0.7304 0.2059 sec/batch\n",
      "Epoch 15/20  Iteration 26639/35720 Training loss: 0.7304 0.2163 sec/batch\n",
      "Epoch 15/20  Iteration 26640/35720 Training loss: 0.7304 0.2224 sec/batch\n",
      "Epoch 15/20  Iteration 26641/35720 Training loss: 0.7304 0.2341 sec/batch\n",
      "Epoch 15/20  Iteration 26642/35720 Training loss: 0.7303 0.2237 sec/batch\n",
      "Epoch 15/20  Iteration 26643/35720 Training loss: 0.7303 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 26644/35720 Training loss: 0.7303 0.2112 sec/batch\n",
      "Epoch 15/20  Iteration 26645/35720 Training loss: 0.7303 0.2151 sec/batch\n",
      "Epoch 15/20  Iteration 26646/35720 Training loss: 0.7303 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 26647/35720 Training loss: 0.7303 0.2227 sec/batch\n",
      "Epoch 15/20  Iteration 26648/35720 Training loss: 0.7303 0.2108 sec/batch\n",
      "Epoch 15/20  Iteration 26649/35720 Training loss: 0.7302 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 26650/35720 Training loss: 0.7303 0.2162 sec/batch\n",
      "Epoch 15/20  Iteration 26651/35720 Training loss: 0.7303 0.2211 sec/batch\n",
      "Epoch 15/20  Iteration 26652/35720 Training loss: 0.7303 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26653/35720 Training loss: 0.7303 0.2166 sec/batch\n",
      "Epoch 15/20  Iteration 26654/35720 Training loss: 0.7303 0.2069 sec/batch\n",
      "Epoch 15/20  Iteration 26655/35720 Training loss: 0.7303 0.2187 sec/batch\n",
      "Epoch 15/20  Iteration 26656/35720 Training loss: 0.7303 0.2237 sec/batch\n",
      "Epoch 15/20  Iteration 26657/35720 Training loss: 0.7303 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 26658/35720 Training loss: 0.7303 0.2259 sec/batch\n",
      "Epoch 15/20  Iteration 26659/35720 Training loss: 0.7303 0.2060 sec/batch\n",
      "Epoch 15/20  Iteration 26660/35720 Training loss: 0.7303 0.2100 sec/batch\n",
      "Epoch 15/20  Iteration 26661/35720 Training loss: 0.7303 0.2102 sec/batch\n",
      "Epoch 15/20  Iteration 26662/35720 Training loss: 0.7303 0.2149 sec/batch\n",
      "Epoch 15/20  Iteration 26663/35720 Training loss: 0.7303 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 26664/35720 Training loss: 0.7303 0.2187 sec/batch\n",
      "Epoch 15/20  Iteration 26665/35720 Training loss: 0.7303 0.2213 sec/batch\n",
      "Epoch 15/20  Iteration 26666/35720 Training loss: 0.7303 0.2057 sec/batch\n",
      "Epoch 15/20  Iteration 26667/35720 Training loss: 0.7303 0.2206 sec/batch\n",
      "Epoch 15/20  Iteration 26668/35720 Training loss: 0.7303 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 26669/35720 Training loss: 0.7303 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26670/35720 Training loss: 0.7303 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 26671/35720 Training loss: 0.7303 0.2114 sec/batch\n",
      "Epoch 15/20  Iteration 26672/35720 Training loss: 0.7303 0.2139 sec/batch\n",
      "Epoch 15/20  Iteration 26673/35720 Training loss: 0.7303 0.2172 sec/batch\n",
      "Epoch 15/20  Iteration 26674/35720 Training loss: 0.7303 0.2089 sec/batch\n",
      "Epoch 15/20  Iteration 26675/35720 Training loss: 0.7304 0.2233 sec/batch\n",
      "Epoch 15/20  Iteration 26676/35720 Training loss: 0.7304 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26677/35720 Training loss: 0.7304 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 26678/35720 Training loss: 0.7304 0.2279 sec/batch\n",
      "Epoch 15/20  Iteration 26679/35720 Training loss: 0.7304 0.2268 sec/batch\n",
      "Epoch 15/20  Iteration 26680/35720 Training loss: 0.7304 0.2277 sec/batch\n",
      "Epoch 15/20  Iteration 26681/35720 Training loss: 0.7303 0.2215 sec/batch\n",
      "Epoch 15/20  Iteration 26682/35720 Training loss: 0.7303 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 26683/35720 Training loss: 0.7303 0.2202 sec/batch\n",
      "Epoch 15/20  Iteration 26684/35720 Training loss: 0.7303 0.2235 sec/batch\n",
      "Epoch 15/20  Iteration 26685/35720 Training loss: 0.7302 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26686/35720 Training loss: 0.7302 0.2142 sec/batch\n",
      "Epoch 15/20  Iteration 26687/35720 Training loss: 0.7302 0.2056 sec/batch\n",
      "Epoch 15/20  Iteration 26688/35720 Training loss: 0.7302 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26689/35720 Training loss: 0.7302 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26690/35720 Training loss: 0.7302 0.2240 sec/batch\n",
      "Epoch 15/20  Iteration 26691/35720 Training loss: 0.7302 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 26692/35720 Training loss: 0.7302 0.2198 sec/batch\n",
      "Epoch 15/20  Iteration 26693/35720 Training loss: 0.7302 0.2171 sec/batch\n",
      "Epoch 15/20  Iteration 26694/35720 Training loss: 0.7302 0.2065 sec/batch\n",
      "Epoch 15/20  Iteration 26695/35720 Training loss: 0.7301 0.2104 sec/batch\n",
      "Epoch 15/20  Iteration 26696/35720 Training loss: 0.7301 0.2157 sec/batch\n",
      "Epoch 15/20  Iteration 26697/35720 Training loss: 0.7301 0.2359 sec/batch\n",
      "Epoch 15/20  Iteration 26698/35720 Training loss: 0.7301 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 26699/35720 Training loss: 0.7301 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 26700/35720 Training loss: 0.7301 0.2143 sec/batch\n",
      "Epoch 15/20  Iteration 26701/35720 Training loss: 0.7301 0.2139 sec/batch\n",
      "Epoch 15/20  Iteration 26702/35720 Training loss: 0.7301 0.2226 sec/batch\n",
      "Epoch 15/20  Iteration 26703/35720 Training loss: 0.7301 0.2190 sec/batch\n",
      "Epoch 15/20  Iteration 26704/35720 Training loss: 0.7301 0.2063 sec/batch\n",
      "Epoch 15/20  Iteration 26705/35720 Training loss: 0.7301 0.2098 sec/batch\n",
      "Epoch 15/20  Iteration 26706/35720 Training loss: 0.7301 0.2155 sec/batch\n",
      "Epoch 15/20  Iteration 26707/35720 Training loss: 0.7301 0.2269 sec/batch\n",
      "Epoch 15/20  Iteration 26708/35720 Training loss: 0.7300 0.2120 sec/batch\n",
      "Epoch 15/20  Iteration 26709/35720 Training loss: 0.7300 0.2205 sec/batch\n",
      "Epoch 15/20  Iteration 26710/35720 Training loss: 0.7300 0.2169 sec/batch\n",
      "Epoch 15/20  Iteration 26711/35720 Training loss: 0.7300 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 26712/35720 Training loss: 0.7300 0.2155 sec/batch\n",
      "Epoch 15/20  Iteration 26713/35720 Training loss: 0.7300 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 26714/35720 Training loss: 0.7301 0.2177 sec/batch\n",
      "Epoch 15/20  Iteration 26715/35720 Training loss: 0.7301 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 26716/35720 Training loss: 0.7301 0.2080 sec/batch\n",
      "Epoch 15/20  Iteration 26717/35720 Training loss: 0.7302 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 26718/35720 Training loss: 0.7301 0.2201 sec/batch\n",
      "Epoch 15/20  Iteration 26719/35720 Training loss: 0.7302 0.2129 sec/batch\n",
      "Epoch 15/20  Iteration 26720/35720 Training loss: 0.7301 0.2119 sec/batch\n",
      "Epoch 15/20  Iteration 26721/35720 Training loss: 0.7301 0.2150 sec/batch\n",
      "Epoch 15/20  Iteration 26722/35720 Training loss: 0.7301 0.2145 sec/batch\n",
      "Epoch 15/20  Iteration 26723/35720 Training loss: 0.7301 0.2156 sec/batch\n",
      "Epoch 15/20  Iteration 26724/35720 Training loss: 0.7301 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 26725/35720 Training loss: 0.7301 0.2397 sec/batch\n",
      "Epoch 15/20  Iteration 26726/35720 Training loss: 0.7301 0.2082 sec/batch\n",
      "Epoch 15/20  Iteration 26727/35720 Training loss: 0.7301 0.2116 sec/batch\n",
      "Epoch 15/20  Iteration 26728/35720 Training loss: 0.7301 0.2152 sec/batch\n",
      "Epoch 15/20  Iteration 26729/35720 Training loss: 0.7300 0.2323 sec/batch\n",
      "Epoch 15/20  Iteration 26730/35720 Training loss: 0.7301 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26731/35720 Training loss: 0.7301 0.2223 sec/batch\n",
      "Epoch 15/20  Iteration 26732/35720 Training loss: 0.7301 0.2118 sec/batch\n",
      "Epoch 15/20  Iteration 26733/35720 Training loss: 0.7301 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26734/35720 Training loss: 0.7301 0.2180 sec/batch\n",
      "Epoch 15/20  Iteration 26735/35720 Training loss: 0.7301 0.2101 sec/batch\n",
      "Epoch 15/20  Iteration 26736/35720 Training loss: 0.7301 0.2182 sec/batch\n",
      "Epoch 15/20  Iteration 26737/35720 Training loss: 0.7301 0.2139 sec/batch\n",
      "Epoch 15/20  Iteration 26738/35720 Training loss: 0.7301 0.2072 sec/batch\n",
      "Epoch 15/20  Iteration 26739/35720 Training loss: 0.7301 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 26740/35720 Training loss: 0.7302 0.2173 sec/batch\n",
      "Epoch 15/20  Iteration 26741/35720 Training loss: 0.7302 0.2130 sec/batch\n",
      "Epoch 15/20  Iteration 26742/35720 Training loss: 0.7302 0.2164 sec/batch\n",
      "Epoch 15/20  Iteration 26743/35720 Training loss: 0.7302 0.2119 sec/batch\n",
      "Epoch 15/20  Iteration 26744/35720 Training loss: 0.7302 0.2269 sec/batch\n",
      "Epoch 15/20  Iteration 26745/35720 Training loss: 0.7302 0.2151 sec/batch\n",
      "Epoch 15/20  Iteration 26746/35720 Training loss: 0.7302 0.2141 sec/batch\n",
      "Epoch 15/20  Iteration 26747/35720 Training loss: 0.7301 0.2167 sec/batch\n",
      "Epoch 15/20  Iteration 26748/35720 Training loss: 0.7302 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 26749/35720 Training loss: 0.7302 0.2176 sec/batch\n",
      "Epoch 15/20  Iteration 26750/35720 Training loss: 0.7302 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 26751/35720 Training loss: 0.7302 0.2280 sec/batch\n",
      "Epoch 15/20  Iteration 26752/35720 Training loss: 0.7302 0.2219 sec/batch\n",
      "Epoch 15/20  Iteration 26753/35720 Training loss: 0.7302 0.2222 sec/batch\n",
      "Epoch 15/20  Iteration 26754/35720 Training loss: 0.7302 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 26755/35720 Training loss: 0.7302 0.2097 sec/batch\n",
      "Epoch 15/20  Iteration 26756/35720 Training loss: 0.7302 0.2095 sec/batch\n",
      "Epoch 15/20  Iteration 26757/35720 Training loss: 0.7302 0.2316 sec/batch\n",
      "Epoch 15/20  Iteration 26758/35720 Training loss: 0.7302 0.2077 sec/batch\n",
      "Epoch 15/20  Iteration 26759/35720 Training loss: 0.7302 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 26760/35720 Training loss: 0.7302 0.2128 sec/batch\n",
      "Epoch 15/20  Iteration 26761/35720 Training loss: 0.7302 0.2123 sec/batch\n",
      "Epoch 15/20  Iteration 26762/35720 Training loss: 0.7302 0.2240 sec/batch\n",
      "Epoch 15/20  Iteration 26763/35720 Training loss: 0.7302 0.2094 sec/batch\n",
      "Epoch 15/20  Iteration 26764/35720 Training loss: 0.7302 0.2198 sec/batch\n",
      "Epoch 15/20  Iteration 26765/35720 Training loss: 0.7302 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26766/35720 Training loss: 0.7302 0.2083 sec/batch\n",
      "Epoch 15/20  Iteration 26767/35720 Training loss: 0.7302 0.2088 sec/batch\n",
      "Epoch 15/20  Iteration 26768/35720 Training loss: 0.7302 0.2276 sec/batch\n",
      "Epoch 15/20  Iteration 26769/35720 Training loss: 0.7301 0.2258 sec/batch\n",
      "Epoch 15/20  Iteration 26770/35720 Training loss: 0.7301 0.2122 sec/batch\n",
      "Epoch 15/20  Iteration 26771/35720 Training loss: 0.7302 0.2164 sec/batch\n",
      "Epoch 15/20  Iteration 26772/35720 Training loss: 0.7302 0.2096 sec/batch\n",
      "Epoch 15/20  Iteration 26773/35720 Training loss: 0.7301 0.2117 sec/batch\n",
      "Epoch 15/20  Iteration 26774/35720 Training loss: 0.7302 0.2168 sec/batch\n",
      "Epoch 15/20  Iteration 26775/35720 Training loss: 0.7302 0.2057 sec/batch\n",
      "Epoch 15/20  Iteration 26776/35720 Training loss: 0.7301 0.2147 sec/batch\n",
      "Epoch 15/20  Iteration 26777/35720 Training loss: 0.7301 0.2091 sec/batch\n",
      "Epoch 15/20  Iteration 26778/35720 Training loss: 0.7301 0.2090 sec/batch\n",
      "Epoch 15/20  Iteration 26779/35720 Training loss: 0.7301 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 26780/35720 Training loss: 0.7301 0.2107 sec/batch\n",
      "Epoch 15/20  Iteration 26781/35720 Training loss: 0.7301 0.2151 sec/batch\n",
      "Epoch 15/20  Iteration 26782/35720 Training loss: 0.7301 0.2068 sec/batch\n",
      "Epoch 15/20  Iteration 26783/35720 Training loss: 0.7300 0.2105 sec/batch\n",
      "Epoch 15/20  Iteration 26784/35720 Training loss: 0.7300 0.2140 sec/batch\n",
      "Epoch 15/20  Iteration 26785/35720 Training loss: 0.7300 0.2137 sec/batch\n",
      "Epoch 15/20  Iteration 26786/35720 Training loss: 0.7300 0.2161 sec/batch\n",
      "Epoch 15/20  Iteration 26787/35720 Training loss: 0.7300 0.2189 sec/batch\n",
      "Epoch 15/20  Iteration 26788/35720 Training loss: 0.7299 0.2203 sec/batch\n",
      "Epoch 15/20  Iteration 26789/35720 Training loss: 0.7299 0.2092 sec/batch\n",
      "Epoch 15/20  Iteration 26790/35720 Training loss: 0.7299 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 26791/35720 Training loss: 0.7887 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 26792/35720 Training loss: 0.7625 0.2231 sec/batch\n",
      "Epoch 16/20  Iteration 26793/35720 Training loss: 0.7502 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 26794/35720 Training loss: 0.7449 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 26795/35720 Training loss: 0.7470 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 26796/35720 Training loss: 0.7354 0.2266 sec/batch\n",
      "Epoch 16/20  Iteration 26797/35720 Training loss: 0.7341 0.2194 sec/batch\n",
      "Epoch 16/20  Iteration 26798/35720 Training loss: 0.7291 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 26799/35720 Training loss: 0.7254 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 26800/35720 Training loss: 0.7278 0.2061 sec/batch\n",
      "Validation loss: 1.55064 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 26801/35720 Training loss: 0.7700 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 26802/35720 Training loss: 0.7641 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 26803/35720 Training loss: 0.7613 0.2238 sec/batch\n",
      "Epoch 16/20  Iteration 26804/35720 Training loss: 0.7625 0.2207 sec/batch\n",
      "Epoch 16/20  Iteration 26805/35720 Training loss: 0.7621 0.2129 sec/batch\n",
      "Epoch 16/20  Iteration 26806/35720 Training loss: 0.7601 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 26807/35720 Training loss: 0.7584 0.2140 sec/batch\n",
      "Epoch 16/20  Iteration 26808/35720 Training loss: 0.7533 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 26809/35720 Training loss: 0.7490 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 26810/35720 Training loss: 0.7480 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 26811/35720 Training loss: 0.7486 0.2272 sec/batch\n",
      "Epoch 16/20  Iteration 26812/35720 Training loss: 0.7458 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 26813/35720 Training loss: 0.7439 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 26814/35720 Training loss: 0.7449 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 26815/35720 Training loss: 0.7450 0.2079 sec/batch\n",
      "Epoch 16/20  Iteration 26816/35720 Training loss: 0.7447 0.2205 sec/batch\n",
      "Epoch 16/20  Iteration 26817/35720 Training loss: 0.7454 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 26818/35720 Training loss: 0.7455 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 26819/35720 Training loss: 0.7436 0.2217 sec/batch\n",
      "Epoch 16/20  Iteration 26820/35720 Training loss: 0.7440 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 26821/35720 Training loss: 0.7450 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 26822/35720 Training loss: 0.7435 0.2134 sec/batch\n",
      "Epoch 16/20  Iteration 26823/35720 Training loss: 0.7453 0.2232 sec/batch\n",
      "Epoch 16/20  Iteration 26824/35720 Training loss: 0.7467 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 26825/35720 Training loss: 0.7492 0.2236 sec/batch\n",
      "Epoch 16/20  Iteration 26826/35720 Training loss: 0.7491 0.2168 sec/batch\n",
      "Epoch 16/20  Iteration 26827/35720 Training loss: 0.7488 0.2055 sec/batch\n",
      "Epoch 16/20  Iteration 26828/35720 Training loss: 0.7483 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 26829/35720 Training loss: 0.7471 0.2359 sec/batch\n",
      "Epoch 16/20  Iteration 26830/35720 Training loss: 0.7475 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 26831/35720 Training loss: 0.7469 0.2120 sec/batch\n",
      "Epoch 16/20  Iteration 26832/35720 Training loss: 0.7456 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 26833/35720 Training loss: 0.7435 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 26834/35720 Training loss: 0.7423 0.2334 sec/batch\n",
      "Epoch 16/20  Iteration 26835/35720 Training loss: 0.7417 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 26836/35720 Training loss: 0.7408 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 26837/35720 Training loss: 0.7397 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 26838/35720 Training loss: 0.7389 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 26839/35720 Training loss: 0.7374 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 26840/35720 Training loss: 0.7366 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 26841/35720 Training loss: 0.7369 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 26842/35720 Training loss: 0.7374 0.2203 sec/batch\n",
      "Epoch 16/20  Iteration 26843/35720 Training loss: 0.7378 0.2171 sec/batch\n",
      "Epoch 16/20  Iteration 26844/35720 Training loss: 0.7362 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 26845/35720 Training loss: 0.7344 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 26846/35720 Training loss: 0.7335 0.2122 sec/batch\n",
      "Epoch 16/20  Iteration 26847/35720 Training loss: 0.7330 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 26848/35720 Training loss: 0.7322 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 26849/35720 Training loss: 0.7317 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 26850/35720 Training loss: 0.7308 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 26851/35720 Training loss: 0.7302 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 26852/35720 Training loss: 0.7286 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 26853/35720 Training loss: 0.7292 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 26854/35720 Training loss: 0.7292 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 26855/35720 Training loss: 0.7300 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 26856/35720 Training loss: 0.7299 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 26857/35720 Training loss: 0.7295 0.2266 sec/batch\n",
      "Epoch 16/20  Iteration 26858/35720 Training loss: 0.7290 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 26859/35720 Training loss: 0.7298 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 26860/35720 Training loss: 0.7296 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 26861/35720 Training loss: 0.7296 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 26862/35720 Training loss: 0.7300 0.2245 sec/batch\n",
      "Epoch 16/20  Iteration 26863/35720 Training loss: 0.7300 0.2211 sec/batch\n",
      "Epoch 16/20  Iteration 26864/35720 Training loss: 0.7299 0.2187 sec/batch\n",
      "Epoch 16/20  Iteration 26865/35720 Training loss: 0.7294 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 26866/35720 Training loss: 0.7294 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 26867/35720 Training loss: 0.7288 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 26868/35720 Training loss: 0.7293 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 26869/35720 Training loss: 0.7291 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 26870/35720 Training loss: 0.7297 0.2212 sec/batch\n",
      "Epoch 16/20  Iteration 26871/35720 Training loss: 0.7302 0.2325 sec/batch\n",
      "Epoch 16/20  Iteration 26872/35720 Training loss: 0.7301 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 26873/35720 Training loss: 0.7301 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 26874/35720 Training loss: 0.7300 0.2142 sec/batch\n",
      "Epoch 16/20  Iteration 26875/35720 Training loss: 0.7300 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 26876/35720 Training loss: 0.7301 0.2263 sec/batch\n",
      "Epoch 16/20  Iteration 26877/35720 Training loss: 0.7302 0.2057 sec/batch\n",
      "Epoch 16/20  Iteration 26878/35720 Training loss: 0.7301 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 26879/35720 Training loss: 0.7288 0.2301 sec/batch\n",
      "Epoch 16/20  Iteration 26880/35720 Training loss: 0.7283 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 26881/35720 Training loss: 0.7285 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 26882/35720 Training loss: 0.7279 0.2213 sec/batch\n",
      "Epoch 16/20  Iteration 26883/35720 Training loss: 0.7283 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 26884/35720 Training loss: 0.7286 0.2186 sec/batch\n",
      "Epoch 16/20  Iteration 26885/35720 Training loss: 0.7284 0.2166 sec/batch\n",
      "Epoch 16/20  Iteration 26886/35720 Training loss: 0.7278 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 26887/35720 Training loss: 0.7280 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 26888/35720 Training loss: 0.7280 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 26889/35720 Training loss: 0.7279 0.2276 sec/batch\n",
      "Epoch 16/20  Iteration 26890/35720 Training loss: 0.7274 0.2231 sec/batch\n",
      "Epoch 16/20  Iteration 26891/35720 Training loss: 0.7269 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 26892/35720 Training loss: 0.7269 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 26893/35720 Training loss: 0.7263 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 26894/35720 Training loss: 0.7258 0.2075 sec/batch\n",
      "Epoch 16/20  Iteration 26895/35720 Training loss: 0.7255 0.2121 sec/batch\n",
      "Epoch 16/20  Iteration 26896/35720 Training loss: 0.7254 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 26897/35720 Training loss: 0.7255 0.2186 sec/batch\n",
      "Epoch 16/20  Iteration 26898/35720 Training loss: 0.7257 0.2153 sec/batch\n",
      "Epoch 16/20  Iteration 26899/35720 Training loss: 0.7261 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 26900/35720 Training loss: 0.7258 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 26901/35720 Training loss: 0.7260 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 26902/35720 Training loss: 0.7261 0.2225 sec/batch\n",
      "Epoch 16/20  Iteration 26903/35720 Training loss: 0.7261 0.3077 sec/batch\n",
      "Epoch 16/20  Iteration 26904/35720 Training loss: 0.7261 0.2261 sec/batch\n",
      "Epoch 16/20  Iteration 26905/35720 Training loss: 0.7262 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 26906/35720 Training loss: 0.7260 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 26907/35720 Training loss: 0.7259 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 26908/35720 Training loss: 0.7258 0.2211 sec/batch\n",
      "Epoch 16/20  Iteration 26909/35720 Training loss: 0.7256 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 26910/35720 Training loss: 0.7262 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 26911/35720 Training loss: 0.7264 0.2121 sec/batch\n",
      "Epoch 16/20  Iteration 26912/35720 Training loss: 0.7259 0.2362 sec/batch\n",
      "Epoch 16/20  Iteration 26913/35720 Training loss: 0.7260 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 26914/35720 Training loss: 0.7264 0.2153 sec/batch\n",
      "Epoch 16/20  Iteration 26915/35720 Training loss: 0.7261 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 26916/35720 Training loss: 0.7262 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 26917/35720 Training loss: 0.7263 0.2127 sec/batch\n",
      "Epoch 16/20  Iteration 26918/35720 Training loss: 0.7263 0.2217 sec/batch\n",
      "Epoch 16/20  Iteration 26919/35720 Training loss: 0.7260 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 26920/35720 Training loss: 0.7261 0.2247 sec/batch\n",
      "Epoch 16/20  Iteration 26921/35720 Training loss: 0.7260 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 26922/35720 Training loss: 0.7258 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 26923/35720 Training loss: 0.7258 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 26924/35720 Training loss: 0.7259 0.2249 sec/batch\n",
      "Epoch 16/20  Iteration 26925/35720 Training loss: 0.7256 0.2269 sec/batch\n",
      "Epoch 16/20  Iteration 26926/35720 Training loss: 0.7253 0.2257 sec/batch\n",
      "Epoch 16/20  Iteration 26927/35720 Training loss: 0.7257 0.2071 sec/batch\n",
      "Epoch 16/20  Iteration 26928/35720 Training loss: 0.7259 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 26929/35720 Training loss: 0.7259 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 26930/35720 Training loss: 0.7260 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 26931/35720 Training loss: 0.7256 0.2281 sec/batch\n",
      "Epoch 16/20  Iteration 26932/35720 Training loss: 0.7251 0.2264 sec/batch\n",
      "Epoch 16/20  Iteration 26933/35720 Training loss: 0.7247 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 26934/35720 Training loss: 0.7242 0.2072 sec/batch\n",
      "Epoch 16/20  Iteration 26935/35720 Training loss: 0.7243 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 26936/35720 Training loss: 0.7246 0.2463 sec/batch\n",
      "Epoch 16/20  Iteration 26937/35720 Training loss: 0.7244 0.2053 sec/batch\n",
      "Epoch 16/20  Iteration 26938/35720 Training loss: 0.7242 0.2219 sec/batch\n",
      "Epoch 16/20  Iteration 26939/35720 Training loss: 0.7243 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 26940/35720 Training loss: 0.7236 0.2238 sec/batch\n",
      "Epoch 16/20  Iteration 26941/35720 Training loss: 0.7237 0.2301 sec/batch\n",
      "Epoch 16/20  Iteration 26942/35720 Training loss: 0.7239 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 26943/35720 Training loss: 0.7239 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 26944/35720 Training loss: 0.7240 0.2155 sec/batch\n",
      "Epoch 16/20  Iteration 26945/35720 Training loss: 0.7238 0.2249 sec/batch\n",
      "Epoch 16/20  Iteration 26946/35720 Training loss: 0.7240 0.2272 sec/batch\n",
      "Epoch 16/20  Iteration 26947/35720 Training loss: 0.7242 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 26948/35720 Training loss: 0.7243 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 26949/35720 Training loss: 0.7239 0.2082 sec/batch\n",
      "Epoch 16/20  Iteration 26950/35720 Training loss: 0.7239 0.2079 sec/batch\n",
      "Epoch 16/20  Iteration 26951/35720 Training loss: 0.7236 0.2304 sec/batch\n",
      "Epoch 16/20  Iteration 26952/35720 Training loss: 0.7238 0.2267 sec/batch\n",
      "Epoch 16/20  Iteration 26953/35720 Training loss: 0.7237 0.2316 sec/batch\n",
      "Epoch 16/20  Iteration 26954/35720 Training loss: 0.7237 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 26955/35720 Training loss: 0.7239 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 26956/35720 Training loss: 0.7239 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 26957/35720 Training loss: 0.7239 0.2143 sec/batch\n",
      "Epoch 16/20  Iteration 26958/35720 Training loss: 0.7242 0.2257 sec/batch\n",
      "Epoch 16/20  Iteration 26959/35720 Training loss: 0.7246 0.2187 sec/batch\n",
      "Epoch 16/20  Iteration 26960/35720 Training loss: 0.7249 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 26961/35720 Training loss: 0.7254 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 26962/35720 Training loss: 0.7257 0.2139 sec/batch\n",
      "Epoch 16/20  Iteration 26963/35720 Training loss: 0.7260 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 26964/35720 Training loss: 0.7263 0.2168 sec/batch\n",
      "Epoch 16/20  Iteration 26965/35720 Training loss: 0.7266 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 26966/35720 Training loss: 0.7266 0.2118 sec/batch\n",
      "Epoch 16/20  Iteration 26967/35720 Training loss: 0.7268 0.2169 sec/batch\n",
      "Epoch 16/20  Iteration 26968/35720 Training loss: 0.7266 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 26969/35720 Training loss: 0.7264 0.2185 sec/batch\n",
      "Epoch 16/20  Iteration 26970/35720 Training loss: 0.7260 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 26971/35720 Training loss: 0.7260 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 26972/35720 Training loss: 0.7260 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 26973/35720 Training loss: 0.7262 0.2199 sec/batch\n",
      "Epoch 16/20  Iteration 26974/35720 Training loss: 0.7263 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 26975/35720 Training loss: 0.7261 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 26976/35720 Training loss: 0.7260 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 26977/35720 Training loss: 0.7259 0.2219 sec/batch\n",
      "Epoch 16/20  Iteration 26978/35720 Training loss: 0.7260 0.2158 sec/batch\n",
      "Epoch 16/20  Iteration 26979/35720 Training loss: 0.7260 0.2166 sec/batch\n",
      "Epoch 16/20  Iteration 26980/35720 Training loss: 0.7259 0.2176 sec/batch\n",
      "Epoch 16/20  Iteration 26981/35720 Training loss: 0.7259 0.2220 sec/batch\n",
      "Epoch 16/20  Iteration 26982/35720 Training loss: 0.7263 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 26983/35720 Training loss: 0.7266 0.2140 sec/batch\n",
      "Epoch 16/20  Iteration 26984/35720 Training loss: 0.7268 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 26985/35720 Training loss: 0.7269 0.2228 sec/batch\n",
      "Epoch 16/20  Iteration 26986/35720 Training loss: 0.7270 0.2211 sec/batch\n",
      "Epoch 16/20  Iteration 26987/35720 Training loss: 0.7269 0.2199 sec/batch\n",
      "Epoch 16/20  Iteration 26988/35720 Training loss: 0.7269 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 26989/35720 Training loss: 0.7270 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 26990/35720 Training loss: 0.7271 0.2275 sec/batch\n",
      "Epoch 16/20  Iteration 26991/35720 Training loss: 0.7270 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 26992/35720 Training loss: 0.7271 0.2277 sec/batch\n",
      "Epoch 16/20  Iteration 26993/35720 Training loss: 0.7272 0.2198 sec/batch\n",
      "Epoch 16/20  Iteration 26994/35720 Training loss: 0.7270 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 26995/35720 Training loss: 0.7271 0.2211 sec/batch\n",
      "Epoch 16/20  Iteration 26996/35720 Training loss: 0.7272 0.2240 sec/batch\n",
      "Epoch 16/20  Iteration 26997/35720 Training loss: 0.7275 0.2328 sec/batch\n",
      "Epoch 16/20  Iteration 26998/35720 Training loss: 0.7278 0.2074 sec/batch\n",
      "Epoch 16/20  Iteration 26999/35720 Training loss: 0.7282 0.2111 sec/batch\n",
      "Epoch 16/20  Iteration 27000/35720 Training loss: 0.7281 0.2093 sec/batch\n",
      "Validation loss: 1.5661 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 27001/35720 Training loss: 0.7305 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27002/35720 Training loss: 0.7305 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 27003/35720 Training loss: 0.7303 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 27004/35720 Training loss: 0.7302 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 27005/35720 Training loss: 0.7302 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 27006/35720 Training loss: 0.7301 0.2221 sec/batch\n",
      "Epoch 16/20  Iteration 27007/35720 Training loss: 0.7300 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 27008/35720 Training loss: 0.7299 0.2262 sec/batch\n",
      "Epoch 16/20  Iteration 27009/35720 Training loss: 0.7299 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 27010/35720 Training loss: 0.7299 0.2184 sec/batch\n",
      "Epoch 16/20  Iteration 27011/35720 Training loss: 0.7300 0.2078 sec/batch\n",
      "Epoch 16/20  Iteration 27012/35720 Training loss: 0.7299 0.2162 sec/batch\n",
      "Epoch 16/20  Iteration 27013/35720 Training loss: 0.7303 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 27014/35720 Training loss: 0.7304 0.2169 sec/batch\n",
      "Epoch 16/20  Iteration 27015/35720 Training loss: 0.7305 0.2057 sec/batch\n",
      "Epoch 16/20  Iteration 27016/35720 Training loss: 0.7304 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 27017/35720 Training loss: 0.7303 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 27018/35720 Training loss: 0.7302 0.2343 sec/batch\n",
      "Epoch 16/20  Iteration 27019/35720 Training loss: 0.7300 0.2255 sec/batch\n",
      "Epoch 16/20  Iteration 27020/35720 Training loss: 0.7300 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 27021/35720 Training loss: 0.7304 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 27022/35720 Training loss: 0.7304 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 27023/35720 Training loss: 0.7306 0.2297 sec/batch\n",
      "Epoch 16/20  Iteration 27024/35720 Training loss: 0.7304 0.2219 sec/batch\n",
      "Epoch 16/20  Iteration 27025/35720 Training loss: 0.7304 0.2212 sec/batch\n",
      "Epoch 16/20  Iteration 27026/35720 Training loss: 0.7305 0.2079 sec/batch\n",
      "Epoch 16/20  Iteration 27027/35720 Training loss: 0.7306 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 27028/35720 Training loss: 0.7305 0.2151 sec/batch\n",
      "Epoch 16/20  Iteration 27029/35720 Training loss: 0.7305 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 27030/35720 Training loss: 0.7304 0.2414 sec/batch\n",
      "Epoch 16/20  Iteration 27031/35720 Training loss: 0.7302 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27032/35720 Training loss: 0.7301 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 27033/35720 Training loss: 0.7300 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 27034/35720 Training loss: 0.7299 0.2082 sec/batch\n",
      "Epoch 16/20  Iteration 27035/35720 Training loss: 0.7295 0.2287 sec/batch\n",
      "Epoch 16/20  Iteration 27036/35720 Training loss: 0.7296 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 27037/35720 Training loss: 0.7294 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27038/35720 Training loss: 0.7294 0.2071 sec/batch\n",
      "Epoch 16/20  Iteration 27039/35720 Training loss: 0.7293 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 27040/35720 Training loss: 0.7292 0.2145 sec/batch\n",
      "Epoch 16/20  Iteration 27041/35720 Training loss: 0.7292 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27042/35720 Training loss: 0.7291 0.2275 sec/batch\n",
      "Epoch 16/20  Iteration 27043/35720 Training loss: 0.7288 0.2199 sec/batch\n",
      "Epoch 16/20  Iteration 27044/35720 Training loss: 0.7289 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27045/35720 Training loss: 0.7290 0.2277 sec/batch\n",
      "Epoch 16/20  Iteration 27046/35720 Training loss: 0.7291 0.2080 sec/batch\n",
      "Epoch 16/20  Iteration 27047/35720 Training loss: 0.7292 0.2200 sec/batch\n",
      "Epoch 16/20  Iteration 27048/35720 Training loss: 0.7291 0.2201 sec/batch\n",
      "Epoch 16/20  Iteration 27049/35720 Training loss: 0.7293 0.2057 sec/batch\n",
      "Epoch 16/20  Iteration 27050/35720 Training loss: 0.7294 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27051/35720 Training loss: 0.7293 0.2143 sec/batch\n",
      "Epoch 16/20  Iteration 27052/35720 Training loss: 0.7292 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 27053/35720 Training loss: 0.7291 0.2218 sec/batch\n",
      "Epoch 16/20  Iteration 27054/35720 Training loss: 0.7292 0.2293 sec/batch\n",
      "Epoch 16/20  Iteration 27055/35720 Training loss: 0.7290 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27056/35720 Training loss: 0.7293 0.2157 sec/batch\n",
      "Epoch 16/20  Iteration 27057/35720 Training loss: 0.7293 0.2160 sec/batch\n",
      "Epoch 16/20  Iteration 27058/35720 Training loss: 0.7292 0.2184 sec/batch\n",
      "Epoch 16/20  Iteration 27059/35720 Training loss: 0.7291 0.2071 sec/batch\n",
      "Epoch 16/20  Iteration 27060/35720 Training loss: 0.7288 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 27061/35720 Training loss: 0.7285 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27062/35720 Training loss: 0.7285 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 27063/35720 Training loss: 0.7285 0.2120 sec/batch\n",
      "Epoch 16/20  Iteration 27064/35720 Training loss: 0.7285 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 27065/35720 Training loss: 0.7284 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 27066/35720 Training loss: 0.7284 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 27067/35720 Training loss: 0.7282 0.2127 sec/batch\n",
      "Epoch 16/20  Iteration 27068/35720 Training loss: 0.7280 0.2200 sec/batch\n",
      "Epoch 16/20  Iteration 27069/35720 Training loss: 0.7278 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 27070/35720 Training loss: 0.7278 0.2255 sec/batch\n",
      "Epoch 16/20  Iteration 27071/35720 Training loss: 0.7277 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 27072/35720 Training loss: 0.7275 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 27073/35720 Training loss: 0.7272 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 27074/35720 Training loss: 0.7270 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 27075/35720 Training loss: 0.7271 0.2186 sec/batch\n",
      "Epoch 16/20  Iteration 27076/35720 Training loss: 0.7271 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 27077/35720 Training loss: 0.7268 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 27078/35720 Training loss: 0.7269 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27079/35720 Training loss: 0.7269 0.2550 sec/batch\n",
      "Epoch 16/20  Iteration 27080/35720 Training loss: 0.7270 0.2262 sec/batch\n",
      "Epoch 16/20  Iteration 27081/35720 Training loss: 0.7270 0.2282 sec/batch\n",
      "Epoch 16/20  Iteration 27082/35720 Training loss: 0.7270 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 27083/35720 Training loss: 0.7268 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 27084/35720 Training loss: 0.7268 0.2219 sec/batch\n",
      "Epoch 16/20  Iteration 27085/35720 Training loss: 0.7269 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 27086/35720 Training loss: 0.7270 0.2259 sec/batch\n",
      "Epoch 16/20  Iteration 27087/35720 Training loss: 0.7268 0.2118 sec/batch\n",
      "Epoch 16/20  Iteration 27088/35720 Training loss: 0.7270 0.2184 sec/batch\n",
      "Epoch 16/20  Iteration 27089/35720 Training loss: 0.7268 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 27090/35720 Training loss: 0.7268 0.2210 sec/batch\n",
      "Epoch 16/20  Iteration 27091/35720 Training loss: 0.7267 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27092/35720 Training loss: 0.7265 0.2158 sec/batch\n",
      "Epoch 16/20  Iteration 27093/35720 Training loss: 0.7266 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 27094/35720 Training loss: 0.7267 0.2162 sec/batch\n",
      "Epoch 16/20  Iteration 27095/35720 Training loss: 0.7266 0.2366 sec/batch\n",
      "Epoch 16/20  Iteration 27096/35720 Training loss: 0.7264 0.2099 sec/batch\n",
      "Epoch 16/20  Iteration 27097/35720 Training loss: 0.7264 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 27098/35720 Training loss: 0.7266 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 27099/35720 Training loss: 0.7264 0.2059 sec/batch\n",
      "Epoch 16/20  Iteration 27100/35720 Training loss: 0.7263 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 27101/35720 Training loss: 0.7262 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 27102/35720 Training loss: 0.7261 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 27103/35720 Training loss: 0.7261 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 27104/35720 Training loss: 0.7260 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 27105/35720 Training loss: 0.7260 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 27106/35720 Training loss: 0.7259 0.2536 sec/batch\n",
      "Epoch 16/20  Iteration 27107/35720 Training loss: 0.7258 0.2266 sec/batch\n",
      "Epoch 16/20  Iteration 27108/35720 Training loss: 0.7257 0.2151 sec/batch\n",
      "Epoch 16/20  Iteration 27109/35720 Training loss: 0.7259 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27110/35720 Training loss: 0.7258 0.2077 sec/batch\n",
      "Epoch 16/20  Iteration 27111/35720 Training loss: 0.7258 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27112/35720 Training loss: 0.7258 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 27113/35720 Training loss: 0.7258 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27114/35720 Training loss: 0.7258 0.2478 sec/batch\n",
      "Epoch 16/20  Iteration 27115/35720 Training loss: 0.7257 0.2075 sec/batch\n",
      "Epoch 16/20  Iteration 27116/35720 Training loss: 0.7257 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 27117/35720 Training loss: 0.7257 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 27118/35720 Training loss: 0.7257 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 27119/35720 Training loss: 0.7257 0.2157 sec/batch\n",
      "Epoch 16/20  Iteration 27120/35720 Training loss: 0.7257 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27121/35720 Training loss: 0.7257 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27122/35720 Training loss: 0.7257 0.2151 sec/batch\n",
      "Epoch 16/20  Iteration 27123/35720 Training loss: 0.7257 0.2301 sec/batch\n",
      "Epoch 16/20  Iteration 27124/35720 Training loss: 0.7257 0.2201 sec/batch\n",
      "Epoch 16/20  Iteration 27125/35720 Training loss: 0.7256 0.2427 sec/batch\n",
      "Epoch 16/20  Iteration 27126/35720 Training loss: 0.7253 0.3007 sec/batch\n",
      "Epoch 16/20  Iteration 27127/35720 Training loss: 0.7253 0.2179 sec/batch\n",
      "Epoch 16/20  Iteration 27128/35720 Training loss: 0.7252 0.2189 sec/batch\n",
      "Epoch 16/20  Iteration 27129/35720 Training loss: 0.7252 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 27130/35720 Training loss: 0.7251 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 27131/35720 Training loss: 0.7250 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 27132/35720 Training loss: 0.7249 0.2080 sec/batch\n",
      "Epoch 16/20  Iteration 27133/35720 Training loss: 0.7249 0.2221 sec/batch\n",
      "Epoch 16/20  Iteration 27134/35720 Training loss: 0.7249 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 27135/35720 Training loss: 0.7247 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 27136/35720 Training loss: 0.7249 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27137/35720 Training loss: 0.7250 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27138/35720 Training loss: 0.7251 0.2099 sec/batch\n",
      "Epoch 16/20  Iteration 27139/35720 Training loss: 0.7252 0.2202 sec/batch\n",
      "Epoch 16/20  Iteration 27140/35720 Training loss: 0.7253 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27141/35720 Training loss: 0.7253 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 27142/35720 Training loss: 0.7252 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27143/35720 Training loss: 0.7252 0.2216 sec/batch\n",
      "Epoch 16/20  Iteration 27144/35720 Training loss: 0.7252 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27145/35720 Training loss: 0.7253 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 27146/35720 Training loss: 0.7253 0.2129 sec/batch\n",
      "Epoch 16/20  Iteration 27147/35720 Training loss: 0.7254 0.2179 sec/batch\n",
      "Epoch 16/20  Iteration 27148/35720 Training loss: 0.7254 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 27149/35720 Training loss: 0.7254 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 27150/35720 Training loss: 0.7253 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27151/35720 Training loss: 0.7252 0.2207 sec/batch\n",
      "Epoch 16/20  Iteration 27152/35720 Training loss: 0.7252 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 27153/35720 Training loss: 0.7253 0.2254 sec/batch\n",
      "Epoch 16/20  Iteration 27154/35720 Training loss: 0.7252 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27155/35720 Training loss: 0.7251 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 27156/35720 Training loss: 0.7252 0.2293 sec/batch\n",
      "Epoch 16/20  Iteration 27157/35720 Training loss: 0.7252 0.2118 sec/batch\n",
      "Epoch 16/20  Iteration 27158/35720 Training loss: 0.7251 0.2370 sec/batch\n",
      "Epoch 16/20  Iteration 27159/35720 Training loss: 0.7250 0.2157 sec/batch\n",
      "Epoch 16/20  Iteration 27160/35720 Training loss: 0.7249 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 27161/35720 Training loss: 0.7248 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 27162/35720 Training loss: 0.7248 0.2245 sec/batch\n",
      "Epoch 16/20  Iteration 27163/35720 Training loss: 0.7247 0.2464 sec/batch\n",
      "Epoch 16/20  Iteration 27164/35720 Training loss: 0.7246 0.2047 sec/batch\n",
      "Epoch 16/20  Iteration 27165/35720 Training loss: 0.7246 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 27166/35720 Training loss: 0.7247 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 27167/35720 Training loss: 0.7248 0.2188 sec/batch\n",
      "Epoch 16/20  Iteration 27168/35720 Training loss: 0.7248 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 27169/35720 Training loss: 0.7246 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 27170/35720 Training loss: 0.7245 0.2290 sec/batch\n",
      "Epoch 16/20  Iteration 27171/35720 Training loss: 0.7244 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27172/35720 Training loss: 0.7243 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 27173/35720 Training loss: 0.7243 0.2292 sec/batch\n",
      "Epoch 16/20  Iteration 27174/35720 Training loss: 0.7244 0.2333 sec/batch\n",
      "Epoch 16/20  Iteration 27175/35720 Training loss: 0.7244 0.2200 sec/batch\n",
      "Epoch 16/20  Iteration 27176/35720 Training loss: 0.7244 0.2120 sec/batch\n",
      "Epoch 16/20  Iteration 27177/35720 Training loss: 0.7244 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27178/35720 Training loss: 0.7245 0.2201 sec/batch\n",
      "Epoch 16/20  Iteration 27179/35720 Training loss: 0.7244 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27180/35720 Training loss: 0.7243 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 27181/35720 Training loss: 0.7243 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27182/35720 Training loss: 0.7242 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 27183/35720 Training loss: 0.7243 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27184/35720 Training loss: 0.7240 0.2330 sec/batch\n",
      "Epoch 16/20  Iteration 27185/35720 Training loss: 0.7240 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 27186/35720 Training loss: 0.7240 0.2223 sec/batch\n",
      "Epoch 16/20  Iteration 27187/35720 Training loss: 0.7241 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27188/35720 Training loss: 0.7239 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 27189/35720 Training loss: 0.7239 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 27190/35720 Training loss: 0.7238 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27191/35720 Training loss: 0.7238 0.2306 sec/batch\n",
      "Epoch 16/20  Iteration 27192/35720 Training loss: 0.7240 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27193/35720 Training loss: 0.7240 0.2148 sec/batch\n",
      "Epoch 16/20  Iteration 27194/35720 Training loss: 0.7239 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27195/35720 Training loss: 0.7238 0.2241 sec/batch\n",
      "Epoch 16/20  Iteration 27196/35720 Training loss: 0.7238 0.2247 sec/batch\n",
      "Epoch 16/20  Iteration 27197/35720 Training loss: 0.7238 0.2270 sec/batch\n",
      "Epoch 16/20  Iteration 27198/35720 Training loss: 0.7238 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 27199/35720 Training loss: 0.7237 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 27200/35720 Training loss: 0.7236 0.2389 sec/batch\n",
      "Validation loss: 1.56514 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 27201/35720 Training loss: 0.7245 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 27202/35720 Training loss: 0.7244 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27203/35720 Training loss: 0.7242 0.2054 sec/batch\n",
      "Epoch 16/20  Iteration 27204/35720 Training loss: 0.7243 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27205/35720 Training loss: 0.7242 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27206/35720 Training loss: 0.7242 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 27207/35720 Training loss: 0.7241 0.2263 sec/batch\n",
      "Epoch 16/20  Iteration 27208/35720 Training loss: 0.7240 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 27209/35720 Training loss: 0.7240 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 27210/35720 Training loss: 0.7239 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 27211/35720 Training loss: 0.7239 0.2293 sec/batch\n",
      "Epoch 16/20  Iteration 27212/35720 Training loss: 0.7239 0.2283 sec/batch\n",
      "Epoch 16/20  Iteration 27213/35720 Training loss: 0.7240 0.2071 sec/batch\n",
      "Epoch 16/20  Iteration 27214/35720 Training loss: 0.7240 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 27215/35720 Training loss: 0.7240 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27216/35720 Training loss: 0.7239 0.2249 sec/batch\n",
      "Epoch 16/20  Iteration 27217/35720 Training loss: 0.7238 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 27218/35720 Training loss: 0.7238 0.2171 sec/batch\n",
      "Epoch 16/20  Iteration 27219/35720 Training loss: 0.7237 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 27220/35720 Training loss: 0.7238 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27221/35720 Training loss: 0.7239 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 27222/35720 Training loss: 0.7238 0.2291 sec/batch\n",
      "Epoch 16/20  Iteration 27223/35720 Training loss: 0.7239 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27224/35720 Training loss: 0.7239 0.2297 sec/batch\n",
      "Epoch 16/20  Iteration 27225/35720 Training loss: 0.7240 0.2050 sec/batch\n",
      "Epoch 16/20  Iteration 27226/35720 Training loss: 0.7239 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 27227/35720 Training loss: 0.7240 0.2184 sec/batch\n",
      "Epoch 16/20  Iteration 27228/35720 Training loss: 0.7241 0.2319 sec/batch\n",
      "Epoch 16/20  Iteration 27229/35720 Training loss: 0.7242 0.2156 sec/batch\n",
      "Epoch 16/20  Iteration 27230/35720 Training loss: 0.7242 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27231/35720 Training loss: 0.7243 0.2057 sec/batch\n",
      "Epoch 16/20  Iteration 27232/35720 Training loss: 0.7244 0.2235 sec/batch\n",
      "Epoch 16/20  Iteration 27233/35720 Training loss: 0.7243 0.2204 sec/batch\n",
      "Epoch 16/20  Iteration 27234/35720 Training loss: 0.7244 0.2340 sec/batch\n",
      "Epoch 16/20  Iteration 27235/35720 Training loss: 0.7244 0.2317 sec/batch\n",
      "Epoch 16/20  Iteration 27236/35720 Training loss: 0.7246 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 27237/35720 Training loss: 0.7248 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 27238/35720 Training loss: 0.7249 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 27239/35720 Training loss: 0.7250 0.2241 sec/batch\n",
      "Epoch 16/20  Iteration 27240/35720 Training loss: 0.7250 0.2369 sec/batch\n",
      "Epoch 16/20  Iteration 27241/35720 Training loss: 0.7248 0.2176 sec/batch\n",
      "Epoch 16/20  Iteration 27242/35720 Training loss: 0.7248 0.2243 sec/batch\n",
      "Epoch 16/20  Iteration 27243/35720 Training loss: 0.7248 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 27244/35720 Training loss: 0.7250 0.2118 sec/batch\n",
      "Epoch 16/20  Iteration 27245/35720 Training loss: 0.7251 0.2174 sec/batch\n",
      "Epoch 16/20  Iteration 27246/35720 Training loss: 0.7253 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 27247/35720 Training loss: 0.7254 0.2122 sec/batch\n",
      "Epoch 16/20  Iteration 27248/35720 Training loss: 0.7255 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 27249/35720 Training loss: 0.7255 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 27250/35720 Training loss: 0.7254 0.2099 sec/batch\n",
      "Epoch 16/20  Iteration 27251/35720 Training loss: 0.7255 0.2195 sec/batch\n",
      "Epoch 16/20  Iteration 27252/35720 Training loss: 0.7255 0.2218 sec/batch\n",
      "Epoch 16/20  Iteration 27253/35720 Training loss: 0.7257 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27254/35720 Training loss: 0.7257 0.2191 sec/batch\n",
      "Epoch 16/20  Iteration 27255/35720 Training loss: 0.7257 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 27256/35720 Training loss: 0.7257 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 27257/35720 Training loss: 0.7257 0.2159 sec/batch\n",
      "Epoch 16/20  Iteration 27258/35720 Training loss: 0.7257 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27259/35720 Training loss: 0.7257 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 27260/35720 Training loss: 0.7257 0.2262 sec/batch\n",
      "Epoch 16/20  Iteration 27261/35720 Training loss: 0.7257 0.2245 sec/batch\n",
      "Epoch 16/20  Iteration 27262/35720 Training loss: 0.7256 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 27263/35720 Training loss: 0.7257 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 27264/35720 Training loss: 0.7255 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 27265/35720 Training loss: 0.7257 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 27266/35720 Training loss: 0.7257 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 27267/35720 Training loss: 0.7257 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 27268/35720 Training loss: 0.7256 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 27269/35720 Training loss: 0.7256 0.2243 sec/batch\n",
      "Epoch 16/20  Iteration 27270/35720 Training loss: 0.7256 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27271/35720 Training loss: 0.7255 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 27272/35720 Training loss: 0.7254 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 27273/35720 Training loss: 0.7255 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27274/35720 Training loss: 0.7255 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 27275/35720 Training loss: 0.7256 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 27276/35720 Training loss: 0.7255 0.2118 sec/batch\n",
      "Epoch 16/20  Iteration 27277/35720 Training loss: 0.7255 0.2204 sec/batch\n",
      "Epoch 16/20  Iteration 27278/35720 Training loss: 0.7254 0.2222 sec/batch\n",
      "Epoch 16/20  Iteration 27279/35720 Training loss: 0.7254 0.2172 sec/batch\n",
      "Epoch 16/20  Iteration 27280/35720 Training loss: 0.7253 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 27281/35720 Training loss: 0.7252 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27282/35720 Training loss: 0.7253 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 27283/35720 Training loss: 0.7253 0.2158 sec/batch\n",
      "Epoch 16/20  Iteration 27284/35720 Training loss: 0.7252 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 27285/35720 Training loss: 0.7252 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 27286/35720 Training loss: 0.7250 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 27287/35720 Training loss: 0.7250 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 27288/35720 Training loss: 0.7251 0.2248 sec/batch\n",
      "Epoch 16/20  Iteration 27289/35720 Training loss: 0.7250 0.2139 sec/batch\n",
      "Epoch 16/20  Iteration 27290/35720 Training loss: 0.7251 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 27291/35720 Training loss: 0.7250 0.2116 sec/batch\n",
      "Epoch 16/20  Iteration 27292/35720 Training loss: 0.7249 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 27293/35720 Training loss: 0.7248 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 27294/35720 Training loss: 0.7247 0.2226 sec/batch\n",
      "Epoch 16/20  Iteration 27295/35720 Training loss: 0.7247 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27296/35720 Training loss: 0.7248 0.2352 sec/batch\n",
      "Epoch 16/20  Iteration 27297/35720 Training loss: 0.7247 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 27298/35720 Training loss: 0.7247 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 27299/35720 Training loss: 0.7248 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 27300/35720 Training loss: 0.7247 0.2181 sec/batch\n",
      "Epoch 16/20  Iteration 27301/35720 Training loss: 0.7247 0.2247 sec/batch\n",
      "Epoch 16/20  Iteration 27302/35720 Training loss: 0.7247 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 27303/35720 Training loss: 0.7247 0.2059 sec/batch\n",
      "Epoch 16/20  Iteration 27304/35720 Training loss: 0.7248 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 27305/35720 Training loss: 0.7248 0.2158 sec/batch\n",
      "Epoch 16/20  Iteration 27306/35720 Training loss: 0.7249 0.2190 sec/batch\n",
      "Epoch 16/20  Iteration 27307/35720 Training loss: 0.7249 0.2157 sec/batch\n",
      "Epoch 16/20  Iteration 27308/35720 Training loss: 0.7249 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 27309/35720 Training loss: 0.7249 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 27310/35720 Training loss: 0.7250 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 27311/35720 Training loss: 0.7250 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 27312/35720 Training loss: 0.7250 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27313/35720 Training loss: 0.7249 0.2174 sec/batch\n",
      "Epoch 16/20  Iteration 27314/35720 Training loss: 0.7248 0.2166 sec/batch\n",
      "Epoch 16/20  Iteration 27315/35720 Training loss: 0.7248 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 27316/35720 Training loss: 0.7248 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 27317/35720 Training loss: 0.7249 0.2082 sec/batch\n",
      "Epoch 16/20  Iteration 27318/35720 Training loss: 0.7248 0.2431 sec/batch\n",
      "Epoch 16/20  Iteration 27319/35720 Training loss: 0.7249 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27320/35720 Training loss: 0.7248 0.2176 sec/batch\n",
      "Epoch 16/20  Iteration 27321/35720 Training loss: 0.7249 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 27322/35720 Training loss: 0.7249 0.2319 sec/batch\n",
      "Epoch 16/20  Iteration 27323/35720 Training loss: 0.7249 0.2176 sec/batch\n",
      "Epoch 16/20  Iteration 27324/35720 Training loss: 0.7249 0.2437 sec/batch\n",
      "Epoch 16/20  Iteration 27325/35720 Training loss: 0.7248 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 27326/35720 Training loss: 0.7247 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 27327/35720 Training loss: 0.7246 0.2302 sec/batch\n",
      "Epoch 16/20  Iteration 27328/35720 Training loss: 0.7246 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 27329/35720 Training loss: 0.7246 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27330/35720 Training loss: 0.7246 0.2072 sec/batch\n",
      "Epoch 16/20  Iteration 27331/35720 Training loss: 0.7246 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27332/35720 Training loss: 0.7245 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27333/35720 Training loss: 0.7244 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 27334/35720 Training loss: 0.7244 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 27335/35720 Training loss: 0.7244 0.2189 sec/batch\n",
      "Epoch 16/20  Iteration 27336/35720 Training loss: 0.7245 0.2271 sec/batch\n",
      "Epoch 16/20  Iteration 27337/35720 Training loss: 0.7245 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 27338/35720 Training loss: 0.7246 0.2082 sec/batch\n",
      "Epoch 16/20  Iteration 27339/35720 Training loss: 0.7245 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 27340/35720 Training loss: 0.7246 0.2284 sec/batch\n",
      "Epoch 16/20  Iteration 27341/35720 Training loss: 0.7245 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 27342/35720 Training loss: 0.7244 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27343/35720 Training loss: 0.7243 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 27344/35720 Training loss: 0.7244 0.2225 sec/batch\n",
      "Epoch 16/20  Iteration 27345/35720 Training loss: 0.7244 0.2324 sec/batch\n",
      "Epoch 16/20  Iteration 27346/35720 Training loss: 0.7245 0.2312 sec/batch\n",
      "Epoch 16/20  Iteration 27347/35720 Training loss: 0.7244 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 27348/35720 Training loss: 0.7245 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 27349/35720 Training loss: 0.7245 0.2647 sec/batch\n",
      "Epoch 16/20  Iteration 27350/35720 Training loss: 0.7244 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27351/35720 Training loss: 0.7245 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 27352/35720 Training loss: 0.7244 0.2077 sec/batch\n",
      "Epoch 16/20  Iteration 27353/35720 Training loss: 0.7244 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 27354/35720 Training loss: 0.7243 0.2223 sec/batch\n",
      "Epoch 16/20  Iteration 27355/35720 Training loss: 0.7242 0.2157 sec/batch\n",
      "Epoch 16/20  Iteration 27356/35720 Training loss: 0.7243 0.2145 sec/batch\n",
      "Epoch 16/20  Iteration 27357/35720 Training loss: 0.7243 0.2342 sec/batch\n",
      "Epoch 16/20  Iteration 27358/35720 Training loss: 0.7242 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27359/35720 Training loss: 0.7242 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 27360/35720 Training loss: 0.7241 0.2243 sec/batch\n",
      "Epoch 16/20  Iteration 27361/35720 Training loss: 0.7241 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 27362/35720 Training loss: 0.7241 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 27363/35720 Training loss: 0.7242 0.2116 sec/batch\n",
      "Epoch 16/20  Iteration 27364/35720 Training loss: 0.7242 0.2291 sec/batch\n",
      "Epoch 16/20  Iteration 27365/35720 Training loss: 0.7242 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 27366/35720 Training loss: 0.7242 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 27367/35720 Training loss: 0.7243 0.2140 sec/batch\n",
      "Epoch 16/20  Iteration 27368/35720 Training loss: 0.7243 0.2259 sec/batch\n",
      "Epoch 16/20  Iteration 27369/35720 Training loss: 0.7243 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 27370/35720 Training loss: 0.7243 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 27371/35720 Training loss: 0.7243 0.2221 sec/batch\n",
      "Epoch 16/20  Iteration 27372/35720 Training loss: 0.7243 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 27373/35720 Training loss: 0.7244 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 27374/35720 Training loss: 0.7244 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 27375/35720 Training loss: 0.7243 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 27376/35720 Training loss: 0.7242 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27377/35720 Training loss: 0.7242 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 27378/35720 Training loss: 0.7241 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27379/35720 Training loss: 0.7240 0.2202 sec/batch\n",
      "Epoch 16/20  Iteration 27380/35720 Training loss: 0.7240 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 27381/35720 Training loss: 0.7240 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 27382/35720 Training loss: 0.7240 0.2202 sec/batch\n",
      "Epoch 16/20  Iteration 27383/35720 Training loss: 0.7240 0.2134 sec/batch\n",
      "Epoch 16/20  Iteration 27384/35720 Training loss: 0.7240 0.2099 sec/batch\n",
      "Epoch 16/20  Iteration 27385/35720 Training loss: 0.7239 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27386/35720 Training loss: 0.7239 0.2075 sec/batch\n",
      "Epoch 16/20  Iteration 27387/35720 Training loss: 0.7239 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 27388/35720 Training loss: 0.7239 0.2145 sec/batch\n",
      "Epoch 16/20  Iteration 27389/35720 Training loss: 0.7238 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27390/35720 Training loss: 0.7237 0.2228 sec/batch\n",
      "Epoch 16/20  Iteration 27391/35720 Training loss: 0.7236 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27392/35720 Training loss: 0.7235 0.2166 sec/batch\n",
      "Epoch 16/20  Iteration 27393/35720 Training loss: 0.7234 0.2183 sec/batch\n",
      "Epoch 16/20  Iteration 27394/35720 Training loss: 0.7233 0.2228 sec/batch\n",
      "Epoch 16/20  Iteration 27395/35720 Training loss: 0.7233 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27396/35720 Training loss: 0.7233 0.2264 sec/batch\n",
      "Epoch 16/20  Iteration 27397/35720 Training loss: 0.7233 0.2194 sec/batch\n",
      "Epoch 16/20  Iteration 27398/35720 Training loss: 0.7233 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 27399/35720 Training loss: 0.7234 0.2396 sec/batch\n",
      "Epoch 16/20  Iteration 27400/35720 Training loss: 0.7234 0.2102 sec/batch\n",
      "Validation loss: 1.55947 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 27401/35720 Training loss: 0.7238 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27402/35720 Training loss: 0.7238 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27403/35720 Training loss: 0.7237 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 27404/35720 Training loss: 0.7238 0.2171 sec/batch\n",
      "Epoch 16/20  Iteration 27405/35720 Training loss: 0.7238 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 27406/35720 Training loss: 0.7237 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 27407/35720 Training loss: 0.7236 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27408/35720 Training loss: 0.7236 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27409/35720 Training loss: 0.7235 0.2075 sec/batch\n",
      "Epoch 16/20  Iteration 27410/35720 Training loss: 0.7234 0.2221 sec/batch\n",
      "Epoch 16/20  Iteration 27411/35720 Training loss: 0.7234 0.2879 sec/batch\n",
      "Epoch 16/20  Iteration 27412/35720 Training loss: 0.7236 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 27413/35720 Training loss: 0.7234 0.2120 sec/batch\n",
      "Epoch 16/20  Iteration 27414/35720 Training loss: 0.7234 0.2127 sec/batch\n",
      "Epoch 16/20  Iteration 27415/35720 Training loss: 0.7233 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 27416/35720 Training loss: 0.7234 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 27417/35720 Training loss: 0.7234 0.2188 sec/batch\n",
      "Epoch 16/20  Iteration 27418/35720 Training loss: 0.7233 0.2231 sec/batch\n",
      "Epoch 16/20  Iteration 27419/35720 Training loss: 0.7234 0.2270 sec/batch\n",
      "Epoch 16/20  Iteration 27420/35720 Training loss: 0.7233 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27421/35720 Training loss: 0.7234 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 27422/35720 Training loss: 0.7233 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27423/35720 Training loss: 0.7232 0.2158 sec/batch\n",
      "Epoch 16/20  Iteration 27424/35720 Training loss: 0.7232 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 27425/35720 Training loss: 0.7232 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 27426/35720 Training loss: 0.7232 0.2248 sec/batch\n",
      "Epoch 16/20  Iteration 27427/35720 Training loss: 0.7232 0.2196 sec/batch\n",
      "Epoch 16/20  Iteration 27428/35720 Training loss: 0.7231 0.2220 sec/batch\n",
      "Epoch 16/20  Iteration 27429/35720 Training loss: 0.7232 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27430/35720 Training loss: 0.7232 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 27431/35720 Training loss: 0.7232 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27432/35720 Training loss: 0.7233 0.2243 sec/batch\n",
      "Epoch 16/20  Iteration 27433/35720 Training loss: 0.7233 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 27434/35720 Training loss: 0.7232 0.2164 sec/batch\n",
      "Epoch 16/20  Iteration 27435/35720 Training loss: 0.7231 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 27436/35720 Training loss: 0.7231 0.2204 sec/batch\n",
      "Epoch 16/20  Iteration 27437/35720 Training loss: 0.7231 0.2278 sec/batch\n",
      "Epoch 16/20  Iteration 27438/35720 Training loss: 0.7230 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 27439/35720 Training loss: 0.7230 0.2195 sec/batch\n",
      "Epoch 16/20  Iteration 27440/35720 Training loss: 0.7229 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 27441/35720 Training loss: 0.7228 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 27442/35720 Training loss: 0.7228 0.2199 sec/batch\n",
      "Epoch 16/20  Iteration 27443/35720 Training loss: 0.7228 0.2195 sec/batch\n",
      "Epoch 16/20  Iteration 27444/35720 Training loss: 0.7228 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 27445/35720 Training loss: 0.7228 0.2168 sec/batch\n",
      "Epoch 16/20  Iteration 27446/35720 Training loss: 0.7228 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 27447/35720 Training loss: 0.7229 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 27448/35720 Training loss: 0.7229 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 27449/35720 Training loss: 0.7229 0.2285 sec/batch\n",
      "Epoch 16/20  Iteration 27450/35720 Training loss: 0.7230 0.2116 sec/batch\n",
      "Epoch 16/20  Iteration 27451/35720 Training loss: 0.7230 0.2159 sec/batch\n",
      "Epoch 16/20  Iteration 27452/35720 Training loss: 0.7230 0.2176 sec/batch\n",
      "Epoch 16/20  Iteration 27453/35720 Training loss: 0.7230 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 27454/35720 Training loss: 0.7230 0.2250 sec/batch\n",
      "Epoch 16/20  Iteration 27455/35720 Training loss: 0.7230 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 27456/35720 Training loss: 0.7230 0.2253 sec/batch\n",
      "Epoch 16/20  Iteration 27457/35720 Training loss: 0.7231 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 27458/35720 Training loss: 0.7231 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 27459/35720 Training loss: 0.7230 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 27460/35720 Training loss: 0.7230 0.2222 sec/batch\n",
      "Epoch 16/20  Iteration 27461/35720 Training loss: 0.7229 0.2218 sec/batch\n",
      "Epoch 16/20  Iteration 27462/35720 Training loss: 0.7229 0.2259 sec/batch\n",
      "Epoch 16/20  Iteration 27463/35720 Training loss: 0.7229 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 27464/35720 Training loss: 0.7228 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 27465/35720 Training loss: 0.7228 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 27466/35720 Training loss: 0.7228 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 27467/35720 Training loss: 0.7228 0.2327 sec/batch\n",
      "Epoch 16/20  Iteration 27468/35720 Training loss: 0.7228 0.2236 sec/batch\n",
      "Epoch 16/20  Iteration 27469/35720 Training loss: 0.7228 0.2111 sec/batch\n",
      "Epoch 16/20  Iteration 27470/35720 Training loss: 0.7228 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 27471/35720 Training loss: 0.7228 0.2205 sec/batch\n",
      "Epoch 16/20  Iteration 27472/35720 Training loss: 0.7228 0.2174 sec/batch\n",
      "Epoch 16/20  Iteration 27473/35720 Training loss: 0.7228 0.2158 sec/batch\n",
      "Epoch 16/20  Iteration 27474/35720 Training loss: 0.7227 0.2078 sec/batch\n",
      "Epoch 16/20  Iteration 27475/35720 Training loss: 0.7227 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 27476/35720 Training loss: 0.7227 0.2348 sec/batch\n",
      "Epoch 16/20  Iteration 27477/35720 Training loss: 0.7226 0.2262 sec/batch\n",
      "Epoch 16/20  Iteration 27478/35720 Training loss: 0.7226 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 27479/35720 Training loss: 0.7226 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 27480/35720 Training loss: 0.7226 0.2055 sec/batch\n",
      "Epoch 16/20  Iteration 27481/35720 Training loss: 0.7226 0.2143 sec/batch\n",
      "Epoch 16/20  Iteration 27482/35720 Training loss: 0.7227 0.2210 sec/batch\n",
      "Epoch 16/20  Iteration 27483/35720 Training loss: 0.7228 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 27484/35720 Training loss: 0.7228 0.2175 sec/batch\n",
      "Epoch 16/20  Iteration 27485/35720 Training loss: 0.7228 0.2120 sec/batch\n",
      "Epoch 16/20  Iteration 27486/35720 Training loss: 0.7228 0.2346 sec/batch\n",
      "Epoch 16/20  Iteration 27487/35720 Training loss: 0.7228 0.2265 sec/batch\n",
      "Epoch 16/20  Iteration 27488/35720 Training loss: 0.7228 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 27489/35720 Training loss: 0.7227 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 27490/35720 Training loss: 0.7227 0.2099 sec/batch\n",
      "Epoch 16/20  Iteration 27491/35720 Training loss: 0.7227 0.2072 sec/batch\n",
      "Epoch 16/20  Iteration 27492/35720 Training loss: 0.7227 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 27493/35720 Training loss: 0.7226 0.2194 sec/batch\n",
      "Epoch 16/20  Iteration 27494/35720 Training loss: 0.7227 0.2309 sec/batch\n",
      "Epoch 16/20  Iteration 27495/35720 Training loss: 0.7226 0.2164 sec/batch\n",
      "Epoch 16/20  Iteration 27496/35720 Training loss: 0.7226 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 27497/35720 Training loss: 0.7227 0.2071 sec/batch\n",
      "Epoch 16/20  Iteration 27498/35720 Training loss: 0.7227 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27499/35720 Training loss: 0.7228 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 27500/35720 Training loss: 0.7228 0.2333 sec/batch\n",
      "Epoch 16/20  Iteration 27501/35720 Training loss: 0.7229 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 27502/35720 Training loss: 0.7229 0.2116 sec/batch\n",
      "Epoch 16/20  Iteration 27503/35720 Training loss: 0.7229 0.2369 sec/batch\n",
      "Epoch 16/20  Iteration 27504/35720 Training loss: 0.7228 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27505/35720 Training loss: 0.7229 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 27506/35720 Training loss: 0.7229 0.2288 sec/batch\n",
      "Epoch 16/20  Iteration 27507/35720 Training loss: 0.7230 0.2219 sec/batch\n",
      "Epoch 16/20  Iteration 27508/35720 Training loss: 0.7230 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27509/35720 Training loss: 0.7229 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 27510/35720 Training loss: 0.7230 0.2156 sec/batch\n",
      "Epoch 16/20  Iteration 27511/35720 Training loss: 0.7231 0.2212 sec/batch\n",
      "Epoch 16/20  Iteration 27512/35720 Training loss: 0.7231 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27513/35720 Training loss: 0.7232 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 27514/35720 Training loss: 0.7231 0.2179 sec/batch\n",
      "Epoch 16/20  Iteration 27515/35720 Training loss: 0.7231 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 27516/35720 Training loss: 0.7231 0.2174 sec/batch\n",
      "Epoch 16/20  Iteration 27517/35720 Training loss: 0.7232 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 27518/35720 Training loss: 0.7232 0.2111 sec/batch\n",
      "Epoch 16/20  Iteration 27519/35720 Training loss: 0.7233 0.2117 sec/batch\n",
      "Epoch 16/20  Iteration 27520/35720 Training loss: 0.7233 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 27521/35720 Training loss: 0.7233 0.2295 sec/batch\n",
      "Epoch 16/20  Iteration 27522/35720 Training loss: 0.7233 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 27523/35720 Training loss: 0.7233 0.2243 sec/batch\n",
      "Epoch 16/20  Iteration 27524/35720 Training loss: 0.7233 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27525/35720 Training loss: 0.7232 0.2082 sec/batch\n",
      "Epoch 16/20  Iteration 27526/35720 Training loss: 0.7232 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 27527/35720 Training loss: 0.7233 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 27528/35720 Training loss: 0.7232 0.2361 sec/batch\n",
      "Epoch 16/20  Iteration 27529/35720 Training loss: 0.7233 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 27530/35720 Training loss: 0.7233 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27531/35720 Training loss: 0.7233 0.2239 sec/batch\n",
      "Epoch 16/20  Iteration 27532/35720 Training loss: 0.7233 0.2276 sec/batch\n",
      "Epoch 16/20  Iteration 27533/35720 Training loss: 0.7233 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 27534/35720 Training loss: 0.7232 0.2078 sec/batch\n",
      "Epoch 16/20  Iteration 27535/35720 Training loss: 0.7232 0.2111 sec/batch\n",
      "Epoch 16/20  Iteration 27536/35720 Training loss: 0.7232 0.2231 sec/batch\n",
      "Epoch 16/20  Iteration 27537/35720 Training loss: 0.7232 0.2245 sec/batch\n",
      "Epoch 16/20  Iteration 27538/35720 Training loss: 0.7231 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 27539/35720 Training loss: 0.7231 0.2288 sec/batch\n",
      "Epoch 16/20  Iteration 27540/35720 Training loss: 0.7231 0.2209 sec/batch\n",
      "Epoch 16/20  Iteration 27541/35720 Training loss: 0.7231 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 27542/35720 Training loss: 0.7231 0.2106 sec/batch\n",
      "Epoch 16/20  Iteration 27543/35720 Training loss: 0.7230 0.2299 sec/batch\n",
      "Epoch 16/20  Iteration 27544/35720 Training loss: 0.7230 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 27545/35720 Training loss: 0.7230 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 27546/35720 Training loss: 0.7230 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 27547/35720 Training loss: 0.7229 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27548/35720 Training loss: 0.7229 0.2122 sec/batch\n",
      "Epoch 16/20  Iteration 27549/35720 Training loss: 0.7229 0.2116 sec/batch\n",
      "Epoch 16/20  Iteration 27550/35720 Training loss: 0.7229 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 27551/35720 Training loss: 0.7229 0.2205 sec/batch\n",
      "Epoch 16/20  Iteration 27552/35720 Training loss: 0.7230 0.2106 sec/batch\n",
      "Epoch 16/20  Iteration 27553/35720 Training loss: 0.7229 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 27554/35720 Training loss: 0.7229 0.2341 sec/batch\n",
      "Epoch 16/20  Iteration 27555/35720 Training loss: 0.7229 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27556/35720 Training loss: 0.7229 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 27557/35720 Training loss: 0.7229 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 27558/35720 Training loss: 0.7228 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27559/35720 Training loss: 0.7229 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27560/35720 Training loss: 0.7230 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 27561/35720 Training loss: 0.7230 0.2078 sec/batch\n",
      "Epoch 16/20  Iteration 27562/35720 Training loss: 0.7231 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27563/35720 Training loss: 0.7231 0.2235 sec/batch\n",
      "Epoch 16/20  Iteration 27564/35720 Training loss: 0.7230 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 27565/35720 Training loss: 0.7230 0.2261 sec/batch\n",
      "Epoch 16/20  Iteration 27566/35720 Training loss: 0.7229 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 27567/35720 Training loss: 0.7229 0.2203 sec/batch\n",
      "Epoch 16/20  Iteration 27568/35720 Training loss: 0.7229 0.2222 sec/batch\n",
      "Epoch 16/20  Iteration 27569/35720 Training loss: 0.7229 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 27570/35720 Training loss: 0.7228 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27571/35720 Training loss: 0.7228 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 27572/35720 Training loss: 0.7229 0.2099 sec/batch\n",
      "Epoch 16/20  Iteration 27573/35720 Training loss: 0.7229 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 27574/35720 Training loss: 0.7228 0.2187 sec/batch\n",
      "Epoch 16/20  Iteration 27575/35720 Training loss: 0.7228 0.2194 sec/batch\n",
      "Epoch 16/20  Iteration 27576/35720 Training loss: 0.7227 0.2169 sec/batch\n",
      "Epoch 16/20  Iteration 27577/35720 Training loss: 0.7228 0.2206 sec/batch\n",
      "Epoch 16/20  Iteration 27578/35720 Training loss: 0.7228 0.2196 sec/batch\n",
      "Epoch 16/20  Iteration 27579/35720 Training loss: 0.7228 0.2148 sec/batch\n",
      "Epoch 16/20  Iteration 27580/35720 Training loss: 0.7228 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27581/35720 Training loss: 0.7228 0.2134 sec/batch\n",
      "Epoch 16/20  Iteration 27582/35720 Training loss: 0.7228 0.2232 sec/batch\n",
      "Epoch 16/20  Iteration 27583/35720 Training loss: 0.7228 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27584/35720 Training loss: 0.7227 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 27585/35720 Training loss: 0.7228 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27586/35720 Training loss: 0.7228 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 27587/35720 Training loss: 0.7228 0.2106 sec/batch\n",
      "Epoch 16/20  Iteration 27588/35720 Training loss: 0.7228 0.2206 sec/batch\n",
      "Epoch 16/20  Iteration 27589/35720 Training loss: 0.7228 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 27590/35720 Training loss: 0.7229 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27591/35720 Training loss: 0.7228 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 27592/35720 Training loss: 0.7229 0.2139 sec/batch\n",
      "Epoch 16/20  Iteration 27593/35720 Training loss: 0.7228 0.2222 sec/batch\n",
      "Epoch 16/20  Iteration 27594/35720 Training loss: 0.7229 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 27595/35720 Training loss: 0.7229 0.2263 sec/batch\n",
      "Epoch 16/20  Iteration 27596/35720 Training loss: 0.7229 0.2071 sec/batch\n",
      "Epoch 16/20  Iteration 27597/35720 Training loss: 0.7229 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 27598/35720 Training loss: 0.7229 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27599/35720 Training loss: 0.7229 0.2226 sec/batch\n",
      "Epoch 16/20  Iteration 27600/35720 Training loss: 0.7230 0.2204 sec/batch\n",
      "Validation loss: 1.56622 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 27601/35720 Training loss: 0.7235 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27602/35720 Training loss: 0.7235 0.2077 sec/batch\n",
      "Epoch 16/20  Iteration 27603/35720 Training loss: 0.7236 0.2153 sec/batch\n",
      "Epoch 16/20  Iteration 27604/35720 Training loss: 0.7236 0.2118 sec/batch\n",
      "Epoch 16/20  Iteration 27605/35720 Training loss: 0.7236 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 27606/35720 Training loss: 0.7236 0.2184 sec/batch\n",
      "Epoch 16/20  Iteration 27607/35720 Training loss: 0.7236 0.2217 sec/batch\n",
      "Epoch 16/20  Iteration 27608/35720 Training loss: 0.7236 0.2128 sec/batch\n",
      "Epoch 16/20  Iteration 27609/35720 Training loss: 0.7236 0.2304 sec/batch\n",
      "Epoch 16/20  Iteration 27610/35720 Training loss: 0.7236 0.2245 sec/batch\n",
      "Epoch 16/20  Iteration 27611/35720 Training loss: 0.7236 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 27612/35720 Training loss: 0.7236 0.2099 sec/batch\n",
      "Epoch 16/20  Iteration 27613/35720 Training loss: 0.7235 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 27614/35720 Training loss: 0.7235 0.2173 sec/batch\n",
      "Epoch 16/20  Iteration 27615/35720 Training loss: 0.7234 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27616/35720 Training loss: 0.7234 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 27617/35720 Training loss: 0.7234 0.2140 sec/batch\n",
      "Epoch 16/20  Iteration 27618/35720 Training loss: 0.7233 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27619/35720 Training loss: 0.7233 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 27620/35720 Training loss: 0.7233 0.2159 sec/batch\n",
      "Epoch 16/20  Iteration 27621/35720 Training loss: 0.7233 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 27622/35720 Training loss: 0.7233 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 27623/35720 Training loss: 0.7234 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 27624/35720 Training loss: 0.7234 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 27625/35720 Training loss: 0.7233 0.2082 sec/batch\n",
      "Epoch 16/20  Iteration 27626/35720 Training loss: 0.7233 0.2256 sec/batch\n",
      "Epoch 16/20  Iteration 27627/35720 Training loss: 0.7233 0.2332 sec/batch\n",
      "Epoch 16/20  Iteration 27628/35720 Training loss: 0.7233 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 27629/35720 Training loss: 0.7233 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 27630/35720 Training loss: 0.7233 0.2128 sec/batch\n",
      "Epoch 16/20  Iteration 27631/35720 Training loss: 0.7233 0.2171 sec/batch\n",
      "Epoch 16/20  Iteration 27632/35720 Training loss: 0.7233 0.2243 sec/batch\n",
      "Epoch 16/20  Iteration 27633/35720 Training loss: 0.7233 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 27634/35720 Training loss: 0.7233 0.2189 sec/batch\n",
      "Epoch 16/20  Iteration 27635/35720 Training loss: 0.7234 0.2057 sec/batch\n",
      "Epoch 16/20  Iteration 27636/35720 Training loss: 0.7234 0.2262 sec/batch\n",
      "Epoch 16/20  Iteration 27637/35720 Training loss: 0.7234 0.2187 sec/batch\n",
      "Epoch 16/20  Iteration 27638/35720 Training loss: 0.7233 0.2469 sec/batch\n",
      "Epoch 16/20  Iteration 27639/35720 Training loss: 0.7233 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 27640/35720 Training loss: 0.7234 0.2075 sec/batch\n",
      "Epoch 16/20  Iteration 27641/35720 Training loss: 0.7233 0.2122 sec/batch\n",
      "Epoch 16/20  Iteration 27642/35720 Training loss: 0.7233 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27643/35720 Training loss: 0.7233 0.2210 sec/batch\n",
      "Epoch 16/20  Iteration 27644/35720 Training loss: 0.7233 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 27645/35720 Training loss: 0.7233 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 27646/35720 Training loss: 0.7233 0.2111 sec/batch\n",
      "Epoch 16/20  Iteration 27647/35720 Training loss: 0.7232 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 27648/35720 Training loss: 0.7232 0.2143 sec/batch\n",
      "Epoch 16/20  Iteration 27649/35720 Training loss: 0.7231 0.2274 sec/batch\n",
      "Epoch 16/20  Iteration 27650/35720 Training loss: 0.7231 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 27651/35720 Training loss: 0.7231 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 27652/35720 Training loss: 0.7231 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 27653/35720 Training loss: 0.7231 0.2250 sec/batch\n",
      "Epoch 16/20  Iteration 27654/35720 Training loss: 0.7231 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 27655/35720 Training loss: 0.7230 0.2272 sec/batch\n",
      "Epoch 16/20  Iteration 27656/35720 Training loss: 0.7230 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27657/35720 Training loss: 0.7230 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 27658/35720 Training loss: 0.7230 0.2148 sec/batch\n",
      "Epoch 16/20  Iteration 27659/35720 Training loss: 0.7230 0.2347 sec/batch\n",
      "Epoch 16/20  Iteration 27660/35720 Training loss: 0.7230 0.2186 sec/batch\n",
      "Epoch 16/20  Iteration 27661/35720 Training loss: 0.7229 0.2169 sec/batch\n",
      "Epoch 16/20  Iteration 27662/35720 Training loss: 0.7229 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27663/35720 Training loss: 0.7229 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27664/35720 Training loss: 0.7229 0.2078 sec/batch\n",
      "Epoch 16/20  Iteration 27665/35720 Training loss: 0.7228 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 27666/35720 Training loss: 0.7228 0.2151 sec/batch\n",
      "Epoch 16/20  Iteration 27667/35720 Training loss: 0.7227 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 27668/35720 Training loss: 0.7227 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 27669/35720 Training loss: 0.7227 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27670/35720 Training loss: 0.7227 0.2208 sec/batch\n",
      "Epoch 16/20  Iteration 27671/35720 Training loss: 0.7226 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 27672/35720 Training loss: 0.7226 0.2174 sec/batch\n",
      "Epoch 16/20  Iteration 27673/35720 Training loss: 0.7226 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 27674/35720 Training loss: 0.7226 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27675/35720 Training loss: 0.7226 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 27676/35720 Training loss: 0.7225 0.2231 sec/batch\n",
      "Epoch 16/20  Iteration 27677/35720 Training loss: 0.7225 0.2292 sec/batch\n",
      "Epoch 16/20  Iteration 27678/35720 Training loss: 0.7225 0.2151 sec/batch\n",
      "Epoch 16/20  Iteration 27679/35720 Training loss: 0.7224 0.2211 sec/batch\n",
      "Epoch 16/20  Iteration 27680/35720 Training loss: 0.7224 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 27681/35720 Training loss: 0.7223 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 27682/35720 Training loss: 0.7223 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27683/35720 Training loss: 0.7222 0.2127 sec/batch\n",
      "Epoch 16/20  Iteration 27684/35720 Training loss: 0.7222 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 27685/35720 Training loss: 0.7222 0.2054 sec/batch\n",
      "Epoch 16/20  Iteration 27686/35720 Training loss: 0.7221 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 27687/35720 Training loss: 0.7220 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27688/35720 Training loss: 0.7219 0.2337 sec/batch\n",
      "Epoch 16/20  Iteration 27689/35720 Training loss: 0.7219 0.2261 sec/batch\n",
      "Epoch 16/20  Iteration 27690/35720 Training loss: 0.7218 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 27691/35720 Training loss: 0.7218 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 27692/35720 Training loss: 0.7217 0.2249 sec/batch\n",
      "Epoch 16/20  Iteration 27693/35720 Training loss: 0.7217 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 27694/35720 Training loss: 0.7216 0.2325 sec/batch\n",
      "Epoch 16/20  Iteration 27695/35720 Training loss: 0.7216 0.2217 sec/batch\n",
      "Epoch 16/20  Iteration 27696/35720 Training loss: 0.7216 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 27697/35720 Training loss: 0.7215 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 27698/35720 Training loss: 0.7215 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 27699/35720 Training loss: 0.7215 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 27700/35720 Training loss: 0.7215 0.2365 sec/batch\n",
      "Epoch 16/20  Iteration 27701/35720 Training loss: 0.7214 0.2117 sec/batch\n",
      "Epoch 16/20  Iteration 27702/35720 Training loss: 0.7214 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 27703/35720 Training loss: 0.7214 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 27704/35720 Training loss: 0.7214 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 27705/35720 Training loss: 0.7215 0.2329 sec/batch\n",
      "Epoch 16/20  Iteration 27706/35720 Training loss: 0.7215 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 27707/35720 Training loss: 0.7215 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 27708/35720 Training loss: 0.7214 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 27709/35720 Training loss: 0.7214 0.2290 sec/batch\n",
      "Epoch 16/20  Iteration 27710/35720 Training loss: 0.7214 0.2106 sec/batch\n",
      "Epoch 16/20  Iteration 27711/35720 Training loss: 0.7214 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 27712/35720 Training loss: 0.7213 0.2059 sec/batch\n",
      "Epoch 16/20  Iteration 27713/35720 Training loss: 0.7213 0.2051 sec/batch\n",
      "Epoch 16/20  Iteration 27714/35720 Training loss: 0.7213 0.2212 sec/batch\n",
      "Epoch 16/20  Iteration 27715/35720 Training loss: 0.7213 0.2211 sec/batch\n",
      "Epoch 16/20  Iteration 27716/35720 Training loss: 0.7213 0.2302 sec/batch\n",
      "Epoch 16/20  Iteration 27717/35720 Training loss: 0.7213 0.2268 sec/batch\n",
      "Epoch 16/20  Iteration 27718/35720 Training loss: 0.7212 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 27719/35720 Training loss: 0.7213 0.2253 sec/batch\n",
      "Epoch 16/20  Iteration 27720/35720 Training loss: 0.7212 0.2366 sec/batch\n",
      "Epoch 16/20  Iteration 27721/35720 Training loss: 0.7213 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 27722/35720 Training loss: 0.7213 0.2268 sec/batch\n",
      "Epoch 16/20  Iteration 27723/35720 Training loss: 0.7212 0.2352 sec/batch\n",
      "Epoch 16/20  Iteration 27724/35720 Training loss: 0.7213 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27725/35720 Training loss: 0.7212 0.2179 sec/batch\n",
      "Epoch 16/20  Iteration 27726/35720 Training loss: 0.7211 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 27727/35720 Training loss: 0.7210 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 27728/35720 Training loss: 0.7210 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 27729/35720 Training loss: 0.7209 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27730/35720 Training loss: 0.7208 0.2244 sec/batch\n",
      "Epoch 16/20  Iteration 27731/35720 Training loss: 0.7208 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 27732/35720 Training loss: 0.7208 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 27733/35720 Training loss: 0.7207 0.2281 sec/batch\n",
      "Epoch 16/20  Iteration 27734/35720 Training loss: 0.7208 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 27735/35720 Training loss: 0.7207 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 27736/35720 Training loss: 0.7208 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 27737/35720 Training loss: 0.7208 0.2179 sec/batch\n",
      "Epoch 16/20  Iteration 27738/35720 Training loss: 0.7207 0.2078 sec/batch\n",
      "Epoch 16/20  Iteration 27739/35720 Training loss: 0.7207 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27740/35720 Training loss: 0.7206 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 27741/35720 Training loss: 0.7206 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 27742/35720 Training loss: 0.7205 0.2242 sec/batch\n",
      "Epoch 16/20  Iteration 27743/35720 Training loss: 0.7205 0.2134 sec/batch\n",
      "Epoch 16/20  Iteration 27744/35720 Training loss: 0.7205 0.2289 sec/batch\n",
      "Epoch 16/20  Iteration 27745/35720 Training loss: 0.7205 0.2257 sec/batch\n",
      "Epoch 16/20  Iteration 27746/35720 Training loss: 0.7205 0.2111 sec/batch\n",
      "Epoch 16/20  Iteration 27747/35720 Training loss: 0.7204 0.2255 sec/batch\n",
      "Epoch 16/20  Iteration 27748/35720 Training loss: 0.7204 0.2276 sec/batch\n",
      "Epoch 16/20  Iteration 27749/35720 Training loss: 0.7204 0.2268 sec/batch\n",
      "Epoch 16/20  Iteration 27750/35720 Training loss: 0.7203 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 27751/35720 Training loss: 0.7202 0.2257 sec/batch\n",
      "Epoch 16/20  Iteration 27752/35720 Training loss: 0.7202 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 27753/35720 Training loss: 0.7202 0.2312 sec/batch\n",
      "Epoch 16/20  Iteration 27754/35720 Training loss: 0.7201 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 27755/35720 Training loss: 0.7201 0.2210 sec/batch\n",
      "Epoch 16/20  Iteration 27756/35720 Training loss: 0.7201 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27757/35720 Training loss: 0.7201 0.2059 sec/batch\n",
      "Epoch 16/20  Iteration 27758/35720 Training loss: 0.7200 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 27759/35720 Training loss: 0.7201 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 27760/35720 Training loss: 0.7202 0.2345 sec/batch\n",
      "Epoch 16/20  Iteration 27761/35720 Training loss: 0.7201 0.2153 sec/batch\n",
      "Epoch 16/20  Iteration 27762/35720 Training loss: 0.7201 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27763/35720 Training loss: 0.7201 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 27764/35720 Training loss: 0.7200 0.2145 sec/batch\n",
      "Epoch 16/20  Iteration 27765/35720 Training loss: 0.7200 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27766/35720 Training loss: 0.7199 0.2173 sec/batch\n",
      "Epoch 16/20  Iteration 27767/35720 Training loss: 0.7199 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27768/35720 Training loss: 0.7200 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27769/35720 Training loss: 0.7199 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 27770/35720 Training loss: 0.7199 0.2186 sec/batch\n",
      "Epoch 16/20  Iteration 27771/35720 Training loss: 0.7198 0.2176 sec/batch\n",
      "Epoch 16/20  Iteration 27772/35720 Training loss: 0.7198 0.2240 sec/batch\n",
      "Epoch 16/20  Iteration 27773/35720 Training loss: 0.7197 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 27774/35720 Training loss: 0.7197 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 27775/35720 Training loss: 0.7197 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27776/35720 Training loss: 0.7196 0.2168 sec/batch\n",
      "Epoch 16/20  Iteration 27777/35720 Training loss: 0.7196 0.2263 sec/batch\n",
      "Epoch 16/20  Iteration 27778/35720 Training loss: 0.7195 0.2265 sec/batch\n",
      "Epoch 16/20  Iteration 27779/35720 Training loss: 0.7195 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 27780/35720 Training loss: 0.7195 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27781/35720 Training loss: 0.7194 0.2298 sec/batch\n",
      "Epoch 16/20  Iteration 27782/35720 Training loss: 0.7194 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 27783/35720 Training loss: 0.7194 0.2254 sec/batch\n",
      "Epoch 16/20  Iteration 27784/35720 Training loss: 0.7194 0.2082 sec/batch\n",
      "Epoch 16/20  Iteration 27785/35720 Training loss: 0.7195 0.2074 sec/batch\n",
      "Epoch 16/20  Iteration 27786/35720 Training loss: 0.7195 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 27787/35720 Training loss: 0.7195 0.2099 sec/batch\n",
      "Epoch 16/20  Iteration 27788/35720 Training loss: 0.7196 0.2271 sec/batch\n",
      "Epoch 16/20  Iteration 27789/35720 Training loss: 0.7195 0.2121 sec/batch\n",
      "Epoch 16/20  Iteration 27790/35720 Training loss: 0.7195 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27791/35720 Training loss: 0.7194 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 27792/35720 Training loss: 0.7193 0.2175 sec/batch\n",
      "Epoch 16/20  Iteration 27793/35720 Training loss: 0.7192 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27794/35720 Training loss: 0.7191 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 27795/35720 Training loss: 0.7191 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 27796/35720 Training loss: 0.7190 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 27797/35720 Training loss: 0.7190 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 27798/35720 Training loss: 0.7190 0.2228 sec/batch\n",
      "Epoch 16/20  Iteration 27799/35720 Training loss: 0.7189 0.2259 sec/batch\n",
      "Epoch 16/20  Iteration 27800/35720 Training loss: 0.7189 0.2158 sec/batch\n",
      "Validation loss: 1.57814 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 27801/35720 Training loss: 0.7194 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27802/35720 Training loss: 0.7195 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 27803/35720 Training loss: 0.7195 0.2157 sec/batch\n",
      "Epoch 16/20  Iteration 27804/35720 Training loss: 0.7195 0.2194 sec/batch\n",
      "Epoch 16/20  Iteration 27805/35720 Training loss: 0.7195 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 27806/35720 Training loss: 0.7195 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27807/35720 Training loss: 0.7194 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 27808/35720 Training loss: 0.7194 0.2143 sec/batch\n",
      "Epoch 16/20  Iteration 27809/35720 Training loss: 0.7194 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27810/35720 Training loss: 0.7193 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 27811/35720 Training loss: 0.7193 0.2172 sec/batch\n",
      "Epoch 16/20  Iteration 27812/35720 Training loss: 0.7193 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27813/35720 Training loss: 0.7194 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 27814/35720 Training loss: 0.7194 0.2158 sec/batch\n",
      "Epoch 16/20  Iteration 27815/35720 Training loss: 0.7194 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 27816/35720 Training loss: 0.7194 0.2194 sec/batch\n",
      "Epoch 16/20  Iteration 27817/35720 Training loss: 0.7194 0.2174 sec/batch\n",
      "Epoch 16/20  Iteration 27818/35720 Training loss: 0.7194 0.2159 sec/batch\n",
      "Epoch 16/20  Iteration 27819/35720 Training loss: 0.7193 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 27820/35720 Training loss: 0.7193 0.2176 sec/batch\n",
      "Epoch 16/20  Iteration 27821/35720 Training loss: 0.7193 0.2238 sec/batch\n",
      "Epoch 16/20  Iteration 27822/35720 Training loss: 0.7193 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27823/35720 Training loss: 0.7193 0.2080 sec/batch\n",
      "Epoch 16/20  Iteration 27824/35720 Training loss: 0.7193 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27825/35720 Training loss: 0.7193 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 27826/35720 Training loss: 0.7193 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 27827/35720 Training loss: 0.7194 0.2157 sec/batch\n",
      "Epoch 16/20  Iteration 27828/35720 Training loss: 0.7194 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 27829/35720 Training loss: 0.7193 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27830/35720 Training loss: 0.7193 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 27831/35720 Training loss: 0.7194 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 27832/35720 Training loss: 0.7194 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27833/35720 Training loss: 0.7193 0.2210 sec/batch\n",
      "Epoch 16/20  Iteration 27834/35720 Training loss: 0.7194 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 27835/35720 Training loss: 0.7194 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27836/35720 Training loss: 0.7194 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 27837/35720 Training loss: 0.7194 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 27838/35720 Training loss: 0.7194 0.2329 sec/batch\n",
      "Epoch 16/20  Iteration 27839/35720 Training loss: 0.7194 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27840/35720 Training loss: 0.7194 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27841/35720 Training loss: 0.7194 0.2226 sec/batch\n",
      "Epoch 16/20  Iteration 27842/35720 Training loss: 0.7194 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 27843/35720 Training loss: 0.7195 0.2204 sec/batch\n",
      "Epoch 16/20  Iteration 27844/35720 Training loss: 0.7195 0.2229 sec/batch\n",
      "Epoch 16/20  Iteration 27845/35720 Training loss: 0.7196 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 27846/35720 Training loss: 0.7196 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27847/35720 Training loss: 0.7195 0.2208 sec/batch\n",
      "Epoch 16/20  Iteration 27848/35720 Training loss: 0.7196 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 27849/35720 Training loss: 0.7195 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 27850/35720 Training loss: 0.7195 0.2175 sec/batch\n",
      "Epoch 16/20  Iteration 27851/35720 Training loss: 0.7195 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 27852/35720 Training loss: 0.7196 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 27853/35720 Training loss: 0.7197 0.2218 sec/batch\n",
      "Epoch 16/20  Iteration 27854/35720 Training loss: 0.7196 0.2183 sec/batch\n",
      "Epoch 16/20  Iteration 27855/35720 Training loss: 0.7196 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 27856/35720 Training loss: 0.7196 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 27857/35720 Training loss: 0.7195 0.2155 sec/batch\n",
      "Epoch 16/20  Iteration 27858/35720 Training loss: 0.7195 0.2184 sec/batch\n",
      "Epoch 16/20  Iteration 27859/35720 Training loss: 0.7195 0.2286 sec/batch\n",
      "Epoch 16/20  Iteration 27860/35720 Training loss: 0.7196 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 27861/35720 Training loss: 0.7195 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 27862/35720 Training loss: 0.7195 0.2246 sec/batch\n",
      "Epoch 16/20  Iteration 27863/35720 Training loss: 0.7195 0.2183 sec/batch\n",
      "Epoch 16/20  Iteration 27864/35720 Training loss: 0.7195 0.2191 sec/batch\n",
      "Epoch 16/20  Iteration 27865/35720 Training loss: 0.7195 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 27866/35720 Training loss: 0.7195 0.2288 sec/batch\n",
      "Epoch 16/20  Iteration 27867/35720 Training loss: 0.7195 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 27868/35720 Training loss: 0.7195 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 27869/35720 Training loss: 0.7194 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 27870/35720 Training loss: 0.7195 0.2249 sec/batch\n",
      "Epoch 16/20  Iteration 27871/35720 Training loss: 0.7194 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 27872/35720 Training loss: 0.7194 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 27873/35720 Training loss: 0.7194 0.2420 sec/batch\n",
      "Epoch 16/20  Iteration 27874/35720 Training loss: 0.7194 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 27875/35720 Training loss: 0.7194 0.2640 sec/batch\n",
      "Epoch 16/20  Iteration 27876/35720 Training loss: 0.7194 0.2285 sec/batch\n",
      "Epoch 16/20  Iteration 27877/35720 Training loss: 0.7194 0.2400 sec/batch\n",
      "Epoch 16/20  Iteration 27878/35720 Training loss: 0.7195 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27879/35720 Training loss: 0.7195 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 27880/35720 Training loss: 0.7195 0.2145 sec/batch\n",
      "Epoch 16/20  Iteration 27881/35720 Training loss: 0.7195 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 27882/35720 Training loss: 0.7195 0.2291 sec/batch\n",
      "Epoch 16/20  Iteration 27883/35720 Training loss: 0.7195 0.2242 sec/batch\n",
      "Epoch 16/20  Iteration 27884/35720 Training loss: 0.7195 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 27885/35720 Training loss: 0.7195 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 27886/35720 Training loss: 0.7195 0.2272 sec/batch\n",
      "Epoch 16/20  Iteration 27887/35720 Training loss: 0.7194 0.2185 sec/batch\n",
      "Epoch 16/20  Iteration 27888/35720 Training loss: 0.7194 0.2227 sec/batch\n",
      "Epoch 16/20  Iteration 27889/35720 Training loss: 0.7195 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 27890/35720 Training loss: 0.7194 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 27891/35720 Training loss: 0.7195 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 27892/35720 Training loss: 0.7195 0.2297 sec/batch\n",
      "Epoch 16/20  Iteration 27893/35720 Training loss: 0.7195 0.2278 sec/batch\n",
      "Epoch 16/20  Iteration 27894/35720 Training loss: 0.7195 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 27895/35720 Training loss: 0.7195 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 27896/35720 Training loss: 0.7195 0.2139 sec/batch\n",
      "Epoch 16/20  Iteration 27897/35720 Training loss: 0.7195 0.2203 sec/batch\n",
      "Epoch 16/20  Iteration 27898/35720 Training loss: 0.7195 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27899/35720 Training loss: 0.7195 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 27900/35720 Training loss: 0.7195 0.2171 sec/batch\n",
      "Epoch 16/20  Iteration 27901/35720 Training loss: 0.7195 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 27902/35720 Training loss: 0.7194 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 27903/35720 Training loss: 0.7194 0.2168 sec/batch\n",
      "Epoch 16/20  Iteration 27904/35720 Training loss: 0.7194 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 27905/35720 Training loss: 0.7194 0.2304 sec/batch\n",
      "Epoch 16/20  Iteration 27906/35720 Training loss: 0.7194 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 27907/35720 Training loss: 0.7194 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 27908/35720 Training loss: 0.7194 0.2072 sec/batch\n",
      "Epoch 16/20  Iteration 27909/35720 Training loss: 0.7194 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 27910/35720 Training loss: 0.7194 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 27911/35720 Training loss: 0.7195 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27912/35720 Training loss: 0.7195 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 27913/35720 Training loss: 0.7195 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 27914/35720 Training loss: 0.7195 0.2307 sec/batch\n",
      "Epoch 16/20  Iteration 27915/35720 Training loss: 0.7195 0.2140 sec/batch\n",
      "Epoch 16/20  Iteration 27916/35720 Training loss: 0.7195 0.2223 sec/batch\n",
      "Epoch 16/20  Iteration 27917/35720 Training loss: 0.7195 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 27918/35720 Training loss: 0.7195 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 27919/35720 Training loss: 0.7194 0.2166 sec/batch\n",
      "Epoch 16/20  Iteration 27920/35720 Training loss: 0.7194 0.2059 sec/batch\n",
      "Epoch 16/20  Iteration 27921/35720 Training loss: 0.7194 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 27922/35720 Training loss: 0.7193 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27923/35720 Training loss: 0.7193 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 27924/35720 Training loss: 0.7193 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 27925/35720 Training loss: 0.7192 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 27926/35720 Training loss: 0.7192 0.2096 sec/batch\n",
      "Epoch 16/20  Iteration 27927/35720 Training loss: 0.7192 0.2438 sec/batch\n",
      "Epoch 16/20  Iteration 27928/35720 Training loss: 0.7191 0.2174 sec/batch\n",
      "Epoch 16/20  Iteration 27929/35720 Training loss: 0.7191 0.2206 sec/batch\n",
      "Epoch 16/20  Iteration 27930/35720 Training loss: 0.7191 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 27931/35720 Training loss: 0.7191 0.2520 sec/batch\n",
      "Epoch 16/20  Iteration 27932/35720 Training loss: 0.7190 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 27933/35720 Training loss: 0.7190 0.2473 sec/batch\n",
      "Epoch 16/20  Iteration 27934/35720 Training loss: 0.7190 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 27935/35720 Training loss: 0.7189 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 27936/35720 Training loss: 0.7189 0.2156 sec/batch\n",
      "Epoch 16/20  Iteration 27937/35720 Training loss: 0.7189 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 27938/35720 Training loss: 0.7188 0.2420 sec/batch\n",
      "Epoch 16/20  Iteration 27939/35720 Training loss: 0.7188 0.2235 sec/batch\n",
      "Epoch 16/20  Iteration 27940/35720 Training loss: 0.7188 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 27941/35720 Training loss: 0.7188 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 27942/35720 Training loss: 0.7188 0.2285 sec/batch\n",
      "Epoch 16/20  Iteration 27943/35720 Training loss: 0.7188 0.2160 sec/batch\n",
      "Epoch 16/20  Iteration 27944/35720 Training loss: 0.7188 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27945/35720 Training loss: 0.7188 0.2139 sec/batch\n",
      "Epoch 16/20  Iteration 27946/35720 Training loss: 0.7188 0.2218 sec/batch\n",
      "Epoch 16/20  Iteration 27947/35720 Training loss: 0.7188 0.2261 sec/batch\n",
      "Epoch 16/20  Iteration 27948/35720 Training loss: 0.7188 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 27949/35720 Training loss: 0.7188 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 27950/35720 Training loss: 0.7188 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27951/35720 Training loss: 0.7187 0.2055 sec/batch\n",
      "Epoch 16/20  Iteration 27952/35720 Training loss: 0.7187 0.2121 sec/batch\n",
      "Epoch 16/20  Iteration 27953/35720 Training loss: 0.7188 0.2503 sec/batch\n",
      "Epoch 16/20  Iteration 27954/35720 Training loss: 0.7188 0.2241 sec/batch\n",
      "Epoch 16/20  Iteration 27955/35720 Training loss: 0.7188 0.2074 sec/batch\n",
      "Epoch 16/20  Iteration 27956/35720 Training loss: 0.7187 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 27957/35720 Training loss: 0.7187 0.2191 sec/batch\n",
      "Epoch 16/20  Iteration 27958/35720 Training loss: 0.7187 0.2387 sec/batch\n",
      "Epoch 16/20  Iteration 27959/35720 Training loss: 0.7188 0.2134 sec/batch\n",
      "Epoch 16/20  Iteration 27960/35720 Training loss: 0.7188 0.2164 sec/batch\n",
      "Epoch 16/20  Iteration 27961/35720 Training loss: 0.7188 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 27962/35720 Training loss: 0.7188 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 27963/35720 Training loss: 0.7188 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 27964/35720 Training loss: 0.7188 0.2204 sec/batch\n",
      "Epoch 16/20  Iteration 27965/35720 Training loss: 0.7188 0.2309 sec/batch\n",
      "Epoch 16/20  Iteration 27966/35720 Training loss: 0.7189 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 27967/35720 Training loss: 0.7189 0.2111 sec/batch\n",
      "Epoch 16/20  Iteration 27968/35720 Training loss: 0.7189 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 27969/35720 Training loss: 0.7189 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 27970/35720 Training loss: 0.7189 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 27971/35720 Training loss: 0.7189 0.2156 sec/batch\n",
      "Epoch 16/20  Iteration 27972/35720 Training loss: 0.7189 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 27973/35720 Training loss: 0.7189 0.2466 sec/batch\n",
      "Epoch 16/20  Iteration 27974/35720 Training loss: 0.7188 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 27975/35720 Training loss: 0.7188 0.2057 sec/batch\n",
      "Epoch 16/20  Iteration 27976/35720 Training loss: 0.7188 0.2291 sec/batch\n",
      "Epoch 16/20  Iteration 27977/35720 Training loss: 0.7188 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 27978/35720 Training loss: 0.7188 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 27979/35720 Training loss: 0.7187 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 27980/35720 Training loss: 0.7188 0.2253 sec/batch\n",
      "Epoch 16/20  Iteration 27981/35720 Training loss: 0.7188 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 27982/35720 Training loss: 0.7188 0.2338 sec/batch\n",
      "Epoch 16/20  Iteration 27983/35720 Training loss: 0.7188 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 27984/35720 Training loss: 0.7188 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 27985/35720 Training loss: 0.7188 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 27986/35720 Training loss: 0.7188 0.2276 sec/batch\n",
      "Epoch 16/20  Iteration 27987/35720 Training loss: 0.7189 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 27988/35720 Training loss: 0.7189 0.2394 sec/batch\n",
      "Epoch 16/20  Iteration 27989/35720 Training loss: 0.7189 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 27990/35720 Training loss: 0.7188 0.2127 sec/batch\n",
      "Epoch 16/20  Iteration 27991/35720 Training loss: 0.7188 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 27992/35720 Training loss: 0.7188 0.2228 sec/batch\n",
      "Epoch 16/20  Iteration 27993/35720 Training loss: 0.7187 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 27994/35720 Training loss: 0.7187 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 27995/35720 Training loss: 0.7186 0.2239 sec/batch\n",
      "Epoch 16/20  Iteration 27996/35720 Training loss: 0.7186 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 27997/35720 Training loss: 0.7186 0.2272 sec/batch\n",
      "Epoch 16/20  Iteration 27998/35720 Training loss: 0.7186 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 27999/35720 Training loss: 0.7186 0.2218 sec/batch\n",
      "Epoch 16/20  Iteration 28000/35720 Training loss: 0.7186 0.2125 sec/batch\n",
      "Validation loss: 1.56588 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 28001/35720 Training loss: 0.7189 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 28002/35720 Training loss: 0.7190 0.2080 sec/batch\n",
      "Epoch 16/20  Iteration 28003/35720 Training loss: 0.7189 0.2290 sec/batch\n",
      "Epoch 16/20  Iteration 28004/35720 Training loss: 0.7190 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 28005/35720 Training loss: 0.7189 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 28006/35720 Training loss: 0.7190 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 28007/35720 Training loss: 0.7190 0.2269 sec/batch\n",
      "Epoch 16/20  Iteration 28008/35720 Training loss: 0.7189 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 28009/35720 Training loss: 0.7189 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 28010/35720 Training loss: 0.7189 0.2115 sec/batch\n",
      "Epoch 16/20  Iteration 28011/35720 Training loss: 0.7189 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 28012/35720 Training loss: 0.7189 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 28013/35720 Training loss: 0.7189 0.2184 sec/batch\n",
      "Epoch 16/20  Iteration 28014/35720 Training loss: 0.7188 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 28015/35720 Training loss: 0.7188 0.2155 sec/batch\n",
      "Epoch 16/20  Iteration 28016/35720 Training loss: 0.7188 0.2106 sec/batch\n",
      "Epoch 16/20  Iteration 28017/35720 Training loss: 0.7188 0.2214 sec/batch\n",
      "Epoch 16/20  Iteration 28018/35720 Training loss: 0.7188 0.2332 sec/batch\n",
      "Epoch 16/20  Iteration 28019/35720 Training loss: 0.7188 0.2180 sec/batch\n",
      "Epoch 16/20  Iteration 28020/35720 Training loss: 0.7188 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 28021/35720 Training loss: 0.7188 0.2053 sec/batch\n",
      "Epoch 16/20  Iteration 28022/35720 Training loss: 0.7188 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 28023/35720 Training loss: 0.7188 0.2130 sec/batch\n",
      "Epoch 16/20  Iteration 28024/35720 Training loss: 0.7188 0.2233 sec/batch\n",
      "Epoch 16/20  Iteration 28025/35720 Training loss: 0.7188 0.2121 sec/batch\n",
      "Epoch 16/20  Iteration 28026/35720 Training loss: 0.7188 0.2330 sec/batch\n",
      "Epoch 16/20  Iteration 28027/35720 Training loss: 0.7188 0.2171 sec/batch\n",
      "Epoch 16/20  Iteration 28028/35720 Training loss: 0.7187 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 28029/35720 Training loss: 0.7187 0.2132 sec/batch\n",
      "Epoch 16/20  Iteration 28030/35720 Training loss: 0.7186 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 28031/35720 Training loss: 0.7186 0.2218 sec/batch\n",
      "Epoch 16/20  Iteration 28032/35720 Training loss: 0.7186 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 28033/35720 Training loss: 0.7186 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 28034/35720 Training loss: 0.7185 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 28035/35720 Training loss: 0.7185 0.2456 sec/batch\n",
      "Epoch 16/20  Iteration 28036/35720 Training loss: 0.7184 0.2221 sec/batch\n",
      "Epoch 16/20  Iteration 28037/35720 Training loss: 0.7185 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 28038/35720 Training loss: 0.7184 0.2155 sec/batch\n",
      "Epoch 16/20  Iteration 28039/35720 Training loss: 0.7184 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 28040/35720 Training loss: 0.7184 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 28041/35720 Training loss: 0.7183 0.2091 sec/batch\n",
      "Epoch 16/20  Iteration 28042/35720 Training loss: 0.7183 0.2228 sec/batch\n",
      "Epoch 16/20  Iteration 28043/35720 Training loss: 0.7183 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 28044/35720 Training loss: 0.7183 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 28045/35720 Training loss: 0.7183 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 28046/35720 Training loss: 0.7183 0.2202 sec/batch\n",
      "Epoch 16/20  Iteration 28047/35720 Training loss: 0.7182 0.2281 sec/batch\n",
      "Epoch 16/20  Iteration 28048/35720 Training loss: 0.7182 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 28049/35720 Training loss: 0.7182 0.2074 sec/batch\n",
      "Epoch 16/20  Iteration 28050/35720 Training loss: 0.7182 0.2080 sec/batch\n",
      "Epoch 16/20  Iteration 28051/35720 Training loss: 0.7181 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 28052/35720 Training loss: 0.7181 0.2132 sec/batch\n",
      "Epoch 16/20  Iteration 28053/35720 Training loss: 0.7181 0.2057 sec/batch\n",
      "Epoch 16/20  Iteration 28054/35720 Training loss: 0.7180 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 28055/35720 Training loss: 0.7180 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 28056/35720 Training loss: 0.7180 0.2155 sec/batch\n",
      "Epoch 16/20  Iteration 28057/35720 Training loss: 0.7179 0.2348 sec/batch\n",
      "Epoch 16/20  Iteration 28058/35720 Training loss: 0.7179 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 28059/35720 Training loss: 0.7179 0.2252 sec/batch\n",
      "Epoch 16/20  Iteration 28060/35720 Training loss: 0.7179 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 28061/35720 Training loss: 0.7179 0.2226 sec/batch\n",
      "Epoch 16/20  Iteration 28062/35720 Training loss: 0.7178 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 28063/35720 Training loss: 0.7178 0.2241 sec/batch\n",
      "Epoch 16/20  Iteration 28064/35720 Training loss: 0.7178 0.2240 sec/batch\n",
      "Epoch 16/20  Iteration 28065/35720 Training loss: 0.7178 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 28066/35720 Training loss: 0.7178 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 28067/35720 Training loss: 0.7177 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 28068/35720 Training loss: 0.7177 0.2159 sec/batch\n",
      "Epoch 16/20  Iteration 28069/35720 Training loss: 0.7176 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 28070/35720 Training loss: 0.7176 0.2264 sec/batch\n",
      "Epoch 16/20  Iteration 28071/35720 Training loss: 0.7176 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 28072/35720 Training loss: 0.7176 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 28073/35720 Training loss: 0.7176 0.2188 sec/batch\n",
      "Epoch 16/20  Iteration 28074/35720 Training loss: 0.7175 0.2343 sec/batch\n",
      "Epoch 16/20  Iteration 28075/35720 Training loss: 0.7175 0.2246 sec/batch\n",
      "Epoch 16/20  Iteration 28076/35720 Training loss: 0.7175 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 28077/35720 Training loss: 0.7175 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 28078/35720 Training loss: 0.7174 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 28079/35720 Training loss: 0.7174 0.2541 sec/batch\n",
      "Epoch 16/20  Iteration 28080/35720 Training loss: 0.7175 0.2324 sec/batch\n",
      "Epoch 16/20  Iteration 28081/35720 Training loss: 0.7174 0.2188 sec/batch\n",
      "Epoch 16/20  Iteration 28082/35720 Training loss: 0.7174 0.2145 sec/batch\n",
      "Epoch 16/20  Iteration 28083/35720 Training loss: 0.7174 0.2079 sec/batch\n",
      "Epoch 16/20  Iteration 28084/35720 Training loss: 0.7174 0.2411 sec/batch\n",
      "Epoch 16/20  Iteration 28085/35720 Training loss: 0.7174 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 28086/35720 Training loss: 0.7173 0.2200 sec/batch\n",
      "Epoch 16/20  Iteration 28087/35720 Training loss: 0.7173 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 28088/35720 Training loss: 0.7173 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 28089/35720 Training loss: 0.7173 0.2166 sec/batch\n",
      "Epoch 16/20  Iteration 28090/35720 Training loss: 0.7172 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 28091/35720 Training loss: 0.7172 0.2289 sec/batch\n",
      "Epoch 16/20  Iteration 28092/35720 Training loss: 0.7172 0.2266 sec/batch\n",
      "Epoch 16/20  Iteration 28093/35720 Training loss: 0.7171 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 28094/35720 Training loss: 0.7171 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 28095/35720 Training loss: 0.7171 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 28096/35720 Training loss: 0.7171 0.2213 sec/batch\n",
      "Epoch 16/20  Iteration 28097/35720 Training loss: 0.7171 0.2270 sec/batch\n",
      "Epoch 16/20  Iteration 28098/35720 Training loss: 0.7171 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 28099/35720 Training loss: 0.7171 0.2202 sec/batch\n",
      "Epoch 16/20  Iteration 28100/35720 Training loss: 0.7170 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 28101/35720 Training loss: 0.7170 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 28102/35720 Training loss: 0.7170 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 28103/35720 Training loss: 0.7170 0.2224 sec/batch\n",
      "Epoch 16/20  Iteration 28104/35720 Training loss: 0.7170 0.2054 sec/batch\n",
      "Epoch 16/20  Iteration 28105/35720 Training loss: 0.7170 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 28106/35720 Training loss: 0.7170 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 28107/35720 Training loss: 0.7169 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 28108/35720 Training loss: 0.7169 0.2140 sec/batch\n",
      "Epoch 16/20  Iteration 28109/35720 Training loss: 0.7169 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 28110/35720 Training loss: 0.7168 0.2218 sec/batch\n",
      "Epoch 16/20  Iteration 28111/35720 Training loss: 0.7168 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 28112/35720 Training loss: 0.7169 0.2277 sec/batch\n",
      "Epoch 16/20  Iteration 28113/35720 Training loss: 0.7168 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 28114/35720 Training loss: 0.7169 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 28115/35720 Training loss: 0.7169 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 28116/35720 Training loss: 0.7168 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 28117/35720 Training loss: 0.7169 0.2118 sec/batch\n",
      "Epoch 16/20  Iteration 28118/35720 Training loss: 0.7168 0.2224 sec/batch\n",
      "Epoch 16/20  Iteration 28119/35720 Training loss: 0.7168 0.2294 sec/batch\n",
      "Epoch 16/20  Iteration 28120/35720 Training loss: 0.7168 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 28121/35720 Training loss: 0.7168 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 28122/35720 Training loss: 0.7168 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 28123/35720 Training loss: 0.7169 0.2325 sec/batch\n",
      "Epoch 16/20  Iteration 28124/35720 Training loss: 0.7168 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 28125/35720 Training loss: 0.7168 0.2274 sec/batch\n",
      "Epoch 16/20  Iteration 28126/35720 Training loss: 0.7168 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 28127/35720 Training loss: 0.7169 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 28128/35720 Training loss: 0.7169 0.2236 sec/batch\n",
      "Epoch 16/20  Iteration 28129/35720 Training loss: 0.7169 0.2217 sec/batch\n",
      "Epoch 16/20  Iteration 28130/35720 Training loss: 0.7169 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 28131/35720 Training loss: 0.7169 0.2186 sec/batch\n",
      "Epoch 16/20  Iteration 28132/35720 Training loss: 0.7169 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 28133/35720 Training loss: 0.7168 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 28134/35720 Training loss: 0.7168 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 28135/35720 Training loss: 0.7168 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 28136/35720 Training loss: 0.7168 0.2280 sec/batch\n",
      "Epoch 16/20  Iteration 28137/35720 Training loss: 0.7167 0.2175 sec/batch\n",
      "Epoch 16/20  Iteration 28138/35720 Training loss: 0.7167 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 28139/35720 Training loss: 0.7167 0.2222 sec/batch\n",
      "Epoch 16/20  Iteration 28140/35720 Training loss: 0.7167 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 28141/35720 Training loss: 0.7167 0.2143 sec/batch\n",
      "Epoch 16/20  Iteration 28142/35720 Training loss: 0.7168 0.2284 sec/batch\n",
      "Epoch 16/20  Iteration 28143/35720 Training loss: 0.7168 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 28144/35720 Training loss: 0.7168 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 28145/35720 Training loss: 0.7167 0.2305 sec/batch\n",
      "Epoch 16/20  Iteration 28146/35720 Training loss: 0.7167 0.2228 sec/batch\n",
      "Epoch 16/20  Iteration 28147/35720 Training loss: 0.7167 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 28148/35720 Training loss: 0.7167 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 28149/35720 Training loss: 0.7166 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 28150/35720 Training loss: 0.7166 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 28151/35720 Training loss: 0.7165 0.2262 sec/batch\n",
      "Epoch 16/20  Iteration 28152/35720 Training loss: 0.7165 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 28153/35720 Training loss: 0.7165 0.2277 sec/batch\n",
      "Epoch 16/20  Iteration 28154/35720 Training loss: 0.7165 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 28155/35720 Training loss: 0.7165 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 28156/35720 Training loss: 0.7165 0.2247 sec/batch\n",
      "Epoch 16/20  Iteration 28157/35720 Training loss: 0.7165 0.2199 sec/batch\n",
      "Epoch 16/20  Iteration 28158/35720 Training loss: 0.7164 0.2181 sec/batch\n",
      "Epoch 16/20  Iteration 28159/35720 Training loss: 0.7165 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 28160/35720 Training loss: 0.7165 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 28161/35720 Training loss: 0.7165 0.2116 sec/batch\n",
      "Epoch 16/20  Iteration 28162/35720 Training loss: 0.7165 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 28163/35720 Training loss: 0.7165 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 28164/35720 Training loss: 0.7164 0.2279 sec/batch\n",
      "Epoch 16/20  Iteration 28165/35720 Training loss: 0.7164 0.2189 sec/batch\n",
      "Epoch 16/20  Iteration 28166/35720 Training loss: 0.7164 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 28167/35720 Training loss: 0.7164 0.2104 sec/batch\n",
      "Epoch 16/20  Iteration 28168/35720 Training loss: 0.7164 0.2229 sec/batch\n",
      "Epoch 16/20  Iteration 28169/35720 Training loss: 0.7164 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 28170/35720 Training loss: 0.7164 0.2266 sec/batch\n",
      "Epoch 16/20  Iteration 28171/35720 Training loss: 0.7164 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 28172/35720 Training loss: 0.7164 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 28173/35720 Training loss: 0.7164 0.2132 sec/batch\n",
      "Epoch 16/20  Iteration 28174/35720 Training loss: 0.7163 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 28175/35720 Training loss: 0.7164 0.2220 sec/batch\n",
      "Epoch 16/20  Iteration 28176/35720 Training loss: 0.7164 0.2211 sec/batch\n",
      "Epoch 16/20  Iteration 28177/35720 Training loss: 0.7164 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 28178/35720 Training loss: 0.7164 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 28179/35720 Training loss: 0.7164 0.2279 sec/batch\n",
      "Epoch 16/20  Iteration 28180/35720 Training loss: 0.7164 0.2214 sec/batch\n",
      "Epoch 16/20  Iteration 28181/35720 Training loss: 0.7163 0.2145 sec/batch\n",
      "Epoch 16/20  Iteration 28182/35720 Training loss: 0.7163 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 28183/35720 Training loss: 0.7162 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 28184/35720 Training loss: 0.7162 0.2224 sec/batch\n",
      "Epoch 16/20  Iteration 28185/35720 Training loss: 0.7162 0.2173 sec/batch\n",
      "Epoch 16/20  Iteration 28186/35720 Training loss: 0.7161 0.2195 sec/batch\n",
      "Epoch 16/20  Iteration 28187/35720 Training loss: 0.7161 0.2074 sec/batch\n",
      "Epoch 16/20  Iteration 28188/35720 Training loss: 0.7161 0.2081 sec/batch\n",
      "Epoch 16/20  Iteration 28189/35720 Training loss: 0.7161 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 28190/35720 Training loss: 0.7160 0.2238 sec/batch\n",
      "Epoch 16/20  Iteration 28191/35720 Training loss: 0.7160 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 28192/35720 Training loss: 0.7159 0.2223 sec/batch\n",
      "Epoch 16/20  Iteration 28193/35720 Training loss: 0.7160 0.2172 sec/batch\n",
      "Epoch 16/20  Iteration 28194/35720 Training loss: 0.7160 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 28195/35720 Training loss: 0.7159 0.2125 sec/batch\n",
      "Epoch 16/20  Iteration 28196/35720 Training loss: 0.7159 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 28197/35720 Training loss: 0.7159 0.2315 sec/batch\n",
      "Epoch 16/20  Iteration 28198/35720 Training loss: 0.7159 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 28199/35720 Training loss: 0.7159 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 28200/35720 Training loss: 0.7159 0.2095 sec/batch\n",
      "Validation loss: 1.58397 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 28201/35720 Training loss: 0.7162 0.2121 sec/batch\n",
      "Epoch 16/20  Iteration 28202/35720 Training loss: 0.7162 0.2108 sec/batch\n",
      "Epoch 16/20  Iteration 28203/35720 Training loss: 0.7161 0.2071 sec/batch\n",
      "Epoch 16/20  Iteration 28204/35720 Training loss: 0.7161 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 28205/35720 Training loss: 0.7161 0.2175 sec/batch\n",
      "Epoch 16/20  Iteration 28206/35720 Training loss: 0.7161 0.2182 sec/batch\n",
      "Epoch 16/20  Iteration 28207/35720 Training loss: 0.7161 0.2322 sec/batch\n",
      "Epoch 16/20  Iteration 28208/35720 Training loss: 0.7161 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 28209/35720 Training loss: 0.7161 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 28210/35720 Training loss: 0.7161 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 28211/35720 Training loss: 0.7161 0.2275 sec/batch\n",
      "Epoch 16/20  Iteration 28212/35720 Training loss: 0.7161 0.2254 sec/batch\n",
      "Epoch 16/20  Iteration 28213/35720 Training loss: 0.7161 0.2208 sec/batch\n",
      "Epoch 16/20  Iteration 28214/35720 Training loss: 0.7161 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 28215/35720 Training loss: 0.7161 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 28216/35720 Training loss: 0.7161 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 28217/35720 Training loss: 0.7161 0.2240 sec/batch\n",
      "Epoch 16/20  Iteration 28218/35720 Training loss: 0.7161 0.2160 sec/batch\n",
      "Epoch 16/20  Iteration 28219/35720 Training loss: 0.7161 0.2236 sec/batch\n",
      "Epoch 16/20  Iteration 28220/35720 Training loss: 0.7161 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 28221/35720 Training loss: 0.7161 0.2143 sec/batch\n",
      "Epoch 16/20  Iteration 28222/35720 Training loss: 0.7161 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 28223/35720 Training loss: 0.7160 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 28224/35720 Training loss: 0.7160 0.2348 sec/batch\n",
      "Epoch 16/20  Iteration 28225/35720 Training loss: 0.7160 0.2420 sec/batch\n",
      "Epoch 16/20  Iteration 28226/35720 Training loss: 0.7160 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 28227/35720 Training loss: 0.7160 0.2120 sec/batch\n",
      "Epoch 16/20  Iteration 28228/35720 Training loss: 0.7160 0.2252 sec/batch\n",
      "Epoch 16/20  Iteration 28229/35720 Training loss: 0.7160 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 28230/35720 Training loss: 0.7160 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 28231/35720 Training loss: 0.7161 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 28232/35720 Training loss: 0.7161 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 28233/35720 Training loss: 0.7161 0.2448 sec/batch\n",
      "Epoch 16/20  Iteration 28234/35720 Training loss: 0.7161 0.2216 sec/batch\n",
      "Epoch 16/20  Iteration 28235/35720 Training loss: 0.7161 0.2245 sec/batch\n",
      "Epoch 16/20  Iteration 28236/35720 Training loss: 0.7161 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 28237/35720 Training loss: 0.7161 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 28238/35720 Training loss: 0.7161 0.2155 sec/batch\n",
      "Epoch 16/20  Iteration 28239/35720 Training loss: 0.7161 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 28240/35720 Training loss: 0.7160 0.2072 sec/batch\n",
      "Epoch 16/20  Iteration 28241/35720 Training loss: 0.7160 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 28242/35720 Training loss: 0.7160 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 28243/35720 Training loss: 0.7160 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 28244/35720 Training loss: 0.7160 0.2275 sec/batch\n",
      "Epoch 16/20  Iteration 28245/35720 Training loss: 0.7160 0.2255 sec/batch\n",
      "Epoch 16/20  Iteration 28246/35720 Training loss: 0.7160 0.2224 sec/batch\n",
      "Epoch 16/20  Iteration 28247/35720 Training loss: 0.7161 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 28248/35720 Training loss: 0.7161 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 28249/35720 Training loss: 0.7161 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 28250/35720 Training loss: 0.7161 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 28251/35720 Training loss: 0.7161 0.2134 sec/batch\n",
      "Epoch 16/20  Iteration 28252/35720 Training loss: 0.7161 0.2267 sec/batch\n",
      "Epoch 16/20  Iteration 28253/35720 Training loss: 0.7162 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 28254/35720 Training loss: 0.7162 0.2124 sec/batch\n",
      "Epoch 16/20  Iteration 28255/35720 Training loss: 0.7161 0.2183 sec/batch\n",
      "Epoch 16/20  Iteration 28256/35720 Training loss: 0.7161 0.2118 sec/batch\n",
      "Epoch 16/20  Iteration 28257/35720 Training loss: 0.7161 0.2335 sec/batch\n",
      "Epoch 16/20  Iteration 28258/35720 Training loss: 0.7161 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 28259/35720 Training loss: 0.7161 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 28260/35720 Training loss: 0.7161 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 28261/35720 Training loss: 0.7161 0.2180 sec/batch\n",
      "Epoch 16/20  Iteration 28262/35720 Training loss: 0.7161 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 28263/35720 Training loss: 0.7161 0.2309 sec/batch\n",
      "Epoch 16/20  Iteration 28264/35720 Training loss: 0.7160 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 28265/35720 Training loss: 0.7160 0.2185 sec/batch\n",
      "Epoch 16/20  Iteration 28266/35720 Training loss: 0.7159 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 28267/35720 Training loss: 0.7158 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 28268/35720 Training loss: 0.7158 0.2248 sec/batch\n",
      "Epoch 16/20  Iteration 28269/35720 Training loss: 0.7158 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 28270/35720 Training loss: 0.7158 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 28271/35720 Training loss: 0.7158 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 28272/35720 Training loss: 0.7158 0.2276 sec/batch\n",
      "Epoch 16/20  Iteration 28273/35720 Training loss: 0.7158 0.2336 sec/batch\n",
      "Epoch 16/20  Iteration 28274/35720 Training loss: 0.7157 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 28275/35720 Training loss: 0.7157 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 28276/35720 Training loss: 0.7157 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 28277/35720 Training loss: 0.7157 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 28278/35720 Training loss: 0.7157 0.2411 sec/batch\n",
      "Epoch 16/20  Iteration 28279/35720 Training loss: 0.7157 0.2246 sec/batch\n",
      "Epoch 16/20  Iteration 28280/35720 Training loss: 0.7157 0.2183 sec/batch\n",
      "Epoch 16/20  Iteration 28281/35720 Training loss: 0.7157 0.2237 sec/batch\n",
      "Epoch 16/20  Iteration 28282/35720 Training loss: 0.7156 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 28283/35720 Training loss: 0.7156 0.2317 sec/batch\n",
      "Epoch 16/20  Iteration 28284/35720 Training loss: 0.7156 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 28285/35720 Training loss: 0.7156 0.2274 sec/batch\n",
      "Epoch 16/20  Iteration 28286/35720 Training loss: 0.7156 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 28287/35720 Training loss: 0.7156 0.2059 sec/batch\n",
      "Epoch 16/20  Iteration 28288/35720 Training loss: 0.7155 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 28289/35720 Training loss: 0.7155 0.2190 sec/batch\n",
      "Epoch 16/20  Iteration 28290/35720 Training loss: 0.7155 0.2382 sec/batch\n",
      "Epoch 16/20  Iteration 28291/35720 Training loss: 0.7155 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 28292/35720 Training loss: 0.7154 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 28293/35720 Training loss: 0.7154 0.2133 sec/batch\n",
      "Epoch 16/20  Iteration 28294/35720 Training loss: 0.7154 0.2160 sec/batch\n",
      "Epoch 16/20  Iteration 28295/35720 Training loss: 0.7154 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 28296/35720 Training loss: 0.7155 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 28297/35720 Training loss: 0.7154 0.2127 sec/batch\n",
      "Epoch 16/20  Iteration 28298/35720 Training loss: 0.7154 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 28299/35720 Training loss: 0.7154 0.2198 sec/batch\n",
      "Epoch 16/20  Iteration 28300/35720 Training loss: 0.7154 0.2075 sec/batch\n",
      "Epoch 16/20  Iteration 28301/35720 Training loss: 0.7154 0.2250 sec/batch\n",
      "Epoch 16/20  Iteration 28302/35720 Training loss: 0.7154 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 28303/35720 Training loss: 0.7154 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 28304/35720 Training loss: 0.7154 0.2116 sec/batch\n",
      "Epoch 16/20  Iteration 28305/35720 Training loss: 0.7154 0.2159 sec/batch\n",
      "Epoch 16/20  Iteration 28306/35720 Training loss: 0.7154 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 28307/35720 Training loss: 0.7154 0.2203 sec/batch\n",
      "Epoch 16/20  Iteration 28308/35720 Training loss: 0.7153 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 28309/35720 Training loss: 0.7154 0.2147 sec/batch\n",
      "Epoch 16/20  Iteration 28310/35720 Training loss: 0.7153 0.2171 sec/batch\n",
      "Epoch 16/20  Iteration 28311/35720 Training loss: 0.7153 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 28312/35720 Training loss: 0.7153 0.2190 sec/batch\n",
      "Epoch 16/20  Iteration 28313/35720 Training loss: 0.7154 0.2194 sec/batch\n",
      "Epoch 16/20  Iteration 28314/35720 Training loss: 0.7154 0.2120 sec/batch\n",
      "Epoch 16/20  Iteration 28315/35720 Training loss: 0.7154 0.2128 sec/batch\n",
      "Epoch 16/20  Iteration 28316/35720 Training loss: 0.7154 0.2157 sec/batch\n",
      "Epoch 16/20  Iteration 28317/35720 Training loss: 0.7154 0.2164 sec/batch\n",
      "Epoch 16/20  Iteration 28318/35720 Training loss: 0.7154 0.2235 sec/batch\n",
      "Epoch 16/20  Iteration 28319/35720 Training loss: 0.7154 0.2287 sec/batch\n",
      "Epoch 16/20  Iteration 28320/35720 Training loss: 0.7154 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 28321/35720 Training loss: 0.7154 0.2114 sec/batch\n",
      "Epoch 16/20  Iteration 28322/35720 Training loss: 0.7154 0.2262 sec/batch\n",
      "Epoch 16/20  Iteration 28323/35720 Training loss: 0.7154 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 28324/35720 Training loss: 0.7154 0.2158 sec/batch\n",
      "Epoch 16/20  Iteration 28325/35720 Training loss: 0.7154 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 28326/35720 Training loss: 0.7154 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 28327/35720 Training loss: 0.7154 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 28328/35720 Training loss: 0.7153 0.2209 sec/batch\n",
      "Epoch 16/20  Iteration 28329/35720 Training loss: 0.7153 0.2201 sec/batch\n",
      "Epoch 16/20  Iteration 28330/35720 Training loss: 0.7153 0.2224 sec/batch\n",
      "Epoch 16/20  Iteration 28331/35720 Training loss: 0.7153 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 28332/35720 Training loss: 0.7153 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 28333/35720 Training loss: 0.7152 0.2138 sec/batch\n",
      "Epoch 16/20  Iteration 28334/35720 Training loss: 0.7152 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 28335/35720 Training loss: 0.7152 0.2279 sec/batch\n",
      "Epoch 16/20  Iteration 28336/35720 Training loss: 0.7151 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 28337/35720 Training loss: 0.7151 0.2106 sec/batch\n",
      "Epoch 16/20  Iteration 28338/35720 Training loss: 0.7151 0.2164 sec/batch\n",
      "Epoch 16/20  Iteration 28339/35720 Training loss: 0.7152 0.2236 sec/batch\n",
      "Epoch 16/20  Iteration 28340/35720 Training loss: 0.7151 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 28341/35720 Training loss: 0.7151 0.2089 sec/batch\n",
      "Epoch 16/20  Iteration 28342/35720 Training loss: 0.7151 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 28343/35720 Training loss: 0.7151 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 28344/35720 Training loss: 0.7150 0.2127 sec/batch\n",
      "Epoch 16/20  Iteration 28345/35720 Training loss: 0.7150 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 28346/35720 Training loss: 0.7150 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 28347/35720 Training loss: 0.7150 0.2260 sec/batch\n",
      "Epoch 16/20  Iteration 28348/35720 Training loss: 0.7149 0.2065 sec/batch\n",
      "Epoch 16/20  Iteration 28349/35720 Training loss: 0.7149 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 28350/35720 Training loss: 0.7149 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 28351/35720 Training loss: 0.7148 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 28352/35720 Training loss: 0.7148 0.2142 sec/batch\n",
      "Epoch 16/20  Iteration 28353/35720 Training loss: 0.7148 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 28354/35720 Training loss: 0.7148 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 28355/35720 Training loss: 0.7148 0.2107 sec/batch\n",
      "Epoch 16/20  Iteration 28356/35720 Training loss: 0.7147 0.2321 sec/batch\n",
      "Epoch 16/20  Iteration 28357/35720 Training loss: 0.7147 0.2186 sec/batch\n",
      "Epoch 16/20  Iteration 28358/35720 Training loss: 0.7147 0.2264 sec/batch\n",
      "Epoch 16/20  Iteration 28359/35720 Training loss: 0.7147 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 28360/35720 Training loss: 0.7147 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 28361/35720 Training loss: 0.7147 0.2122 sec/batch\n",
      "Epoch 16/20  Iteration 28362/35720 Training loss: 0.7147 0.2242 sec/batch\n",
      "Epoch 16/20  Iteration 28363/35720 Training loss: 0.7147 0.2172 sec/batch\n",
      "Epoch 16/20  Iteration 28364/35720 Training loss: 0.7146 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 28365/35720 Training loss: 0.7146 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 28366/35720 Training loss: 0.7146 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 28367/35720 Training loss: 0.7146 0.2265 sec/batch\n",
      "Epoch 16/20  Iteration 28368/35720 Training loss: 0.7146 0.2231 sec/batch\n",
      "Epoch 16/20  Iteration 28369/35720 Training loss: 0.7146 0.2160 sec/batch\n",
      "Epoch 16/20  Iteration 28370/35720 Training loss: 0.7146 0.2071 sec/batch\n",
      "Epoch 16/20  Iteration 28371/35720 Training loss: 0.7146 0.2090 sec/batch\n",
      "Epoch 16/20  Iteration 28372/35720 Training loss: 0.7146 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 28373/35720 Training loss: 0.7146 0.2085 sec/batch\n",
      "Epoch 16/20  Iteration 28374/35720 Training loss: 0.7145 0.2185 sec/batch\n",
      "Epoch 16/20  Iteration 28375/35720 Training loss: 0.7146 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 28376/35720 Training loss: 0.7145 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 28377/35720 Training loss: 0.7145 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 28378/35720 Training loss: 0.7145 0.2189 sec/batch\n",
      "Epoch 16/20  Iteration 28379/35720 Training loss: 0.7144 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 28380/35720 Training loss: 0.7144 0.2181 sec/batch\n",
      "Epoch 16/20  Iteration 28381/35720 Training loss: 0.7144 0.2111 sec/batch\n",
      "Epoch 16/20  Iteration 28382/35720 Training loss: 0.7144 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 28383/35720 Training loss: 0.7144 0.2228 sec/batch\n",
      "Epoch 16/20  Iteration 28384/35720 Training loss: 0.7144 0.2225 sec/batch\n",
      "Epoch 16/20  Iteration 28385/35720 Training loss: 0.7143 0.2187 sec/batch\n",
      "Epoch 16/20  Iteration 28386/35720 Training loss: 0.7143 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 28387/35720 Training loss: 0.7143 0.2074 sec/batch\n",
      "Epoch 16/20  Iteration 28388/35720 Training loss: 0.7142 0.2075 sec/batch\n",
      "Epoch 16/20  Iteration 28389/35720 Training loss: 0.7142 0.2249 sec/batch\n",
      "Epoch 16/20  Iteration 28390/35720 Training loss: 0.7142 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 28391/35720 Training loss: 0.7142 0.2244 sec/batch\n",
      "Epoch 16/20  Iteration 28392/35720 Training loss: 0.7141 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 28393/35720 Training loss: 0.7141 0.2120 sec/batch\n",
      "Epoch 16/20  Iteration 28394/35720 Training loss: 0.7141 0.2185 sec/batch\n",
      "Epoch 16/20  Iteration 28395/35720 Training loss: 0.7141 0.2169 sec/batch\n",
      "Epoch 16/20  Iteration 28396/35720 Training loss: 0.7141 0.2285 sec/batch\n",
      "Epoch 16/20  Iteration 28397/35720 Training loss: 0.7141 0.2241 sec/batch\n",
      "Epoch 16/20  Iteration 28398/35720 Training loss: 0.7140 0.2097 sec/batch\n",
      "Epoch 16/20  Iteration 28399/35720 Training loss: 0.7140 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 28400/35720 Training loss: 0.7140 0.2134 sec/batch\n",
      "Validation loss: 1.59321 Saving checkpoint!\n",
      "Epoch 16/20  Iteration 28401/35720 Training loss: 0.7141 0.2102 sec/batch\n",
      "Epoch 16/20  Iteration 28402/35720 Training loss: 0.7141 0.2057 sec/batch\n",
      "Epoch 16/20  Iteration 28403/35720 Training loss: 0.7141 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 28404/35720 Training loss: 0.7141 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 28405/35720 Training loss: 0.7141 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 28406/35720 Training loss: 0.7141 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 28407/35720 Training loss: 0.7141 0.2150 sec/batch\n",
      "Epoch 16/20  Iteration 28408/35720 Training loss: 0.7141 0.2132 sec/batch\n",
      "Epoch 16/20  Iteration 28409/35720 Training loss: 0.7141 0.3049 sec/batch\n",
      "Epoch 16/20  Iteration 28410/35720 Training loss: 0.7141 0.2298 sec/batch\n",
      "Epoch 16/20  Iteration 28411/35720 Training loss: 0.7140 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 28412/35720 Training loss: 0.7140 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 28413/35720 Training loss: 0.7140 0.2154 sec/batch\n",
      "Epoch 16/20  Iteration 28414/35720 Training loss: 0.7140 0.2068 sec/batch\n",
      "Epoch 16/20  Iteration 28415/35720 Training loss: 0.7140 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 28416/35720 Training loss: 0.7140 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 28417/35720 Training loss: 0.7140 0.2080 sec/batch\n",
      "Epoch 16/20  Iteration 28418/35720 Training loss: 0.7140 0.2112 sec/batch\n",
      "Epoch 16/20  Iteration 28419/35720 Training loss: 0.7140 0.2344 sec/batch\n",
      "Epoch 16/20  Iteration 28420/35720 Training loss: 0.7140 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 28421/35720 Training loss: 0.7140 0.2216 sec/batch\n",
      "Epoch 16/20  Iteration 28422/35720 Training loss: 0.7140 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 28423/35720 Training loss: 0.7140 0.2239 sec/batch\n",
      "Epoch 16/20  Iteration 28424/35720 Training loss: 0.7140 0.2056 sec/batch\n",
      "Epoch 16/20  Iteration 28425/35720 Training loss: 0.7140 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 28426/35720 Training loss: 0.7140 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 28427/35720 Training loss: 0.7140 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 28428/35720 Training loss: 0.7139 0.2306 sec/batch\n",
      "Epoch 16/20  Iteration 28429/35720 Training loss: 0.7139 0.2177 sec/batch\n",
      "Epoch 16/20  Iteration 28430/35720 Training loss: 0.7139 0.2062 sec/batch\n",
      "Epoch 16/20  Iteration 28431/35720 Training loss: 0.7139 0.2241 sec/batch\n",
      "Epoch 16/20  Iteration 28432/35720 Training loss: 0.7139 0.2241 sec/batch\n",
      "Epoch 16/20  Iteration 28433/35720 Training loss: 0.7139 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 28434/35720 Training loss: 0.7139 0.2173 sec/batch\n",
      "Epoch 16/20  Iteration 28435/35720 Training loss: 0.7139 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 28436/35720 Training loss: 0.7139 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 28437/35720 Training loss: 0.7139 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 28438/35720 Training loss: 0.7139 0.2187 sec/batch\n",
      "Epoch 16/20  Iteration 28439/35720 Training loss: 0.7139 0.2185 sec/batch\n",
      "Epoch 16/20  Iteration 28440/35720 Training loss: 0.7139 0.2252 sec/batch\n",
      "Epoch 16/20  Iteration 28441/35720 Training loss: 0.7139 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 28442/35720 Training loss: 0.7139 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 28443/35720 Training loss: 0.7139 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 28444/35720 Training loss: 0.7139 0.2131 sec/batch\n",
      "Epoch 16/20  Iteration 28445/35720 Training loss: 0.7139 0.2103 sec/batch\n",
      "Epoch 16/20  Iteration 28446/35720 Training loss: 0.7139 0.2197 sec/batch\n",
      "Epoch 16/20  Iteration 28447/35720 Training loss: 0.7139 0.2193 sec/batch\n",
      "Epoch 16/20  Iteration 28448/35720 Training loss: 0.7139 0.2155 sec/batch\n",
      "Epoch 16/20  Iteration 28449/35720 Training loss: 0.7139 0.2213 sec/batch\n",
      "Epoch 16/20  Iteration 28450/35720 Training loss: 0.7139 0.2239 sec/batch\n",
      "Epoch 16/20  Iteration 28451/35720 Training loss: 0.7139 0.2172 sec/batch\n",
      "Epoch 16/20  Iteration 28452/35720 Training loss: 0.7139 0.2146 sec/batch\n",
      "Epoch 16/20  Iteration 28453/35720 Training loss: 0.7139 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 28454/35720 Training loss: 0.7139 0.2253 sec/batch\n",
      "Epoch 16/20  Iteration 28455/35720 Training loss: 0.7139 0.2283 sec/batch\n",
      "Epoch 16/20  Iteration 28456/35720 Training loss: 0.7139 0.2384 sec/batch\n",
      "Epoch 16/20  Iteration 28457/35720 Training loss: 0.7139 0.2122 sec/batch\n",
      "Epoch 16/20  Iteration 28458/35720 Training loss: 0.7139 0.2234 sec/batch\n",
      "Epoch 16/20  Iteration 28459/35720 Training loss: 0.7139 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 28460/35720 Training loss: 0.7139 0.2153 sec/batch\n",
      "Epoch 16/20  Iteration 28461/35720 Training loss: 0.7140 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 28462/35720 Training loss: 0.7140 0.2351 sec/batch\n",
      "Epoch 16/20  Iteration 28463/35720 Training loss: 0.7140 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 28464/35720 Training loss: 0.7140 0.2070 sec/batch\n",
      "Epoch 16/20  Iteration 28465/35720 Training loss: 0.7140 0.2244 sec/batch\n",
      "Epoch 16/20  Iteration 28466/35720 Training loss: 0.7140 0.2219 sec/batch\n",
      "Epoch 16/20  Iteration 28467/35720 Training loss: 0.7139 0.2183 sec/batch\n",
      "Epoch 16/20  Iteration 28468/35720 Training loss: 0.7139 0.2140 sec/batch\n",
      "Epoch 16/20  Iteration 28469/35720 Training loss: 0.7139 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 28470/35720 Training loss: 0.7139 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 28471/35720 Training loss: 0.7139 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 28472/35720 Training loss: 0.7138 0.2087 sec/batch\n",
      "Epoch 16/20  Iteration 28473/35720 Training loss: 0.7138 0.2161 sec/batch\n",
      "Epoch 16/20  Iteration 28474/35720 Training loss: 0.7139 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 28475/35720 Training loss: 0.7138 0.2076 sec/batch\n",
      "Epoch 16/20  Iteration 28476/35720 Training loss: 0.7139 0.2167 sec/batch\n",
      "Epoch 16/20  Iteration 28477/35720 Training loss: 0.7138 0.2213 sec/batch\n",
      "Epoch 16/20  Iteration 28478/35720 Training loss: 0.7138 0.2259 sec/batch\n",
      "Epoch 16/20  Iteration 28479/35720 Training loss: 0.7138 0.2256 sec/batch\n",
      "Epoch 16/20  Iteration 28480/35720 Training loss: 0.7138 0.2105 sec/batch\n",
      "Epoch 16/20  Iteration 28481/35720 Training loss: 0.7138 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 28482/35720 Training loss: 0.7138 0.2302 sec/batch\n",
      "Epoch 16/20  Iteration 28483/35720 Training loss: 0.7137 0.2135 sec/batch\n",
      "Epoch 16/20  Iteration 28484/35720 Training loss: 0.7138 0.2140 sec/batch\n",
      "Epoch 16/20  Iteration 28485/35720 Training loss: 0.7137 0.2117 sec/batch\n",
      "Epoch 16/20  Iteration 28486/35720 Training loss: 0.7137 0.2221 sec/batch\n",
      "Epoch 16/20  Iteration 28487/35720 Training loss: 0.7137 0.2093 sec/batch\n",
      "Epoch 16/20  Iteration 28488/35720 Training loss: 0.7137 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 28489/35720 Training loss: 0.7137 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 28490/35720 Training loss: 0.7137 0.2234 sec/batch\n",
      "Epoch 16/20  Iteration 28491/35720 Training loss: 0.7137 0.2064 sec/batch\n",
      "Epoch 16/20  Iteration 28492/35720 Training loss: 0.7136 0.2067 sec/batch\n",
      "Epoch 16/20  Iteration 28493/35720 Training loss: 0.7136 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 28494/35720 Training loss: 0.7136 0.2232 sec/batch\n",
      "Epoch 16/20  Iteration 28495/35720 Training loss: 0.7136 0.2373 sec/batch\n",
      "Epoch 16/20  Iteration 28496/35720 Training loss: 0.7136 0.2151 sec/batch\n",
      "Epoch 16/20  Iteration 28497/35720 Training loss: 0.7136 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 28498/35720 Training loss: 0.7136 0.2100 sec/batch\n",
      "Epoch 16/20  Iteration 28499/35720 Training loss: 0.7136 0.2172 sec/batch\n",
      "Epoch 16/20  Iteration 28500/35720 Training loss: 0.7136 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 28501/35720 Training loss: 0.7137 0.2148 sec/batch\n",
      "Epoch 16/20  Iteration 28502/35720 Training loss: 0.7137 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 28503/35720 Training loss: 0.7137 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 28504/35720 Training loss: 0.7137 0.2095 sec/batch\n",
      "Epoch 16/20  Iteration 28505/35720 Training loss: 0.7137 0.2178 sec/batch\n",
      "Epoch 16/20  Iteration 28506/35720 Training loss: 0.7137 0.2106 sec/batch\n",
      "Epoch 16/20  Iteration 28507/35720 Training loss: 0.7137 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 28508/35720 Training loss: 0.7136 0.2354 sec/batch\n",
      "Epoch 16/20  Iteration 28509/35720 Training loss: 0.7136 0.2082 sec/batch\n",
      "Epoch 16/20  Iteration 28510/35720 Training loss: 0.7136 0.2192 sec/batch\n",
      "Epoch 16/20  Iteration 28511/35720 Training loss: 0.7136 0.2084 sec/batch\n",
      "Epoch 16/20  Iteration 28512/35720 Training loss: 0.7136 0.2121 sec/batch\n",
      "Epoch 16/20  Iteration 28513/35720 Training loss: 0.7136 0.2063 sec/batch\n",
      "Epoch 16/20  Iteration 28514/35720 Training loss: 0.7136 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 28515/35720 Training loss: 0.7136 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 28516/35720 Training loss: 0.7136 0.2312 sec/batch\n",
      "Epoch 16/20  Iteration 28517/35720 Training loss: 0.7136 0.2246 sec/batch\n",
      "Epoch 16/20  Iteration 28518/35720 Training loss: 0.7136 0.2268 sec/batch\n",
      "Epoch 16/20  Iteration 28519/35720 Training loss: 0.7136 0.2183 sec/batch\n",
      "Epoch 16/20  Iteration 28520/35720 Training loss: 0.7136 0.2311 sec/batch\n",
      "Epoch 16/20  Iteration 28521/35720 Training loss: 0.7136 0.2094 sec/batch\n",
      "Epoch 16/20  Iteration 28522/35720 Training loss: 0.7136 0.2152 sec/batch\n",
      "Epoch 16/20  Iteration 28523/35720 Training loss: 0.7136 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 28524/35720 Training loss: 0.7136 0.2151 sec/batch\n",
      "Epoch 16/20  Iteration 28525/35720 Training loss: 0.7137 0.2163 sec/batch\n",
      "Epoch 16/20  Iteration 28526/35720 Training loss: 0.7137 0.2149 sec/batch\n",
      "Epoch 16/20  Iteration 28527/35720 Training loss: 0.7137 0.2073 sec/batch\n",
      "Epoch 16/20  Iteration 28528/35720 Training loss: 0.7137 0.2092 sec/batch\n",
      "Epoch 16/20  Iteration 28529/35720 Training loss: 0.7137 0.2312 sec/batch\n",
      "Epoch 16/20  Iteration 28530/35720 Training loss: 0.7137 0.2187 sec/batch\n",
      "Epoch 16/20  Iteration 28531/35720 Training loss: 0.7137 0.2101 sec/batch\n",
      "Epoch 16/20  Iteration 28532/35720 Training loss: 0.7137 0.2088 sec/batch\n",
      "Epoch 16/20  Iteration 28533/35720 Training loss: 0.7137 0.2300 sec/batch\n",
      "Epoch 16/20  Iteration 28534/35720 Training loss: 0.7137 0.2098 sec/batch\n",
      "Epoch 16/20  Iteration 28535/35720 Training loss: 0.7137 0.2199 sec/batch\n",
      "Epoch 16/20  Iteration 28536/35720 Training loss: 0.7138 0.2123 sec/batch\n",
      "Epoch 16/20  Iteration 28537/35720 Training loss: 0.7138 0.2137 sec/batch\n",
      "Epoch 16/20  Iteration 28538/35720 Training loss: 0.7138 0.2221 sec/batch\n",
      "Epoch 16/20  Iteration 28539/35720 Training loss: 0.7137 0.2083 sec/batch\n",
      "Epoch 16/20  Iteration 28540/35720 Training loss: 0.7137 0.2284 sec/batch\n",
      "Epoch 16/20  Iteration 28541/35720 Training loss: 0.7137 0.2066 sec/batch\n",
      "Epoch 16/20  Iteration 28542/35720 Training loss: 0.7137 0.2109 sec/batch\n",
      "Epoch 16/20  Iteration 28543/35720 Training loss: 0.7137 0.2129 sec/batch\n",
      "Epoch 16/20  Iteration 28544/35720 Training loss: 0.7137 0.2196 sec/batch\n",
      "Epoch 16/20  Iteration 28545/35720 Training loss: 0.7138 0.2086 sec/batch\n",
      "Epoch 16/20  Iteration 28546/35720 Training loss: 0.7138 0.2277 sec/batch\n",
      "Epoch 16/20  Iteration 28547/35720 Training loss: 0.7138 0.2274 sec/batch\n",
      "Epoch 16/20  Iteration 28548/35720 Training loss: 0.7137 0.2215 sec/batch\n",
      "Epoch 16/20  Iteration 28549/35720 Training loss: 0.7137 0.2243 sec/batch\n",
      "Epoch 16/20  Iteration 28550/35720 Training loss: 0.7137 0.2188 sec/batch\n",
      "Epoch 16/20  Iteration 28551/35720 Training loss: 0.7137 0.2141 sec/batch\n",
      "Epoch 16/20  Iteration 28552/35720 Training loss: 0.7137 0.2134 sec/batch\n",
      "Epoch 16/20  Iteration 28553/35720 Training loss: 0.7137 0.2113 sec/batch\n",
      "Epoch 16/20  Iteration 28554/35720 Training loss: 0.7137 0.2166 sec/batch\n",
      "Epoch 16/20  Iteration 28555/35720 Training loss: 0.7137 0.2144 sec/batch\n",
      "Epoch 16/20  Iteration 28556/35720 Training loss: 0.7137 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 28557/35720 Training loss: 0.7137 0.2254 sec/batch\n",
      "Epoch 16/20  Iteration 28558/35720 Training loss: 0.7137 0.2058 sec/batch\n",
      "Epoch 16/20  Iteration 28559/35720 Training loss: 0.7137 0.2069 sec/batch\n",
      "Epoch 16/20  Iteration 28560/35720 Training loss: 0.7137 0.2259 sec/batch\n",
      "Epoch 16/20  Iteration 28561/35720 Training loss: 0.7137 0.2119 sec/batch\n",
      "Epoch 16/20  Iteration 28562/35720 Training loss: 0.7137 0.2299 sec/batch\n",
      "Epoch 16/20  Iteration 28563/35720 Training loss: 0.7137 0.2060 sec/batch\n",
      "Epoch 16/20  Iteration 28564/35720 Training loss: 0.7136 0.2165 sec/batch\n",
      "Epoch 16/20  Iteration 28565/35720 Training loss: 0.7136 0.2170 sec/batch\n",
      "Epoch 16/20  Iteration 28566/35720 Training loss: 0.7136 0.2225 sec/batch\n",
      "Epoch 16/20  Iteration 28567/35720 Training loss: 0.7136 0.2132 sec/batch\n",
      "Epoch 16/20  Iteration 28568/35720 Training loss: 0.7136 0.2169 sec/batch\n",
      "Epoch 16/20  Iteration 28569/35720 Training loss: 0.7136 0.2061 sec/batch\n",
      "Epoch 16/20  Iteration 28570/35720 Training loss: 0.7136 0.2126 sec/batch\n",
      "Epoch 16/20  Iteration 28571/35720 Training loss: 0.7135 0.2171 sec/batch\n",
      "Epoch 16/20  Iteration 28572/35720 Training loss: 0.7135 0.2155 sec/batch\n",
      "Epoch 16/20  Iteration 28573/35720 Training loss: 0.7135 0.2136 sec/batch\n",
      "Epoch 16/20  Iteration 28574/35720 Training loss: 0.7135 0.2159 sec/batch\n",
      "Epoch 16/20  Iteration 28575/35720 Training loss: 0.7135 0.2110 sec/batch\n",
      "Epoch 16/20  Iteration 28576/35720 Training loss: 0.7135 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 28577/35720 Training loss: 0.7613 0.2226 sec/batch\n",
      "Epoch 17/20  Iteration 28578/35720 Training loss: 0.7495 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 28579/35720 Training loss: 0.7330 0.2247 sec/batch\n",
      "Epoch 17/20  Iteration 28580/35720 Training loss: 0.7289 0.2126 sec/batch\n",
      "Epoch 17/20  Iteration 28581/35720 Training loss: 0.7329 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 28582/35720 Training loss: 0.7174 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 28583/35720 Training loss: 0.7157 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 28584/35720 Training loss: 0.7104 0.2225 sec/batch\n",
      "Epoch 17/20  Iteration 28585/35720 Training loss: 0.7078 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 28586/35720 Training loss: 0.7097 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 28587/35720 Training loss: 0.7104 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 28588/35720 Training loss: 0.7059 0.2204 sec/batch\n",
      "Epoch 17/20  Iteration 28589/35720 Training loss: 0.7045 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 28590/35720 Training loss: 0.7068 0.2201 sec/batch\n",
      "Epoch 17/20  Iteration 28591/35720 Training loss: 0.7074 0.2151 sec/batch\n",
      "Epoch 17/20  Iteration 28592/35720 Training loss: 0.7077 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 28593/35720 Training loss: 0.7089 0.2131 sec/batch\n",
      "Epoch 17/20  Iteration 28594/35720 Training loss: 0.7052 0.2214 sec/batch\n",
      "Epoch 17/20  Iteration 28595/35720 Training loss: 0.7039 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 28596/35720 Training loss: 0.7042 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 28597/35720 Training loss: 0.7056 0.2052 sec/batch\n",
      "Epoch 17/20  Iteration 28598/35720 Training loss: 0.7033 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 28599/35720 Training loss: 0.7021 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 28600/35720 Training loss: 0.7049 0.2387 sec/batch\n",
      "Validation loss: 1.58392 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 28601/35720 Training loss: 0.7267 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 28602/35720 Training loss: 0.7257 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 28603/35720 Training loss: 0.7265 0.2083 sec/batch\n",
      "Epoch 17/20  Iteration 28604/35720 Training loss: 0.7268 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 28605/35720 Training loss: 0.7254 0.2122 sec/batch\n",
      "Epoch 17/20  Iteration 28606/35720 Training loss: 0.7254 0.2183 sec/batch\n",
      "Epoch 17/20  Iteration 28607/35720 Training loss: 0.7259 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 28608/35720 Training loss: 0.7244 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 28609/35720 Training loss: 0.7252 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 28610/35720 Training loss: 0.7267 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 28611/35720 Training loss: 0.7284 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 28612/35720 Training loss: 0.7289 0.2187 sec/batch\n",
      "Epoch 17/20  Iteration 28613/35720 Training loss: 0.7289 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 28614/35720 Training loss: 0.7277 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 28615/35720 Training loss: 0.7268 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 28616/35720 Training loss: 0.7279 0.2162 sec/batch\n",
      "Epoch 17/20  Iteration 28617/35720 Training loss: 0.7270 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 28618/35720 Training loss: 0.7258 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 28619/35720 Training loss: 0.7238 0.2170 sec/batch\n",
      "Epoch 17/20  Iteration 28620/35720 Training loss: 0.7228 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 28621/35720 Training loss: 0.7221 0.2199 sec/batch\n",
      "Epoch 17/20  Iteration 28622/35720 Training loss: 0.7212 0.2133 sec/batch\n",
      "Epoch 17/20  Iteration 28623/35720 Training loss: 0.7199 0.2151 sec/batch\n",
      "Epoch 17/20  Iteration 28624/35720 Training loss: 0.7188 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 28625/35720 Training loss: 0.7182 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 28626/35720 Training loss: 0.7173 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 28627/35720 Training loss: 0.7175 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 28628/35720 Training loss: 0.7173 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 28629/35720 Training loss: 0.7172 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 28630/35720 Training loss: 0.7157 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 28631/35720 Training loss: 0.7143 0.2159 sec/batch\n",
      "Epoch 17/20  Iteration 28632/35720 Training loss: 0.7132 0.2174 sec/batch\n",
      "Epoch 17/20  Iteration 28633/35720 Training loss: 0.7128 0.2566 sec/batch\n",
      "Epoch 17/20  Iteration 28634/35720 Training loss: 0.7121 0.2596 sec/batch\n",
      "Epoch 17/20  Iteration 28635/35720 Training loss: 0.7114 0.2191 sec/batch\n",
      "Epoch 17/20  Iteration 28636/35720 Training loss: 0.7108 0.2195 sec/batch\n",
      "Epoch 17/20  Iteration 28637/35720 Training loss: 0.7104 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 28638/35720 Training loss: 0.7088 0.2269 sec/batch\n",
      "Epoch 17/20  Iteration 28639/35720 Training loss: 0.7091 0.2210 sec/batch\n",
      "Epoch 17/20  Iteration 28640/35720 Training loss: 0.7091 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 28641/35720 Training loss: 0.7097 0.2058 sec/batch\n",
      "Epoch 17/20  Iteration 28642/35720 Training loss: 0.7099 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 28643/35720 Training loss: 0.7095 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 28644/35720 Training loss: 0.7089 0.2140 sec/batch\n",
      "Epoch 17/20  Iteration 28645/35720 Training loss: 0.7098 0.2233 sec/batch\n",
      "Epoch 17/20  Iteration 28646/35720 Training loss: 0.7097 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 28647/35720 Training loss: 0.7099 0.2075 sec/batch\n",
      "Epoch 17/20  Iteration 28648/35720 Training loss: 0.7101 0.2147 sec/batch\n",
      "Epoch 17/20  Iteration 28649/35720 Training loss: 0.7103 0.2252 sec/batch\n",
      "Epoch 17/20  Iteration 28650/35720 Training loss: 0.7101 0.2237 sec/batch\n",
      "Epoch 17/20  Iteration 28651/35720 Training loss: 0.7095 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 28652/35720 Training loss: 0.7096 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 28653/35720 Training loss: 0.7091 0.2078 sec/batch\n",
      "Epoch 17/20  Iteration 28654/35720 Training loss: 0.7098 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 28655/35720 Training loss: 0.7099 0.2155 sec/batch\n",
      "Epoch 17/20  Iteration 28656/35720 Training loss: 0.7104 0.2182 sec/batch\n",
      "Epoch 17/20  Iteration 28657/35720 Training loss: 0.7107 0.2260 sec/batch\n",
      "Epoch 17/20  Iteration 28658/35720 Training loss: 0.7109 0.2070 sec/batch\n",
      "Epoch 17/20  Iteration 28659/35720 Training loss: 0.7111 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 28660/35720 Training loss: 0.7109 0.2206 sec/batch\n",
      "Epoch 17/20  Iteration 28661/35720 Training loss: 0.7108 0.2109 sec/batch\n",
      "Epoch 17/20  Iteration 28662/35720 Training loss: 0.7109 0.2230 sec/batch\n",
      "Epoch 17/20  Iteration 28663/35720 Training loss: 0.7105 0.2159 sec/batch\n",
      "Epoch 17/20  Iteration 28664/35720 Training loss: 0.7106 0.2335 sec/batch\n",
      "Epoch 17/20  Iteration 28665/35720 Training loss: 0.7099 0.2214 sec/batch\n",
      "Epoch 17/20  Iteration 28666/35720 Training loss: 0.7094 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 28667/35720 Training loss: 0.7099 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 28668/35720 Training loss: 0.7094 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 28669/35720 Training loss: 0.7097 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 28670/35720 Training loss: 0.7102 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 28671/35720 Training loss: 0.7100 0.2255 sec/batch\n",
      "Epoch 17/20  Iteration 28672/35720 Training loss: 0.7094 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 28673/35720 Training loss: 0.7096 0.2109 sec/batch\n",
      "Epoch 17/20  Iteration 28674/35720 Training loss: 0.7093 0.2058 sec/batch\n",
      "Epoch 17/20  Iteration 28675/35720 Training loss: 0.7090 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 28676/35720 Training loss: 0.7086 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 28677/35720 Training loss: 0.7082 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 28678/35720 Training loss: 0.7083 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 28679/35720 Training loss: 0.7076 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 28680/35720 Training loss: 0.7073 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 28681/35720 Training loss: 0.7071 0.2186 sec/batch\n",
      "Epoch 17/20  Iteration 28682/35720 Training loss: 0.7067 0.2163 sec/batch\n",
      "Epoch 17/20  Iteration 28683/35720 Training loss: 0.7064 0.2131 sec/batch\n",
      "Epoch 17/20  Iteration 28684/35720 Training loss: 0.7066 0.2319 sec/batch\n",
      "Epoch 17/20  Iteration 28685/35720 Training loss: 0.7070 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 28686/35720 Training loss: 0.7068 0.2335 sec/batch\n",
      "Epoch 17/20  Iteration 28687/35720 Training loss: 0.7068 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 28688/35720 Training loss: 0.7071 0.2208 sec/batch\n",
      "Epoch 17/20  Iteration 28689/35720 Training loss: 0.7073 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 28690/35720 Training loss: 0.7075 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 28691/35720 Training loss: 0.7075 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 28692/35720 Training loss: 0.7075 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 28693/35720 Training loss: 0.7073 0.2265 sec/batch\n",
      "Epoch 17/20  Iteration 28694/35720 Training loss: 0.7073 0.2272 sec/batch\n",
      "Epoch 17/20  Iteration 28695/35720 Training loss: 0.7074 0.2198 sec/batch\n",
      "Epoch 17/20  Iteration 28696/35720 Training loss: 0.7078 0.2125 sec/batch\n",
      "Epoch 17/20  Iteration 28697/35720 Training loss: 0.7080 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 28698/35720 Training loss: 0.7074 0.2077 sec/batch\n",
      "Epoch 17/20  Iteration 28699/35720 Training loss: 0.7073 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 28700/35720 Training loss: 0.7077 0.2127 sec/batch\n",
      "Epoch 17/20  Iteration 28701/35720 Training loss: 0.7074 0.2309 sec/batch\n",
      "Epoch 17/20  Iteration 28702/35720 Training loss: 0.7073 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 28703/35720 Training loss: 0.7075 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 28704/35720 Training loss: 0.7074 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 28705/35720 Training loss: 0.7072 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 28706/35720 Training loss: 0.7076 0.2270 sec/batch\n",
      "Epoch 17/20  Iteration 28707/35720 Training loss: 0.7075 0.2232 sec/batch\n",
      "Epoch 17/20  Iteration 28708/35720 Training loss: 0.7071 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 28709/35720 Training loss: 0.7072 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 28710/35720 Training loss: 0.7074 0.2421 sec/batch\n",
      "Epoch 17/20  Iteration 28711/35720 Training loss: 0.7071 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 28712/35720 Training loss: 0.7069 0.2241 sec/batch\n",
      "Epoch 17/20  Iteration 28713/35720 Training loss: 0.7072 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 28714/35720 Training loss: 0.7074 0.2057 sec/batch\n",
      "Epoch 17/20  Iteration 28715/35720 Training loss: 0.7075 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 28716/35720 Training loss: 0.7075 0.2258 sec/batch\n",
      "Epoch 17/20  Iteration 28717/35720 Training loss: 0.7073 0.2233 sec/batch\n",
      "Epoch 17/20  Iteration 28718/35720 Training loss: 0.7070 0.2269 sec/batch\n",
      "Epoch 17/20  Iteration 28719/35720 Training loss: 0.7067 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 28720/35720 Training loss: 0.7061 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 28721/35720 Training loss: 0.7060 0.2248 sec/batch\n",
      "Epoch 17/20  Iteration 28722/35720 Training loss: 0.7062 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 28723/35720 Training loss: 0.7059 0.2177 sec/batch\n",
      "Epoch 17/20  Iteration 28724/35720 Training loss: 0.7059 0.2269 sec/batch\n",
      "Epoch 17/20  Iteration 28725/35720 Training loss: 0.7059 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 28726/35720 Training loss: 0.7055 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 28727/35720 Training loss: 0.7054 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 28728/35720 Training loss: 0.7054 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 28729/35720 Training loss: 0.7053 0.2224 sec/batch\n",
      "Epoch 17/20  Iteration 28730/35720 Training loss: 0.7056 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 28731/35720 Training loss: 0.7056 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 28732/35720 Training loss: 0.7058 0.2335 sec/batch\n",
      "Epoch 17/20  Iteration 28733/35720 Training loss: 0.7058 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 28734/35720 Training loss: 0.7060 0.2232 sec/batch\n",
      "Epoch 17/20  Iteration 28735/35720 Training loss: 0.7058 0.2188 sec/batch\n",
      "Epoch 17/20  Iteration 28736/35720 Training loss: 0.7059 0.2122 sec/batch\n",
      "Epoch 17/20  Iteration 28737/35720 Training loss: 0.7056 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 28738/35720 Training loss: 0.7059 0.2223 sec/batch\n",
      "Epoch 17/20  Iteration 28739/35720 Training loss: 0.7059 0.2200 sec/batch\n",
      "Epoch 17/20  Iteration 28740/35720 Training loss: 0.7061 0.2290 sec/batch\n",
      "Epoch 17/20  Iteration 28741/35720 Training loss: 0.7062 0.2074 sec/batch\n",
      "Epoch 17/20  Iteration 28742/35720 Training loss: 0.7061 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 28743/35720 Training loss: 0.7062 0.2199 sec/batch\n",
      "Epoch 17/20  Iteration 28744/35720 Training loss: 0.7064 0.2341 sec/batch\n",
      "Epoch 17/20  Iteration 28745/35720 Training loss: 0.7068 0.2258 sec/batch\n",
      "Epoch 17/20  Iteration 28746/35720 Training loss: 0.7071 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 28747/35720 Training loss: 0.7074 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 28748/35720 Training loss: 0.7079 0.2126 sec/batch\n",
      "Epoch 17/20  Iteration 28749/35720 Training loss: 0.7081 0.2192 sec/batch\n",
      "Epoch 17/20  Iteration 28750/35720 Training loss: 0.7084 0.2338 sec/batch\n",
      "Epoch 17/20  Iteration 28751/35720 Training loss: 0.7086 0.2161 sec/batch\n",
      "Epoch 17/20  Iteration 28752/35720 Training loss: 0.7087 0.2248 sec/batch\n",
      "Epoch 17/20  Iteration 28753/35720 Training loss: 0.7088 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 28754/35720 Training loss: 0.7086 0.2299 sec/batch\n",
      "Epoch 17/20  Iteration 28755/35720 Training loss: 0.7085 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 28756/35720 Training loss: 0.7082 0.2355 sec/batch\n",
      "Epoch 17/20  Iteration 28757/35720 Training loss: 0.7082 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 28758/35720 Training loss: 0.7082 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 28759/35720 Training loss: 0.7083 0.2213 sec/batch\n",
      "Epoch 17/20  Iteration 28760/35720 Training loss: 0.7085 0.2188 sec/batch\n",
      "Epoch 17/20  Iteration 28761/35720 Training loss: 0.7083 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 28762/35720 Training loss: 0.7083 0.2690 sec/batch\n",
      "Epoch 17/20  Iteration 28763/35720 Training loss: 0.7082 0.2487 sec/batch\n",
      "Epoch 17/20  Iteration 28764/35720 Training loss: 0.7083 0.2133 sec/batch\n",
      "Epoch 17/20  Iteration 28765/35720 Training loss: 0.7083 0.2189 sec/batch\n",
      "Epoch 17/20  Iteration 28766/35720 Training loss: 0.7084 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 28767/35720 Training loss: 0.7085 0.2272 sec/batch\n",
      "Epoch 17/20  Iteration 28768/35720 Training loss: 0.7088 0.2212 sec/batch\n",
      "Epoch 17/20  Iteration 28769/35720 Training loss: 0.7090 0.2145 sec/batch\n",
      "Epoch 17/20  Iteration 28770/35720 Training loss: 0.7091 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 28771/35720 Training loss: 0.7092 0.2120 sec/batch\n",
      "Epoch 17/20  Iteration 28772/35720 Training loss: 0.7093 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 28773/35720 Training loss: 0.7091 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 28774/35720 Training loss: 0.7092 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 28775/35720 Training loss: 0.7092 0.2133 sec/batch\n",
      "Epoch 17/20  Iteration 28776/35720 Training loss: 0.7093 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 28777/35720 Training loss: 0.7092 0.2224 sec/batch\n",
      "Epoch 17/20  Iteration 28778/35720 Training loss: 0.7092 0.2219 sec/batch\n",
      "Epoch 17/20  Iteration 28779/35720 Training loss: 0.7093 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 28780/35720 Training loss: 0.7092 0.2147 sec/batch\n",
      "Epoch 17/20  Iteration 28781/35720 Training loss: 0.7094 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 28782/35720 Training loss: 0.7094 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 28783/35720 Training loss: 0.7097 0.2199 sec/batch\n",
      "Epoch 17/20  Iteration 28784/35720 Training loss: 0.7100 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 28785/35720 Training loss: 0.7104 0.2050 sec/batch\n",
      "Epoch 17/20  Iteration 28786/35720 Training loss: 0.7104 0.2124 sec/batch\n",
      "Epoch 17/20  Iteration 28787/35720 Training loss: 0.7106 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 28788/35720 Training loss: 0.7107 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 28789/35720 Training loss: 0.7106 0.2346 sec/batch\n",
      "Epoch 17/20  Iteration 28790/35720 Training loss: 0.7105 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 28791/35720 Training loss: 0.7105 0.2075 sec/batch\n",
      "Epoch 17/20  Iteration 28792/35720 Training loss: 0.7104 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 28793/35720 Training loss: 0.7102 0.2304 sec/batch\n",
      "Epoch 17/20  Iteration 28794/35720 Training loss: 0.7101 0.2227 sec/batch\n",
      "Epoch 17/20  Iteration 28795/35720 Training loss: 0.7101 0.2053 sec/batch\n",
      "Epoch 17/20  Iteration 28796/35720 Training loss: 0.7101 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 28797/35720 Training loss: 0.7101 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 28798/35720 Training loss: 0.7103 0.2126 sec/batch\n",
      "Epoch 17/20  Iteration 28799/35720 Training loss: 0.7106 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 28800/35720 Training loss: 0.7106 0.2091 sec/batch\n",
      "Validation loss: 1.60496 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 28801/35720 Training loss: 0.7128 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 28802/35720 Training loss: 0.7130 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 28803/35720 Training loss: 0.7128 0.2194 sec/batch\n",
      "Epoch 17/20  Iteration 28804/35720 Training loss: 0.7127 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 28805/35720 Training loss: 0.7125 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 28806/35720 Training loss: 0.7126 0.2070 sec/batch\n",
      "Epoch 17/20  Iteration 28807/35720 Training loss: 0.7129 0.2200 sec/batch\n",
      "Epoch 17/20  Iteration 28808/35720 Training loss: 0.7128 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 28809/35720 Training loss: 0.7128 0.2380 sec/batch\n",
      "Epoch 17/20  Iteration 28810/35720 Training loss: 0.7127 0.2147 sec/batch\n",
      "Epoch 17/20  Iteration 28811/35720 Training loss: 0.7127 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 28812/35720 Training loss: 0.7128 0.2313 sec/batch\n",
      "Epoch 17/20  Iteration 28813/35720 Training loss: 0.7130 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 28814/35720 Training loss: 0.7128 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 28815/35720 Training loss: 0.7128 0.2107 sec/batch\n",
      "Epoch 17/20  Iteration 28816/35720 Training loss: 0.7128 0.2243 sec/batch\n",
      "Epoch 17/20  Iteration 28817/35720 Training loss: 0.7127 0.2147 sec/batch\n",
      "Epoch 17/20  Iteration 28818/35720 Training loss: 0.7126 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 28819/35720 Training loss: 0.7127 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 28820/35720 Training loss: 0.7124 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 28821/35720 Training loss: 0.7122 0.2184 sec/batch\n",
      "Epoch 17/20  Iteration 28822/35720 Training loss: 0.7122 0.2244 sec/batch\n",
      "Epoch 17/20  Iteration 28823/35720 Training loss: 0.7122 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 28824/35720 Training loss: 0.7122 0.2057 sec/batch\n",
      "Epoch 17/20  Iteration 28825/35720 Training loss: 0.7120 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 28826/35720 Training loss: 0.7118 0.2217 sec/batch\n",
      "Epoch 17/20  Iteration 28827/35720 Training loss: 0.7119 0.2204 sec/batch\n",
      "Epoch 17/20  Iteration 28828/35720 Training loss: 0.7118 0.2298 sec/batch\n",
      "Epoch 17/20  Iteration 28829/35720 Training loss: 0.7115 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 28830/35720 Training loss: 0.7116 0.2231 sec/batch\n",
      "Epoch 17/20  Iteration 28831/35720 Training loss: 0.7118 0.2197 sec/batch\n",
      "Epoch 17/20  Iteration 28832/35720 Training loss: 0.7119 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 28833/35720 Training loss: 0.7119 0.2158 sec/batch\n",
      "Epoch 17/20  Iteration 28834/35720 Training loss: 0.7118 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 28835/35720 Training loss: 0.7120 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 28836/35720 Training loss: 0.7120 0.2283 sec/batch\n",
      "Epoch 17/20  Iteration 28837/35720 Training loss: 0.7119 0.2221 sec/batch\n",
      "Epoch 17/20  Iteration 28838/35720 Training loss: 0.7118 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 28839/35720 Training loss: 0.7119 0.2207 sec/batch\n",
      "Epoch 17/20  Iteration 28840/35720 Training loss: 0.7119 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 28841/35720 Training loss: 0.7118 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 28842/35720 Training loss: 0.7120 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 28843/35720 Training loss: 0.7119 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 28844/35720 Training loss: 0.7118 0.2233 sec/batch\n",
      "Epoch 17/20  Iteration 28845/35720 Training loss: 0.7116 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 28846/35720 Training loss: 0.7112 0.2255 sec/batch\n",
      "Epoch 17/20  Iteration 28847/35720 Training loss: 0.7110 0.2099 sec/batch\n",
      "Epoch 17/20  Iteration 28848/35720 Training loss: 0.7110 0.2135 sec/batch\n",
      "Epoch 17/20  Iteration 28849/35720 Training loss: 0.7109 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 28850/35720 Training loss: 0.7109 0.2192 sec/batch\n",
      "Epoch 17/20  Iteration 28851/35720 Training loss: 0.7108 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 28852/35720 Training loss: 0.7109 0.2099 sec/batch\n",
      "Epoch 17/20  Iteration 28853/35720 Training loss: 0.7107 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 28854/35720 Training loss: 0.7105 0.2321 sec/batch\n",
      "Epoch 17/20  Iteration 28855/35720 Training loss: 0.7103 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 28856/35720 Training loss: 0.7103 0.2247 sec/batch\n",
      "Epoch 17/20  Iteration 28857/35720 Training loss: 0.7103 0.2135 sec/batch\n",
      "Epoch 17/20  Iteration 28858/35720 Training loss: 0.7100 0.2258 sec/batch\n",
      "Epoch 17/20  Iteration 28859/35720 Training loss: 0.7098 0.2293 sec/batch\n",
      "Epoch 17/20  Iteration 28860/35720 Training loss: 0.7096 0.2305 sec/batch\n",
      "Epoch 17/20  Iteration 28861/35720 Training loss: 0.7097 0.2213 sec/batch\n",
      "Epoch 17/20  Iteration 28862/35720 Training loss: 0.7097 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 28863/35720 Training loss: 0.7094 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 28864/35720 Training loss: 0.7095 0.2325 sec/batch\n",
      "Epoch 17/20  Iteration 28865/35720 Training loss: 0.7095 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 28866/35720 Training loss: 0.7096 0.2351 sec/batch\n",
      "Epoch 17/20  Iteration 28867/35720 Training loss: 0.7096 0.2257 sec/batch\n",
      "Epoch 17/20  Iteration 28868/35720 Training loss: 0.7096 0.2117 sec/batch\n",
      "Epoch 17/20  Iteration 28869/35720 Training loss: 0.7095 0.2143 sec/batch\n",
      "Epoch 17/20  Iteration 28870/35720 Training loss: 0.7095 0.2260 sec/batch\n",
      "Epoch 17/20  Iteration 28871/35720 Training loss: 0.7096 0.2170 sec/batch\n",
      "Epoch 17/20  Iteration 28872/35720 Training loss: 0.7096 0.2209 sec/batch\n",
      "Epoch 17/20  Iteration 28873/35720 Training loss: 0.7095 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 28874/35720 Training loss: 0.7097 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 28875/35720 Training loss: 0.7096 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 28876/35720 Training loss: 0.7095 0.2151 sec/batch\n",
      "Epoch 17/20  Iteration 28877/35720 Training loss: 0.7095 0.2302 sec/batch\n",
      "Epoch 17/20  Iteration 28878/35720 Training loss: 0.7093 0.2344 sec/batch\n",
      "Epoch 17/20  Iteration 28879/35720 Training loss: 0.7095 0.2182 sec/batch\n",
      "Epoch 17/20  Iteration 28880/35720 Training loss: 0.7095 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 28881/35720 Training loss: 0.7094 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 28882/35720 Training loss: 0.7092 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 28883/35720 Training loss: 0.7093 0.2293 sec/batch\n",
      "Epoch 17/20  Iteration 28884/35720 Training loss: 0.7094 0.2277 sec/batch\n",
      "Epoch 17/20  Iteration 28885/35720 Training loss: 0.7092 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 28886/35720 Training loss: 0.7091 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 28887/35720 Training loss: 0.7091 0.2168 sec/batch\n",
      "Epoch 17/20  Iteration 28888/35720 Training loss: 0.7089 0.2364 sec/batch\n",
      "Epoch 17/20  Iteration 28889/35720 Training loss: 0.7088 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 28890/35720 Training loss: 0.7088 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 28891/35720 Training loss: 0.7089 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 28892/35720 Training loss: 0.7089 0.2285 sec/batch\n",
      "Epoch 17/20  Iteration 28893/35720 Training loss: 0.7088 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 28894/35720 Training loss: 0.7088 0.2240 sec/batch\n",
      "Epoch 17/20  Iteration 28895/35720 Training loss: 0.7089 0.2277 sec/batch\n",
      "Epoch 17/20  Iteration 28896/35720 Training loss: 0.7089 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 28897/35720 Training loss: 0.7090 0.2209 sec/batch\n",
      "Epoch 17/20  Iteration 28898/35720 Training loss: 0.7090 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 28899/35720 Training loss: 0.7090 0.2296 sec/batch\n",
      "Epoch 17/20  Iteration 28900/35720 Training loss: 0.7090 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 28901/35720 Training loss: 0.7089 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 28902/35720 Training loss: 0.7091 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 28903/35720 Training loss: 0.7091 0.2133 sec/batch\n",
      "Epoch 17/20  Iteration 28904/35720 Training loss: 0.7090 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 28905/35720 Training loss: 0.7090 0.2279 sec/batch\n",
      "Epoch 17/20  Iteration 28906/35720 Training loss: 0.7090 0.2136 sec/batch\n",
      "Epoch 17/20  Iteration 28907/35720 Training loss: 0.7090 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 28908/35720 Training loss: 0.7090 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 28909/35720 Training loss: 0.7089 0.2225 sec/batch\n",
      "Epoch 17/20  Iteration 28910/35720 Training loss: 0.7089 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 28911/35720 Training loss: 0.7088 0.2135 sec/batch\n",
      "Epoch 17/20  Iteration 28912/35720 Training loss: 0.7085 0.2107 sec/batch\n",
      "Epoch 17/20  Iteration 28913/35720 Training loss: 0.7085 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 28914/35720 Training loss: 0.7083 0.2211 sec/batch\n",
      "Epoch 17/20  Iteration 28915/35720 Training loss: 0.7083 0.2226 sec/batch\n",
      "Epoch 17/20  Iteration 28916/35720 Training loss: 0.7082 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 28917/35720 Training loss: 0.7081 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 28918/35720 Training loss: 0.7081 0.2072 sec/batch\n",
      "Epoch 17/20  Iteration 28919/35720 Training loss: 0.7081 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 28920/35720 Training loss: 0.7081 0.2254 sec/batch\n",
      "Epoch 17/20  Iteration 28921/35720 Training loss: 0.7079 0.2142 sec/batch\n",
      "Epoch 17/20  Iteration 28922/35720 Training loss: 0.7081 0.2255 sec/batch\n",
      "Epoch 17/20  Iteration 28923/35720 Training loss: 0.7081 0.2055 sec/batch\n",
      "Epoch 17/20  Iteration 28924/35720 Training loss: 0.7082 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 28925/35720 Training loss: 0.7082 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 28926/35720 Training loss: 0.7082 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 28927/35720 Training loss: 0.7082 0.2203 sec/batch\n",
      "Epoch 17/20  Iteration 28928/35720 Training loss: 0.7081 0.2051 sec/batch\n",
      "Epoch 17/20  Iteration 28929/35720 Training loss: 0.7080 0.2057 sec/batch\n",
      "Epoch 17/20  Iteration 28930/35720 Training loss: 0.7080 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 28931/35720 Training loss: 0.7081 0.2127 sec/batch\n",
      "Epoch 17/20  Iteration 28932/35720 Training loss: 0.7081 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 28933/35720 Training loss: 0.7082 0.2149 sec/batch\n",
      "Epoch 17/20  Iteration 28934/35720 Training loss: 0.7083 0.2284 sec/batch\n",
      "Epoch 17/20  Iteration 28935/35720 Training loss: 0.7082 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 28936/35720 Training loss: 0.7082 0.2280 sec/batch\n",
      "Epoch 17/20  Iteration 28937/35720 Training loss: 0.7081 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 28938/35720 Training loss: 0.7081 0.2233 sec/batch\n",
      "Epoch 17/20  Iteration 28939/35720 Training loss: 0.7082 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 28940/35720 Training loss: 0.7082 0.2055 sec/batch\n",
      "Epoch 17/20  Iteration 28941/35720 Training loss: 0.7081 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 28942/35720 Training loss: 0.7082 0.2266 sec/batch\n",
      "Epoch 17/20  Iteration 28943/35720 Training loss: 0.7082 0.2211 sec/batch\n",
      "Epoch 17/20  Iteration 28944/35720 Training loss: 0.7080 0.2239 sec/batch\n",
      "Epoch 17/20  Iteration 28945/35720 Training loss: 0.7080 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 28946/35720 Training loss: 0.7079 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 28947/35720 Training loss: 0.7078 0.2215 sec/batch\n",
      "Epoch 17/20  Iteration 28948/35720 Training loss: 0.7078 0.2210 sec/batch\n",
      "Epoch 17/20  Iteration 28949/35720 Training loss: 0.7078 0.2183 sec/batch\n",
      "Epoch 17/20  Iteration 28950/35720 Training loss: 0.7077 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 28951/35720 Training loss: 0.7077 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 28952/35720 Training loss: 0.7077 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 28953/35720 Training loss: 0.7078 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 28954/35720 Training loss: 0.7078 0.2174 sec/batch\n",
      "Epoch 17/20  Iteration 28955/35720 Training loss: 0.7076 0.2203 sec/batch\n",
      "Epoch 17/20  Iteration 28956/35720 Training loss: 0.7076 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 28957/35720 Training loss: 0.7075 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 28958/35720 Training loss: 0.7074 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 28959/35720 Training loss: 0.7074 0.2117 sec/batch\n",
      "Epoch 17/20  Iteration 28960/35720 Training loss: 0.7074 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 28961/35720 Training loss: 0.7075 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 28962/35720 Training loss: 0.7074 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 28963/35720 Training loss: 0.7074 0.2115 sec/batch\n",
      "Epoch 17/20  Iteration 28964/35720 Training loss: 0.7074 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 28965/35720 Training loss: 0.7073 0.2099 sec/batch\n",
      "Epoch 17/20  Iteration 28966/35720 Training loss: 0.7073 0.2142 sec/batch\n",
      "Epoch 17/20  Iteration 28967/35720 Training loss: 0.7072 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 28968/35720 Training loss: 0.7071 0.2046 sec/batch\n",
      "Epoch 17/20  Iteration 28969/35720 Training loss: 0.7071 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 28970/35720 Training loss: 0.7070 0.2129 sec/batch\n",
      "Epoch 17/20  Iteration 28971/35720 Training loss: 0.7070 0.2140 sec/batch\n",
      "Epoch 17/20  Iteration 28972/35720 Training loss: 0.7070 0.2322 sec/batch\n",
      "Epoch 17/20  Iteration 28973/35720 Training loss: 0.7071 0.2320 sec/batch\n",
      "Epoch 17/20  Iteration 28974/35720 Training loss: 0.7070 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 28975/35720 Training loss: 0.7070 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 28976/35720 Training loss: 0.7069 0.2311 sec/batch\n",
      "Epoch 17/20  Iteration 28977/35720 Training loss: 0.7069 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 28978/35720 Training loss: 0.7070 0.2147 sec/batch\n",
      "Epoch 17/20  Iteration 28979/35720 Training loss: 0.7070 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 28980/35720 Training loss: 0.7070 0.2151 sec/batch\n",
      "Epoch 17/20  Iteration 28981/35720 Training loss: 0.7069 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 28982/35720 Training loss: 0.7068 0.2158 sec/batch\n",
      "Epoch 17/20  Iteration 28983/35720 Training loss: 0.7068 0.2245 sec/batch\n",
      "Epoch 17/20  Iteration 28984/35720 Training loss: 0.7069 0.2195 sec/batch\n",
      "Epoch 17/20  Iteration 28985/35720 Training loss: 0.7068 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 28986/35720 Training loss: 0.7066 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 28987/35720 Training loss: 0.7068 0.2294 sec/batch\n",
      "Epoch 17/20  Iteration 28988/35720 Training loss: 0.7067 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 28989/35720 Training loss: 0.7065 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 28990/35720 Training loss: 0.7065 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 28991/35720 Training loss: 0.7065 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 28992/35720 Training loss: 0.7065 0.2124 sec/batch\n",
      "Epoch 17/20  Iteration 28993/35720 Training loss: 0.7065 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 28994/35720 Training loss: 0.7064 0.2150 sec/batch\n",
      "Epoch 17/20  Iteration 28995/35720 Training loss: 0.7064 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 28996/35720 Training loss: 0.7064 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 28997/35720 Training loss: 0.7064 0.2099 sec/batch\n",
      "Epoch 17/20  Iteration 28998/35720 Training loss: 0.7064 0.2215 sec/batch\n",
      "Epoch 17/20  Iteration 28999/35720 Training loss: 0.7065 0.2164 sec/batch\n",
      "Epoch 17/20  Iteration 29000/35720 Training loss: 0.7065 0.2406 sec/batch\n",
      "Validation loss: 1.59695 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 29001/35720 Training loss: 0.7079 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29002/35720 Training loss: 0.7079 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29003/35720 Training loss: 0.7078 0.2234 sec/batch\n",
      "Epoch 17/20  Iteration 29004/35720 Training loss: 0.7078 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29005/35720 Training loss: 0.7078 0.2161 sec/batch\n",
      "Epoch 17/20  Iteration 29006/35720 Training loss: 0.7078 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29007/35720 Training loss: 0.7079 0.2070 sec/batch\n",
      "Epoch 17/20  Iteration 29008/35720 Training loss: 0.7078 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29009/35720 Training loss: 0.7079 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 29010/35720 Training loss: 0.7079 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 29011/35720 Training loss: 0.7080 0.2197 sec/batch\n",
      "Epoch 17/20  Iteration 29012/35720 Training loss: 0.7080 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 29013/35720 Training loss: 0.7081 0.2125 sec/batch\n",
      "Epoch 17/20  Iteration 29014/35720 Training loss: 0.7082 0.2309 sec/batch\n",
      "Epoch 17/20  Iteration 29015/35720 Training loss: 0.7083 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29016/35720 Training loss: 0.7083 0.2305 sec/batch\n",
      "Epoch 17/20  Iteration 29017/35720 Training loss: 0.7083 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 29018/35720 Training loss: 0.7084 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 29019/35720 Training loss: 0.7084 0.2140 sec/batch\n",
      "Epoch 17/20  Iteration 29020/35720 Training loss: 0.7084 0.2287 sec/batch\n",
      "Epoch 17/20  Iteration 29021/35720 Training loss: 0.7084 0.2220 sec/batch\n",
      "Epoch 17/20  Iteration 29022/35720 Training loss: 0.7086 0.2208 sec/batch\n",
      "Epoch 17/20  Iteration 29023/35720 Training loss: 0.7087 0.2075 sec/batch\n",
      "Epoch 17/20  Iteration 29024/35720 Training loss: 0.7089 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29025/35720 Training loss: 0.7090 0.2178 sec/batch\n",
      "Epoch 17/20  Iteration 29026/35720 Training loss: 0.7089 0.2198 sec/batch\n",
      "Epoch 17/20  Iteration 29027/35720 Training loss: 0.7088 0.2174 sec/batch\n",
      "Epoch 17/20  Iteration 29028/35720 Training loss: 0.7088 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 29029/35720 Training loss: 0.7088 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29030/35720 Training loss: 0.7089 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29031/35720 Training loss: 0.7090 0.2237 sec/batch\n",
      "Epoch 17/20  Iteration 29032/35720 Training loss: 0.7092 0.2136 sec/batch\n",
      "Epoch 17/20  Iteration 29033/35720 Training loss: 0.7093 0.2191 sec/batch\n",
      "Epoch 17/20  Iteration 29034/35720 Training loss: 0.7093 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 29035/35720 Training loss: 0.7092 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29036/35720 Training loss: 0.7092 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 29037/35720 Training loss: 0.7092 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 29038/35720 Training loss: 0.7093 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29039/35720 Training loss: 0.7094 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29040/35720 Training loss: 0.7094 0.2199 sec/batch\n",
      "Epoch 17/20  Iteration 29041/35720 Training loss: 0.7095 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 29042/35720 Training loss: 0.7094 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 29043/35720 Training loss: 0.7094 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 29044/35720 Training loss: 0.7094 0.2219 sec/batch\n",
      "Epoch 17/20  Iteration 29045/35720 Training loss: 0.7095 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29046/35720 Training loss: 0.7094 0.2077 sec/batch\n",
      "Epoch 17/20  Iteration 29047/35720 Training loss: 0.7095 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29048/35720 Training loss: 0.7094 0.2201 sec/batch\n",
      "Epoch 17/20  Iteration 29049/35720 Training loss: 0.7095 0.2202 sec/batch\n",
      "Epoch 17/20  Iteration 29050/35720 Training loss: 0.7094 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 29051/35720 Training loss: 0.7095 0.2117 sec/batch\n",
      "Epoch 17/20  Iteration 29052/35720 Training loss: 0.7095 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 29053/35720 Training loss: 0.7096 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 29054/35720 Training loss: 0.7095 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29055/35720 Training loss: 0.7095 0.2186 sec/batch\n",
      "Epoch 17/20  Iteration 29056/35720 Training loss: 0.7094 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 29057/35720 Training loss: 0.7095 0.2202 sec/batch\n",
      "Epoch 17/20  Iteration 29058/35720 Training loss: 0.7093 0.2260 sec/batch\n",
      "Epoch 17/20  Iteration 29059/35720 Training loss: 0.7094 0.2249 sec/batch\n",
      "Epoch 17/20  Iteration 29060/35720 Training loss: 0.7094 0.2107 sec/batch\n",
      "Epoch 17/20  Iteration 29061/35720 Training loss: 0.7095 0.2260 sec/batch\n",
      "Epoch 17/20  Iteration 29062/35720 Training loss: 0.7094 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 29063/35720 Training loss: 0.7094 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 29064/35720 Training loss: 0.7093 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29065/35720 Training loss: 0.7093 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29066/35720 Training loss: 0.7092 0.2083 sec/batch\n",
      "Epoch 17/20  Iteration 29067/35720 Training loss: 0.7092 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29068/35720 Training loss: 0.7092 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 29069/35720 Training loss: 0.7092 0.2133 sec/batch\n",
      "Epoch 17/20  Iteration 29070/35720 Training loss: 0.7091 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29071/35720 Training loss: 0.7092 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29072/35720 Training loss: 0.7090 0.2182 sec/batch\n",
      "Epoch 17/20  Iteration 29073/35720 Training loss: 0.7090 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 29074/35720 Training loss: 0.7091 0.2072 sec/batch\n",
      "Epoch 17/20  Iteration 29075/35720 Training loss: 0.7090 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29076/35720 Training loss: 0.7090 0.2326 sec/batch\n",
      "Epoch 17/20  Iteration 29077/35720 Training loss: 0.7089 0.2237 sec/batch\n",
      "Epoch 17/20  Iteration 29078/35720 Training loss: 0.7089 0.2181 sec/batch\n",
      "Epoch 17/20  Iteration 29079/35720 Training loss: 0.7088 0.2233 sec/batch\n",
      "Epoch 17/20  Iteration 29080/35720 Training loss: 0.7086 0.2174 sec/batch\n",
      "Epoch 17/20  Iteration 29081/35720 Training loss: 0.7086 0.2261 sec/batch\n",
      "Epoch 17/20  Iteration 29082/35720 Training loss: 0.7086 0.2182 sec/batch\n",
      "Epoch 17/20  Iteration 29083/35720 Training loss: 0.7086 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 29084/35720 Training loss: 0.7086 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 29085/35720 Training loss: 0.7087 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 29086/35720 Training loss: 0.7086 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 29087/35720 Training loss: 0.7086 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29088/35720 Training loss: 0.7086 0.2244 sec/batch\n",
      "Epoch 17/20  Iteration 29089/35720 Training loss: 0.7086 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29090/35720 Training loss: 0.7086 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29091/35720 Training loss: 0.7086 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29092/35720 Training loss: 0.7087 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 29093/35720 Training loss: 0.7087 0.2261 sec/batch\n",
      "Epoch 17/20  Iteration 29094/35720 Training loss: 0.7087 0.2194 sec/batch\n",
      "Epoch 17/20  Iteration 29095/35720 Training loss: 0.7087 0.2240 sec/batch\n",
      "Epoch 17/20  Iteration 29096/35720 Training loss: 0.7088 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29097/35720 Training loss: 0.7087 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29098/35720 Training loss: 0.7087 0.2231 sec/batch\n",
      "Epoch 17/20  Iteration 29099/35720 Training loss: 0.7087 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 29100/35720 Training loss: 0.7086 0.2249 sec/batch\n",
      "Epoch 17/20  Iteration 29101/35720 Training loss: 0.7086 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29102/35720 Training loss: 0.7085 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 29103/35720 Training loss: 0.7086 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 29104/35720 Training loss: 0.7086 0.2196 sec/batch\n",
      "Epoch 17/20  Iteration 29105/35720 Training loss: 0.7087 0.2099 sec/batch\n",
      "Epoch 17/20  Iteration 29106/35720 Training loss: 0.7086 0.2293 sec/batch\n",
      "Epoch 17/20  Iteration 29107/35720 Training loss: 0.7086 0.2188 sec/batch\n",
      "Epoch 17/20  Iteration 29108/35720 Training loss: 0.7086 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 29109/35720 Training loss: 0.7086 0.2272 sec/batch\n",
      "Epoch 17/20  Iteration 29110/35720 Training loss: 0.7086 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29111/35720 Training loss: 0.7085 0.2240 sec/batch\n",
      "Epoch 17/20  Iteration 29112/35720 Training loss: 0.7084 0.2109 sec/batch\n",
      "Epoch 17/20  Iteration 29113/35720 Training loss: 0.7084 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29114/35720 Training loss: 0.7083 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29115/35720 Training loss: 0.7083 0.2305 sec/batch\n",
      "Epoch 17/20  Iteration 29116/35720 Training loss: 0.7082 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 29117/35720 Training loss: 0.7082 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 29118/35720 Training loss: 0.7081 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 29119/35720 Training loss: 0.7081 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 29120/35720 Training loss: 0.7081 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29121/35720 Training loss: 0.7081 0.2083 sec/batch\n",
      "Epoch 17/20  Iteration 29122/35720 Training loss: 0.7082 0.2277 sec/batch\n",
      "Epoch 17/20  Iteration 29123/35720 Training loss: 0.7082 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 29124/35720 Training loss: 0.7083 0.2248 sec/batch\n",
      "Epoch 17/20  Iteration 29125/35720 Training loss: 0.7082 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 29126/35720 Training loss: 0.7083 0.2225 sec/batch\n",
      "Epoch 17/20  Iteration 29127/35720 Training loss: 0.7083 0.2196 sec/batch\n",
      "Epoch 17/20  Iteration 29128/35720 Training loss: 0.7082 0.2260 sec/batch\n",
      "Epoch 17/20  Iteration 29129/35720 Training loss: 0.7082 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 29130/35720 Training loss: 0.7082 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29131/35720 Training loss: 0.7083 0.2208 sec/batch\n",
      "Epoch 17/20  Iteration 29132/35720 Training loss: 0.7084 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 29133/35720 Training loss: 0.7083 0.2381 sec/batch\n",
      "Epoch 17/20  Iteration 29134/35720 Training loss: 0.7084 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29135/35720 Training loss: 0.7084 0.2136 sec/batch\n",
      "Epoch 17/20  Iteration 29136/35720 Training loss: 0.7083 0.2257 sec/batch\n",
      "Epoch 17/20  Iteration 29137/35720 Training loss: 0.7084 0.2297 sec/batch\n",
      "Epoch 17/20  Iteration 29138/35720 Training loss: 0.7083 0.2246 sec/batch\n",
      "Epoch 17/20  Iteration 29139/35720 Training loss: 0.7083 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 29140/35720 Training loss: 0.7083 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29141/35720 Training loss: 0.7082 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 29142/35720 Training loss: 0.7082 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29143/35720 Training loss: 0.7082 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29144/35720 Training loss: 0.7082 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 29145/35720 Training loss: 0.7081 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 29146/35720 Training loss: 0.7081 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 29147/35720 Training loss: 0.7080 0.2328 sec/batch\n",
      "Epoch 17/20  Iteration 29148/35720 Training loss: 0.7081 0.2300 sec/batch\n",
      "Epoch 17/20  Iteration 29149/35720 Training loss: 0.7082 0.2198 sec/batch\n",
      "Epoch 17/20  Iteration 29150/35720 Training loss: 0.7082 0.2161 sec/batch\n",
      "Epoch 17/20  Iteration 29151/35720 Training loss: 0.7083 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 29152/35720 Training loss: 0.7084 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 29153/35720 Training loss: 0.7084 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 29154/35720 Training loss: 0.7084 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29155/35720 Training loss: 0.7084 0.2220 sec/batch\n",
      "Epoch 17/20  Iteration 29156/35720 Training loss: 0.7084 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 29157/35720 Training loss: 0.7085 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29158/35720 Training loss: 0.7084 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29159/35720 Training loss: 0.7085 0.2209 sec/batch\n",
      "Epoch 17/20  Iteration 29160/35720 Training loss: 0.7085 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29161/35720 Training loss: 0.7084 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 29162/35720 Training loss: 0.7083 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29163/35720 Training loss: 0.7083 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 29164/35720 Training loss: 0.7083 0.2246 sec/batch\n",
      "Epoch 17/20  Iteration 29165/35720 Training loss: 0.7082 0.2245 sec/batch\n",
      "Epoch 17/20  Iteration 29166/35720 Training loss: 0.7082 0.2058 sec/batch\n",
      "Epoch 17/20  Iteration 29167/35720 Training loss: 0.7082 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29168/35720 Training loss: 0.7082 0.2074 sec/batch\n",
      "Epoch 17/20  Iteration 29169/35720 Training loss: 0.7081 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 29170/35720 Training loss: 0.7081 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 29171/35720 Training loss: 0.7080 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 29172/35720 Training loss: 0.7080 0.2203 sec/batch\n",
      "Epoch 17/20  Iteration 29173/35720 Training loss: 0.7079 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 29174/35720 Training loss: 0.7080 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 29175/35720 Training loss: 0.7079 0.2174 sec/batch\n",
      "Epoch 17/20  Iteration 29176/35720 Training loss: 0.7078 0.2145 sec/batch\n",
      "Epoch 17/20  Iteration 29177/35720 Training loss: 0.7077 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 29178/35720 Training loss: 0.7076 0.2143 sec/batch\n",
      "Epoch 17/20  Iteration 29179/35720 Training loss: 0.7076 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29180/35720 Training loss: 0.7076 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 29181/35720 Training loss: 0.7076 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29182/35720 Training loss: 0.7076 0.2163 sec/batch\n",
      "Epoch 17/20  Iteration 29183/35720 Training loss: 0.7075 0.2206 sec/batch\n",
      "Epoch 17/20  Iteration 29184/35720 Training loss: 0.7075 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 29185/35720 Training loss: 0.7076 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 29186/35720 Training loss: 0.7076 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 29187/35720 Training loss: 0.7075 0.2262 sec/batch\n",
      "Epoch 17/20  Iteration 29188/35720 Training loss: 0.7075 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29189/35720 Training loss: 0.7074 0.2296 sec/batch\n",
      "Epoch 17/20  Iteration 29190/35720 Training loss: 0.7075 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 29191/35720 Training loss: 0.7075 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 29192/35720 Training loss: 0.7074 0.2255 sec/batch\n",
      "Epoch 17/20  Iteration 29193/35720 Training loss: 0.7074 0.2266 sec/batch\n",
      "Epoch 17/20  Iteration 29194/35720 Training loss: 0.7073 0.2292 sec/batch\n",
      "Epoch 17/20  Iteration 29195/35720 Training loss: 0.7073 0.2268 sec/batch\n",
      "Epoch 17/20  Iteration 29196/35720 Training loss: 0.7072 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 29197/35720 Training loss: 0.7072 0.2150 sec/batch\n",
      "Epoch 17/20  Iteration 29198/35720 Training loss: 0.7073 0.2233 sec/batch\n",
      "Epoch 17/20  Iteration 29199/35720 Training loss: 0.7072 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 29200/35720 Training loss: 0.7072 0.2189 sec/batch\n",
      "Validation loss: 1.58947 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 29201/35720 Training loss: 0.7078 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29202/35720 Training loss: 0.7079 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 29203/35720 Training loss: 0.7079 0.2296 sec/batch\n",
      "Epoch 17/20  Iteration 29204/35720 Training loss: 0.7079 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 29205/35720 Training loss: 0.7079 0.2197 sec/batch\n",
      "Epoch 17/20  Iteration 29206/35720 Training loss: 0.7079 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29207/35720 Training loss: 0.7079 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 29208/35720 Training loss: 0.7078 0.2145 sec/batch\n",
      "Epoch 17/20  Iteration 29209/35720 Training loss: 0.7078 0.2168 sec/batch\n",
      "Epoch 17/20  Iteration 29210/35720 Training loss: 0.7078 0.2464 sec/batch\n",
      "Epoch 17/20  Iteration 29211/35720 Training loss: 0.7078 0.2274 sec/batch\n",
      "Epoch 17/20  Iteration 29212/35720 Training loss: 0.7078 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 29213/35720 Training loss: 0.7078 0.2072 sec/batch\n",
      "Epoch 17/20  Iteration 29214/35720 Training loss: 0.7077 0.2332 sec/batch\n",
      "Epoch 17/20  Iteration 29215/35720 Training loss: 0.7077 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29216/35720 Training loss: 0.7077 0.2241 sec/batch\n",
      "Epoch 17/20  Iteration 29217/35720 Training loss: 0.7077 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 29218/35720 Training loss: 0.7079 0.2077 sec/batch\n",
      "Epoch 17/20  Iteration 29219/35720 Training loss: 0.7079 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29220/35720 Training loss: 0.7078 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29221/35720 Training loss: 0.7078 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 29222/35720 Training loss: 0.7078 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 29223/35720 Training loss: 0.7078 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29224/35720 Training loss: 0.7077 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29225/35720 Training loss: 0.7076 0.2246 sec/batch\n",
      "Epoch 17/20  Iteration 29226/35720 Training loss: 0.7075 0.2226 sec/batch\n",
      "Epoch 17/20  Iteration 29227/35720 Training loss: 0.7075 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 29228/35720 Training loss: 0.7075 0.2054 sec/batch\n",
      "Epoch 17/20  Iteration 29229/35720 Training loss: 0.7074 0.2054 sec/batch\n",
      "Epoch 17/20  Iteration 29230/35720 Training loss: 0.7075 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29231/35720 Training loss: 0.7075 0.2291 sec/batch\n",
      "Epoch 17/20  Iteration 29232/35720 Training loss: 0.7075 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29233/35720 Training loss: 0.7076 0.2213 sec/batch\n",
      "Epoch 17/20  Iteration 29234/35720 Training loss: 0.7077 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 29235/35720 Training loss: 0.7077 0.2197 sec/batch\n",
      "Epoch 17/20  Iteration 29236/35720 Training loss: 0.7077 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29237/35720 Training loss: 0.7077 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 29238/35720 Training loss: 0.7078 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29239/35720 Training loss: 0.7077 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29240/35720 Training loss: 0.7077 0.2218 sec/batch\n",
      "Epoch 17/20  Iteration 29241/35720 Training loss: 0.7078 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 29242/35720 Training loss: 0.7078 0.2185 sec/batch\n",
      "Epoch 17/20  Iteration 29243/35720 Training loss: 0.7079 0.2203 sec/batch\n",
      "Epoch 17/20  Iteration 29244/35720 Training loss: 0.7078 0.2180 sec/batch\n",
      "Epoch 17/20  Iteration 29245/35720 Training loss: 0.7078 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 29246/35720 Training loss: 0.7078 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29247/35720 Training loss: 0.7077 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 29248/35720 Training loss: 0.7077 0.2249 sec/batch\n",
      "Epoch 17/20  Iteration 29249/35720 Training loss: 0.7078 0.2198 sec/batch\n",
      "Epoch 17/20  Iteration 29250/35720 Training loss: 0.7077 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 29251/35720 Training loss: 0.7077 0.2185 sec/batch\n",
      "Epoch 17/20  Iteration 29252/35720 Training loss: 0.7076 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29253/35720 Training loss: 0.7076 0.2254 sec/batch\n",
      "Epoch 17/20  Iteration 29254/35720 Training loss: 0.7076 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29255/35720 Training loss: 0.7077 0.2147 sec/batch\n",
      "Epoch 17/20  Iteration 29256/35720 Training loss: 0.7076 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29257/35720 Training loss: 0.7077 0.2190 sec/batch\n",
      "Epoch 17/20  Iteration 29258/35720 Training loss: 0.7076 0.2174 sec/batch\n",
      "Epoch 17/20  Iteration 29259/35720 Training loss: 0.7076 0.2203 sec/batch\n",
      "Epoch 17/20  Iteration 29260/35720 Training loss: 0.7076 0.2216 sec/batch\n",
      "Epoch 17/20  Iteration 29261/35720 Training loss: 0.7076 0.2162 sec/batch\n",
      "Epoch 17/20  Iteration 29262/35720 Training loss: 0.7075 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29263/35720 Training loss: 0.7075 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29264/35720 Training loss: 0.7075 0.2340 sec/batch\n",
      "Epoch 17/20  Iteration 29265/35720 Training loss: 0.7075 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29266/35720 Training loss: 0.7074 0.2274 sec/batch\n",
      "Epoch 17/20  Iteration 29267/35720 Training loss: 0.7075 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 29268/35720 Training loss: 0.7075 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 29269/35720 Training loss: 0.7076 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 29270/35720 Training loss: 0.7077 0.2208 sec/batch\n",
      "Epoch 17/20  Iteration 29271/35720 Training loss: 0.7077 0.2253 sec/batch\n",
      "Epoch 17/20  Iteration 29272/35720 Training loss: 0.7077 0.2243 sec/batch\n",
      "Epoch 17/20  Iteration 29273/35720 Training loss: 0.7077 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29274/35720 Training loss: 0.7077 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29275/35720 Training loss: 0.7077 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 29276/35720 Training loss: 0.7076 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 29277/35720 Training loss: 0.7075 0.2269 sec/batch\n",
      "Epoch 17/20  Iteration 29278/35720 Training loss: 0.7076 0.2115 sec/batch\n",
      "Epoch 17/20  Iteration 29279/35720 Training loss: 0.7075 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29280/35720 Training loss: 0.7075 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 29281/35720 Training loss: 0.7075 0.2186 sec/batch\n",
      "Epoch 17/20  Iteration 29282/35720 Training loss: 0.7074 0.2184 sec/batch\n",
      "Epoch 17/20  Iteration 29283/35720 Training loss: 0.7075 0.2181 sec/batch\n",
      "Epoch 17/20  Iteration 29284/35720 Training loss: 0.7075 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 29285/35720 Training loss: 0.7075 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 29286/35720 Training loss: 0.7075 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 29287/35720 Training loss: 0.7076 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 29288/35720 Training loss: 0.7076 0.2292 sec/batch\n",
      "Epoch 17/20  Iteration 29289/35720 Training loss: 0.7076 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29290/35720 Training loss: 0.7076 0.2213 sec/batch\n",
      "Epoch 17/20  Iteration 29291/35720 Training loss: 0.7076 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 29292/35720 Training loss: 0.7077 0.2308 sec/batch\n",
      "Epoch 17/20  Iteration 29293/35720 Training loss: 0.7078 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29294/35720 Training loss: 0.7078 0.2185 sec/batch\n",
      "Epoch 17/20  Iteration 29295/35720 Training loss: 0.7077 0.2204 sec/batch\n",
      "Epoch 17/20  Iteration 29296/35720 Training loss: 0.7078 0.2124 sec/batch\n",
      "Epoch 17/20  Iteration 29297/35720 Training loss: 0.7078 0.2335 sec/batch\n",
      "Epoch 17/20  Iteration 29298/35720 Training loss: 0.7079 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 29299/35720 Training loss: 0.7080 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 29300/35720 Training loss: 0.7079 0.2236 sec/batch\n",
      "Epoch 17/20  Iteration 29301/35720 Training loss: 0.7079 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 29302/35720 Training loss: 0.7079 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29303/35720 Training loss: 0.7080 0.2184 sec/batch\n",
      "Epoch 17/20  Iteration 29304/35720 Training loss: 0.7081 0.2371 sec/batch\n",
      "Epoch 17/20  Iteration 29305/35720 Training loss: 0.7082 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 29306/35720 Training loss: 0.7082 0.2177 sec/batch\n",
      "Epoch 17/20  Iteration 29307/35720 Training loss: 0.7082 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 29308/35720 Training loss: 0.7082 0.2220 sec/batch\n",
      "Epoch 17/20  Iteration 29309/35720 Training loss: 0.7082 0.2194 sec/batch\n",
      "Epoch 17/20  Iteration 29310/35720 Training loss: 0.7082 0.2163 sec/batch\n",
      "Epoch 17/20  Iteration 29311/35720 Training loss: 0.7081 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29312/35720 Training loss: 0.7081 0.2052 sec/batch\n",
      "Epoch 17/20  Iteration 29313/35720 Training loss: 0.7081 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 29314/35720 Training loss: 0.7081 0.2159 sec/batch\n",
      "Epoch 17/20  Iteration 29315/35720 Training loss: 0.7081 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 29316/35720 Training loss: 0.7082 0.2185 sec/batch\n",
      "Epoch 17/20  Iteration 29317/35720 Training loss: 0.7082 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 29318/35720 Training loss: 0.7082 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29319/35720 Training loss: 0.7081 0.2163 sec/batch\n",
      "Epoch 17/20  Iteration 29320/35720 Training loss: 0.7081 0.2149 sec/batch\n",
      "Epoch 17/20  Iteration 29321/35720 Training loss: 0.7081 0.2129 sec/batch\n",
      "Epoch 17/20  Iteration 29322/35720 Training loss: 0.7081 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 29323/35720 Training loss: 0.7081 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 29324/35720 Training loss: 0.7080 0.2279 sec/batch\n",
      "Epoch 17/20  Iteration 29325/35720 Training loss: 0.7080 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29326/35720 Training loss: 0.7080 0.2205 sec/batch\n",
      "Epoch 17/20  Iteration 29327/35720 Training loss: 0.7080 0.2310 sec/batch\n",
      "Epoch 17/20  Iteration 29328/35720 Training loss: 0.7080 0.2142 sec/batch\n",
      "Epoch 17/20  Iteration 29329/35720 Training loss: 0.7079 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 29330/35720 Training loss: 0.7079 0.2211 sec/batch\n",
      "Epoch 17/20  Iteration 29331/35720 Training loss: 0.7078 0.2277 sec/batch\n",
      "Epoch 17/20  Iteration 29332/35720 Training loss: 0.7078 0.2120 sec/batch\n",
      "Epoch 17/20  Iteration 29333/35720 Training loss: 0.7078 0.2146 sec/batch\n",
      "Epoch 17/20  Iteration 29334/35720 Training loss: 0.7078 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29335/35720 Training loss: 0.7078 0.2115 sec/batch\n",
      "Epoch 17/20  Iteration 29336/35720 Training loss: 0.7078 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29337/35720 Training loss: 0.7079 0.2252 sec/batch\n",
      "Epoch 17/20  Iteration 29338/35720 Training loss: 0.7079 0.2282 sec/batch\n",
      "Epoch 17/20  Iteration 29339/35720 Training loss: 0.7078 0.2248 sec/batch\n",
      "Epoch 17/20  Iteration 29340/35720 Training loss: 0.7078 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29341/35720 Training loss: 0.7078 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29342/35720 Training loss: 0.7078 0.2182 sec/batch\n",
      "Epoch 17/20  Iteration 29343/35720 Training loss: 0.7078 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 29344/35720 Training loss: 0.7078 0.2187 sec/batch\n",
      "Epoch 17/20  Iteration 29345/35720 Training loss: 0.7079 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 29346/35720 Training loss: 0.7080 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 29347/35720 Training loss: 0.7081 0.2145 sec/batch\n",
      "Epoch 17/20  Iteration 29348/35720 Training loss: 0.7082 0.2178 sec/batch\n",
      "Epoch 17/20  Iteration 29349/35720 Training loss: 0.7081 0.2057 sec/batch\n",
      "Epoch 17/20  Iteration 29350/35720 Training loss: 0.7081 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29351/35720 Training loss: 0.7080 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29352/35720 Training loss: 0.7080 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29353/35720 Training loss: 0.7080 0.2174 sec/batch\n",
      "Epoch 17/20  Iteration 29354/35720 Training loss: 0.7079 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29355/35720 Training loss: 0.7080 0.2186 sec/batch\n",
      "Epoch 17/20  Iteration 29356/35720 Training loss: 0.7080 0.2185 sec/batch\n",
      "Epoch 17/20  Iteration 29357/35720 Training loss: 0.7080 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29358/35720 Training loss: 0.7080 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 29359/35720 Training loss: 0.7080 0.2241 sec/batch\n",
      "Epoch 17/20  Iteration 29360/35720 Training loss: 0.7079 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 29361/35720 Training loss: 0.7079 0.2147 sec/batch\n",
      "Epoch 17/20  Iteration 29362/35720 Training loss: 0.7078 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 29363/35720 Training loss: 0.7079 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 29364/35720 Training loss: 0.7079 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29365/35720 Training loss: 0.7080 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 29366/35720 Training loss: 0.7080 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 29367/35720 Training loss: 0.7080 0.2188 sec/batch\n",
      "Epoch 17/20  Iteration 29368/35720 Training loss: 0.7080 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 29369/35720 Training loss: 0.7080 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 29370/35720 Training loss: 0.7080 0.2145 sec/batch\n",
      "Epoch 17/20  Iteration 29371/35720 Training loss: 0.7080 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29372/35720 Training loss: 0.7080 0.2193 sec/batch\n",
      "Epoch 17/20  Iteration 29373/35720 Training loss: 0.7080 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29374/35720 Training loss: 0.7080 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29375/35720 Training loss: 0.7080 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 29376/35720 Training loss: 0.7081 0.2242 sec/batch\n",
      "Epoch 17/20  Iteration 29377/35720 Training loss: 0.7081 0.2310 sec/batch\n",
      "Epoch 17/20  Iteration 29378/35720 Training loss: 0.7082 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29379/35720 Training loss: 0.7082 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 29380/35720 Training loss: 0.7082 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 29381/35720 Training loss: 0.7083 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29382/35720 Training loss: 0.7082 0.2143 sec/batch\n",
      "Epoch 17/20  Iteration 29383/35720 Training loss: 0.7083 0.2250 sec/batch\n",
      "Epoch 17/20  Iteration 29384/35720 Training loss: 0.7082 0.2164 sec/batch\n",
      "Epoch 17/20  Iteration 29385/35720 Training loss: 0.7083 0.2178 sec/batch\n",
      "Epoch 17/20  Iteration 29386/35720 Training loss: 0.7083 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 29387/35720 Training loss: 0.7083 0.2164 sec/batch\n",
      "Epoch 17/20  Iteration 29388/35720 Training loss: 0.7083 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 29389/35720 Training loss: 0.7083 0.2271 sec/batch\n",
      "Epoch 17/20  Iteration 29390/35720 Training loss: 0.7083 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 29391/35720 Training loss: 0.7084 0.2124 sec/batch\n",
      "Epoch 17/20  Iteration 29392/35720 Training loss: 0.7083 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 29393/35720 Training loss: 0.7083 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 29394/35720 Training loss: 0.7084 0.2219 sec/batch\n",
      "Epoch 17/20  Iteration 29395/35720 Training loss: 0.7083 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 29396/35720 Training loss: 0.7083 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29397/35720 Training loss: 0.7083 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29398/35720 Training loss: 0.7083 0.2278 sec/batch\n",
      "Epoch 17/20  Iteration 29399/35720 Training loss: 0.7082 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 29400/35720 Training loss: 0.7082 0.2164 sec/batch\n",
      "Validation loss: 1.59157 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 29401/35720 Training loss: 0.7084 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 29402/35720 Training loss: 0.7084 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 29403/35720 Training loss: 0.7084 0.2162 sec/batch\n",
      "Epoch 17/20  Iteration 29404/35720 Training loss: 0.7083 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 29405/35720 Training loss: 0.7083 0.2150 sec/batch\n",
      "Epoch 17/20  Iteration 29406/35720 Training loss: 0.7083 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 29407/35720 Training loss: 0.7083 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29408/35720 Training loss: 0.7083 0.2164 sec/batch\n",
      "Epoch 17/20  Iteration 29409/35720 Training loss: 0.7084 0.2142 sec/batch\n",
      "Epoch 17/20  Iteration 29410/35720 Training loss: 0.7084 0.2077 sec/batch\n",
      "Epoch 17/20  Iteration 29411/35720 Training loss: 0.7084 0.2245 sec/batch\n",
      "Epoch 17/20  Iteration 29412/35720 Training loss: 0.7083 0.2149 sec/batch\n",
      "Epoch 17/20  Iteration 29413/35720 Training loss: 0.7084 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29414/35720 Training loss: 0.7084 0.2258 sec/batch\n",
      "Epoch 17/20  Iteration 29415/35720 Training loss: 0.7084 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 29416/35720 Training loss: 0.7084 0.2257 sec/batch\n",
      "Epoch 17/20  Iteration 29417/35720 Training loss: 0.7084 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 29418/35720 Training loss: 0.7084 0.2177 sec/batch\n",
      "Epoch 17/20  Iteration 29419/35720 Training loss: 0.7084 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 29420/35720 Training loss: 0.7084 0.2195 sec/batch\n",
      "Epoch 17/20  Iteration 29421/35720 Training loss: 0.7085 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 29422/35720 Training loss: 0.7085 0.2129 sec/batch\n",
      "Epoch 17/20  Iteration 29423/35720 Training loss: 0.7085 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29424/35720 Training loss: 0.7085 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29425/35720 Training loss: 0.7084 0.2260 sec/batch\n",
      "Epoch 17/20  Iteration 29426/35720 Training loss: 0.7085 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 29427/35720 Training loss: 0.7085 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29428/35720 Training loss: 0.7084 0.2127 sec/batch\n",
      "Epoch 17/20  Iteration 29429/35720 Training loss: 0.7084 0.2072 sec/batch\n",
      "Epoch 17/20  Iteration 29430/35720 Training loss: 0.7084 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29431/35720 Training loss: 0.7084 0.2174 sec/batch\n",
      "Epoch 17/20  Iteration 29432/35720 Training loss: 0.7084 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29433/35720 Training loss: 0.7083 0.2220 sec/batch\n",
      "Epoch 17/20  Iteration 29434/35720 Training loss: 0.7084 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29435/35720 Training loss: 0.7083 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 29436/35720 Training loss: 0.7083 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 29437/35720 Training loss: 0.7083 0.2207 sec/batch\n",
      "Epoch 17/20  Iteration 29438/35720 Training loss: 0.7083 0.2176 sec/batch\n",
      "Epoch 17/20  Iteration 29439/35720 Training loss: 0.7083 0.2193 sec/batch\n",
      "Epoch 17/20  Iteration 29440/35720 Training loss: 0.7082 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29441/35720 Training loss: 0.7082 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 29442/35720 Training loss: 0.7082 0.2158 sec/batch\n",
      "Epoch 17/20  Iteration 29443/35720 Training loss: 0.7082 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 29444/35720 Training loss: 0.7082 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29445/35720 Training loss: 0.7082 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 29446/35720 Training loss: 0.7081 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29447/35720 Training loss: 0.7081 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29448/35720 Training loss: 0.7081 0.2242 sec/batch\n",
      "Epoch 17/20  Iteration 29449/35720 Training loss: 0.7080 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 29450/35720 Training loss: 0.7080 0.2168 sec/batch\n",
      "Epoch 17/20  Iteration 29451/35720 Training loss: 0.7080 0.2075 sec/batch\n",
      "Epoch 17/20  Iteration 29452/35720 Training loss: 0.7079 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29453/35720 Training loss: 0.7079 0.2279 sec/batch\n",
      "Epoch 17/20  Iteration 29454/35720 Training loss: 0.7078 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 29455/35720 Training loss: 0.7079 0.2226 sec/batch\n",
      "Epoch 17/20  Iteration 29456/35720 Training loss: 0.7078 0.2150 sec/batch\n",
      "Epoch 17/20  Iteration 29457/35720 Training loss: 0.7077 0.2053 sec/batch\n",
      "Epoch 17/20  Iteration 29458/35720 Training loss: 0.7077 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29459/35720 Training loss: 0.7077 0.2253 sec/batch\n",
      "Epoch 17/20  Iteration 29460/35720 Training loss: 0.7077 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 29461/35720 Training loss: 0.7076 0.2245 sec/batch\n",
      "Epoch 17/20  Iteration 29462/35720 Training loss: 0.7076 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29463/35720 Training loss: 0.7076 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29464/35720 Training loss: 0.7076 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 29465/35720 Training loss: 0.7076 0.2232 sec/batch\n",
      "Epoch 17/20  Iteration 29466/35720 Training loss: 0.7075 0.2226 sec/batch\n",
      "Epoch 17/20  Iteration 29467/35720 Training loss: 0.7075 0.2302 sec/batch\n",
      "Epoch 17/20  Iteration 29468/35720 Training loss: 0.7075 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 29469/35720 Training loss: 0.7074 0.2183 sec/batch\n",
      "Epoch 17/20  Iteration 29470/35720 Training loss: 0.7074 0.2276 sec/batch\n",
      "Epoch 17/20  Iteration 29471/35720 Training loss: 0.7074 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29472/35720 Training loss: 0.7074 0.2250 sec/batch\n",
      "Epoch 17/20  Iteration 29473/35720 Training loss: 0.7073 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29474/35720 Training loss: 0.7072 0.2206 sec/batch\n",
      "Epoch 17/20  Iteration 29475/35720 Training loss: 0.7071 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 29476/35720 Training loss: 0.7071 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 29477/35720 Training loss: 0.7071 0.2297 sec/batch\n",
      "Epoch 17/20  Iteration 29478/35720 Training loss: 0.7070 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 29479/35720 Training loss: 0.7070 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 29480/35720 Training loss: 0.7070 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 29481/35720 Training loss: 0.7069 0.2186 sec/batch\n",
      "Epoch 17/20  Iteration 29482/35720 Training loss: 0.7069 0.2107 sec/batch\n",
      "Epoch 17/20  Iteration 29483/35720 Training loss: 0.7069 0.2243 sec/batch\n",
      "Epoch 17/20  Iteration 29484/35720 Training loss: 0.7069 0.2177 sec/batch\n",
      "Epoch 17/20  Iteration 29485/35720 Training loss: 0.7069 0.2135 sec/batch\n",
      "Epoch 17/20  Iteration 29486/35720 Training loss: 0.7068 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29487/35720 Training loss: 0.7068 0.2117 sec/batch\n",
      "Epoch 17/20  Iteration 29488/35720 Training loss: 0.7068 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29489/35720 Training loss: 0.7068 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29490/35720 Training loss: 0.7068 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 29491/35720 Training loss: 0.7069 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 29492/35720 Training loss: 0.7069 0.2263 sec/batch\n",
      "Epoch 17/20  Iteration 29493/35720 Training loss: 0.7069 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 29494/35720 Training loss: 0.7068 0.2246 sec/batch\n",
      "Epoch 17/20  Iteration 29495/35720 Training loss: 0.7068 0.2075 sec/batch\n",
      "Epoch 17/20  Iteration 29496/35720 Training loss: 0.7068 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 29497/35720 Training loss: 0.7068 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29498/35720 Training loss: 0.7067 0.2159 sec/batch\n",
      "Epoch 17/20  Iteration 29499/35720 Training loss: 0.7067 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29500/35720 Training loss: 0.7067 0.2245 sec/batch\n",
      "Epoch 17/20  Iteration 29501/35720 Training loss: 0.7067 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29502/35720 Training loss: 0.7067 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 29503/35720 Training loss: 0.7068 0.2253 sec/batch\n",
      "Epoch 17/20  Iteration 29504/35720 Training loss: 0.7067 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29505/35720 Training loss: 0.7068 0.2245 sec/batch\n",
      "Epoch 17/20  Iteration 29506/35720 Training loss: 0.7067 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29507/35720 Training loss: 0.7067 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 29508/35720 Training loss: 0.7068 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29509/35720 Training loss: 0.7068 0.2254 sec/batch\n",
      "Epoch 17/20  Iteration 29510/35720 Training loss: 0.7068 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29511/35720 Training loss: 0.7067 0.2280 sec/batch\n",
      "Epoch 17/20  Iteration 29512/35720 Training loss: 0.7066 0.2125 sec/batch\n",
      "Epoch 17/20  Iteration 29513/35720 Training loss: 0.7066 0.2072 sec/batch\n",
      "Epoch 17/20  Iteration 29514/35720 Training loss: 0.7065 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29515/35720 Training loss: 0.7065 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 29516/35720 Training loss: 0.7064 0.2212 sec/batch\n",
      "Epoch 17/20  Iteration 29517/35720 Training loss: 0.7064 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29518/35720 Training loss: 0.7064 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 29519/35720 Training loss: 0.7063 0.2143 sec/batch\n",
      "Epoch 17/20  Iteration 29520/35720 Training loss: 0.7064 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 29521/35720 Training loss: 0.7063 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29522/35720 Training loss: 0.7064 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 29523/35720 Training loss: 0.7064 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 29524/35720 Training loss: 0.7063 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 29525/35720 Training loss: 0.7063 0.2083 sec/batch\n",
      "Epoch 17/20  Iteration 29526/35720 Training loss: 0.7063 0.2264 sec/batch\n",
      "Epoch 17/20  Iteration 29527/35720 Training loss: 0.7062 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 29528/35720 Training loss: 0.7062 0.2310 sec/batch\n",
      "Epoch 17/20  Iteration 29529/35720 Training loss: 0.7061 0.2724 sec/batch\n",
      "Epoch 17/20  Iteration 29530/35720 Training loss: 0.7062 0.2558 sec/batch\n",
      "Epoch 17/20  Iteration 29531/35720 Training loss: 0.7062 0.2887 sec/batch\n",
      "Epoch 17/20  Iteration 29532/35720 Training loss: 0.7062 0.2360 sec/batch\n",
      "Epoch 17/20  Iteration 29533/35720 Training loss: 0.7061 0.2263 sec/batch\n",
      "Epoch 17/20  Iteration 29534/35720 Training loss: 0.7061 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 29535/35720 Training loss: 0.7061 0.2238 sec/batch\n",
      "Epoch 17/20  Iteration 29536/35720 Training loss: 0.7060 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 29537/35720 Training loss: 0.7060 0.2149 sec/batch\n",
      "Epoch 17/20  Iteration 29538/35720 Training loss: 0.7059 0.2191 sec/batch\n",
      "Epoch 17/20  Iteration 29539/35720 Training loss: 0.7059 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 29540/35720 Training loss: 0.7059 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 29541/35720 Training loss: 0.7059 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29542/35720 Training loss: 0.7059 0.2149 sec/batch\n",
      "Epoch 17/20  Iteration 29543/35720 Training loss: 0.7059 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 29544/35720 Training loss: 0.7058 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29545/35720 Training loss: 0.7059 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29546/35720 Training loss: 0.7060 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 29547/35720 Training loss: 0.7059 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29548/35720 Training loss: 0.7058 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29549/35720 Training loss: 0.7058 0.2212 sec/batch\n",
      "Epoch 17/20  Iteration 29550/35720 Training loss: 0.7058 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 29551/35720 Training loss: 0.7057 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29552/35720 Training loss: 0.7057 0.2240 sec/batch\n",
      "Epoch 17/20  Iteration 29553/35720 Training loss: 0.7057 0.2145 sec/batch\n",
      "Epoch 17/20  Iteration 29554/35720 Training loss: 0.7057 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 29555/35720 Training loss: 0.7056 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 29556/35720 Training loss: 0.7056 0.2194 sec/batch\n",
      "Epoch 17/20  Iteration 29557/35720 Training loss: 0.7056 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 29558/35720 Training loss: 0.7055 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 29559/35720 Training loss: 0.7054 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 29560/35720 Training loss: 0.7054 0.2319 sec/batch\n",
      "Epoch 17/20  Iteration 29561/35720 Training loss: 0.7054 0.2168 sec/batch\n",
      "Epoch 17/20  Iteration 29562/35720 Training loss: 0.7053 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29563/35720 Training loss: 0.7053 0.2260 sec/batch\n",
      "Epoch 17/20  Iteration 29564/35720 Training loss: 0.7053 0.2291 sec/batch\n",
      "Epoch 17/20  Iteration 29565/35720 Training loss: 0.7052 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 29566/35720 Training loss: 0.7052 0.2151 sec/batch\n",
      "Epoch 17/20  Iteration 29567/35720 Training loss: 0.7052 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 29568/35720 Training loss: 0.7051 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29569/35720 Training loss: 0.7051 0.2278 sec/batch\n",
      "Epoch 17/20  Iteration 29570/35720 Training loss: 0.7051 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29571/35720 Training loss: 0.7052 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29572/35720 Training loss: 0.7052 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 29573/35720 Training loss: 0.7052 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29574/35720 Training loss: 0.7052 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29575/35720 Training loss: 0.7052 0.2248 sec/batch\n",
      "Epoch 17/20  Iteration 29576/35720 Training loss: 0.7052 0.2145 sec/batch\n",
      "Epoch 17/20  Iteration 29577/35720 Training loss: 0.7051 0.2193 sec/batch\n",
      "Epoch 17/20  Iteration 29578/35720 Training loss: 0.7050 0.2165 sec/batch\n",
      "Epoch 17/20  Iteration 29579/35720 Training loss: 0.7050 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29580/35720 Training loss: 0.7049 0.2133 sec/batch\n",
      "Epoch 17/20  Iteration 29581/35720 Training loss: 0.7048 0.2203 sec/batch\n",
      "Epoch 17/20  Iteration 29582/35720 Training loss: 0.7048 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29583/35720 Training loss: 0.7048 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 29584/35720 Training loss: 0.7048 0.2165 sec/batch\n",
      "Epoch 17/20  Iteration 29585/35720 Training loss: 0.7047 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29586/35720 Training loss: 0.7047 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29587/35720 Training loss: 0.7047 0.2115 sec/batch\n",
      "Epoch 17/20  Iteration 29588/35720 Training loss: 0.7047 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29589/35720 Training loss: 0.7046 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 29590/35720 Training loss: 0.7046 0.2189 sec/batch\n",
      "Epoch 17/20  Iteration 29591/35720 Training loss: 0.7046 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 29592/35720 Training loss: 0.7047 0.2165 sec/batch\n",
      "Epoch 17/20  Iteration 29593/35720 Training loss: 0.7046 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29594/35720 Training loss: 0.7046 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 29595/35720 Training loss: 0.7045 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 29596/35720 Training loss: 0.7045 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29597/35720 Training loss: 0.7045 0.2201 sec/batch\n",
      "Epoch 17/20  Iteration 29598/35720 Training loss: 0.7046 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29599/35720 Training loss: 0.7046 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29600/35720 Training loss: 0.7046 0.2086 sec/batch\n",
      "Validation loss: 1.60719 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 29601/35720 Training loss: 0.7050 0.2120 sec/batch\n",
      "Epoch 17/20  Iteration 29602/35720 Training loss: 0.7050 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29603/35720 Training loss: 0.7050 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 29604/35720 Training loss: 0.7050 0.2260 sec/batch\n",
      "Epoch 17/20  Iteration 29605/35720 Training loss: 0.7049 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 29606/35720 Training loss: 0.7049 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29607/35720 Training loss: 0.7049 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 29608/35720 Training loss: 0.7049 0.2255 sec/batch\n",
      "Epoch 17/20  Iteration 29609/35720 Training loss: 0.7048 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 29610/35720 Training loss: 0.7048 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 29611/35720 Training loss: 0.7049 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 29612/35720 Training loss: 0.7049 0.2204 sec/batch\n",
      "Epoch 17/20  Iteration 29613/35720 Training loss: 0.7049 0.2250 sec/batch\n",
      "Epoch 17/20  Iteration 29614/35720 Training loss: 0.7049 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 29615/35720 Training loss: 0.7049 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 29616/35720 Training loss: 0.7049 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 29617/35720 Training loss: 0.7049 0.2078 sec/batch\n",
      "Epoch 17/20  Iteration 29618/35720 Training loss: 0.7049 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29619/35720 Training loss: 0.7049 0.2158 sec/batch\n",
      "Epoch 17/20  Iteration 29620/35720 Training loss: 0.7049 0.2195 sec/batch\n",
      "Epoch 17/20  Iteration 29621/35720 Training loss: 0.7049 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 29622/35720 Training loss: 0.7049 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 29623/35720 Training loss: 0.7049 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29624/35720 Training loss: 0.7049 0.2095 sec/batch\n",
      "Epoch 17/20  Iteration 29625/35720 Training loss: 0.7049 0.2136 sec/batch\n",
      "Epoch 17/20  Iteration 29626/35720 Training loss: 0.7049 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29627/35720 Training loss: 0.7049 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 29628/35720 Training loss: 0.7050 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 29629/35720 Training loss: 0.7050 0.2198 sec/batch\n",
      "Epoch 17/20  Iteration 29630/35720 Training loss: 0.7050 0.2131 sec/batch\n",
      "Epoch 17/20  Iteration 29631/35720 Training loss: 0.7051 0.2117 sec/batch\n",
      "Epoch 17/20  Iteration 29632/35720 Training loss: 0.7051 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 29633/35720 Training loss: 0.7050 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29634/35720 Training loss: 0.7051 0.2078 sec/batch\n",
      "Epoch 17/20  Iteration 29635/35720 Training loss: 0.7051 0.2107 sec/batch\n",
      "Epoch 17/20  Iteration 29636/35720 Training loss: 0.7050 0.2248 sec/batch\n",
      "Epoch 17/20  Iteration 29637/35720 Training loss: 0.7050 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29638/35720 Training loss: 0.7050 0.2184 sec/batch\n",
      "Epoch 17/20  Iteration 29639/35720 Training loss: 0.7051 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29640/35720 Training loss: 0.7051 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 29641/35720 Training loss: 0.7051 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29642/35720 Training loss: 0.7051 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 29643/35720 Training loss: 0.7050 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 29644/35720 Training loss: 0.7050 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 29645/35720 Training loss: 0.7050 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29646/35720 Training loss: 0.7051 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29647/35720 Training loss: 0.7050 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 29648/35720 Training loss: 0.7050 0.2303 sec/batch\n",
      "Epoch 17/20  Iteration 29649/35720 Training loss: 0.7050 0.2241 sec/batch\n",
      "Epoch 17/20  Iteration 29650/35720 Training loss: 0.7050 0.2225 sec/batch\n",
      "Epoch 17/20  Iteration 29651/35720 Training loss: 0.7050 0.2049 sec/batch\n",
      "Epoch 17/20  Iteration 29652/35720 Training loss: 0.7050 0.2159 sec/batch\n",
      "Epoch 17/20  Iteration 29653/35720 Training loss: 0.7050 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 29654/35720 Training loss: 0.7050 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29655/35720 Training loss: 0.7049 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 29656/35720 Training loss: 0.7050 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 29657/35720 Training loss: 0.7049 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29658/35720 Training loss: 0.7049 0.2281 sec/batch\n",
      "Epoch 17/20  Iteration 29659/35720 Training loss: 0.7049 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 29660/35720 Training loss: 0.7049 0.2229 sec/batch\n",
      "Epoch 17/20  Iteration 29661/35720 Training loss: 0.7049 0.2170 sec/batch\n",
      "Epoch 17/20  Iteration 29662/35720 Training loss: 0.7049 0.2058 sec/batch\n",
      "Epoch 17/20  Iteration 29663/35720 Training loss: 0.7049 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29664/35720 Training loss: 0.7049 0.2247 sec/batch\n",
      "Epoch 17/20  Iteration 29665/35720 Training loss: 0.7049 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 29666/35720 Training loss: 0.7050 0.2258 sec/batch\n",
      "Epoch 17/20  Iteration 29667/35720 Training loss: 0.7049 0.2124 sec/batch\n",
      "Epoch 17/20  Iteration 29668/35720 Training loss: 0.7049 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 29669/35720 Training loss: 0.7049 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 29670/35720 Training loss: 0.7049 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 29671/35720 Training loss: 0.7049 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 29672/35720 Training loss: 0.7049 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 29673/35720 Training loss: 0.7049 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 29674/35720 Training loss: 0.7048 0.2143 sec/batch\n",
      "Epoch 17/20  Iteration 29675/35720 Training loss: 0.7049 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 29676/35720 Training loss: 0.7049 0.2246 sec/batch\n",
      "Epoch 17/20  Iteration 29677/35720 Training loss: 0.7049 0.2268 sec/batch\n",
      "Epoch 17/20  Iteration 29678/35720 Training loss: 0.7049 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29679/35720 Training loss: 0.7049 0.2049 sec/batch\n",
      "Epoch 17/20  Iteration 29680/35720 Training loss: 0.7049 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29681/35720 Training loss: 0.7050 0.2346 sec/batch\n",
      "Epoch 17/20  Iteration 29682/35720 Training loss: 0.7049 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 29683/35720 Training loss: 0.7049 0.2193 sec/batch\n",
      "Epoch 17/20  Iteration 29684/35720 Training loss: 0.7050 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 29685/35720 Training loss: 0.7050 0.2155 sec/batch\n",
      "Epoch 17/20  Iteration 29686/35720 Training loss: 0.7050 0.2380 sec/batch\n",
      "Epoch 17/20  Iteration 29687/35720 Training loss: 0.7049 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29688/35720 Training loss: 0.7049 0.2210 sec/batch\n",
      "Epoch 17/20  Iteration 29689/35720 Training loss: 0.7048 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 29690/35720 Training loss: 0.7048 0.2140 sec/batch\n",
      "Epoch 17/20  Iteration 29691/35720 Training loss: 0.7048 0.2415 sec/batch\n",
      "Epoch 17/20  Iteration 29692/35720 Training loss: 0.7048 0.2199 sec/batch\n",
      "Epoch 17/20  Iteration 29693/35720 Training loss: 0.7048 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 29694/35720 Training loss: 0.7048 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 29695/35720 Training loss: 0.7048 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29696/35720 Training loss: 0.7048 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 29697/35720 Training loss: 0.7048 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 29698/35720 Training loss: 0.7049 0.2142 sec/batch\n",
      "Epoch 17/20  Iteration 29699/35720 Training loss: 0.7049 0.2357 sec/batch\n",
      "Epoch 17/20  Iteration 29700/35720 Training loss: 0.7049 0.2187 sec/batch\n",
      "Epoch 17/20  Iteration 29701/35720 Training loss: 0.7048 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29702/35720 Training loss: 0.7048 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29703/35720 Training loss: 0.7048 0.2209 sec/batch\n",
      "Epoch 17/20  Iteration 29704/35720 Training loss: 0.7048 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 29705/35720 Training loss: 0.7047 0.2143 sec/batch\n",
      "Epoch 17/20  Iteration 29706/35720 Training loss: 0.7047 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 29707/35720 Training loss: 0.7047 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29708/35720 Training loss: 0.7046 0.2274 sec/batch\n",
      "Epoch 17/20  Iteration 29709/35720 Training loss: 0.7045 0.2279 sec/batch\n",
      "Epoch 17/20  Iteration 29710/35720 Training loss: 0.7045 0.2184 sec/batch\n",
      "Epoch 17/20  Iteration 29711/35720 Training loss: 0.7045 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 29712/35720 Training loss: 0.7044 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29713/35720 Training loss: 0.7044 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29714/35720 Training loss: 0.7044 0.2269 sec/batch\n",
      "Epoch 17/20  Iteration 29715/35720 Training loss: 0.7043 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 29716/35720 Training loss: 0.7043 0.2391 sec/batch\n",
      "Epoch 17/20  Iteration 29717/35720 Training loss: 0.7043 0.2126 sec/batch\n",
      "Epoch 17/20  Iteration 29718/35720 Training loss: 0.7043 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29719/35720 Training loss: 0.7042 0.2175 sec/batch\n",
      "Epoch 17/20  Iteration 29720/35720 Training loss: 0.7042 0.2272 sec/batch\n",
      "Epoch 17/20  Iteration 29721/35720 Training loss: 0.7042 0.2286 sec/batch\n",
      "Epoch 17/20  Iteration 29722/35720 Training loss: 0.7042 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 29723/35720 Training loss: 0.7042 0.2058 sec/batch\n",
      "Epoch 17/20  Iteration 29724/35720 Training loss: 0.7041 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 29725/35720 Training loss: 0.7041 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29726/35720 Training loss: 0.7041 0.2140 sec/batch\n",
      "Epoch 17/20  Iteration 29727/35720 Training loss: 0.7041 0.2207 sec/batch\n",
      "Epoch 17/20  Iteration 29728/35720 Training loss: 0.7041 0.2315 sec/batch\n",
      "Epoch 17/20  Iteration 29729/35720 Training loss: 0.7041 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29730/35720 Training loss: 0.7041 0.2234 sec/batch\n",
      "Epoch 17/20  Iteration 29731/35720 Training loss: 0.7041 0.2143 sec/batch\n",
      "Epoch 17/20  Iteration 29732/35720 Training loss: 0.7041 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 29733/35720 Training loss: 0.7041 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 29734/35720 Training loss: 0.7040 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 29735/35720 Training loss: 0.7040 0.2234 sec/batch\n",
      "Epoch 17/20  Iteration 29736/35720 Training loss: 0.7040 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29737/35720 Training loss: 0.7040 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 29738/35720 Training loss: 0.7040 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 29739/35720 Training loss: 0.7040 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29740/35720 Training loss: 0.7041 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 29741/35720 Training loss: 0.7040 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 29742/35720 Training loss: 0.7040 0.2360 sec/batch\n",
      "Epoch 17/20  Iteration 29743/35720 Training loss: 0.7039 0.2231 sec/batch\n",
      "Epoch 17/20  Iteration 29744/35720 Training loss: 0.7040 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29745/35720 Training loss: 0.7040 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29746/35720 Training loss: 0.7040 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 29747/35720 Training loss: 0.7040 0.2120 sec/batch\n",
      "Epoch 17/20  Iteration 29748/35720 Training loss: 0.7041 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29749/35720 Training loss: 0.7040 0.2363 sec/batch\n",
      "Epoch 17/20  Iteration 29750/35720 Training loss: 0.7040 0.2158 sec/batch\n",
      "Epoch 17/20  Iteration 29751/35720 Training loss: 0.7040 0.2151 sec/batch\n",
      "Epoch 17/20  Iteration 29752/35720 Training loss: 0.7041 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 29753/35720 Training loss: 0.7041 0.2280 sec/batch\n",
      "Epoch 17/20  Iteration 29754/35720 Training loss: 0.7041 0.2695 sec/batch\n",
      "Epoch 17/20  Iteration 29755/35720 Training loss: 0.7041 0.2168 sec/batch\n",
      "Epoch 17/20  Iteration 29756/35720 Training loss: 0.7041 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 29757/35720 Training loss: 0.7041 0.2249 sec/batch\n",
      "Epoch 17/20  Iteration 29758/35720 Training loss: 0.7041 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 29759/35720 Training loss: 0.7041 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29760/35720 Training loss: 0.7040 0.2076 sec/batch\n",
      "Epoch 17/20  Iteration 29761/35720 Training loss: 0.7040 0.2216 sec/batch\n",
      "Epoch 17/20  Iteration 29762/35720 Training loss: 0.7040 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 29763/35720 Training loss: 0.7040 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29764/35720 Training loss: 0.7040 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 29765/35720 Training loss: 0.7040 0.2077 sec/batch\n",
      "Epoch 17/20  Iteration 29766/35720 Training loss: 0.7040 0.2264 sec/batch\n",
      "Epoch 17/20  Iteration 29767/35720 Training loss: 0.7040 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 29768/35720 Training loss: 0.7040 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 29769/35720 Training loss: 0.7040 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 29770/35720 Training loss: 0.7040 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29771/35720 Training loss: 0.7040 0.2230 sec/batch\n",
      "Epoch 17/20  Iteration 29772/35720 Training loss: 0.7040 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29773/35720 Training loss: 0.7040 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29774/35720 Training loss: 0.7040 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29775/35720 Training loss: 0.7040 0.2247 sec/batch\n",
      "Epoch 17/20  Iteration 29776/35720 Training loss: 0.7040 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 29777/35720 Training loss: 0.7039 0.2306 sec/batch\n",
      "Epoch 17/20  Iteration 29778/35720 Training loss: 0.7039 0.2257 sec/batch\n",
      "Epoch 17/20  Iteration 29779/35720 Training loss: 0.7038 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29780/35720 Training loss: 0.7038 0.2425 sec/batch\n",
      "Epoch 17/20  Iteration 29781/35720 Training loss: 0.7038 0.2259 sec/batch\n",
      "Epoch 17/20  Iteration 29782/35720 Training loss: 0.7038 0.2251 sec/batch\n",
      "Epoch 17/20  Iteration 29783/35720 Training loss: 0.7037 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 29784/35720 Training loss: 0.7038 0.2120 sec/batch\n",
      "Epoch 17/20  Iteration 29785/35720 Training loss: 0.7037 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 29786/35720 Training loss: 0.7037 0.2142 sec/batch\n",
      "Epoch 17/20  Iteration 29787/35720 Training loss: 0.7037 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 29788/35720 Training loss: 0.7037 0.2192 sec/batch\n",
      "Epoch 17/20  Iteration 29789/35720 Training loss: 0.7037 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 29790/35720 Training loss: 0.7037 0.2196 sec/batch\n",
      "Epoch 17/20  Iteration 29791/35720 Training loss: 0.7037 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29792/35720 Training loss: 0.7037 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 29793/35720 Training loss: 0.7037 0.2454 sec/batch\n",
      "Epoch 17/20  Iteration 29794/35720 Training loss: 0.7037 0.2219 sec/batch\n",
      "Epoch 17/20  Iteration 29795/35720 Training loss: 0.7037 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 29796/35720 Training loss: 0.7037 0.2117 sec/batch\n",
      "Epoch 17/20  Iteration 29797/35720 Training loss: 0.7037 0.2164 sec/batch\n",
      "Epoch 17/20  Iteration 29798/35720 Training loss: 0.7037 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29799/35720 Training loss: 0.7037 0.2397 sec/batch\n",
      "Epoch 17/20  Iteration 29800/35720 Training loss: 0.7037 0.2140 sec/batch\n",
      "Validation loss: 1.60749 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 29801/35720 Training loss: 0.7040 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29802/35720 Training loss: 0.7040 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29803/35720 Training loss: 0.7040 0.2074 sec/batch\n",
      "Epoch 17/20  Iteration 29804/35720 Training loss: 0.7040 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29805/35720 Training loss: 0.7040 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29806/35720 Training loss: 0.7040 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29807/35720 Training loss: 0.7040 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29808/35720 Training loss: 0.7040 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29809/35720 Training loss: 0.7040 0.2254 sec/batch\n",
      "Epoch 17/20  Iteration 29810/35720 Training loss: 0.7040 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 29811/35720 Training loss: 0.7040 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 29812/35720 Training loss: 0.7039 0.2206 sec/batch\n",
      "Epoch 17/20  Iteration 29813/35720 Training loss: 0.7039 0.2279 sec/batch\n",
      "Epoch 17/20  Iteration 29814/35720 Training loss: 0.7038 0.2127 sec/batch\n",
      "Epoch 17/20  Iteration 29815/35720 Training loss: 0.7038 0.2149 sec/batch\n",
      "Epoch 17/20  Iteration 29816/35720 Training loss: 0.7037 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 29817/35720 Training loss: 0.7037 0.2155 sec/batch\n",
      "Epoch 17/20  Iteration 29818/35720 Training loss: 0.7037 0.2214 sec/batch\n",
      "Epoch 17/20  Iteration 29819/35720 Training loss: 0.7037 0.2165 sec/batch\n",
      "Epoch 17/20  Iteration 29820/35720 Training loss: 0.7037 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 29821/35720 Training loss: 0.7036 0.2218 sec/batch\n",
      "Epoch 17/20  Iteration 29822/35720 Training loss: 0.7036 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29823/35720 Training loss: 0.7035 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 29824/35720 Training loss: 0.7035 0.2129 sec/batch\n",
      "Epoch 17/20  Iteration 29825/35720 Training loss: 0.7035 0.2124 sec/batch\n",
      "Epoch 17/20  Iteration 29826/35720 Training loss: 0.7035 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 29827/35720 Training loss: 0.7035 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 29828/35720 Training loss: 0.7035 0.2264 sec/batch\n",
      "Epoch 17/20  Iteration 29829/35720 Training loss: 0.7035 0.2070 sec/batch\n",
      "Epoch 17/20  Iteration 29830/35720 Training loss: 0.7034 0.2233 sec/batch\n",
      "Epoch 17/20  Iteration 29831/35720 Training loss: 0.7035 0.2318 sec/batch\n",
      "Epoch 17/20  Iteration 29832/35720 Training loss: 0.7034 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 29833/35720 Training loss: 0.7034 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29834/35720 Training loss: 0.7033 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 29835/35720 Training loss: 0.7033 0.2293 sec/batch\n",
      "Epoch 17/20  Iteration 29836/35720 Training loss: 0.7033 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29837/35720 Training loss: 0.7033 0.2147 sec/batch\n",
      "Epoch 17/20  Iteration 29838/35720 Training loss: 0.7033 0.2070 sec/batch\n",
      "Epoch 17/20  Iteration 29839/35720 Training loss: 0.7032 0.2355 sec/batch\n",
      "Epoch 17/20  Iteration 29840/35720 Training loss: 0.7032 0.2204 sec/batch\n",
      "Epoch 17/20  Iteration 29841/35720 Training loss: 0.7032 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 29842/35720 Training loss: 0.7031 0.2277 sec/batch\n",
      "Epoch 17/20  Iteration 29843/35720 Training loss: 0.7031 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 29844/35720 Training loss: 0.7031 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 29845/35720 Training loss: 0.7031 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 29846/35720 Training loss: 0.7031 0.2205 sec/batch\n",
      "Epoch 17/20  Iteration 29847/35720 Training loss: 0.7031 0.2272 sec/batch\n",
      "Epoch 17/20  Iteration 29848/35720 Training loss: 0.7031 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 29849/35720 Training loss: 0.7031 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29850/35720 Training loss: 0.7030 0.2055 sec/batch\n",
      "Epoch 17/20  Iteration 29851/35720 Training loss: 0.7030 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 29852/35720 Training loss: 0.7030 0.2267 sec/batch\n",
      "Epoch 17/20  Iteration 29853/35720 Training loss: 0.7029 0.2122 sec/batch\n",
      "Epoch 17/20  Iteration 29854/35720 Training loss: 0.7029 0.2254 sec/batch\n",
      "Epoch 17/20  Iteration 29855/35720 Training loss: 0.7028 0.2051 sec/batch\n",
      "Epoch 17/20  Iteration 29856/35720 Training loss: 0.7027 0.2083 sec/batch\n",
      "Epoch 17/20  Iteration 29857/35720 Training loss: 0.7028 0.2191 sec/batch\n",
      "Epoch 17/20  Iteration 29858/35720 Training loss: 0.7028 0.2177 sec/batch\n",
      "Epoch 17/20  Iteration 29859/35720 Training loss: 0.7027 0.2217 sec/batch\n",
      "Epoch 17/20  Iteration 29860/35720 Training loss: 0.7027 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 29861/35720 Training loss: 0.7027 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29862/35720 Training loss: 0.7027 0.2155 sec/batch\n",
      "Epoch 17/20  Iteration 29863/35720 Training loss: 0.7026 0.2224 sec/batch\n",
      "Epoch 17/20  Iteration 29864/35720 Training loss: 0.7026 0.2149 sec/batch\n",
      "Epoch 17/20  Iteration 29865/35720 Training loss: 0.7026 0.2256 sec/batch\n",
      "Epoch 17/20  Iteration 29866/35720 Training loss: 0.7026 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 29867/35720 Training loss: 0.7026 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29868/35720 Training loss: 0.7025 0.2318 sec/batch\n",
      "Epoch 17/20  Iteration 29869/35720 Training loss: 0.7025 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 29870/35720 Training loss: 0.7025 0.2113 sec/batch\n",
      "Epoch 17/20  Iteration 29871/35720 Training loss: 0.7025 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29872/35720 Training loss: 0.7024 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 29873/35720 Training loss: 0.7025 0.2151 sec/batch\n",
      "Epoch 17/20  Iteration 29874/35720 Training loss: 0.7025 0.2238 sec/batch\n",
      "Epoch 17/20  Iteration 29875/35720 Training loss: 0.7024 0.2307 sec/batch\n",
      "Epoch 17/20  Iteration 29876/35720 Training loss: 0.7024 0.2164 sec/batch\n",
      "Epoch 17/20  Iteration 29877/35720 Training loss: 0.7023 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29878/35720 Training loss: 0.7023 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 29879/35720 Training loss: 0.7023 0.2263 sec/batch\n",
      "Epoch 17/20  Iteration 29880/35720 Training loss: 0.7023 0.2469 sec/batch\n",
      "Epoch 17/20  Iteration 29881/35720 Training loss: 0.7023 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 29882/35720 Training loss: 0.7022 0.2162 sec/batch\n",
      "Epoch 17/20  Iteration 29883/35720 Training loss: 0.7022 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29884/35720 Training loss: 0.7022 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 29885/35720 Training loss: 0.7022 0.2263 sec/batch\n",
      "Epoch 17/20  Iteration 29886/35720 Training loss: 0.7021 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 29887/35720 Training loss: 0.7021 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 29888/35720 Training loss: 0.7021 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 29889/35720 Training loss: 0.7021 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 29890/35720 Training loss: 0.7021 0.2287 sec/batch\n",
      "Epoch 17/20  Iteration 29891/35720 Training loss: 0.7020 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29892/35720 Training loss: 0.7020 0.2150 sec/batch\n",
      "Epoch 17/20  Iteration 29893/35720 Training loss: 0.7020 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 29894/35720 Training loss: 0.7019 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 29895/35720 Training loss: 0.7019 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29896/35720 Training loss: 0.7019 0.2075 sec/batch\n",
      "Epoch 17/20  Iteration 29897/35720 Training loss: 0.7019 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 29898/35720 Training loss: 0.7019 0.2288 sec/batch\n",
      "Epoch 17/20  Iteration 29899/35720 Training loss: 0.7019 0.2077 sec/batch\n",
      "Epoch 17/20  Iteration 29900/35720 Training loss: 0.7019 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 29901/35720 Training loss: 0.7019 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 29902/35720 Training loss: 0.7019 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29903/35720 Training loss: 0.7019 0.2074 sec/batch\n",
      "Epoch 17/20  Iteration 29904/35720 Training loss: 0.7019 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 29905/35720 Training loss: 0.7019 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29906/35720 Training loss: 0.7018 0.2123 sec/batch\n",
      "Epoch 17/20  Iteration 29907/35720 Training loss: 0.7018 0.2296 sec/batch\n",
      "Epoch 17/20  Iteration 29908/35720 Training loss: 0.7018 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 29909/35720 Training loss: 0.7019 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 29910/35720 Training loss: 0.7019 0.2072 sec/batch\n",
      "Epoch 17/20  Iteration 29911/35720 Training loss: 0.7019 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 29912/35720 Training loss: 0.7018 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 29913/35720 Training loss: 0.7019 0.2326 sec/batch\n",
      "Epoch 17/20  Iteration 29914/35720 Training loss: 0.7019 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 29915/35720 Training loss: 0.7019 0.2215 sec/batch\n",
      "Epoch 17/20  Iteration 29916/35720 Training loss: 0.7019 0.2074 sec/batch\n",
      "Epoch 17/20  Iteration 29917/35720 Training loss: 0.7019 0.2072 sec/batch\n",
      "Epoch 17/20  Iteration 29918/35720 Training loss: 0.7019 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29919/35720 Training loss: 0.7019 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 29920/35720 Training loss: 0.7018 0.2246 sec/batch\n",
      "Epoch 17/20  Iteration 29921/35720 Training loss: 0.7018 0.2165 sec/batch\n",
      "Epoch 17/20  Iteration 29922/35720 Training loss: 0.7018 0.2274 sec/batch\n",
      "Epoch 17/20  Iteration 29923/35720 Training loss: 0.7018 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 29924/35720 Training loss: 0.7018 0.2262 sec/batch\n",
      "Epoch 17/20  Iteration 29925/35720 Training loss: 0.7018 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 29926/35720 Training loss: 0.7018 0.2407 sec/batch\n",
      "Epoch 17/20  Iteration 29927/35720 Training loss: 0.7018 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29928/35720 Training loss: 0.7019 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 29929/35720 Training loss: 0.7019 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29930/35720 Training loss: 0.7019 0.2131 sec/batch\n",
      "Epoch 17/20  Iteration 29931/35720 Training loss: 0.7018 0.2107 sec/batch\n",
      "Epoch 17/20  Iteration 29932/35720 Training loss: 0.7018 0.2182 sec/batch\n",
      "Epoch 17/20  Iteration 29933/35720 Training loss: 0.7018 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 29934/35720 Training loss: 0.7018 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 29935/35720 Training loss: 0.7017 0.2180 sec/batch\n",
      "Epoch 17/20  Iteration 29936/35720 Training loss: 0.7017 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29937/35720 Training loss: 0.7017 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 29938/35720 Training loss: 0.7016 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 29939/35720 Training loss: 0.7016 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 29940/35720 Training loss: 0.7016 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 29941/35720 Training loss: 0.7016 0.2143 sec/batch\n",
      "Epoch 17/20  Iteration 29942/35720 Training loss: 0.7015 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29943/35720 Training loss: 0.7015 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 29944/35720 Training loss: 0.7015 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 29945/35720 Training loss: 0.7015 0.2072 sec/batch\n",
      "Epoch 17/20  Iteration 29946/35720 Training loss: 0.7015 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 29947/35720 Training loss: 0.7015 0.2155 sec/batch\n",
      "Epoch 17/20  Iteration 29948/35720 Training loss: 0.7015 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 29949/35720 Training loss: 0.7015 0.2115 sec/batch\n",
      "Epoch 17/20  Iteration 29950/35720 Training loss: 0.7014 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 29951/35720 Training loss: 0.7014 0.2130 sec/batch\n",
      "Epoch 17/20  Iteration 29952/35720 Training loss: 0.7014 0.2251 sec/batch\n",
      "Epoch 17/20  Iteration 29953/35720 Training loss: 0.7014 0.2201 sec/batch\n",
      "Epoch 17/20  Iteration 29954/35720 Training loss: 0.7014 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 29955/35720 Training loss: 0.7014 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 29956/35720 Training loss: 0.7014 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 29957/35720 Training loss: 0.7014 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 29958/35720 Training loss: 0.7014 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 29959/35720 Training loss: 0.7014 0.2142 sec/batch\n",
      "Epoch 17/20  Iteration 29960/35720 Training loss: 0.7014 0.2239 sec/batch\n",
      "Epoch 17/20  Iteration 29961/35720 Training loss: 0.7014 0.2136 sec/batch\n",
      "Epoch 17/20  Iteration 29962/35720 Training loss: 0.7014 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 29963/35720 Training loss: 0.7014 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 29964/35720 Training loss: 0.7014 0.2308 sec/batch\n",
      "Epoch 17/20  Iteration 29965/35720 Training loss: 0.7014 0.2266 sec/batch\n",
      "Epoch 17/20  Iteration 29966/35720 Training loss: 0.7014 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 29967/35720 Training loss: 0.7014 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 29968/35720 Training loss: 0.7013 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29969/35720 Training loss: 0.7013 0.2344 sec/batch\n",
      "Epoch 17/20  Iteration 29970/35720 Training loss: 0.7012 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 29971/35720 Training loss: 0.7012 0.2194 sec/batch\n",
      "Epoch 17/20  Iteration 29972/35720 Training loss: 0.7012 0.2175 sec/batch\n",
      "Epoch 17/20  Iteration 29973/35720 Training loss: 0.7012 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 29974/35720 Training loss: 0.7011 0.2165 sec/batch\n",
      "Epoch 17/20  Iteration 29975/35720 Training loss: 0.7011 0.2121 sec/batch\n",
      "Epoch 17/20  Iteration 29976/35720 Training loss: 0.7011 0.2253 sec/batch\n",
      "Epoch 17/20  Iteration 29977/35720 Training loss: 0.7010 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 29978/35720 Training loss: 0.7010 0.3177 sec/batch\n",
      "Epoch 17/20  Iteration 29979/35720 Training loss: 0.7010 0.2859 sec/batch\n",
      "Epoch 17/20  Iteration 29980/35720 Training loss: 0.7010 0.2155 sec/batch\n",
      "Epoch 17/20  Iteration 29981/35720 Training loss: 0.7010 0.2241 sec/batch\n",
      "Epoch 17/20  Iteration 29982/35720 Training loss: 0.7010 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 29983/35720 Training loss: 0.7009 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 29984/35720 Training loss: 0.7010 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 29985/35720 Training loss: 0.7010 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 29986/35720 Training loss: 0.7010 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 29987/35720 Training loss: 0.7010 0.2176 sec/batch\n",
      "Epoch 17/20  Iteration 29988/35720 Training loss: 0.7009 0.2186 sec/batch\n",
      "Epoch 17/20  Iteration 29989/35720 Training loss: 0.7009 0.2212 sec/batch\n",
      "Epoch 17/20  Iteration 29990/35720 Training loss: 0.7009 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 29991/35720 Training loss: 0.7009 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 29992/35720 Training loss: 0.7009 0.2183 sec/batch\n",
      "Epoch 17/20  Iteration 29993/35720 Training loss: 0.7008 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 29994/35720 Training loss: 0.7009 0.2231 sec/batch\n",
      "Epoch 17/20  Iteration 29995/35720 Training loss: 0.7009 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 29996/35720 Training loss: 0.7009 0.2155 sec/batch\n",
      "Epoch 17/20  Iteration 29997/35720 Training loss: 0.7009 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 29998/35720 Training loss: 0.7009 0.2188 sec/batch\n",
      "Epoch 17/20  Iteration 29999/35720 Training loss: 0.7009 0.2374 sec/batch\n",
      "Epoch 17/20  Iteration 30000/35720 Training loss: 0.7009 0.2097 sec/batch\n",
      "Validation loss: 1.61997 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 30001/35720 Training loss: 0.7011 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30002/35720 Training loss: 0.7012 0.2341 sec/batch\n",
      "Epoch 17/20  Iteration 30003/35720 Training loss: 0.7012 0.2070 sec/batch\n",
      "Epoch 17/20  Iteration 30004/35720 Training loss: 0.7012 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 30005/35720 Training loss: 0.7012 0.2150 sec/batch\n",
      "Epoch 17/20  Iteration 30006/35720 Training loss: 0.7011 0.2216 sec/batch\n",
      "Epoch 17/20  Iteration 30007/35720 Training loss: 0.7011 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 30008/35720 Training loss: 0.7011 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 30009/35720 Training loss: 0.7011 0.2185 sec/batch\n",
      "Epoch 17/20  Iteration 30010/35720 Training loss: 0.7011 0.2361 sec/batch\n",
      "Epoch 17/20  Iteration 30011/35720 Training loss: 0.7011 0.2269 sec/batch\n",
      "Epoch 17/20  Iteration 30012/35720 Training loss: 0.7011 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 30013/35720 Training loss: 0.7010 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30014/35720 Training loss: 0.7011 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 30015/35720 Training loss: 0.7011 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 30016/35720 Training loss: 0.7011 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 30017/35720 Training loss: 0.7011 0.2203 sec/batch\n",
      "Epoch 17/20  Iteration 30018/35720 Training loss: 0.7011 0.2202 sec/batch\n",
      "Epoch 17/20  Iteration 30019/35720 Training loss: 0.7011 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 30020/35720 Training loss: 0.7011 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 30021/35720 Training loss: 0.7011 0.2211 sec/batch\n",
      "Epoch 17/20  Iteration 30022/35720 Training loss: 0.7011 0.2185 sec/batch\n",
      "Epoch 17/20  Iteration 30023/35720 Training loss: 0.7011 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 30024/35720 Training loss: 0.7011 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 30025/35720 Training loss: 0.7011 0.2213 sec/batch\n",
      "Epoch 17/20  Iteration 30026/35720 Training loss: 0.7010 0.2208 sec/batch\n",
      "Epoch 17/20  Iteration 30027/35720 Training loss: 0.7010 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 30028/35720 Training loss: 0.7010 0.2241 sec/batch\n",
      "Epoch 17/20  Iteration 30029/35720 Training loss: 0.7010 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 30030/35720 Training loss: 0.7010 0.2151 sec/batch\n",
      "Epoch 17/20  Iteration 30031/35720 Training loss: 0.7010 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 30032/35720 Training loss: 0.7010 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 30033/35720 Training loss: 0.7011 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 30034/35720 Training loss: 0.7011 0.2292 sec/batch\n",
      "Epoch 17/20  Iteration 30035/35720 Training loss: 0.7011 0.2225 sec/batch\n",
      "Epoch 17/20  Iteration 30036/35720 Training loss: 0.7011 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 30037/35720 Training loss: 0.7011 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 30038/35720 Training loss: 0.7012 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 30039/35720 Training loss: 0.7012 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 30040/35720 Training loss: 0.7012 0.2119 sec/batch\n",
      "Epoch 17/20  Iteration 30041/35720 Training loss: 0.7012 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 30042/35720 Training loss: 0.7011 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 30043/35720 Training loss: 0.7011 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 30044/35720 Training loss: 0.7012 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 30045/35720 Training loss: 0.7011 0.2206 sec/batch\n",
      "Epoch 17/20  Iteration 30046/35720 Training loss: 0.7011 0.2219 sec/batch\n",
      "Epoch 17/20  Iteration 30047/35720 Training loss: 0.7011 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 30048/35720 Training loss: 0.7011 0.2106 sec/batch\n",
      "Epoch 17/20  Iteration 30049/35720 Training loss: 0.7011 0.2083 sec/batch\n",
      "Epoch 17/20  Iteration 30050/35720 Training loss: 0.7011 0.2127 sec/batch\n",
      "Epoch 17/20  Iteration 30051/35720 Training loss: 0.7010 0.2163 sec/batch\n",
      "Epoch 17/20  Iteration 30052/35720 Training loss: 0.7009 0.2241 sec/batch\n",
      "Epoch 17/20  Iteration 30053/35720 Training loss: 0.7009 0.2234 sec/batch\n",
      "Epoch 17/20  Iteration 30054/35720 Training loss: 0.7008 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 30055/35720 Training loss: 0.7008 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 30056/35720 Training loss: 0.7008 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 30057/35720 Training loss: 0.7009 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 30058/35720 Training loss: 0.7008 0.2176 sec/batch\n",
      "Epoch 17/20  Iteration 30059/35720 Training loss: 0.7008 0.2129 sec/batch\n",
      "Epoch 17/20  Iteration 30060/35720 Training loss: 0.7008 0.2057 sec/batch\n",
      "Epoch 17/20  Iteration 30061/35720 Training loss: 0.7008 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 30062/35720 Training loss: 0.7008 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 30063/35720 Training loss: 0.7007 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 30064/35720 Training loss: 0.7007 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 30065/35720 Training loss: 0.7007 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 30066/35720 Training loss: 0.7007 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 30067/35720 Training loss: 0.7007 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 30068/35720 Training loss: 0.7006 0.2192 sec/batch\n",
      "Epoch 17/20  Iteration 30069/35720 Training loss: 0.7006 0.2212 sec/batch\n",
      "Epoch 17/20  Iteration 30070/35720 Training loss: 0.7006 0.2139 sec/batch\n",
      "Epoch 17/20  Iteration 30071/35720 Training loss: 0.7006 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 30072/35720 Training loss: 0.7006 0.2090 sec/batch\n",
      "Epoch 17/20  Iteration 30073/35720 Training loss: 0.7006 0.2229 sec/batch\n",
      "Epoch 17/20  Iteration 30074/35720 Training loss: 0.7005 0.2229 sec/batch\n",
      "Epoch 17/20  Iteration 30075/35720 Training loss: 0.7005 0.2176 sec/batch\n",
      "Epoch 17/20  Iteration 30076/35720 Training loss: 0.7005 0.2176 sec/batch\n",
      "Epoch 17/20  Iteration 30077/35720 Training loss: 0.7005 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 30078/35720 Training loss: 0.7004 0.2136 sec/batch\n",
      "Epoch 17/20  Iteration 30079/35720 Training loss: 0.7004 0.2245 sec/batch\n",
      "Epoch 17/20  Iteration 30080/35720 Training loss: 0.7004 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 30081/35720 Training loss: 0.7004 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 30082/35720 Training loss: 0.7005 0.2104 sec/batch\n",
      "Epoch 17/20  Iteration 30083/35720 Training loss: 0.7004 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 30084/35720 Training loss: 0.7004 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 30085/35720 Training loss: 0.7004 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 30086/35720 Training loss: 0.7004 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 30087/35720 Training loss: 0.7004 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 30088/35720 Training loss: 0.7004 0.2163 sec/batch\n",
      "Epoch 17/20  Iteration 30089/35720 Training loss: 0.7003 0.2115 sec/batch\n",
      "Epoch 17/20  Iteration 30090/35720 Training loss: 0.7004 0.2149 sec/batch\n",
      "Epoch 17/20  Iteration 30091/35720 Training loss: 0.7004 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 30092/35720 Training loss: 0.7004 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 30093/35720 Training loss: 0.7003 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 30094/35720 Training loss: 0.7003 0.2075 sec/batch\n",
      "Epoch 17/20  Iteration 30095/35720 Training loss: 0.7003 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 30096/35720 Training loss: 0.7003 0.2202 sec/batch\n",
      "Epoch 17/20  Iteration 30097/35720 Training loss: 0.7003 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 30098/35720 Training loss: 0.7003 0.2164 sec/batch\n",
      "Epoch 17/20  Iteration 30099/35720 Training loss: 0.7004 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 30100/35720 Training loss: 0.7004 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 30101/35720 Training loss: 0.7004 0.2116 sec/batch\n",
      "Epoch 17/20  Iteration 30102/35720 Training loss: 0.7004 0.2183 sec/batch\n",
      "Epoch 17/20  Iteration 30103/35720 Training loss: 0.7004 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30104/35720 Training loss: 0.7004 0.2191 sec/batch\n",
      "Epoch 17/20  Iteration 30105/35720 Training loss: 0.7004 0.2235 sec/batch\n",
      "Epoch 17/20  Iteration 30106/35720 Training loss: 0.7005 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 30107/35720 Training loss: 0.7005 0.2226 sec/batch\n",
      "Epoch 17/20  Iteration 30108/35720 Training loss: 0.7005 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 30109/35720 Training loss: 0.7005 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 30110/35720 Training loss: 0.7005 0.2058 sec/batch\n",
      "Epoch 17/20  Iteration 30111/35720 Training loss: 0.7005 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 30112/35720 Training loss: 0.7005 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 30113/35720 Training loss: 0.7004 0.2194 sec/batch\n",
      "Epoch 17/20  Iteration 30114/35720 Training loss: 0.7004 0.2109 sec/batch\n",
      "Epoch 17/20  Iteration 30115/35720 Training loss: 0.7004 0.2175 sec/batch\n",
      "Epoch 17/20  Iteration 30116/35720 Training loss: 0.7004 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 30117/35720 Training loss: 0.7003 0.2096 sec/batch\n",
      "Epoch 17/20  Iteration 30118/35720 Training loss: 0.7003 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 30119/35720 Training loss: 0.7003 0.2137 sec/batch\n",
      "Epoch 17/20  Iteration 30120/35720 Training loss: 0.7003 0.2115 sec/batch\n",
      "Epoch 17/20  Iteration 30121/35720 Training loss: 0.7003 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 30122/35720 Training loss: 0.7002 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 30123/35720 Training loss: 0.7002 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 30124/35720 Training loss: 0.7002 0.2249 sec/batch\n",
      "Epoch 17/20  Iteration 30125/35720 Training loss: 0.7002 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 30126/35720 Training loss: 0.7002 0.2219 sec/batch\n",
      "Epoch 17/20  Iteration 30127/35720 Training loss: 0.7002 0.2210 sec/batch\n",
      "Epoch 17/20  Iteration 30128/35720 Training loss: 0.7002 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 30129/35720 Training loss: 0.7002 0.2170 sec/batch\n",
      "Epoch 17/20  Iteration 30130/35720 Training loss: 0.7002 0.2165 sec/batch\n",
      "Epoch 17/20  Iteration 30131/35720 Training loss: 0.7001 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 30132/35720 Training loss: 0.7001 0.2098 sec/batch\n",
      "Epoch 17/20  Iteration 30133/35720 Training loss: 0.7001 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 30134/35720 Training loss: 0.7000 0.2117 sec/batch\n",
      "Epoch 17/20  Iteration 30135/35720 Training loss: 0.7000 0.2249 sec/batch\n",
      "Epoch 17/20  Iteration 30136/35720 Training loss: 0.7000 0.2299 sec/batch\n",
      "Epoch 17/20  Iteration 30137/35720 Training loss: 0.6999 0.2273 sec/batch\n",
      "Epoch 17/20  Iteration 30138/35720 Training loss: 0.6999 0.2056 sec/batch\n",
      "Epoch 17/20  Iteration 30139/35720 Training loss: 0.6999 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 30140/35720 Training loss: 0.6999 0.2218 sec/batch\n",
      "Epoch 17/20  Iteration 30141/35720 Training loss: 0.6999 0.2265 sec/batch\n",
      "Epoch 17/20  Iteration 30142/35720 Training loss: 0.6998 0.2156 sec/batch\n",
      "Epoch 17/20  Iteration 30143/35720 Training loss: 0.6998 0.2175 sec/batch\n",
      "Epoch 17/20  Iteration 30144/35720 Training loss: 0.6998 0.2180 sec/batch\n",
      "Epoch 17/20  Iteration 30145/35720 Training loss: 0.6998 0.2099 sec/batch\n",
      "Epoch 17/20  Iteration 30146/35720 Training loss: 0.6998 0.2079 sec/batch\n",
      "Epoch 17/20  Iteration 30147/35720 Training loss: 0.6998 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30148/35720 Training loss: 0.6997 0.2135 sec/batch\n",
      "Epoch 17/20  Iteration 30149/35720 Training loss: 0.6997 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 30150/35720 Training loss: 0.6997 0.2073 sec/batch\n",
      "Epoch 17/20  Iteration 30151/35720 Training loss: 0.6997 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30152/35720 Training loss: 0.6997 0.2166 sec/batch\n",
      "Epoch 17/20  Iteration 30153/35720 Training loss: 0.6996 0.2212 sec/batch\n",
      "Epoch 17/20  Iteration 30154/35720 Training loss: 0.6996 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 30155/35720 Training loss: 0.6997 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 30156/35720 Training loss: 0.6996 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 30157/35720 Training loss: 0.6997 0.2086 sec/batch\n",
      "Epoch 17/20  Iteration 30158/35720 Training loss: 0.6996 0.2227 sec/batch\n",
      "Epoch 17/20  Iteration 30159/35720 Training loss: 0.6996 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 30160/35720 Training loss: 0.6996 0.2212 sec/batch\n",
      "Epoch 17/20  Iteration 30161/35720 Training loss: 0.6996 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 30162/35720 Training loss: 0.6995 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 30163/35720 Training loss: 0.6995 0.2276 sec/batch\n",
      "Epoch 17/20  Iteration 30164/35720 Training loss: 0.6994 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 30165/35720 Training loss: 0.6994 0.2304 sec/batch\n",
      "Epoch 17/20  Iteration 30166/35720 Training loss: 0.6994 0.2188 sec/batch\n",
      "Epoch 17/20  Iteration 30167/35720 Training loss: 0.6993 0.2054 sec/batch\n",
      "Epoch 17/20  Iteration 30168/35720 Training loss: 0.6993 0.2232 sec/batch\n",
      "Epoch 17/20  Iteration 30169/35720 Training loss: 0.6993 0.2182 sec/batch\n",
      "Epoch 17/20  Iteration 30170/35720 Training loss: 0.6993 0.2207 sec/batch\n",
      "Epoch 17/20  Iteration 30171/35720 Training loss: 0.6993 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 30172/35720 Training loss: 0.6992 0.2053 sec/batch\n",
      "Epoch 17/20  Iteration 30173/35720 Training loss: 0.6992 0.2085 sec/batch\n",
      "Epoch 17/20  Iteration 30174/35720 Training loss: 0.6992 0.2206 sec/batch\n",
      "Epoch 17/20  Iteration 30175/35720 Training loss: 0.6991 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 30176/35720 Training loss: 0.6992 0.2299 sec/batch\n",
      "Epoch 17/20  Iteration 30177/35720 Training loss: 0.6991 0.2146 sec/batch\n",
      "Epoch 17/20  Iteration 30178/35720 Training loss: 0.6991 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 30179/35720 Training loss: 0.6991 0.2081 sec/batch\n",
      "Epoch 17/20  Iteration 30180/35720 Training loss: 0.6991 0.2372 sec/batch\n",
      "Epoch 17/20  Iteration 30181/35720 Training loss: 0.6990 0.2103 sec/batch\n",
      "Epoch 17/20  Iteration 30182/35720 Training loss: 0.6990 0.2242 sec/batch\n",
      "Epoch 17/20  Iteration 30183/35720 Training loss: 0.6990 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 30184/35720 Training loss: 0.6990 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30185/35720 Training loss: 0.6989 0.2052 sec/batch\n",
      "Epoch 17/20  Iteration 30186/35720 Training loss: 0.6989 0.2218 sec/batch\n",
      "Epoch 17/20  Iteration 30187/35720 Training loss: 0.6989 0.2084 sec/batch\n",
      "Epoch 17/20  Iteration 30188/35720 Training loss: 0.6988 0.2247 sec/batch\n",
      "Epoch 17/20  Iteration 30189/35720 Training loss: 0.6988 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 30190/35720 Training loss: 0.6988 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 30191/35720 Training loss: 0.6988 0.2290 sec/batch\n",
      "Epoch 17/20  Iteration 30192/35720 Training loss: 0.6988 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 30193/35720 Training loss: 0.6988 0.2301 sec/batch\n",
      "Epoch 17/20  Iteration 30194/35720 Training loss: 0.6988 0.2057 sec/batch\n",
      "Epoch 17/20  Iteration 30195/35720 Training loss: 0.6988 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 30196/35720 Training loss: 0.6988 0.2312 sec/batch\n",
      "Epoch 17/20  Iteration 30197/35720 Training loss: 0.6988 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 30198/35720 Training loss: 0.6988 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 30199/35720 Training loss: 0.6988 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 30200/35720 Training loss: 0.6988 0.2052 sec/batch\n",
      "Validation loss: 1.62879 Saving checkpoint!\n",
      "Epoch 17/20  Iteration 30201/35720 Training loss: 0.6990 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 30202/35720 Training loss: 0.6990 0.2181 sec/batch\n",
      "Epoch 17/20  Iteration 30203/35720 Training loss: 0.6990 0.2234 sec/batch\n",
      "Epoch 17/20  Iteration 30204/35720 Training loss: 0.6990 0.2214 sec/batch\n",
      "Epoch 17/20  Iteration 30205/35720 Training loss: 0.6990 0.2125 sec/batch\n",
      "Epoch 17/20  Iteration 30206/35720 Training loss: 0.6990 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 30207/35720 Training loss: 0.6990 0.2258 sec/batch\n",
      "Epoch 17/20  Iteration 30208/35720 Training loss: 0.6990 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 30209/35720 Training loss: 0.6990 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 30210/35720 Training loss: 0.6990 0.2132 sec/batch\n",
      "Epoch 17/20  Iteration 30211/35720 Training loss: 0.6990 0.2281 sec/batch\n",
      "Epoch 17/20  Iteration 30212/35720 Training loss: 0.6990 0.2238 sec/batch\n",
      "Epoch 17/20  Iteration 30213/35720 Training loss: 0.6990 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 30214/35720 Training loss: 0.6989 0.2273 sec/batch\n",
      "Epoch 17/20  Iteration 30215/35720 Training loss: 0.6990 0.2222 sec/batch\n",
      "Epoch 17/20  Iteration 30216/35720 Training loss: 0.6989 0.2105 sec/batch\n",
      "Epoch 17/20  Iteration 30217/35720 Training loss: 0.6989 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 30218/35720 Training loss: 0.6990 0.2315 sec/batch\n",
      "Epoch 17/20  Iteration 30219/35720 Training loss: 0.6989 0.2219 sec/batch\n",
      "Epoch 17/20  Iteration 30220/35720 Training loss: 0.6989 0.2265 sec/batch\n",
      "Epoch 17/20  Iteration 30221/35720 Training loss: 0.6989 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 30222/35720 Training loss: 0.6989 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 30223/35720 Training loss: 0.6989 0.2188 sec/batch\n",
      "Epoch 17/20  Iteration 30224/35720 Training loss: 0.6990 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 30225/35720 Training loss: 0.6990 0.2141 sec/batch\n",
      "Epoch 17/20  Iteration 30226/35720 Training loss: 0.6990 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 30227/35720 Training loss: 0.6990 0.2241 sec/batch\n",
      "Epoch 17/20  Iteration 30228/35720 Training loss: 0.6990 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 30229/35720 Training loss: 0.6990 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 30230/35720 Training loss: 0.6990 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30231/35720 Training loss: 0.6990 0.2163 sec/batch\n",
      "Epoch 17/20  Iteration 30232/35720 Training loss: 0.6990 0.2114 sec/batch\n",
      "Epoch 17/20  Iteration 30233/35720 Training loss: 0.6990 0.2146 sec/batch\n",
      "Epoch 17/20  Iteration 30234/35720 Training loss: 0.6990 0.2205 sec/batch\n",
      "Epoch 17/20  Iteration 30235/35720 Training loss: 0.6990 0.2171 sec/batch\n",
      "Epoch 17/20  Iteration 30236/35720 Training loss: 0.6989 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 30237/35720 Training loss: 0.6989 0.2124 sec/batch\n",
      "Epoch 17/20  Iteration 30238/35720 Training loss: 0.6990 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 30239/35720 Training loss: 0.6990 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 30240/35720 Training loss: 0.6990 0.2191 sec/batch\n",
      "Epoch 17/20  Iteration 30241/35720 Training loss: 0.6990 0.2196 sec/batch\n",
      "Epoch 17/20  Iteration 30242/35720 Training loss: 0.6990 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 30243/35720 Training loss: 0.6990 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 30244/35720 Training loss: 0.6990 0.2068 sec/batch\n",
      "Epoch 17/20  Iteration 30245/35720 Training loss: 0.6990 0.2080 sec/batch\n",
      "Epoch 17/20  Iteration 30246/35720 Training loss: 0.6990 0.2129 sec/batch\n",
      "Epoch 17/20  Iteration 30247/35720 Training loss: 0.6990 0.2092 sec/batch\n",
      "Epoch 17/20  Iteration 30248/35720 Training loss: 0.6990 0.2117 sec/batch\n",
      "Epoch 17/20  Iteration 30249/35720 Training loss: 0.6990 0.2076 sec/batch\n",
      "Epoch 17/20  Iteration 30250/35720 Training loss: 0.6991 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 30251/35720 Training loss: 0.6990 0.2208 sec/batch\n",
      "Epoch 17/20  Iteration 30252/35720 Training loss: 0.6990 0.2185 sec/batch\n",
      "Epoch 17/20  Iteration 30253/35720 Training loss: 0.6990 0.2148 sec/batch\n",
      "Epoch 17/20  Iteration 30254/35720 Training loss: 0.6990 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 30255/35720 Training loss: 0.6990 0.2053 sec/batch\n",
      "Epoch 17/20  Iteration 30256/35720 Training loss: 0.6989 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 30257/35720 Training loss: 0.6989 0.2168 sec/batch\n",
      "Epoch 17/20  Iteration 30258/35720 Training loss: 0.6988 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 30259/35720 Training loss: 0.6988 0.2200 sec/batch\n",
      "Epoch 17/20  Iteration 30260/35720 Training loss: 0.6989 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 30261/35720 Training loss: 0.6989 0.2071 sec/batch\n",
      "Epoch 17/20  Iteration 30262/35720 Training loss: 0.6989 0.2118 sec/batch\n",
      "Epoch 17/20  Iteration 30263/35720 Training loss: 0.6989 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 30264/35720 Training loss: 0.6988 0.2099 sec/batch\n",
      "Epoch 17/20  Iteration 30265/35720 Training loss: 0.6988 0.2278 sec/batch\n",
      "Epoch 17/20  Iteration 30266/35720 Training loss: 0.6988 0.2061 sec/batch\n",
      "Epoch 17/20  Iteration 30267/35720 Training loss: 0.6988 0.2059 sec/batch\n",
      "Epoch 17/20  Iteration 30268/35720 Training loss: 0.6988 0.2140 sec/batch\n",
      "Epoch 17/20  Iteration 30269/35720 Training loss: 0.6988 0.2162 sec/batch\n",
      "Epoch 17/20  Iteration 30270/35720 Training loss: 0.6988 0.2193 sec/batch\n",
      "Epoch 17/20  Iteration 30271/35720 Training loss: 0.6988 0.2172 sec/batch\n",
      "Epoch 17/20  Iteration 30272/35720 Training loss: 0.6987 0.2097 sec/batch\n",
      "Epoch 17/20  Iteration 30273/35720 Training loss: 0.6987 0.2083 sec/batch\n",
      "Epoch 17/20  Iteration 30274/35720 Training loss: 0.6987 0.2145 sec/batch\n",
      "Epoch 17/20  Iteration 30275/35720 Training loss: 0.6987 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 30276/35720 Training loss: 0.6988 0.2274 sec/batch\n",
      "Epoch 17/20  Iteration 30277/35720 Training loss: 0.6988 0.2060 sec/batch\n",
      "Epoch 17/20  Iteration 30278/35720 Training loss: 0.6987 0.2064 sec/batch\n",
      "Epoch 17/20  Iteration 30279/35720 Training loss: 0.6987 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 30280/35720 Training loss: 0.6987 0.2246 sec/batch\n",
      "Epoch 17/20  Iteration 30281/35720 Training loss: 0.6987 0.2321 sec/batch\n",
      "Epoch 17/20  Iteration 30282/35720 Training loss: 0.6987 0.2261 sec/batch\n",
      "Epoch 17/20  Iteration 30283/35720 Training loss: 0.6986 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 30284/35720 Training loss: 0.6986 0.2128 sec/batch\n",
      "Epoch 17/20  Iteration 30285/35720 Training loss: 0.6987 0.2216 sec/batch\n",
      "Epoch 17/20  Iteration 30286/35720 Training loss: 0.6987 0.2102 sec/batch\n",
      "Epoch 17/20  Iteration 30287/35720 Training loss: 0.6988 0.2259 sec/batch\n",
      "Epoch 17/20  Iteration 30288/35720 Training loss: 0.6988 0.2063 sec/batch\n",
      "Epoch 17/20  Iteration 30289/35720 Training loss: 0.6988 0.2111 sec/batch\n",
      "Epoch 17/20  Iteration 30290/35720 Training loss: 0.6988 0.2278 sec/batch\n",
      "Epoch 17/20  Iteration 30291/35720 Training loss: 0.6988 0.2275 sec/batch\n",
      "Epoch 17/20  Iteration 30292/35720 Training loss: 0.6988 0.2265 sec/batch\n",
      "Epoch 17/20  Iteration 30293/35720 Training loss: 0.6988 0.2069 sec/batch\n",
      "Epoch 17/20  Iteration 30294/35720 Training loss: 0.6987 0.2181 sec/batch\n",
      "Epoch 17/20  Iteration 30295/35720 Training loss: 0.6987 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 30296/35720 Training loss: 0.6987 0.2225 sec/batch\n",
      "Epoch 17/20  Iteration 30297/35720 Training loss: 0.6987 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 30298/35720 Training loss: 0.6987 0.2250 sec/batch\n",
      "Epoch 17/20  Iteration 30299/35720 Training loss: 0.6987 0.2186 sec/batch\n",
      "Epoch 17/20  Iteration 30300/35720 Training loss: 0.6987 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 30301/35720 Training loss: 0.6987 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 30302/35720 Training loss: 0.6987 0.2160 sec/batch\n",
      "Epoch 17/20  Iteration 30303/35720 Training loss: 0.6987 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 30304/35720 Training loss: 0.6987 0.2173 sec/batch\n",
      "Epoch 17/20  Iteration 30305/35720 Training loss: 0.6987 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 30306/35720 Training loss: 0.6987 0.2100 sec/batch\n",
      "Epoch 17/20  Iteration 30307/35720 Training loss: 0.6987 0.2093 sec/batch\n",
      "Epoch 17/20  Iteration 30308/35720 Training loss: 0.6987 0.2299 sec/batch\n",
      "Epoch 17/20  Iteration 30309/35720 Training loss: 0.6987 0.2167 sec/batch\n",
      "Epoch 17/20  Iteration 30310/35720 Training loss: 0.6987 0.2164 sec/batch\n",
      "Epoch 17/20  Iteration 30311/35720 Training loss: 0.6987 0.2193 sec/batch\n",
      "Epoch 17/20  Iteration 30312/35720 Training loss: 0.6987 0.2087 sec/batch\n",
      "Epoch 17/20  Iteration 30313/35720 Training loss: 0.6988 0.2345 sec/batch\n",
      "Epoch 17/20  Iteration 30314/35720 Training loss: 0.6987 0.2140 sec/batch\n",
      "Epoch 17/20  Iteration 30315/35720 Training loss: 0.6988 0.2163 sec/batch\n",
      "Epoch 17/20  Iteration 30316/35720 Training loss: 0.6987 0.2066 sec/batch\n",
      "Epoch 17/20  Iteration 30317/35720 Training loss: 0.6987 0.2109 sec/batch\n",
      "Epoch 17/20  Iteration 30318/35720 Training loss: 0.6987 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30319/35720 Training loss: 0.6987 0.2200 sec/batch\n",
      "Epoch 17/20  Iteration 30320/35720 Training loss: 0.6987 0.2110 sec/batch\n",
      "Epoch 17/20  Iteration 30321/35720 Training loss: 0.6988 0.2122 sec/batch\n",
      "Epoch 17/20  Iteration 30322/35720 Training loss: 0.6988 0.2094 sec/batch\n",
      "Epoch 17/20  Iteration 30323/35720 Training loss: 0.6988 0.2082 sec/batch\n",
      "Epoch 17/20  Iteration 30324/35720 Training loss: 0.6988 0.2161 sec/batch\n",
      "Epoch 17/20  Iteration 30325/35720 Training loss: 0.6988 0.2108 sec/batch\n",
      "Epoch 17/20  Iteration 30326/35720 Training loss: 0.6988 0.2269 sec/batch\n",
      "Epoch 17/20  Iteration 30327/35720 Training loss: 0.6988 0.2234 sec/batch\n",
      "Epoch 17/20  Iteration 30328/35720 Training loss: 0.6988 0.2065 sec/batch\n",
      "Epoch 17/20  Iteration 30329/35720 Training loss: 0.6988 0.2088 sec/batch\n",
      "Epoch 17/20  Iteration 30330/35720 Training loss: 0.6988 0.2125 sec/batch\n",
      "Epoch 17/20  Iteration 30331/35720 Training loss: 0.6988 0.2153 sec/batch\n",
      "Epoch 17/20  Iteration 30332/35720 Training loss: 0.6988 0.2157 sec/batch\n",
      "Epoch 17/20  Iteration 30333/35720 Training loss: 0.6988 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 30334/35720 Training loss: 0.6988 0.2179 sec/batch\n",
      "Epoch 17/20  Iteration 30335/35720 Training loss: 0.6988 0.2192 sec/batch\n",
      "Epoch 17/20  Iteration 30336/35720 Training loss: 0.6988 0.2138 sec/batch\n",
      "Epoch 17/20  Iteration 30337/35720 Training loss: 0.6988 0.2055 sec/batch\n",
      "Epoch 17/20  Iteration 30338/35720 Training loss: 0.6988 0.2298 sec/batch\n",
      "Epoch 17/20  Iteration 30339/35720 Training loss: 0.6987 0.2101 sec/batch\n",
      "Epoch 17/20  Iteration 30340/35720 Training loss: 0.6987 0.2133 sec/batch\n",
      "Epoch 17/20  Iteration 30341/35720 Training loss: 0.6987 0.2224 sec/batch\n",
      "Epoch 17/20  Iteration 30342/35720 Training loss: 0.6987 0.2144 sec/batch\n",
      "Epoch 17/20  Iteration 30343/35720 Training loss: 0.6987 0.2274 sec/batch\n",
      "Epoch 17/20  Iteration 30344/35720 Training loss: 0.6987 0.2112 sec/batch\n",
      "Epoch 17/20  Iteration 30345/35720 Training loss: 0.6987 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 30346/35720 Training loss: 0.6987 0.2154 sec/batch\n",
      "Epoch 17/20  Iteration 30347/35720 Training loss: 0.6987 0.2122 sec/batch\n",
      "Epoch 17/20  Iteration 30348/35720 Training loss: 0.6987 0.2205 sec/batch\n",
      "Epoch 17/20  Iteration 30349/35720 Training loss: 0.6987 0.2255 sec/batch\n",
      "Epoch 17/20  Iteration 30350/35720 Training loss: 0.6987 0.2062 sec/batch\n",
      "Epoch 17/20  Iteration 30351/35720 Training loss: 0.6987 0.2089 sec/batch\n",
      "Epoch 17/20  Iteration 30352/35720 Training loss: 0.6986 0.2169 sec/batch\n",
      "Epoch 17/20  Iteration 30353/35720 Training loss: 0.6986 0.2091 sec/batch\n",
      "Epoch 17/20  Iteration 30354/35720 Training loss: 0.6986 0.2150 sec/batch\n",
      "Epoch 17/20  Iteration 30355/35720 Training loss: 0.6986 0.2057 sec/batch\n",
      "Epoch 17/20  Iteration 30356/35720 Training loss: 0.6986 0.2152 sec/batch\n",
      "Epoch 17/20  Iteration 30357/35720 Training loss: 0.6986 0.2248 sec/batch\n",
      "Epoch 17/20  Iteration 30358/35720 Training loss: 0.6986 0.2298 sec/batch\n",
      "Epoch 17/20  Iteration 30359/35720 Training loss: 0.6986 0.2134 sec/batch\n",
      "Epoch 17/20  Iteration 30360/35720 Training loss: 0.6985 0.2230 sec/batch\n",
      "Epoch 17/20  Iteration 30361/35720 Training loss: 0.6985 0.2067 sec/batch\n",
      "Epoch 17/20  Iteration 30362/35720 Training loss: 0.6985 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 30363/35720 Training loss: 0.7373 0.2143 sec/batch\n",
      "Epoch 18/20  Iteration 30364/35720 Training loss: 0.7352 0.2119 sec/batch\n",
      "Epoch 18/20  Iteration 30365/35720 Training loss: 0.7280 0.2338 sec/batch\n",
      "Epoch 18/20  Iteration 30366/35720 Training loss: 0.7175 0.2074 sec/batch\n",
      "Epoch 18/20  Iteration 30367/35720 Training loss: 0.7201 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 30368/35720 Training loss: 0.7075 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 30369/35720 Training loss: 0.7072 0.2121 sec/batch\n",
      "Epoch 18/20  Iteration 30370/35720 Training loss: 0.7020 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 30371/35720 Training loss: 0.6948 0.2125 sec/batch\n",
      "Epoch 18/20  Iteration 30372/35720 Training loss: 0.6960 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 30373/35720 Training loss: 0.6966 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30374/35720 Training loss: 0.6921 0.2280 sec/batch\n",
      "Epoch 18/20  Iteration 30375/35720 Training loss: 0.6923 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 30376/35720 Training loss: 0.6961 0.2134 sec/batch\n",
      "Epoch 18/20  Iteration 30377/35720 Training loss: 0.6958 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 30378/35720 Training loss: 0.6962 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 30379/35720 Training loss: 0.6971 0.2102 sec/batch\n",
      "Epoch 18/20  Iteration 30380/35720 Training loss: 0.6951 0.2323 sec/batch\n",
      "Epoch 18/20  Iteration 30381/35720 Training loss: 0.6915 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 30382/35720 Training loss: 0.6922 0.2237 sec/batch\n",
      "Epoch 18/20  Iteration 30383/35720 Training loss: 0.6928 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 30384/35720 Training loss: 0.6905 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 30385/35720 Training loss: 0.6910 0.2228 sec/batch\n",
      "Epoch 18/20  Iteration 30386/35720 Training loss: 0.6931 0.2179 sec/batch\n",
      "Epoch 18/20  Iteration 30387/35720 Training loss: 0.6955 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 30388/35720 Training loss: 0.6944 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 30389/35720 Training loss: 0.6941 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 30390/35720 Training loss: 0.6945 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30391/35720 Training loss: 0.6936 0.2224 sec/batch\n",
      "Epoch 18/20  Iteration 30392/35720 Training loss: 0.6947 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 30393/35720 Training loss: 0.6950 0.2123 sec/batch\n",
      "Epoch 18/20  Iteration 30394/35720 Training loss: 0.6945 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 30395/35720 Training loss: 0.6960 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 30396/35720 Training loss: 0.6966 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 30397/35720 Training loss: 0.6987 0.2143 sec/batch\n",
      "Epoch 18/20  Iteration 30398/35720 Training loss: 0.6993 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 30399/35720 Training loss: 0.6995 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 30400/35720 Training loss: 0.6990 0.2159 sec/batch\n",
      "Validation loss: 1.60938 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 30401/35720 Training loss: 0.7103 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 30402/35720 Training loss: 0.7110 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 30403/35720 Training loss: 0.7100 0.2237 sec/batch\n",
      "Epoch 18/20  Iteration 30404/35720 Training loss: 0.7090 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 30405/35720 Training loss: 0.7072 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 30406/35720 Training loss: 0.7059 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 30407/35720 Training loss: 0.7061 0.2167 sec/batch\n",
      "Epoch 18/20  Iteration 30408/35720 Training loss: 0.7053 0.2214 sec/batch\n",
      "Epoch 18/20  Iteration 30409/35720 Training loss: 0.7038 0.2056 sec/batch\n",
      "Epoch 18/20  Iteration 30410/35720 Training loss: 0.7028 0.2114 sec/batch\n",
      "Epoch 18/20  Iteration 30411/35720 Training loss: 0.7014 0.2354 sec/batch\n",
      "Epoch 18/20  Iteration 30412/35720 Training loss: 0.7003 0.2333 sec/batch\n",
      "Epoch 18/20  Iteration 30413/35720 Training loss: 0.7004 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 30414/35720 Training loss: 0.7012 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 30415/35720 Training loss: 0.7014 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 30416/35720 Training loss: 0.7000 0.2121 sec/batch\n",
      "Epoch 18/20  Iteration 30417/35720 Training loss: 0.6984 0.2102 sec/batch\n",
      "Epoch 18/20  Iteration 30418/35720 Training loss: 0.6979 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 30419/35720 Training loss: 0.6979 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 30420/35720 Training loss: 0.6967 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 30421/35720 Training loss: 0.6961 0.2057 sec/batch\n",
      "Epoch 18/20  Iteration 30422/35720 Training loss: 0.6952 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 30423/35720 Training loss: 0.6951 0.2152 sec/batch\n",
      "Epoch 18/20  Iteration 30424/35720 Training loss: 0.6938 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 30425/35720 Training loss: 0.6941 0.2279 sec/batch\n",
      "Epoch 18/20  Iteration 30426/35720 Training loss: 0.6942 0.2152 sec/batch\n",
      "Epoch 18/20  Iteration 30427/35720 Training loss: 0.6949 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 30428/35720 Training loss: 0.6947 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 30429/35720 Training loss: 0.6940 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30430/35720 Training loss: 0.6935 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30431/35720 Training loss: 0.6943 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 30432/35720 Training loss: 0.6944 0.2056 sec/batch\n",
      "Epoch 18/20  Iteration 30433/35720 Training loss: 0.6948 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 30434/35720 Training loss: 0.6950 0.2229 sec/batch\n",
      "Epoch 18/20  Iteration 30435/35720 Training loss: 0.6950 0.2117 sec/batch\n",
      "Epoch 18/20  Iteration 30436/35720 Training loss: 0.6948 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 30437/35720 Training loss: 0.6943 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30438/35720 Training loss: 0.6943 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 30439/35720 Training loss: 0.6939 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 30440/35720 Training loss: 0.6945 0.2293 sec/batch\n",
      "Epoch 18/20  Iteration 30441/35720 Training loss: 0.6946 0.2124 sec/batch\n",
      "Epoch 18/20  Iteration 30442/35720 Training loss: 0.6955 0.2267 sec/batch\n",
      "Epoch 18/20  Iteration 30443/35720 Training loss: 0.6957 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 30444/35720 Training loss: 0.6955 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 30445/35720 Training loss: 0.6956 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 30446/35720 Training loss: 0.6955 0.2202 sec/batch\n",
      "Epoch 18/20  Iteration 30447/35720 Training loss: 0.6955 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30448/35720 Training loss: 0.6956 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 30449/35720 Training loss: 0.6953 0.2249 sec/batch\n",
      "Epoch 18/20  Iteration 30450/35720 Training loss: 0.6951 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 30451/35720 Training loss: 0.6942 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 30452/35720 Training loss: 0.6940 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 30453/35720 Training loss: 0.6942 0.2169 sec/batch\n",
      "Epoch 18/20  Iteration 30454/35720 Training loss: 0.6938 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 30455/35720 Training loss: 0.6945 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 30456/35720 Training loss: 0.6950 0.2231 sec/batch\n",
      "Epoch 18/20  Iteration 30457/35720 Training loss: 0.6950 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 30458/35720 Training loss: 0.6947 0.2178 sec/batch\n",
      "Epoch 18/20  Iteration 30459/35720 Training loss: 0.6948 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 30460/35720 Training loss: 0.6949 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 30461/35720 Training loss: 0.6949 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 30462/35720 Training loss: 0.6944 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 30463/35720 Training loss: 0.6941 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30464/35720 Training loss: 0.6941 0.2117 sec/batch\n",
      "Epoch 18/20  Iteration 30465/35720 Training loss: 0.6936 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 30466/35720 Training loss: 0.6933 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 30467/35720 Training loss: 0.6931 0.2118 sec/batch\n",
      "Epoch 18/20  Iteration 30468/35720 Training loss: 0.6928 0.2166 sec/batch\n",
      "Epoch 18/20  Iteration 30469/35720 Training loss: 0.6930 0.2180 sec/batch\n",
      "Epoch 18/20  Iteration 30470/35720 Training loss: 0.6931 0.2167 sec/batch\n",
      "Epoch 18/20  Iteration 30471/35720 Training loss: 0.6934 0.2151 sec/batch\n",
      "Epoch 18/20  Iteration 30472/35720 Training loss: 0.6932 0.2218 sec/batch\n",
      "Epoch 18/20  Iteration 30473/35720 Training loss: 0.6933 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 30474/35720 Training loss: 0.6935 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30475/35720 Training loss: 0.6935 0.2303 sec/batch\n",
      "Epoch 18/20  Iteration 30476/35720 Training loss: 0.6937 0.2195 sec/batch\n",
      "Epoch 18/20  Iteration 30477/35720 Training loss: 0.6938 0.2151 sec/batch\n",
      "Epoch 18/20  Iteration 30478/35720 Training loss: 0.6938 0.2844 sec/batch\n",
      "Epoch 18/20  Iteration 30479/35720 Training loss: 0.6936 0.2254 sec/batch\n",
      "Epoch 18/20  Iteration 30480/35720 Training loss: 0.6936 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 30481/35720 Training loss: 0.6934 0.2212 sec/batch\n",
      "Epoch 18/20  Iteration 30482/35720 Training loss: 0.6942 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 30483/35720 Training loss: 0.6943 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 30484/35720 Training loss: 0.6937 0.2296 sec/batch\n",
      "Epoch 18/20  Iteration 30485/35720 Training loss: 0.6936 0.2162 sec/batch\n",
      "Epoch 18/20  Iteration 30486/35720 Training loss: 0.6938 0.2330 sec/batch\n",
      "Epoch 18/20  Iteration 30487/35720 Training loss: 0.6935 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 30488/35720 Training loss: 0.6935 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 30489/35720 Training loss: 0.6939 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 30490/35720 Training loss: 0.6938 0.2187 sec/batch\n",
      "Epoch 18/20  Iteration 30491/35720 Training loss: 0.6937 0.2144 sec/batch\n",
      "Epoch 18/20  Iteration 30492/35720 Training loss: 0.6940 0.2149 sec/batch\n",
      "Epoch 18/20  Iteration 30493/35720 Training loss: 0.6939 0.2114 sec/batch\n",
      "Epoch 18/20  Iteration 30494/35720 Training loss: 0.6937 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30495/35720 Training loss: 0.6938 0.2165 sec/batch\n",
      "Epoch 18/20  Iteration 30496/35720 Training loss: 0.6938 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 30497/35720 Training loss: 0.6935 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 30498/35720 Training loss: 0.6934 0.2264 sec/batch\n",
      "Epoch 18/20  Iteration 30499/35720 Training loss: 0.6937 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 30500/35720 Training loss: 0.6937 0.2159 sec/batch\n",
      "Epoch 18/20  Iteration 30501/35720 Training loss: 0.6938 0.2239 sec/batch\n",
      "Epoch 18/20  Iteration 30502/35720 Training loss: 0.6940 0.2159 sec/batch\n",
      "Epoch 18/20  Iteration 30503/35720 Training loss: 0.6938 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 30504/35720 Training loss: 0.6935 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 30505/35720 Training loss: 0.6930 0.2141 sec/batch\n",
      "Epoch 18/20  Iteration 30506/35720 Training loss: 0.6925 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30507/35720 Training loss: 0.6927 0.2166 sec/batch\n",
      "Epoch 18/20  Iteration 30508/35720 Training loss: 0.6927 0.2139 sec/batch\n",
      "Epoch 18/20  Iteration 30509/35720 Training loss: 0.6924 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 30510/35720 Training loss: 0.6923 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 30511/35720 Training loss: 0.6924 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 30512/35720 Training loss: 0.6919 0.2243 sec/batch\n",
      "Epoch 18/20  Iteration 30513/35720 Training loss: 0.6918 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 30514/35720 Training loss: 0.6918 0.2253 sec/batch\n",
      "Epoch 18/20  Iteration 30515/35720 Training loss: 0.6917 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 30516/35720 Training loss: 0.6919 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 30517/35720 Training loss: 0.6920 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 30518/35720 Training loss: 0.6923 0.2225 sec/batch\n",
      "Epoch 18/20  Iteration 30519/35720 Training loss: 0.6923 0.2232 sec/batch\n",
      "Epoch 18/20  Iteration 30520/35720 Training loss: 0.6925 0.2134 sec/batch\n",
      "Epoch 18/20  Iteration 30521/35720 Training loss: 0.6922 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 30522/35720 Training loss: 0.6921 0.2221 sec/batch\n",
      "Epoch 18/20  Iteration 30523/35720 Training loss: 0.6917 0.2271 sec/batch\n",
      "Epoch 18/20  Iteration 30524/35720 Training loss: 0.6918 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 30525/35720 Training loss: 0.6918 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 30526/35720 Training loss: 0.6919 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 30527/35720 Training loss: 0.6921 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 30528/35720 Training loss: 0.6920 0.2384 sec/batch\n",
      "Epoch 18/20  Iteration 30529/35720 Training loss: 0.6919 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 30530/35720 Training loss: 0.6921 0.2258 sec/batch\n",
      "Epoch 18/20  Iteration 30531/35720 Training loss: 0.6923 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 30532/35720 Training loss: 0.6926 0.2056 sec/batch\n",
      "Epoch 18/20  Iteration 30533/35720 Training loss: 0.6930 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 30534/35720 Training loss: 0.6934 0.2234 sec/batch\n",
      "Epoch 18/20  Iteration 30535/35720 Training loss: 0.6936 0.2165 sec/batch\n",
      "Epoch 18/20  Iteration 30536/35720 Training loss: 0.6940 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 30537/35720 Training loss: 0.6942 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 30538/35720 Training loss: 0.6943 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 30539/35720 Training loss: 0.6943 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30540/35720 Training loss: 0.6941 0.2285 sec/batch\n",
      "Epoch 18/20  Iteration 30541/35720 Training loss: 0.6940 0.2305 sec/batch\n",
      "Epoch 18/20  Iteration 30542/35720 Training loss: 0.6938 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 30543/35720 Training loss: 0.6938 0.2075 sec/batch\n",
      "Epoch 18/20  Iteration 30544/35720 Training loss: 0.6939 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 30545/35720 Training loss: 0.6939 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 30546/35720 Training loss: 0.6941 0.2233 sec/batch\n",
      "Epoch 18/20  Iteration 30547/35720 Training loss: 0.6940 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 30548/35720 Training loss: 0.6939 0.2121 sec/batch\n",
      "Epoch 18/20  Iteration 30549/35720 Training loss: 0.6937 0.2106 sec/batch\n",
      "Epoch 18/20  Iteration 30550/35720 Training loss: 0.6938 0.2139 sec/batch\n",
      "Epoch 18/20  Iteration 30551/35720 Training loss: 0.6939 0.2249 sec/batch\n",
      "Epoch 18/20  Iteration 30552/35720 Training loss: 0.6937 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 30553/35720 Training loss: 0.6938 0.2139 sec/batch\n",
      "Epoch 18/20  Iteration 30554/35720 Training loss: 0.6940 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 30555/35720 Training loss: 0.6941 0.2072 sec/batch\n",
      "Epoch 18/20  Iteration 30556/35720 Training loss: 0.6943 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 30557/35720 Training loss: 0.6943 0.2353 sec/batch\n",
      "Epoch 18/20  Iteration 30558/35720 Training loss: 0.6945 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 30559/35720 Training loss: 0.6944 0.2171 sec/batch\n",
      "Epoch 18/20  Iteration 30560/35720 Training loss: 0.6942 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 30561/35720 Training loss: 0.6943 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 30562/35720 Training loss: 0.6946 0.2386 sec/batch\n",
      "Epoch 18/20  Iteration 30563/35720 Training loss: 0.6946 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30564/35720 Training loss: 0.6945 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 30565/35720 Training loss: 0.6947 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 30566/35720 Training loss: 0.6946 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 30567/35720 Training loss: 0.6947 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30568/35720 Training loss: 0.6946 0.2200 sec/batch\n",
      "Epoch 18/20  Iteration 30569/35720 Training loss: 0.6948 0.2223 sec/batch\n",
      "Epoch 18/20  Iteration 30570/35720 Training loss: 0.6951 0.2210 sec/batch\n",
      "Epoch 18/20  Iteration 30571/35720 Training loss: 0.6954 0.2142 sec/batch\n",
      "Epoch 18/20  Iteration 30572/35720 Training loss: 0.6953 0.2202 sec/batch\n",
      "Epoch 18/20  Iteration 30573/35720 Training loss: 0.6954 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 30574/35720 Training loss: 0.6954 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 30575/35720 Training loss: 0.6954 0.2208 sec/batch\n",
      "Epoch 18/20  Iteration 30576/35720 Training loss: 0.6954 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 30577/35720 Training loss: 0.6954 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 30578/35720 Training loss: 0.6954 0.2114 sec/batch\n",
      "Epoch 18/20  Iteration 30579/35720 Training loss: 0.6953 0.2272 sec/batch\n",
      "Epoch 18/20  Iteration 30580/35720 Training loss: 0.6953 0.2263 sec/batch\n",
      "Epoch 18/20  Iteration 30581/35720 Training loss: 0.6953 0.2230 sec/batch\n",
      "Epoch 18/20  Iteration 30582/35720 Training loss: 0.6954 0.2054 sec/batch\n",
      "Epoch 18/20  Iteration 30583/35720 Training loss: 0.6954 0.2078 sec/batch\n",
      "Epoch 18/20  Iteration 30584/35720 Training loss: 0.6954 0.2149 sec/batch\n",
      "Epoch 18/20  Iteration 30585/35720 Training loss: 0.6956 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 30586/35720 Training loss: 0.6957 0.2290 sec/batch\n",
      "Epoch 18/20  Iteration 30587/35720 Training loss: 0.6960 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 30588/35720 Training loss: 0.6961 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 30589/35720 Training loss: 0.6959 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 30590/35720 Training loss: 0.6958 0.2313 sec/batch\n",
      "Epoch 18/20  Iteration 30591/35720 Training loss: 0.6956 0.2281 sec/batch\n",
      "Epoch 18/20  Iteration 30592/35720 Training loss: 0.6956 0.2247 sec/batch\n",
      "Epoch 18/20  Iteration 30593/35720 Training loss: 0.6959 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 30594/35720 Training loss: 0.6958 0.2159 sec/batch\n",
      "Epoch 18/20  Iteration 30595/35720 Training loss: 0.6959 0.2229 sec/batch\n",
      "Epoch 18/20  Iteration 30596/35720 Training loss: 0.6960 0.2146 sec/batch\n",
      "Epoch 18/20  Iteration 30597/35720 Training loss: 0.6959 0.2258 sec/batch\n",
      "Epoch 18/20  Iteration 30598/35720 Training loss: 0.6960 0.2124 sec/batch\n",
      "Epoch 18/20  Iteration 30599/35720 Training loss: 0.6962 0.2318 sec/batch\n",
      "Epoch 18/20  Iteration 30600/35720 Training loss: 0.6959 0.2096 sec/batch\n",
      "Validation loss: 1.62497 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 30601/35720 Training loss: 0.6975 0.2080 sec/batch\n",
      "Epoch 18/20  Iteration 30602/35720 Training loss: 0.6976 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 30603/35720 Training loss: 0.6974 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 30604/35720 Training loss: 0.6973 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 30605/35720 Training loss: 0.6974 0.2243 sec/batch\n",
      "Epoch 18/20  Iteration 30606/35720 Training loss: 0.6972 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 30607/35720 Training loss: 0.6969 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 30608/35720 Training loss: 0.6971 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 30609/35720 Training loss: 0.6969 0.2163 sec/batch\n",
      "Epoch 18/20  Iteration 30610/35720 Training loss: 0.6969 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 30611/35720 Training loss: 0.6968 0.2161 sec/batch\n",
      "Epoch 18/20  Iteration 30612/35720 Training loss: 0.6966 0.2170 sec/batch\n",
      "Epoch 18/20  Iteration 30613/35720 Training loss: 0.6966 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 30614/35720 Training loss: 0.6966 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 30615/35720 Training loss: 0.6963 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 30616/35720 Training loss: 0.6963 0.2216 sec/batch\n",
      "Epoch 18/20  Iteration 30617/35720 Training loss: 0.6965 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 30618/35720 Training loss: 0.6966 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30619/35720 Training loss: 0.6966 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 30620/35720 Training loss: 0.6965 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 30621/35720 Training loss: 0.6967 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 30622/35720 Training loss: 0.6967 0.2179 sec/batch\n",
      "Epoch 18/20  Iteration 30623/35720 Training loss: 0.6966 0.2259 sec/batch\n",
      "Epoch 18/20  Iteration 30624/35720 Training loss: 0.6965 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 30625/35720 Training loss: 0.6964 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 30626/35720 Training loss: 0.6964 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 30627/35720 Training loss: 0.6963 0.2293 sec/batch\n",
      "Epoch 18/20  Iteration 30628/35720 Training loss: 0.6966 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 30629/35720 Training loss: 0.6965 0.2367 sec/batch\n",
      "Epoch 18/20  Iteration 30630/35720 Training loss: 0.6965 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 30631/35720 Training loss: 0.6964 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 30632/35720 Training loss: 0.6960 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 30633/35720 Training loss: 0.6958 0.2227 sec/batch\n",
      "Epoch 18/20  Iteration 30634/35720 Training loss: 0.6957 0.2164 sec/batch\n",
      "Epoch 18/20  Iteration 30635/35720 Training loss: 0.6957 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 30636/35720 Training loss: 0.6957 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 30637/35720 Training loss: 0.6957 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 30638/35720 Training loss: 0.6957 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 30639/35720 Training loss: 0.6954 0.2290 sec/batch\n",
      "Epoch 18/20  Iteration 30640/35720 Training loss: 0.6952 0.2331 sec/batch\n",
      "Epoch 18/20  Iteration 30641/35720 Training loss: 0.6950 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 30642/35720 Training loss: 0.6950 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 30643/35720 Training loss: 0.6950 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30644/35720 Training loss: 0.6948 0.2194 sec/batch\n",
      "Epoch 18/20  Iteration 30645/35720 Training loss: 0.6946 0.2123 sec/batch\n",
      "Epoch 18/20  Iteration 30646/35720 Training loss: 0.6945 0.2161 sec/batch\n",
      "Epoch 18/20  Iteration 30647/35720 Training loss: 0.6946 0.2183 sec/batch\n",
      "Epoch 18/20  Iteration 30648/35720 Training loss: 0.6945 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 30649/35720 Training loss: 0.6944 0.2218 sec/batch\n",
      "Epoch 18/20  Iteration 30650/35720 Training loss: 0.6944 0.2117 sec/batch\n",
      "Epoch 18/20  Iteration 30651/35720 Training loss: 0.6945 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 30652/35720 Training loss: 0.6945 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 30653/35720 Training loss: 0.6944 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 30654/35720 Training loss: 0.6944 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 30655/35720 Training loss: 0.6943 0.2137 sec/batch\n",
      "Epoch 18/20  Iteration 30656/35720 Training loss: 0.6943 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 30657/35720 Training loss: 0.6943 0.2151 sec/batch\n",
      "Epoch 18/20  Iteration 30658/35720 Training loss: 0.6943 0.2075 sec/batch\n",
      "Epoch 18/20  Iteration 30659/35720 Training loss: 0.6941 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 30660/35720 Training loss: 0.6942 0.2213 sec/batch\n",
      "Epoch 18/20  Iteration 30661/35720 Training loss: 0.6940 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 30662/35720 Training loss: 0.6941 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 30663/35720 Training loss: 0.6941 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 30664/35720 Training loss: 0.6939 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 30665/35720 Training loss: 0.6940 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30666/35720 Training loss: 0.6940 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 30667/35720 Training loss: 0.6939 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 30668/35720 Training loss: 0.6937 0.2214 sec/batch\n",
      "Epoch 18/20  Iteration 30669/35720 Training loss: 0.6936 0.2171 sec/batch\n",
      "Epoch 18/20  Iteration 30670/35720 Training loss: 0.6937 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 30671/35720 Training loss: 0.6934 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30672/35720 Training loss: 0.6934 0.2149 sec/batch\n",
      "Epoch 18/20  Iteration 30673/35720 Training loss: 0.6934 0.2102 sec/batch\n",
      "Epoch 18/20  Iteration 30674/35720 Training loss: 0.6932 0.2350 sec/batch\n",
      "Epoch 18/20  Iteration 30675/35720 Training loss: 0.6932 0.2349 sec/batch\n",
      "Epoch 18/20  Iteration 30676/35720 Training loss: 0.6931 0.2320 sec/batch\n",
      "Epoch 18/20  Iteration 30677/35720 Training loss: 0.6930 0.2123 sec/batch\n",
      "Epoch 18/20  Iteration 30678/35720 Training loss: 0.6931 0.2151 sec/batch\n",
      "Epoch 18/20  Iteration 30679/35720 Training loss: 0.6929 0.2218 sec/batch\n",
      "Epoch 18/20  Iteration 30680/35720 Training loss: 0.6930 0.2170 sec/batch\n",
      "Epoch 18/20  Iteration 30681/35720 Training loss: 0.6932 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 30682/35720 Training loss: 0.6931 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30683/35720 Training loss: 0.6932 0.2053 sec/batch\n",
      "Epoch 18/20  Iteration 30684/35720 Training loss: 0.6932 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 30685/35720 Training loss: 0.6933 0.2331 sec/batch\n",
      "Epoch 18/20  Iteration 30686/35720 Training loss: 0.6934 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 30687/35720 Training loss: 0.6933 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 30688/35720 Training loss: 0.6934 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 30689/35720 Training loss: 0.6935 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 30690/35720 Training loss: 0.6934 0.2250 sec/batch\n",
      "Epoch 18/20  Iteration 30691/35720 Training loss: 0.6934 0.2137 sec/batch\n",
      "Epoch 18/20  Iteration 30692/35720 Training loss: 0.6934 0.2288 sec/batch\n",
      "Epoch 18/20  Iteration 30693/35720 Training loss: 0.6934 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30694/35720 Training loss: 0.6935 0.2238 sec/batch\n",
      "Epoch 18/20  Iteration 30695/35720 Training loss: 0.6934 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 30696/35720 Training loss: 0.6933 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 30697/35720 Training loss: 0.6933 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 30698/35720 Training loss: 0.6930 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 30699/35720 Training loss: 0.6931 0.2186 sec/batch\n",
      "Epoch 18/20  Iteration 30700/35720 Training loss: 0.6930 0.2185 sec/batch\n",
      "Epoch 18/20  Iteration 30701/35720 Training loss: 0.6929 0.2144 sec/batch\n",
      "Epoch 18/20  Iteration 30702/35720 Training loss: 0.6929 0.2234 sec/batch\n",
      "Epoch 18/20  Iteration 30703/35720 Training loss: 0.6927 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30704/35720 Training loss: 0.6926 0.2132 sec/batch\n",
      "Epoch 18/20  Iteration 30705/35720 Training loss: 0.6927 0.2345 sec/batch\n",
      "Epoch 18/20  Iteration 30706/35720 Training loss: 0.6928 0.2080 sec/batch\n",
      "Epoch 18/20  Iteration 30707/35720 Training loss: 0.6925 0.2362 sec/batch\n",
      "Epoch 18/20  Iteration 30708/35720 Training loss: 0.6926 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 30709/35720 Training loss: 0.6927 0.2055 sec/batch\n",
      "Epoch 18/20  Iteration 30710/35720 Training loss: 0.6928 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 30711/35720 Training loss: 0.6927 0.2193 sec/batch\n",
      "Epoch 18/20  Iteration 30712/35720 Training loss: 0.6927 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30713/35720 Training loss: 0.6927 0.2223 sec/batch\n",
      "Epoch 18/20  Iteration 30714/35720 Training loss: 0.6925 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30715/35720 Training loss: 0.6926 0.2116 sec/batch\n",
      "Epoch 18/20  Iteration 30716/35720 Training loss: 0.6926 0.2186 sec/batch\n",
      "Epoch 18/20  Iteration 30717/35720 Training loss: 0.6927 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 30718/35720 Training loss: 0.6927 0.2260 sec/batch\n",
      "Epoch 18/20  Iteration 30719/35720 Training loss: 0.6929 0.2055 sec/batch\n",
      "Epoch 18/20  Iteration 30720/35720 Training loss: 0.6929 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 30721/35720 Training loss: 0.6929 0.2186 sec/batch\n",
      "Epoch 18/20  Iteration 30722/35720 Training loss: 0.6929 0.2227 sec/batch\n",
      "Epoch 18/20  Iteration 30723/35720 Training loss: 0.6928 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 30724/35720 Training loss: 0.6929 0.2162 sec/batch\n",
      "Epoch 18/20  Iteration 30725/35720 Training loss: 0.6929 0.2056 sec/batch\n",
      "Epoch 18/20  Iteration 30726/35720 Training loss: 0.6929 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 30727/35720 Training loss: 0.6929 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 30728/35720 Training loss: 0.6929 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 30729/35720 Training loss: 0.6929 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 30730/35720 Training loss: 0.6928 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 30731/35720 Training loss: 0.6928 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 30732/35720 Training loss: 0.6927 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 30733/35720 Training loss: 0.6927 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30734/35720 Training loss: 0.6927 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 30735/35720 Training loss: 0.6926 0.2130 sec/batch\n",
      "Epoch 18/20  Iteration 30736/35720 Training loss: 0.6926 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 30737/35720 Training loss: 0.6926 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 30738/35720 Training loss: 0.6926 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30739/35720 Training loss: 0.6927 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 30740/35720 Training loss: 0.6927 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 30741/35720 Training loss: 0.6926 0.2230 sec/batch\n",
      "Epoch 18/20  Iteration 30742/35720 Training loss: 0.6925 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 30743/35720 Training loss: 0.6924 0.2185 sec/batch\n",
      "Epoch 18/20  Iteration 30744/35720 Training loss: 0.6923 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 30745/35720 Training loss: 0.6923 0.2292 sec/batch\n",
      "Epoch 18/20  Iteration 30746/35720 Training loss: 0.6924 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 30747/35720 Training loss: 0.6924 0.2206 sec/batch\n",
      "Epoch 18/20  Iteration 30748/35720 Training loss: 0.6924 0.2116 sec/batch\n",
      "Epoch 18/20  Iteration 30749/35720 Training loss: 0.6924 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 30750/35720 Training loss: 0.6924 0.2187 sec/batch\n",
      "Epoch 18/20  Iteration 30751/35720 Training loss: 0.6924 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 30752/35720 Training loss: 0.6924 0.2166 sec/batch\n",
      "Epoch 18/20  Iteration 30753/35720 Training loss: 0.6923 0.2153 sec/batch\n",
      "Epoch 18/20  Iteration 30754/35720 Training loss: 0.6922 0.2132 sec/batch\n",
      "Epoch 18/20  Iteration 30755/35720 Training loss: 0.6922 0.2270 sec/batch\n",
      "Epoch 18/20  Iteration 30756/35720 Training loss: 0.6921 0.2332 sec/batch\n",
      "Epoch 18/20  Iteration 30757/35720 Training loss: 0.6921 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 30758/35720 Training loss: 0.6921 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 30759/35720 Training loss: 0.6921 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 30760/35720 Training loss: 0.6920 0.2151 sec/batch\n",
      "Epoch 18/20  Iteration 30761/35720 Training loss: 0.6919 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 30762/35720 Training loss: 0.6918 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 30763/35720 Training loss: 0.6918 0.2152 sec/batch\n",
      "Epoch 18/20  Iteration 30764/35720 Training loss: 0.6919 0.2231 sec/batch\n",
      "Epoch 18/20  Iteration 30765/35720 Training loss: 0.6920 0.2165 sec/batch\n",
      "Epoch 18/20  Iteration 30766/35720 Training loss: 0.6920 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 30767/35720 Training loss: 0.6919 0.2364 sec/batch\n",
      "Epoch 18/20  Iteration 30768/35720 Training loss: 0.6918 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 30769/35720 Training loss: 0.6917 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 30770/35720 Training loss: 0.6917 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 30771/35720 Training loss: 0.6916 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 30772/35720 Training loss: 0.6915 0.2178 sec/batch\n",
      "Epoch 18/20  Iteration 30773/35720 Training loss: 0.6916 0.2197 sec/batch\n",
      "Epoch 18/20  Iteration 30774/35720 Training loss: 0.6915 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 30775/35720 Training loss: 0.6913 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 30776/35720 Training loss: 0.6914 0.2206 sec/batch\n",
      "Epoch 18/20  Iteration 30777/35720 Training loss: 0.6913 0.2119 sec/batch\n",
      "Epoch 18/20  Iteration 30778/35720 Training loss: 0.6913 0.2143 sec/batch\n",
      "Epoch 18/20  Iteration 30779/35720 Training loss: 0.6912 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 30780/35720 Training loss: 0.6911 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30781/35720 Training loss: 0.6911 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 30782/35720 Training loss: 0.6911 0.2162 sec/batch\n",
      "Epoch 18/20  Iteration 30783/35720 Training loss: 0.6910 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30784/35720 Training loss: 0.6910 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 30785/35720 Training loss: 0.6911 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 30786/35720 Training loss: 0.6912 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 30787/35720 Training loss: 0.6912 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 30788/35720 Training loss: 0.6911 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30789/35720 Training loss: 0.6910 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 30790/35720 Training loss: 0.6910 0.2143 sec/batch\n",
      "Epoch 18/20  Iteration 30791/35720 Training loss: 0.6910 0.2263 sec/batch\n",
      "Epoch 18/20  Iteration 30792/35720 Training loss: 0.6910 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 30793/35720 Training loss: 0.6911 0.2100 sec/batch\n",
      "Epoch 18/20  Iteration 30794/35720 Training loss: 0.6911 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 30795/35720 Training loss: 0.6911 0.2144 sec/batch\n",
      "Epoch 18/20  Iteration 30796/35720 Training loss: 0.6912 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30797/35720 Training loss: 0.6913 0.2162 sec/batch\n",
      "Epoch 18/20  Iteration 30798/35720 Training loss: 0.6913 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 30799/35720 Training loss: 0.6914 0.2114 sec/batch\n",
      "Epoch 18/20  Iteration 30800/35720 Training loss: 0.6915 0.2082 sec/batch\n",
      "Validation loss: 1.62795 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 30801/35720 Training loss: 0.6929 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 30802/35720 Training loss: 0.6931 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 30803/35720 Training loss: 0.6931 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 30804/35720 Training loss: 0.6932 0.2058 sec/batch\n",
      "Epoch 18/20  Iteration 30805/35720 Training loss: 0.6932 0.2272 sec/batch\n",
      "Epoch 18/20  Iteration 30806/35720 Training loss: 0.6932 0.2125 sec/batch\n",
      "Epoch 18/20  Iteration 30807/35720 Training loss: 0.6933 0.2296 sec/batch\n",
      "Epoch 18/20  Iteration 30808/35720 Training loss: 0.6934 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30809/35720 Training loss: 0.6935 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 30810/35720 Training loss: 0.6937 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 30811/35720 Training loss: 0.6938 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 30812/35720 Training loss: 0.6938 0.2246 sec/batch\n",
      "Epoch 18/20  Iteration 30813/35720 Training loss: 0.6937 0.2279 sec/batch\n",
      "Epoch 18/20  Iteration 30814/35720 Training loss: 0.6936 0.2700 sec/batch\n",
      "Epoch 18/20  Iteration 30815/35720 Training loss: 0.6936 0.2664 sec/batch\n",
      "Epoch 18/20  Iteration 30816/35720 Training loss: 0.6937 0.2398 sec/batch\n",
      "Epoch 18/20  Iteration 30817/35720 Training loss: 0.6939 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 30818/35720 Training loss: 0.6940 0.2257 sec/batch\n",
      "Epoch 18/20  Iteration 30819/35720 Training loss: 0.6941 0.2142 sec/batch\n",
      "Epoch 18/20  Iteration 30820/35720 Training loss: 0.6941 0.2205 sec/batch\n",
      "Epoch 18/20  Iteration 30821/35720 Training loss: 0.6941 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 30822/35720 Training loss: 0.6940 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 30823/35720 Training loss: 0.6940 0.2168 sec/batch\n",
      "Epoch 18/20  Iteration 30824/35720 Training loss: 0.6942 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 30825/35720 Training loss: 0.6943 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 30826/35720 Training loss: 0.6943 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 30827/35720 Training loss: 0.6944 0.2170 sec/batch\n",
      "Epoch 18/20  Iteration 30828/35720 Training loss: 0.6944 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 30829/35720 Training loss: 0.6944 0.2179 sec/batch\n",
      "Epoch 18/20  Iteration 30830/35720 Training loss: 0.6944 0.2183 sec/batch\n",
      "Epoch 18/20  Iteration 30831/35720 Training loss: 0.6944 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 30832/35720 Training loss: 0.6944 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30833/35720 Training loss: 0.6944 0.2221 sec/batch\n",
      "Epoch 18/20  Iteration 30834/35720 Training loss: 0.6943 0.2335 sec/batch\n",
      "Epoch 18/20  Iteration 30835/35720 Training loss: 0.6944 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 30836/35720 Training loss: 0.6943 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 30837/35720 Training loss: 0.6944 0.2200 sec/batch\n",
      "Epoch 18/20  Iteration 30838/35720 Training loss: 0.6943 0.2173 sec/batch\n",
      "Epoch 18/20  Iteration 30839/35720 Training loss: 0.6944 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 30840/35720 Training loss: 0.6943 0.2243 sec/batch\n",
      "Epoch 18/20  Iteration 30841/35720 Training loss: 0.6943 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 30842/35720 Training loss: 0.6942 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 30843/35720 Training loss: 0.6942 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 30844/35720 Training loss: 0.6941 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 30845/35720 Training loss: 0.6941 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30846/35720 Training loss: 0.6941 0.2141 sec/batch\n",
      "Epoch 18/20  Iteration 30847/35720 Training loss: 0.6942 0.2072 sec/batch\n",
      "Epoch 18/20  Iteration 30848/35720 Training loss: 0.6941 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 30849/35720 Training loss: 0.6941 0.2225 sec/batch\n",
      "Epoch 18/20  Iteration 30850/35720 Training loss: 0.6941 0.2195 sec/batch\n",
      "Epoch 18/20  Iteration 30851/35720 Training loss: 0.6940 0.2546 sec/batch\n",
      "Epoch 18/20  Iteration 30852/35720 Training loss: 0.6939 0.2282 sec/batch\n",
      "Epoch 18/20  Iteration 30853/35720 Training loss: 0.6939 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 30854/35720 Training loss: 0.6940 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 30855/35720 Training loss: 0.6940 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 30856/35720 Training loss: 0.6939 0.2243 sec/batch\n",
      "Epoch 18/20  Iteration 30857/35720 Training loss: 0.6939 0.2247 sec/batch\n",
      "Epoch 18/20  Iteration 30858/35720 Training loss: 0.6938 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 30859/35720 Training loss: 0.6938 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 30860/35720 Training loss: 0.6939 0.2189 sec/batch\n",
      "Epoch 18/20  Iteration 30861/35720 Training loss: 0.6938 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 30862/35720 Training loss: 0.6938 0.2238 sec/batch\n",
      "Epoch 18/20  Iteration 30863/35720 Training loss: 0.6937 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 30864/35720 Training loss: 0.6936 0.2196 sec/batch\n",
      "Epoch 18/20  Iteration 30865/35720 Training loss: 0.6935 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 30866/35720 Training loss: 0.6934 0.2244 sec/batch\n",
      "Epoch 18/20  Iteration 30867/35720 Training loss: 0.6934 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 30868/35720 Training loss: 0.6934 0.2209 sec/batch\n",
      "Epoch 18/20  Iteration 30869/35720 Training loss: 0.6934 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 30870/35720 Training loss: 0.6934 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 30871/35720 Training loss: 0.6935 0.2175 sec/batch\n",
      "Epoch 18/20  Iteration 30872/35720 Training loss: 0.6935 0.2238 sec/batch\n",
      "Epoch 18/20  Iteration 30873/35720 Training loss: 0.6934 0.2144 sec/batch\n",
      "Epoch 18/20  Iteration 30874/35720 Training loss: 0.6934 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 30875/35720 Training loss: 0.6934 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 30876/35720 Training loss: 0.6935 0.2143 sec/batch\n",
      "Epoch 18/20  Iteration 30877/35720 Training loss: 0.6935 0.2265 sec/batch\n",
      "Epoch 18/20  Iteration 30878/35720 Training loss: 0.6936 0.2119 sec/batch\n",
      "Epoch 18/20  Iteration 30879/35720 Training loss: 0.6936 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 30880/35720 Training loss: 0.6936 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 30881/35720 Training loss: 0.6936 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 30882/35720 Training loss: 0.6937 0.2173 sec/batch\n",
      "Epoch 18/20  Iteration 30883/35720 Training loss: 0.6936 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30884/35720 Training loss: 0.6936 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 30885/35720 Training loss: 0.6935 0.2146 sec/batch\n",
      "Epoch 18/20  Iteration 30886/35720 Training loss: 0.6934 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 30887/35720 Training loss: 0.6934 0.2073 sec/batch\n",
      "Epoch 18/20  Iteration 30888/35720 Training loss: 0.6934 0.2146 sec/batch\n",
      "Epoch 18/20  Iteration 30889/35720 Training loss: 0.6934 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30890/35720 Training loss: 0.6934 0.2141 sec/batch\n",
      "Epoch 18/20  Iteration 30891/35720 Training loss: 0.6935 0.2235 sec/batch\n",
      "Epoch 18/20  Iteration 30892/35720 Training loss: 0.6934 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 30893/35720 Training loss: 0.6935 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 30894/35720 Training loss: 0.6934 0.2162 sec/batch\n",
      "Epoch 18/20  Iteration 30895/35720 Training loss: 0.6934 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 30896/35720 Training loss: 0.6934 0.2191 sec/batch\n",
      "Epoch 18/20  Iteration 30897/35720 Training loss: 0.6933 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 30898/35720 Training loss: 0.6932 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 30899/35720 Training loss: 0.6932 0.2163 sec/batch\n",
      "Epoch 18/20  Iteration 30900/35720 Training loss: 0.6931 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 30901/35720 Training loss: 0.6931 0.2072 sec/batch\n",
      "Epoch 18/20  Iteration 30902/35720 Training loss: 0.6931 0.2271 sec/batch\n",
      "Epoch 18/20  Iteration 30903/35720 Training loss: 0.6931 0.2222 sec/batch\n",
      "Epoch 18/20  Iteration 30904/35720 Training loss: 0.6930 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 30905/35720 Training loss: 0.6929 0.2163 sec/batch\n",
      "Epoch 18/20  Iteration 30906/35720 Training loss: 0.6929 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 30907/35720 Training loss: 0.6930 0.2275 sec/batch\n",
      "Epoch 18/20  Iteration 30908/35720 Training loss: 0.6931 0.2123 sec/batch\n",
      "Epoch 18/20  Iteration 30909/35720 Training loss: 0.6931 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 30910/35720 Training loss: 0.6931 0.2144 sec/batch\n",
      "Epoch 18/20  Iteration 30911/35720 Training loss: 0.6930 0.2149 sec/batch\n",
      "Epoch 18/20  Iteration 30912/35720 Training loss: 0.6931 0.2168 sec/batch\n",
      "Epoch 18/20  Iteration 30913/35720 Training loss: 0.6930 0.2161 sec/batch\n",
      "Epoch 18/20  Iteration 30914/35720 Training loss: 0.6929 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 30915/35720 Training loss: 0.6928 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 30916/35720 Training loss: 0.6928 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 30917/35720 Training loss: 0.6929 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 30918/35720 Training loss: 0.6929 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 30919/35720 Training loss: 0.6929 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 30920/35720 Training loss: 0.6930 0.2046 sec/batch\n",
      "Epoch 18/20  Iteration 30921/35720 Training loss: 0.6930 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 30922/35720 Training loss: 0.6929 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 30923/35720 Training loss: 0.6930 0.2191 sec/batch\n",
      "Epoch 18/20  Iteration 30924/35720 Training loss: 0.6929 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 30925/35720 Training loss: 0.6928 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 30926/35720 Training loss: 0.6929 0.2123 sec/batch\n",
      "Epoch 18/20  Iteration 30927/35720 Training loss: 0.6927 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 30928/35720 Training loss: 0.6928 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 30929/35720 Training loss: 0.6928 0.2173 sec/batch\n",
      "Epoch 18/20  Iteration 30930/35720 Training loss: 0.6927 0.2056 sec/batch\n",
      "Epoch 18/20  Iteration 30931/35720 Training loss: 0.6927 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 30932/35720 Training loss: 0.6927 0.2167 sec/batch\n",
      "Epoch 18/20  Iteration 30933/35720 Training loss: 0.6927 0.2343 sec/batch\n",
      "Epoch 18/20  Iteration 30934/35720 Training loss: 0.6927 0.2268 sec/batch\n",
      "Epoch 18/20  Iteration 30935/35720 Training loss: 0.6928 0.2266 sec/batch\n",
      "Epoch 18/20  Iteration 30936/35720 Training loss: 0.6929 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 30937/35720 Training loss: 0.6929 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 30938/35720 Training loss: 0.6929 0.2238 sec/batch\n",
      "Epoch 18/20  Iteration 30939/35720 Training loss: 0.6930 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 30940/35720 Training loss: 0.6929 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 30941/35720 Training loss: 0.6929 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 30942/35720 Training loss: 0.6929 0.2250 sec/batch\n",
      "Epoch 18/20  Iteration 30943/35720 Training loss: 0.6929 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30944/35720 Training loss: 0.6929 0.2170 sec/batch\n",
      "Epoch 18/20  Iteration 30945/35720 Training loss: 0.6929 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 30946/35720 Training loss: 0.6929 0.2258 sec/batch\n",
      "Epoch 18/20  Iteration 30947/35720 Training loss: 0.6929 0.2134 sec/batch\n",
      "Epoch 18/20  Iteration 30948/35720 Training loss: 0.6928 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 30949/35720 Training loss: 0.6927 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 30950/35720 Training loss: 0.6927 0.2217 sec/batch\n",
      "Epoch 18/20  Iteration 30951/35720 Training loss: 0.6926 0.2222 sec/batch\n",
      "Epoch 18/20  Iteration 30952/35720 Training loss: 0.6926 0.2331 sec/batch\n",
      "Epoch 18/20  Iteration 30953/35720 Training loss: 0.6926 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 30954/35720 Training loss: 0.6926 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 30955/35720 Training loss: 0.6925 0.2192 sec/batch\n",
      "Epoch 18/20  Iteration 30956/35720 Training loss: 0.6925 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 30957/35720 Training loss: 0.6924 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 30958/35720 Training loss: 0.6924 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 30959/35720 Training loss: 0.6924 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 30960/35720 Training loss: 0.6924 0.2176 sec/batch\n",
      "Epoch 18/20  Iteration 30961/35720 Training loss: 0.6923 0.2179 sec/batch\n",
      "Epoch 18/20  Iteration 30962/35720 Training loss: 0.6922 0.2263 sec/batch\n",
      "Epoch 18/20  Iteration 30963/35720 Training loss: 0.6921 0.2153 sec/batch\n",
      "Epoch 18/20  Iteration 30964/35720 Training loss: 0.6920 0.2173 sec/batch\n",
      "Epoch 18/20  Iteration 30965/35720 Training loss: 0.6920 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 30966/35720 Training loss: 0.6919 0.2244 sec/batch\n",
      "Epoch 18/20  Iteration 30967/35720 Training loss: 0.6919 0.2146 sec/batch\n",
      "Epoch 18/20  Iteration 30968/35720 Training loss: 0.6920 0.2416 sec/batch\n",
      "Epoch 18/20  Iteration 30969/35720 Training loss: 0.6919 0.2056 sec/batch\n",
      "Epoch 18/20  Iteration 30970/35720 Training loss: 0.6919 0.2100 sec/batch\n",
      "Epoch 18/20  Iteration 30971/35720 Training loss: 0.6920 0.2184 sec/batch\n",
      "Epoch 18/20  Iteration 30972/35720 Training loss: 0.6919 0.2280 sec/batch\n",
      "Epoch 18/20  Iteration 30973/35720 Training loss: 0.6919 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 30974/35720 Training loss: 0.6918 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 30975/35720 Training loss: 0.6917 0.2124 sec/batch\n",
      "Epoch 18/20  Iteration 30976/35720 Training loss: 0.6917 0.2123 sec/batch\n",
      "Epoch 18/20  Iteration 30977/35720 Training loss: 0.6917 0.2161 sec/batch\n",
      "Epoch 18/20  Iteration 30978/35720 Training loss: 0.6916 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 30979/35720 Training loss: 0.6916 0.2293 sec/batch\n",
      "Epoch 18/20  Iteration 30980/35720 Training loss: 0.6915 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 30981/35720 Training loss: 0.6914 0.2203 sec/batch\n",
      "Epoch 18/20  Iteration 30982/35720 Training loss: 0.6913 0.2219 sec/batch\n",
      "Epoch 18/20  Iteration 30983/35720 Training loss: 0.6913 0.2299 sec/batch\n",
      "Epoch 18/20  Iteration 30984/35720 Training loss: 0.6914 0.2220 sec/batch\n",
      "Epoch 18/20  Iteration 30985/35720 Training loss: 0.6913 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 30986/35720 Training loss: 0.6913 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 30987/35720 Training loss: 0.6913 0.2125 sec/batch\n",
      "Epoch 18/20  Iteration 30988/35720 Training loss: 0.6913 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 30989/35720 Training loss: 0.6913 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 30990/35720 Training loss: 0.6912 0.2257 sec/batch\n",
      "Epoch 18/20  Iteration 30991/35720 Training loss: 0.6913 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 30992/35720 Training loss: 0.6913 0.2072 sec/batch\n",
      "Epoch 18/20  Iteration 30993/35720 Training loss: 0.6913 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 30994/35720 Training loss: 0.6912 0.2192 sec/batch\n",
      "Epoch 18/20  Iteration 30995/35720 Training loss: 0.6912 0.2242 sec/batch\n",
      "Epoch 18/20  Iteration 30996/35720 Training loss: 0.6912 0.2264 sec/batch\n",
      "Epoch 18/20  Iteration 30997/35720 Training loss: 0.6912 0.2102 sec/batch\n",
      "Epoch 18/20  Iteration 30998/35720 Training loss: 0.6912 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 30999/35720 Training loss: 0.6912 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31000/35720 Training loss: 0.6911 0.2165 sec/batch\n",
      "Validation loss: 1.61833 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 31001/35720 Training loss: 0.6918 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 31002/35720 Training loss: 0.6918 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31003/35720 Training loss: 0.6919 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31004/35720 Training loss: 0.6919 0.2054 sec/batch\n",
      "Epoch 18/20  Iteration 31005/35720 Training loss: 0.6920 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 31006/35720 Training loss: 0.6919 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 31007/35720 Training loss: 0.6918 0.2074 sec/batch\n",
      "Epoch 18/20  Iteration 31008/35720 Training loss: 0.6919 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31009/35720 Training loss: 0.6918 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31010/35720 Training loss: 0.6918 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 31011/35720 Training loss: 0.6917 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31012/35720 Training loss: 0.6916 0.2252 sec/batch\n",
      "Epoch 18/20  Iteration 31013/35720 Training loss: 0.6916 0.2242 sec/batch\n",
      "Epoch 18/20  Iteration 31014/35720 Training loss: 0.6916 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 31015/35720 Training loss: 0.6915 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 31016/35720 Training loss: 0.6916 0.2239 sec/batch\n",
      "Epoch 18/20  Iteration 31017/35720 Training loss: 0.6915 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 31018/35720 Training loss: 0.6916 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 31019/35720 Training loss: 0.6916 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31020/35720 Training loss: 0.6917 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31021/35720 Training loss: 0.6917 0.2171 sec/batch\n",
      "Epoch 18/20  Iteration 31022/35720 Training loss: 0.6917 0.2191 sec/batch\n",
      "Epoch 18/20  Iteration 31023/35720 Training loss: 0.6917 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 31024/35720 Training loss: 0.6918 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 31025/35720 Training loss: 0.6917 0.2224 sec/batch\n",
      "Epoch 18/20  Iteration 31026/35720 Training loss: 0.6917 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31027/35720 Training loss: 0.6918 0.2250 sec/batch\n",
      "Epoch 18/20  Iteration 31028/35720 Training loss: 0.6918 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 31029/35720 Training loss: 0.6919 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 31030/35720 Training loss: 0.6918 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 31031/35720 Training loss: 0.6918 0.2221 sec/batch\n",
      "Epoch 18/20  Iteration 31032/35720 Training loss: 0.6918 0.2074 sec/batch\n",
      "Epoch 18/20  Iteration 31033/35720 Training loss: 0.6917 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 31034/35720 Training loss: 0.6917 0.2252 sec/batch\n",
      "Epoch 18/20  Iteration 31035/35720 Training loss: 0.6917 0.2058 sec/batch\n",
      "Epoch 18/20  Iteration 31036/35720 Training loss: 0.6916 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31037/35720 Training loss: 0.6916 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 31038/35720 Training loss: 0.6916 0.2297 sec/batch\n",
      "Epoch 18/20  Iteration 31039/35720 Training loss: 0.6917 0.2308 sec/batch\n",
      "Epoch 18/20  Iteration 31040/35720 Training loss: 0.6917 0.2648 sec/batch\n",
      "Epoch 18/20  Iteration 31041/35720 Training loss: 0.6917 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 31042/35720 Training loss: 0.6916 0.2075 sec/batch\n",
      "Epoch 18/20  Iteration 31043/35720 Training loss: 0.6916 0.2055 sec/batch\n",
      "Epoch 18/20  Iteration 31044/35720 Training loss: 0.6916 0.2193 sec/batch\n",
      "Epoch 18/20  Iteration 31045/35720 Training loss: 0.6916 0.2287 sec/batch\n",
      "Epoch 18/20  Iteration 31046/35720 Training loss: 0.6915 0.2078 sec/batch\n",
      "Epoch 18/20  Iteration 31047/35720 Training loss: 0.6915 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31048/35720 Training loss: 0.6915 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 31049/35720 Training loss: 0.6914 0.2176 sec/batch\n",
      "Epoch 18/20  Iteration 31050/35720 Training loss: 0.6914 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 31051/35720 Training loss: 0.6914 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 31052/35720 Training loss: 0.6914 0.2250 sec/batch\n",
      "Epoch 18/20  Iteration 31053/35720 Training loss: 0.6914 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31054/35720 Training loss: 0.6915 0.2106 sec/batch\n",
      "Epoch 18/20  Iteration 31055/35720 Training loss: 0.6916 0.2168 sec/batch\n",
      "Epoch 18/20  Iteration 31056/35720 Training loss: 0.6916 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31057/35720 Training loss: 0.6916 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 31058/35720 Training loss: 0.6916 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 31059/35720 Training loss: 0.6916 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31060/35720 Training loss: 0.6916 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 31061/35720 Training loss: 0.6915 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 31062/35720 Training loss: 0.6915 0.2233 sec/batch\n",
      "Epoch 18/20  Iteration 31063/35720 Training loss: 0.6914 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31064/35720 Training loss: 0.6914 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31065/35720 Training loss: 0.6913 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 31066/35720 Training loss: 0.6913 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 31067/35720 Training loss: 0.6913 0.2163 sec/batch\n",
      "Epoch 18/20  Iteration 31068/35720 Training loss: 0.6913 0.2213 sec/batch\n",
      "Epoch 18/20  Iteration 31069/35720 Training loss: 0.6914 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31070/35720 Training loss: 0.6914 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31071/35720 Training loss: 0.6915 0.2240 sec/batch\n",
      "Epoch 18/20  Iteration 31072/35720 Training loss: 0.6915 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 31073/35720 Training loss: 0.6915 0.2328 sec/batch\n",
      "Epoch 18/20  Iteration 31074/35720 Training loss: 0.6916 0.2107 sec/batch\n",
      "Epoch 18/20  Iteration 31075/35720 Training loss: 0.6916 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 31076/35720 Training loss: 0.6916 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 31077/35720 Training loss: 0.6916 0.2180 sec/batch\n",
      "Epoch 18/20  Iteration 31078/35720 Training loss: 0.6916 0.2193 sec/batch\n",
      "Epoch 18/20  Iteration 31079/35720 Training loss: 0.6917 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 31080/35720 Training loss: 0.6917 0.2130 sec/batch\n",
      "Epoch 18/20  Iteration 31081/35720 Training loss: 0.6916 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 31082/35720 Training loss: 0.6916 0.2142 sec/batch\n",
      "Epoch 18/20  Iteration 31083/35720 Training loss: 0.6916 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 31084/35720 Training loss: 0.6917 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 31085/35720 Training loss: 0.6918 0.2262 sec/batch\n",
      "Epoch 18/20  Iteration 31086/35720 Training loss: 0.6917 0.2165 sec/batch\n",
      "Epoch 18/20  Iteration 31087/35720 Training loss: 0.6917 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 31088/35720 Training loss: 0.6917 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 31089/35720 Training loss: 0.6918 0.2135 sec/batch\n",
      "Epoch 18/20  Iteration 31090/35720 Training loss: 0.6918 0.2170 sec/batch\n",
      "Epoch 18/20  Iteration 31091/35720 Training loss: 0.6919 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31092/35720 Training loss: 0.6920 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31093/35720 Training loss: 0.6919 0.2191 sec/batch\n",
      "Epoch 18/20  Iteration 31094/35720 Training loss: 0.6919 0.2261 sec/batch\n",
      "Epoch 18/20  Iteration 31095/35720 Training loss: 0.6919 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 31096/35720 Training loss: 0.6919 0.2116 sec/batch\n",
      "Epoch 18/20  Iteration 31097/35720 Training loss: 0.6919 0.2073 sec/batch\n",
      "Epoch 18/20  Iteration 31098/35720 Training loss: 0.6919 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 31099/35720 Training loss: 0.6919 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31100/35720 Training loss: 0.6918 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31101/35720 Training loss: 0.6919 0.2173 sec/batch\n",
      "Epoch 18/20  Iteration 31102/35720 Training loss: 0.6919 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 31103/35720 Training loss: 0.6919 0.2191 sec/batch\n",
      "Epoch 18/20  Iteration 31104/35720 Training loss: 0.6920 0.2141 sec/batch\n",
      "Epoch 18/20  Iteration 31105/35720 Training loss: 0.6919 0.2210 sec/batch\n",
      "Epoch 18/20  Iteration 31106/35720 Training loss: 0.6919 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31107/35720 Training loss: 0.6919 0.2286 sec/batch\n",
      "Epoch 18/20  Iteration 31108/35720 Training loss: 0.6919 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 31109/35720 Training loss: 0.6919 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31110/35720 Training loss: 0.6919 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 31111/35720 Training loss: 0.6918 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 31112/35720 Training loss: 0.6918 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 31113/35720 Training loss: 0.6918 0.2200 sec/batch\n",
      "Epoch 18/20  Iteration 31114/35720 Training loss: 0.6919 0.2163 sec/batch\n",
      "Epoch 18/20  Iteration 31115/35720 Training loss: 0.6918 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 31116/35720 Training loss: 0.6917 0.2414 sec/batch\n",
      "Epoch 18/20  Iteration 31117/35720 Training loss: 0.6917 0.2213 sec/batch\n",
      "Epoch 18/20  Iteration 31118/35720 Training loss: 0.6917 0.2175 sec/batch\n",
      "Epoch 18/20  Iteration 31119/35720 Training loss: 0.6916 0.2107 sec/batch\n",
      "Epoch 18/20  Iteration 31120/35720 Training loss: 0.6917 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 31121/35720 Training loss: 0.6917 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31122/35720 Training loss: 0.6916 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 31123/35720 Training loss: 0.6917 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 31124/35720 Training loss: 0.6917 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31125/35720 Training loss: 0.6916 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31126/35720 Training loss: 0.6915 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31127/35720 Training loss: 0.6915 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 31128/35720 Training loss: 0.6915 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 31129/35720 Training loss: 0.6915 0.2278 sec/batch\n",
      "Epoch 18/20  Iteration 31130/35720 Training loss: 0.6914 0.2216 sec/batch\n",
      "Epoch 18/20  Iteration 31131/35720 Training loss: 0.6914 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31132/35720 Training loss: 0.6915 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31133/35720 Training loss: 0.6916 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 31134/35720 Training loss: 0.6917 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31135/35720 Training loss: 0.6916 0.2226 sec/batch\n",
      "Epoch 18/20  Iteration 31136/35720 Training loss: 0.6915 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31137/35720 Training loss: 0.6915 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 31138/35720 Training loss: 0.6914 0.2176 sec/batch\n",
      "Epoch 18/20  Iteration 31139/35720 Training loss: 0.6914 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 31140/35720 Training loss: 0.6913 0.2210 sec/batch\n",
      "Epoch 18/20  Iteration 31141/35720 Training loss: 0.6914 0.2390 sec/batch\n",
      "Epoch 18/20  Iteration 31142/35720 Training loss: 0.6914 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 31143/35720 Training loss: 0.6913 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31144/35720 Training loss: 0.6914 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 31145/35720 Training loss: 0.6914 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31146/35720 Training loss: 0.6914 0.2321 sec/batch\n",
      "Epoch 18/20  Iteration 31147/35720 Training loss: 0.6914 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 31148/35720 Training loss: 0.6913 0.2164 sec/batch\n",
      "Epoch 18/20  Iteration 31149/35720 Training loss: 0.6914 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 31150/35720 Training loss: 0.6914 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 31151/35720 Training loss: 0.6914 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31152/35720 Training loss: 0.6914 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31153/35720 Training loss: 0.6914 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31154/35720 Training loss: 0.6914 0.2080 sec/batch\n",
      "Epoch 18/20  Iteration 31155/35720 Training loss: 0.6914 0.2471 sec/batch\n",
      "Epoch 18/20  Iteration 31156/35720 Training loss: 0.6913 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 31157/35720 Training loss: 0.6914 0.2272 sec/batch\n",
      "Epoch 18/20  Iteration 31158/35720 Training loss: 0.6914 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31159/35720 Training loss: 0.6914 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 31160/35720 Training loss: 0.6914 0.2184 sec/batch\n",
      "Epoch 18/20  Iteration 31161/35720 Training loss: 0.6914 0.2283 sec/batch\n",
      "Epoch 18/20  Iteration 31162/35720 Training loss: 0.6915 0.2141 sec/batch\n",
      "Epoch 18/20  Iteration 31163/35720 Training loss: 0.6915 0.2218 sec/batch\n",
      "Epoch 18/20  Iteration 31164/35720 Training loss: 0.6915 0.2164 sec/batch\n",
      "Epoch 18/20  Iteration 31165/35720 Training loss: 0.6915 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31166/35720 Training loss: 0.6916 0.2325 sec/batch\n",
      "Epoch 18/20  Iteration 31167/35720 Training loss: 0.6916 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31168/35720 Training loss: 0.6916 0.2227 sec/batch\n",
      "Epoch 18/20  Iteration 31169/35720 Training loss: 0.6916 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 31170/35720 Training loss: 0.6915 0.2305 sec/batch\n",
      "Epoch 18/20  Iteration 31171/35720 Training loss: 0.6916 0.2239 sec/batch\n",
      "Epoch 18/20  Iteration 31172/35720 Training loss: 0.6917 0.2229 sec/batch\n",
      "Epoch 18/20  Iteration 31173/35720 Training loss: 0.6917 0.2249 sec/batch\n",
      "Epoch 18/20  Iteration 31174/35720 Training loss: 0.6917 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 31175/35720 Training loss: 0.6917 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 31176/35720 Training loss: 0.6917 0.2146 sec/batch\n",
      "Epoch 18/20  Iteration 31177/35720 Training loss: 0.6917 0.2326 sec/batch\n",
      "Epoch 18/20  Iteration 31178/35720 Training loss: 0.6917 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31179/35720 Training loss: 0.6917 0.2327 sec/batch\n",
      "Epoch 18/20  Iteration 31180/35720 Training loss: 0.6917 0.2185 sec/batch\n",
      "Epoch 18/20  Iteration 31181/35720 Training loss: 0.6917 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 31182/35720 Training loss: 0.6917 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 31183/35720 Training loss: 0.6917 0.2278 sec/batch\n",
      "Epoch 18/20  Iteration 31184/35720 Training loss: 0.6917 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 31185/35720 Training loss: 0.6916 0.2124 sec/batch\n",
      "Epoch 18/20  Iteration 31186/35720 Training loss: 0.6916 0.2180 sec/batch\n",
      "Epoch 18/20  Iteration 31187/35720 Training loss: 0.6915 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31188/35720 Training loss: 0.6915 0.2199 sec/batch\n",
      "Epoch 18/20  Iteration 31189/35720 Training loss: 0.6914 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 31190/35720 Training loss: 0.6914 0.2324 sec/batch\n",
      "Epoch 18/20  Iteration 31191/35720 Training loss: 0.6914 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 31192/35720 Training loss: 0.6914 0.2419 sec/batch\n",
      "Epoch 18/20  Iteration 31193/35720 Training loss: 0.6913 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 31194/35720 Training loss: 0.6914 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 31195/35720 Training loss: 0.6914 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 31196/35720 Training loss: 0.6914 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31197/35720 Training loss: 0.6914 0.2072 sec/batch\n",
      "Epoch 18/20  Iteration 31198/35720 Training loss: 0.6914 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31199/35720 Training loss: 0.6914 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 31200/35720 Training loss: 0.6914 0.2094 sec/batch\n",
      "Validation loss: 1.60638 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 31201/35720 Training loss: 0.6920 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 31202/35720 Training loss: 0.6920 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31203/35720 Training loss: 0.6920 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31204/35720 Training loss: 0.6920 0.2179 sec/batch\n",
      "Epoch 18/20  Iteration 31205/35720 Training loss: 0.6920 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31206/35720 Training loss: 0.6920 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 31207/35720 Training loss: 0.6921 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 31208/35720 Training loss: 0.6921 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 31209/35720 Training loss: 0.6921 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 31210/35720 Training loss: 0.6920 0.2346 sec/batch\n",
      "Epoch 18/20  Iteration 31211/35720 Training loss: 0.6920 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 31212/35720 Training loss: 0.6921 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 31213/35720 Training loss: 0.6921 0.2386 sec/batch\n",
      "Epoch 18/20  Iteration 31214/35720 Training loss: 0.6921 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31215/35720 Training loss: 0.6920 0.2167 sec/batch\n",
      "Epoch 18/20  Iteration 31216/35720 Training loss: 0.6920 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31217/35720 Training loss: 0.6921 0.2268 sec/batch\n",
      "Epoch 18/20  Iteration 31218/35720 Training loss: 0.6920 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 31219/35720 Training loss: 0.6919 0.2057 sec/batch\n",
      "Epoch 18/20  Iteration 31220/35720 Training loss: 0.6920 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31221/35720 Training loss: 0.6919 0.2171 sec/batch\n",
      "Epoch 18/20  Iteration 31222/35720 Training loss: 0.6919 0.2182 sec/batch\n",
      "Epoch 18/20  Iteration 31223/35720 Training loss: 0.6919 0.2260 sec/batch\n",
      "Epoch 18/20  Iteration 31224/35720 Training loss: 0.6919 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31225/35720 Training loss: 0.6919 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31226/35720 Training loss: 0.6918 0.2079 sec/batch\n",
      "Epoch 18/20  Iteration 31227/35720 Training loss: 0.6918 0.2217 sec/batch\n",
      "Epoch 18/20  Iteration 31228/35720 Training loss: 0.6918 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 31229/35720 Training loss: 0.6918 0.2190 sec/batch\n",
      "Epoch 18/20  Iteration 31230/35720 Training loss: 0.6918 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 31231/35720 Training loss: 0.6918 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31232/35720 Training loss: 0.6917 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31233/35720 Training loss: 0.6917 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31234/35720 Training loss: 0.6917 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 31235/35720 Training loss: 0.6916 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31236/35720 Training loss: 0.6916 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31237/35720 Training loss: 0.6916 0.2232 sec/batch\n",
      "Epoch 18/20  Iteration 31238/35720 Training loss: 0.6916 0.2199 sec/batch\n",
      "Epoch 18/20  Iteration 31239/35720 Training loss: 0.6915 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 31240/35720 Training loss: 0.6916 0.2265 sec/batch\n",
      "Epoch 18/20  Iteration 31241/35720 Training loss: 0.6915 0.2281 sec/batch\n",
      "Epoch 18/20  Iteration 31242/35720 Training loss: 0.6915 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31243/35720 Training loss: 0.6914 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 31244/35720 Training loss: 0.6913 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31245/35720 Training loss: 0.6913 0.2073 sec/batch\n",
      "Epoch 18/20  Iteration 31246/35720 Training loss: 0.6913 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31247/35720 Training loss: 0.6913 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31248/35720 Training loss: 0.6912 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 31249/35720 Training loss: 0.6912 0.2241 sec/batch\n",
      "Epoch 18/20  Iteration 31250/35720 Training loss: 0.6912 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 31251/35720 Training loss: 0.6911 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 31252/35720 Training loss: 0.6911 0.2231 sec/batch\n",
      "Epoch 18/20  Iteration 31253/35720 Training loss: 0.6911 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 31254/35720 Training loss: 0.6910 0.2238 sec/batch\n",
      "Epoch 18/20  Iteration 31255/35720 Training loss: 0.6910 0.2132 sec/batch\n",
      "Epoch 18/20  Iteration 31256/35720 Training loss: 0.6909 0.2315 sec/batch\n",
      "Epoch 18/20  Iteration 31257/35720 Training loss: 0.6909 0.2117 sec/batch\n",
      "Epoch 18/20  Iteration 31258/35720 Training loss: 0.6909 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 31259/35720 Training loss: 0.6908 0.2221 sec/batch\n",
      "Epoch 18/20  Iteration 31260/35720 Training loss: 0.6908 0.2300 sec/batch\n",
      "Epoch 18/20  Iteration 31261/35720 Training loss: 0.6907 0.2404 sec/batch\n",
      "Epoch 18/20  Iteration 31262/35720 Training loss: 0.6907 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 31263/35720 Training loss: 0.6907 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 31264/35720 Training loss: 0.6906 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 31265/35720 Training loss: 0.6906 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 31266/35720 Training loss: 0.6906 0.2251 sec/batch\n",
      "Epoch 18/20  Iteration 31267/35720 Training loss: 0.6905 0.2135 sec/batch\n",
      "Epoch 18/20  Iteration 31268/35720 Training loss: 0.6905 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 31269/35720 Training loss: 0.6905 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 31270/35720 Training loss: 0.6905 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 31271/35720 Training loss: 0.6905 0.2319 sec/batch\n",
      "Epoch 18/20  Iteration 31272/35720 Training loss: 0.6905 0.2194 sec/batch\n",
      "Epoch 18/20  Iteration 31273/35720 Training loss: 0.6905 0.2227 sec/batch\n",
      "Epoch 18/20  Iteration 31274/35720 Training loss: 0.6905 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 31275/35720 Training loss: 0.6905 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 31276/35720 Training loss: 0.6905 0.2149 sec/batch\n",
      "Epoch 18/20  Iteration 31277/35720 Training loss: 0.6905 0.2199 sec/batch\n",
      "Epoch 18/20  Iteration 31278/35720 Training loss: 0.6905 0.2271 sec/batch\n",
      "Epoch 18/20  Iteration 31279/35720 Training loss: 0.6905 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31280/35720 Training loss: 0.6905 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31281/35720 Training loss: 0.6904 0.2078 sec/batch\n",
      "Epoch 18/20  Iteration 31282/35720 Training loss: 0.6905 0.2264 sec/batch\n",
      "Epoch 18/20  Iteration 31283/35720 Training loss: 0.6904 0.2076 sec/batch\n",
      "Epoch 18/20  Iteration 31284/35720 Training loss: 0.6904 0.2180 sec/batch\n",
      "Epoch 18/20  Iteration 31285/35720 Training loss: 0.6904 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 31286/35720 Training loss: 0.6904 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 31287/35720 Training loss: 0.6904 0.2079 sec/batch\n",
      "Epoch 18/20  Iteration 31288/35720 Training loss: 0.6904 0.2257 sec/batch\n",
      "Epoch 18/20  Iteration 31289/35720 Training loss: 0.6904 0.2237 sec/batch\n",
      "Epoch 18/20  Iteration 31290/35720 Training loss: 0.6903 0.2057 sec/batch\n",
      "Epoch 18/20  Iteration 31291/35720 Training loss: 0.6904 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 31292/35720 Training loss: 0.6904 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31293/35720 Training loss: 0.6904 0.2299 sec/batch\n",
      "Epoch 18/20  Iteration 31294/35720 Training loss: 0.6904 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 31295/35720 Training loss: 0.6904 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 31296/35720 Training loss: 0.6904 0.2116 sec/batch\n",
      "Epoch 18/20  Iteration 31297/35720 Training loss: 0.6903 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31298/35720 Training loss: 0.6902 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 31299/35720 Training loss: 0.6901 0.2162 sec/batch\n",
      "Epoch 18/20  Iteration 31300/35720 Training loss: 0.6901 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31301/35720 Training loss: 0.6900 0.2169 sec/batch\n",
      "Epoch 18/20  Iteration 31302/35720 Training loss: 0.6899 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31303/35720 Training loss: 0.6899 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 31304/35720 Training loss: 0.6899 0.2212 sec/batch\n",
      "Epoch 18/20  Iteration 31305/35720 Training loss: 0.6899 0.2207 sec/batch\n",
      "Epoch 18/20  Iteration 31306/35720 Training loss: 0.6899 0.2291 sec/batch\n",
      "Epoch 18/20  Iteration 31307/35720 Training loss: 0.6899 0.2288 sec/batch\n",
      "Epoch 18/20  Iteration 31308/35720 Training loss: 0.6899 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 31309/35720 Training loss: 0.6899 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 31310/35720 Training loss: 0.6898 0.2320 sec/batch\n",
      "Epoch 18/20  Iteration 31311/35720 Training loss: 0.6898 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31312/35720 Training loss: 0.6898 0.2212 sec/batch\n",
      "Epoch 18/20  Iteration 31313/35720 Training loss: 0.6898 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31314/35720 Training loss: 0.6897 0.2125 sec/batch\n",
      "Epoch 18/20  Iteration 31315/35720 Training loss: 0.6897 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 31316/35720 Training loss: 0.6896 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 31317/35720 Training loss: 0.6896 0.2182 sec/batch\n",
      "Epoch 18/20  Iteration 31318/35720 Training loss: 0.6896 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31319/35720 Training loss: 0.6896 0.2197 sec/batch\n",
      "Epoch 18/20  Iteration 31320/35720 Training loss: 0.6896 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31321/35720 Training loss: 0.6895 0.2307 sec/batch\n",
      "Epoch 18/20  Iteration 31322/35720 Training loss: 0.6894 0.2171 sec/batch\n",
      "Epoch 18/20  Iteration 31323/35720 Training loss: 0.6894 0.2175 sec/batch\n",
      "Epoch 18/20  Iteration 31324/35720 Training loss: 0.6894 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 31325/35720 Training loss: 0.6893 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31326/35720 Training loss: 0.6893 0.2189 sec/batch\n",
      "Epoch 18/20  Iteration 31327/35720 Training loss: 0.6893 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 31328/35720 Training loss: 0.6893 0.2243 sec/batch\n",
      "Epoch 18/20  Iteration 31329/35720 Training loss: 0.6893 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 31330/35720 Training loss: 0.6893 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31331/35720 Training loss: 0.6894 0.2077 sec/batch\n",
      "Epoch 18/20  Iteration 31332/35720 Training loss: 0.6894 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31333/35720 Training loss: 0.6894 0.2078 sec/batch\n",
      "Epoch 18/20  Iteration 31334/35720 Training loss: 0.6893 0.2419 sec/batch\n",
      "Epoch 18/20  Iteration 31335/35720 Training loss: 0.6893 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31336/35720 Training loss: 0.6892 0.2166 sec/batch\n",
      "Epoch 18/20  Iteration 31337/35720 Training loss: 0.6892 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 31338/35720 Training loss: 0.6892 0.2469 sec/batch\n",
      "Epoch 18/20  Iteration 31339/35720 Training loss: 0.6892 0.2280 sec/batch\n",
      "Epoch 18/20  Iteration 31340/35720 Training loss: 0.6892 0.2277 sec/batch\n",
      "Epoch 18/20  Iteration 31341/35720 Training loss: 0.6892 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 31342/35720 Training loss: 0.6892 0.2075 sec/batch\n",
      "Epoch 18/20  Iteration 31343/35720 Training loss: 0.6891 0.2249 sec/batch\n",
      "Epoch 18/20  Iteration 31344/35720 Training loss: 0.6890 0.2076 sec/batch\n",
      "Epoch 18/20  Iteration 31345/35720 Training loss: 0.6890 0.2200 sec/batch\n",
      "Epoch 18/20  Iteration 31346/35720 Training loss: 0.6889 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31347/35720 Training loss: 0.6889 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31348/35720 Training loss: 0.6889 0.2357 sec/batch\n",
      "Epoch 18/20  Iteration 31349/35720 Training loss: 0.6889 0.2555 sec/batch\n",
      "Epoch 18/20  Iteration 31350/35720 Training loss: 0.6888 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 31351/35720 Training loss: 0.6888 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31352/35720 Training loss: 0.6888 0.2072 sec/batch\n",
      "Epoch 18/20  Iteration 31353/35720 Training loss: 0.6887 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31354/35720 Training loss: 0.6888 0.2256 sec/batch\n",
      "Epoch 18/20  Iteration 31355/35720 Training loss: 0.6887 0.2180 sec/batch\n",
      "Epoch 18/20  Iteration 31356/35720 Training loss: 0.6887 0.2364 sec/batch\n",
      "Epoch 18/20  Iteration 31357/35720 Training loss: 0.6888 0.2197 sec/batch\n",
      "Epoch 18/20  Iteration 31358/35720 Training loss: 0.6888 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 31359/35720 Training loss: 0.6888 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31360/35720 Training loss: 0.6888 0.2339 sec/batch\n",
      "Epoch 18/20  Iteration 31361/35720 Training loss: 0.6888 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 31362/35720 Training loss: 0.6888 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 31363/35720 Training loss: 0.6887 0.2078 sec/batch\n",
      "Epoch 18/20  Iteration 31364/35720 Training loss: 0.6887 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31365/35720 Training loss: 0.6886 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 31366/35720 Training loss: 0.6885 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 31367/35720 Training loss: 0.6884 0.2077 sec/batch\n",
      "Epoch 18/20  Iteration 31368/35720 Training loss: 0.6884 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 31369/35720 Training loss: 0.6884 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31370/35720 Training loss: 0.6884 0.2196 sec/batch\n",
      "Epoch 18/20  Iteration 31371/35720 Training loss: 0.6883 0.2158 sec/batch\n",
      "Epoch 18/20  Iteration 31372/35720 Training loss: 0.6883 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 31373/35720 Training loss: 0.6883 0.2151 sec/batch\n",
      "Epoch 18/20  Iteration 31374/35720 Training loss: 0.6883 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31375/35720 Training loss: 0.6882 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 31376/35720 Training loss: 0.6882 0.2217 sec/batch\n",
      "Epoch 18/20  Iteration 31377/35720 Training loss: 0.6882 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31378/35720 Training loss: 0.6882 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31379/35720 Training loss: 0.6882 0.2118 sec/batch\n",
      "Epoch 18/20  Iteration 31380/35720 Training loss: 0.6882 0.2229 sec/batch\n",
      "Epoch 18/20  Iteration 31381/35720 Training loss: 0.6882 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31382/35720 Training loss: 0.6881 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 31383/35720 Training loss: 0.6881 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 31384/35720 Training loss: 0.6881 0.2135 sec/batch\n",
      "Epoch 18/20  Iteration 31385/35720 Training loss: 0.6882 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31386/35720 Training loss: 0.6882 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 31387/35720 Training loss: 0.6883 0.2163 sec/batch\n",
      "Epoch 18/20  Iteration 31388/35720 Training loss: 0.6883 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 31389/35720 Training loss: 0.6882 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31390/35720 Training loss: 0.6882 0.2171 sec/batch\n",
      "Epoch 18/20  Iteration 31391/35720 Training loss: 0.6882 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 31392/35720 Training loss: 0.6882 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 31393/35720 Training loss: 0.6882 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 31394/35720 Training loss: 0.6882 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 31395/35720 Training loss: 0.6882 0.2167 sec/batch\n",
      "Epoch 18/20  Iteration 31396/35720 Training loss: 0.6882 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 31397/35720 Training loss: 0.6882 0.2102 sec/batch\n",
      "Epoch 18/20  Iteration 31398/35720 Training loss: 0.6883 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 31399/35720 Training loss: 0.6883 0.2197 sec/batch\n",
      "Epoch 18/20  Iteration 31400/35720 Training loss: 0.6884 0.2119 sec/batch\n",
      "Validation loss: 1.63204 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 31401/35720 Training loss: 0.6887 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 31402/35720 Training loss: 0.6887 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31403/35720 Training loss: 0.6887 0.2058 sec/batch\n",
      "Epoch 18/20  Iteration 31404/35720 Training loss: 0.6887 0.2259 sec/batch\n",
      "Epoch 18/20  Iteration 31405/35720 Training loss: 0.6886 0.2117 sec/batch\n",
      "Epoch 18/20  Iteration 31406/35720 Training loss: 0.6887 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31407/35720 Training loss: 0.6887 0.2137 sec/batch\n",
      "Epoch 18/20  Iteration 31408/35720 Training loss: 0.6887 0.2137 sec/batch\n",
      "Epoch 18/20  Iteration 31409/35720 Training loss: 0.6887 0.2075 sec/batch\n",
      "Epoch 18/20  Iteration 31410/35720 Training loss: 0.6887 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31411/35720 Training loss: 0.6886 0.2211 sec/batch\n",
      "Epoch 18/20  Iteration 31412/35720 Training loss: 0.6887 0.2258 sec/batch\n",
      "Epoch 18/20  Iteration 31413/35720 Training loss: 0.6886 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31414/35720 Training loss: 0.6887 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 31415/35720 Training loss: 0.6887 0.2236 sec/batch\n",
      "Epoch 18/20  Iteration 31416/35720 Training loss: 0.6887 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 31417/35720 Training loss: 0.6887 0.2216 sec/batch\n",
      "Epoch 18/20  Iteration 31418/35720 Training loss: 0.6888 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31419/35720 Training loss: 0.6888 0.2078 sec/batch\n",
      "Epoch 18/20  Iteration 31420/35720 Training loss: 0.6888 0.2254 sec/batch\n",
      "Epoch 18/20  Iteration 31421/35720 Training loss: 0.6888 0.2149 sec/batch\n",
      "Epoch 18/20  Iteration 31422/35720 Training loss: 0.6888 0.2153 sec/batch\n",
      "Epoch 18/20  Iteration 31423/35720 Training loss: 0.6888 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31424/35720 Training loss: 0.6889 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 31425/35720 Training loss: 0.6890 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 31426/35720 Training loss: 0.6889 0.2239 sec/batch\n",
      "Epoch 18/20  Iteration 31427/35720 Training loss: 0.6889 0.2255 sec/batch\n",
      "Epoch 18/20  Iteration 31428/35720 Training loss: 0.6889 0.2223 sec/batch\n",
      "Epoch 18/20  Iteration 31429/35720 Training loss: 0.6889 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 31430/35720 Training loss: 0.6889 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 31431/35720 Training loss: 0.6889 0.2173 sec/batch\n",
      "Epoch 18/20  Iteration 31432/35720 Training loss: 0.6889 0.2117 sec/batch\n",
      "Epoch 18/20  Iteration 31433/35720 Training loss: 0.6889 0.2289 sec/batch\n",
      "Epoch 18/20  Iteration 31434/35720 Training loss: 0.6889 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 31435/35720 Training loss: 0.6889 0.2124 sec/batch\n",
      "Epoch 18/20  Iteration 31436/35720 Training loss: 0.6889 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 31437/35720 Training loss: 0.6889 0.2231 sec/batch\n",
      "Epoch 18/20  Iteration 31438/35720 Training loss: 0.6889 0.2137 sec/batch\n",
      "Epoch 18/20  Iteration 31439/35720 Training loss: 0.6889 0.2276 sec/batch\n",
      "Epoch 18/20  Iteration 31440/35720 Training loss: 0.6889 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 31441/35720 Training loss: 0.6889 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31442/35720 Training loss: 0.6889 0.2268 sec/batch\n",
      "Epoch 18/20  Iteration 31443/35720 Training loss: 0.6888 0.2130 sec/batch\n",
      "Epoch 18/20  Iteration 31444/35720 Training loss: 0.6888 0.2255 sec/batch\n",
      "Epoch 18/20  Iteration 31445/35720 Training loss: 0.6888 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31446/35720 Training loss: 0.6888 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 31447/35720 Training loss: 0.6888 0.2139 sec/batch\n",
      "Epoch 18/20  Iteration 31448/35720 Training loss: 0.6889 0.2299 sec/batch\n",
      "Epoch 18/20  Iteration 31449/35720 Training loss: 0.6888 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 31450/35720 Training loss: 0.6889 0.2313 sec/batch\n",
      "Epoch 18/20  Iteration 31451/35720 Training loss: 0.6889 0.2167 sec/batch\n",
      "Epoch 18/20  Iteration 31452/35720 Training loss: 0.6889 0.2058 sec/batch\n",
      "Epoch 18/20  Iteration 31453/35720 Training loss: 0.6889 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 31454/35720 Training loss: 0.6889 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 31455/35720 Training loss: 0.6889 0.2134 sec/batch\n",
      "Epoch 18/20  Iteration 31456/35720 Training loss: 0.6888 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 31457/35720 Training loss: 0.6889 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 31458/35720 Training loss: 0.6889 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31459/35720 Training loss: 0.6888 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 31460/35720 Training loss: 0.6888 0.2407 sec/batch\n",
      "Epoch 18/20  Iteration 31461/35720 Training loss: 0.6889 0.2296 sec/batch\n",
      "Epoch 18/20  Iteration 31462/35720 Training loss: 0.6889 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 31463/35720 Training loss: 0.6889 0.2096 sec/batch\n",
      "Epoch 18/20  Iteration 31464/35720 Training loss: 0.6889 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31465/35720 Training loss: 0.6890 0.2240 sec/batch\n",
      "Epoch 18/20  Iteration 31466/35720 Training loss: 0.6890 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 31467/35720 Training loss: 0.6890 0.2236 sec/batch\n",
      "Epoch 18/20  Iteration 31468/35720 Training loss: 0.6889 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 31469/35720 Training loss: 0.6889 0.2166 sec/batch\n",
      "Epoch 18/20  Iteration 31470/35720 Training loss: 0.6889 0.2206 sec/batch\n",
      "Epoch 18/20  Iteration 31471/35720 Training loss: 0.6889 0.2376 sec/batch\n",
      "Epoch 18/20  Iteration 31472/35720 Training loss: 0.6889 0.2265 sec/batch\n",
      "Epoch 18/20  Iteration 31473/35720 Training loss: 0.6889 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 31474/35720 Training loss: 0.6888 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31475/35720 Training loss: 0.6888 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 31476/35720 Training loss: 0.6888 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 31477/35720 Training loss: 0.6888 0.2210 sec/batch\n",
      "Epoch 18/20  Iteration 31478/35720 Training loss: 0.6888 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31479/35720 Training loss: 0.6888 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 31480/35720 Training loss: 0.6887 0.2194 sec/batch\n",
      "Epoch 18/20  Iteration 31481/35720 Training loss: 0.6887 0.2268 sec/batch\n",
      "Epoch 18/20  Iteration 31482/35720 Training loss: 0.6888 0.2130 sec/batch\n",
      "Epoch 18/20  Iteration 31483/35720 Training loss: 0.6888 0.2408 sec/batch\n",
      "Epoch 18/20  Iteration 31484/35720 Training loss: 0.6888 0.2242 sec/batch\n",
      "Epoch 18/20  Iteration 31485/35720 Training loss: 0.6888 0.2125 sec/batch\n",
      "Epoch 18/20  Iteration 31486/35720 Training loss: 0.6888 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 31487/35720 Training loss: 0.6888 0.2539 sec/batch\n",
      "Epoch 18/20  Iteration 31488/35720 Training loss: 0.6888 0.2194 sec/batch\n",
      "Epoch 18/20  Iteration 31489/35720 Training loss: 0.6887 0.2240 sec/batch\n",
      "Epoch 18/20  Iteration 31490/35720 Training loss: 0.6887 0.2176 sec/batch\n",
      "Epoch 18/20  Iteration 31491/35720 Training loss: 0.6887 0.2184 sec/batch\n",
      "Epoch 18/20  Iteration 31492/35720 Training loss: 0.6887 0.2208 sec/batch\n",
      "Epoch 18/20  Iteration 31493/35720 Training loss: 0.6886 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 31494/35720 Training loss: 0.6886 0.2354 sec/batch\n",
      "Epoch 18/20  Iteration 31495/35720 Training loss: 0.6885 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31496/35720 Training loss: 0.6885 0.2075 sec/batch\n",
      "Epoch 18/20  Iteration 31497/35720 Training loss: 0.6885 0.2168 sec/batch\n",
      "Epoch 18/20  Iteration 31498/35720 Training loss: 0.6885 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 31499/35720 Training loss: 0.6884 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 31500/35720 Training loss: 0.6884 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 31501/35720 Training loss: 0.6884 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 31502/35720 Training loss: 0.6884 0.2121 sec/batch\n",
      "Epoch 18/20  Iteration 31503/35720 Training loss: 0.6884 0.2170 sec/batch\n",
      "Epoch 18/20  Iteration 31504/35720 Training loss: 0.6884 0.2185 sec/batch\n",
      "Epoch 18/20  Iteration 31505/35720 Training loss: 0.6883 0.2226 sec/batch\n",
      "Epoch 18/20  Iteration 31506/35720 Training loss: 0.6883 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 31507/35720 Training loss: 0.6883 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 31508/35720 Training loss: 0.6883 0.2197 sec/batch\n",
      "Epoch 18/20  Iteration 31509/35720 Training loss: 0.6883 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 31510/35720 Training loss: 0.6882 0.2100 sec/batch\n",
      "Epoch 18/20  Iteration 31511/35720 Training loss: 0.6882 0.2203 sec/batch\n",
      "Epoch 18/20  Iteration 31512/35720 Training loss: 0.6882 0.2143 sec/batch\n",
      "Epoch 18/20  Iteration 31513/35720 Training loss: 0.6882 0.2234 sec/batch\n",
      "Epoch 18/20  Iteration 31514/35720 Training loss: 0.6882 0.2325 sec/batch\n",
      "Epoch 18/20  Iteration 31515/35720 Training loss: 0.6882 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 31516/35720 Training loss: 0.6882 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 31517/35720 Training loss: 0.6882 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 31518/35720 Training loss: 0.6882 0.2165 sec/batch\n",
      "Epoch 18/20  Iteration 31519/35720 Training loss: 0.6882 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31520/35720 Training loss: 0.6882 0.2221 sec/batch\n",
      "Epoch 18/20  Iteration 31521/35720 Training loss: 0.6882 0.2193 sec/batch\n",
      "Epoch 18/20  Iteration 31522/35720 Training loss: 0.6882 0.2292 sec/batch\n",
      "Epoch 18/20  Iteration 31523/35720 Training loss: 0.6881 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31524/35720 Training loss: 0.6881 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31525/35720 Training loss: 0.6882 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 31526/35720 Training loss: 0.6882 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 31527/35720 Training loss: 0.6881 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31528/35720 Training loss: 0.6881 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 31529/35720 Training loss: 0.6881 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 31530/35720 Training loss: 0.6881 0.2173 sec/batch\n",
      "Epoch 18/20  Iteration 31531/35720 Training loss: 0.6881 0.2437 sec/batch\n",
      "Epoch 18/20  Iteration 31532/35720 Training loss: 0.6881 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 31533/35720 Training loss: 0.6881 0.2232 sec/batch\n",
      "Epoch 18/20  Iteration 31534/35720 Training loss: 0.6882 0.2151 sec/batch\n",
      "Epoch 18/20  Iteration 31535/35720 Training loss: 0.6881 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31536/35720 Training loss: 0.6881 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 31537/35720 Training loss: 0.6882 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31538/35720 Training loss: 0.6882 0.2194 sec/batch\n",
      "Epoch 18/20  Iteration 31539/35720 Training loss: 0.6882 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 31540/35720 Training loss: 0.6882 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 31541/35720 Training loss: 0.6882 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31542/35720 Training loss: 0.6882 0.2386 sec/batch\n",
      "Epoch 18/20  Iteration 31543/35720 Training loss: 0.6882 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31544/35720 Training loss: 0.6882 0.2236 sec/batch\n",
      "Epoch 18/20  Iteration 31545/35720 Training loss: 0.6882 0.2165 sec/batch\n",
      "Epoch 18/20  Iteration 31546/35720 Training loss: 0.6882 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 31547/35720 Training loss: 0.6882 0.2135 sec/batch\n",
      "Epoch 18/20  Iteration 31548/35720 Training loss: 0.6881 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 31549/35720 Training loss: 0.6881 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 31550/35720 Training loss: 0.6882 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 31551/35720 Training loss: 0.6881 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 31552/35720 Training loss: 0.6881 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 31553/35720 Training loss: 0.6881 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 31554/35720 Training loss: 0.6881 0.2220 sec/batch\n",
      "Epoch 18/20  Iteration 31555/35720 Training loss: 0.6881 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 31556/35720 Training loss: 0.6881 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 31557/35720 Training loss: 0.6881 0.2151 sec/batch\n",
      "Epoch 18/20  Iteration 31558/35720 Training loss: 0.6881 0.2143 sec/batch\n",
      "Epoch 18/20  Iteration 31559/35720 Training loss: 0.6881 0.2242 sec/batch\n",
      "Epoch 18/20  Iteration 31560/35720 Training loss: 0.6881 0.2218 sec/batch\n",
      "Epoch 18/20  Iteration 31561/35720 Training loss: 0.6881 0.2162 sec/batch\n",
      "Epoch 18/20  Iteration 31562/35720 Training loss: 0.6881 0.2079 sec/batch\n",
      "Epoch 18/20  Iteration 31563/35720 Training loss: 0.6881 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 31564/35720 Training loss: 0.6881 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31565/35720 Training loss: 0.6880 0.2284 sec/batch\n",
      "Epoch 18/20  Iteration 31566/35720 Training loss: 0.6880 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 31567/35720 Training loss: 0.6879 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 31568/35720 Training loss: 0.6879 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31569/35720 Training loss: 0.6879 0.2130 sec/batch\n",
      "Epoch 18/20  Iteration 31570/35720 Training loss: 0.6879 0.2165 sec/batch\n",
      "Epoch 18/20  Iteration 31571/35720 Training loss: 0.6879 0.2168 sec/batch\n",
      "Epoch 18/20  Iteration 31572/35720 Training loss: 0.6879 0.2178 sec/batch\n",
      "Epoch 18/20  Iteration 31573/35720 Training loss: 0.6879 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 31574/35720 Training loss: 0.6879 0.2196 sec/batch\n",
      "Epoch 18/20  Iteration 31575/35720 Training loss: 0.6879 0.2350 sec/batch\n",
      "Epoch 18/20  Iteration 31576/35720 Training loss: 0.6879 0.2212 sec/batch\n",
      "Epoch 18/20  Iteration 31577/35720 Training loss: 0.6878 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 31578/35720 Training loss: 0.6878 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 31579/35720 Training loss: 0.6879 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 31580/35720 Training loss: 0.6878 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 31581/35720 Training loss: 0.6878 0.2176 sec/batch\n",
      "Epoch 18/20  Iteration 31582/35720 Training loss: 0.6878 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 31583/35720 Training loss: 0.6878 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 31584/35720 Training loss: 0.6878 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 31585/35720 Training loss: 0.6878 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 31586/35720 Training loss: 0.6877 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31587/35720 Training loss: 0.6877 0.2206 sec/batch\n",
      "Epoch 18/20  Iteration 31588/35720 Training loss: 0.6877 0.2291 sec/batch\n",
      "Epoch 18/20  Iteration 31589/35720 Training loss: 0.6877 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31590/35720 Training loss: 0.6877 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31591/35720 Training loss: 0.6877 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31592/35720 Training loss: 0.6877 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 31593/35720 Training loss: 0.6877 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31594/35720 Training loss: 0.6877 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 31595/35720 Training loss: 0.6877 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 31596/35720 Training loss: 0.6877 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 31597/35720 Training loss: 0.6877 0.2169 sec/batch\n",
      "Epoch 18/20  Iteration 31598/35720 Training loss: 0.6876 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 31599/35720 Training loss: 0.6876 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 31600/35720 Training loss: 0.6876 0.2171 sec/batch\n",
      "Validation loss: 1.64088 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 31601/35720 Training loss: 0.6878 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 31602/35720 Training loss: 0.6878 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31603/35720 Training loss: 0.6877 0.2274 sec/batch\n",
      "Epoch 18/20  Iteration 31604/35720 Training loss: 0.6877 0.2181 sec/batch\n",
      "Epoch 18/20  Iteration 31605/35720 Training loss: 0.6877 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 31606/35720 Training loss: 0.6877 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31607/35720 Training loss: 0.6877 0.2079 sec/batch\n",
      "Epoch 18/20  Iteration 31608/35720 Training loss: 0.6876 0.2238 sec/batch\n",
      "Epoch 18/20  Iteration 31609/35720 Training loss: 0.6876 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 31610/35720 Training loss: 0.6876 0.2238 sec/batch\n",
      "Epoch 18/20  Iteration 31611/35720 Training loss: 0.6876 0.2158 sec/batch\n",
      "Epoch 18/20  Iteration 31612/35720 Training loss: 0.6876 0.2071 sec/batch\n",
      "Epoch 18/20  Iteration 31613/35720 Training loss: 0.6875 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31614/35720 Training loss: 0.6876 0.2225 sec/batch\n",
      "Epoch 18/20  Iteration 31615/35720 Training loss: 0.6876 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31616/35720 Training loss: 0.6876 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 31617/35720 Training loss: 0.6876 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 31618/35720 Training loss: 0.6875 0.2100 sec/batch\n",
      "Epoch 18/20  Iteration 31619/35720 Training loss: 0.6875 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 31620/35720 Training loss: 0.6875 0.2305 sec/batch\n",
      "Epoch 18/20  Iteration 31621/35720 Training loss: 0.6875 0.2134 sec/batch\n",
      "Epoch 18/20  Iteration 31622/35720 Training loss: 0.6875 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 31623/35720 Training loss: 0.6875 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 31624/35720 Training loss: 0.6875 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31625/35720 Training loss: 0.6874 0.2256 sec/batch\n",
      "Epoch 18/20  Iteration 31626/35720 Training loss: 0.6874 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31627/35720 Training loss: 0.6874 0.2176 sec/batch\n",
      "Epoch 18/20  Iteration 31628/35720 Training loss: 0.6873 0.2318 sec/batch\n",
      "Epoch 18/20  Iteration 31629/35720 Training loss: 0.6873 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 31630/35720 Training loss: 0.6873 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31631/35720 Training loss: 0.6873 0.2328 sec/batch\n",
      "Epoch 18/20  Iteration 31632/35720 Training loss: 0.6874 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31633/35720 Training loss: 0.6874 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31634/35720 Training loss: 0.6873 0.2107 sec/batch\n",
      "Epoch 18/20  Iteration 31635/35720 Training loss: 0.6873 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31636/35720 Training loss: 0.6873 0.2165 sec/batch\n",
      "Epoch 18/20  Iteration 31637/35720 Training loss: 0.6873 0.2181 sec/batch\n",
      "Epoch 18/20  Iteration 31638/35720 Training loss: 0.6873 0.2183 sec/batch\n",
      "Epoch 18/20  Iteration 31639/35720 Training loss: 0.6872 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 31640/35720 Training loss: 0.6872 0.2167 sec/batch\n",
      "Epoch 18/20  Iteration 31641/35720 Training loss: 0.6871 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 31642/35720 Training loss: 0.6871 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31643/35720 Training loss: 0.6872 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 31644/35720 Training loss: 0.6871 0.2193 sec/batch\n",
      "Epoch 18/20  Iteration 31645/35720 Training loss: 0.6871 0.2119 sec/batch\n",
      "Epoch 18/20  Iteration 31646/35720 Training loss: 0.6871 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 31647/35720 Training loss: 0.6871 0.2283 sec/batch\n",
      "Epoch 18/20  Iteration 31648/35720 Training loss: 0.6871 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 31649/35720 Training loss: 0.6870 0.2219 sec/batch\n",
      "Epoch 18/20  Iteration 31650/35720 Training loss: 0.6870 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 31651/35720 Training loss: 0.6869 0.2100 sec/batch\n",
      "Epoch 18/20  Iteration 31652/35720 Training loss: 0.6870 0.2139 sec/batch\n",
      "Epoch 18/20  Iteration 31653/35720 Training loss: 0.6869 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 31654/35720 Training loss: 0.6868 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31655/35720 Training loss: 0.6869 0.2265 sec/batch\n",
      "Epoch 18/20  Iteration 31656/35720 Training loss: 0.6869 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 31657/35720 Training loss: 0.6869 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 31658/35720 Training loss: 0.6868 0.2233 sec/batch\n",
      "Epoch 18/20  Iteration 31659/35720 Training loss: 0.6869 0.2252 sec/batch\n",
      "Epoch 18/20  Iteration 31660/35720 Training loss: 0.6868 0.2076 sec/batch\n",
      "Epoch 18/20  Iteration 31661/35720 Training loss: 0.6868 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31662/35720 Training loss: 0.6868 0.2118 sec/batch\n",
      "Epoch 18/20  Iteration 31663/35720 Training loss: 0.6868 0.2132 sec/batch\n",
      "Epoch 18/20  Iteration 31664/35720 Training loss: 0.6868 0.2228 sec/batch\n",
      "Epoch 18/20  Iteration 31665/35720 Training loss: 0.6867 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 31666/35720 Training loss: 0.6867 0.2226 sec/batch\n",
      "Epoch 18/20  Iteration 31667/35720 Training loss: 0.6867 0.2106 sec/batch\n",
      "Epoch 18/20  Iteration 31668/35720 Training loss: 0.6867 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 31669/35720 Training loss: 0.6867 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 31670/35720 Training loss: 0.6867 0.2308 sec/batch\n",
      "Epoch 18/20  Iteration 31671/35720 Training loss: 0.6866 0.2310 sec/batch\n",
      "Epoch 18/20  Iteration 31672/35720 Training loss: 0.6866 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 31673/35720 Training loss: 0.6866 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31674/35720 Training loss: 0.6866 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 31675/35720 Training loss: 0.6866 0.2333 sec/batch\n",
      "Epoch 18/20  Iteration 31676/35720 Training loss: 0.6866 0.2134 sec/batch\n",
      "Epoch 18/20  Iteration 31677/35720 Training loss: 0.6866 0.2279 sec/batch\n",
      "Epoch 18/20  Iteration 31678/35720 Training loss: 0.6866 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 31679/35720 Training loss: 0.6865 0.2482 sec/batch\n",
      "Epoch 18/20  Iteration 31680/35720 Training loss: 0.6865 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 31681/35720 Training loss: 0.6865 0.2204 sec/batch\n",
      "Epoch 18/20  Iteration 31682/35720 Training loss: 0.6864 0.2272 sec/batch\n",
      "Epoch 18/20  Iteration 31683/35720 Training loss: 0.6864 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31684/35720 Training loss: 0.6864 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 31685/35720 Training loss: 0.6864 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31686/35720 Training loss: 0.6864 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 31687/35720 Training loss: 0.6864 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 31688/35720 Training loss: 0.6864 0.2294 sec/batch\n",
      "Epoch 18/20  Iteration 31689/35720 Training loss: 0.6864 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31690/35720 Training loss: 0.6864 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 31691/35720 Training loss: 0.6864 0.2279 sec/batch\n",
      "Epoch 18/20  Iteration 31692/35720 Training loss: 0.6864 0.2186 sec/batch\n",
      "Epoch 18/20  Iteration 31693/35720 Training loss: 0.6864 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31694/35720 Training loss: 0.6864 0.2186 sec/batch\n",
      "Epoch 18/20  Iteration 31695/35720 Training loss: 0.6864 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 31696/35720 Training loss: 0.6864 0.2194 sec/batch\n",
      "Epoch 18/20  Iteration 31697/35720 Training loss: 0.6864 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 31698/35720 Training loss: 0.6864 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 31699/35720 Training loss: 0.6864 0.2194 sec/batch\n",
      "Epoch 18/20  Iteration 31700/35720 Training loss: 0.6864 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 31701/35720 Training loss: 0.6864 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 31702/35720 Training loss: 0.6864 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 31703/35720 Training loss: 0.6864 0.2222 sec/batch\n",
      "Epoch 18/20  Iteration 31704/35720 Training loss: 0.6864 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31705/35720 Training loss: 0.6864 0.2211 sec/batch\n",
      "Epoch 18/20  Iteration 31706/35720 Training loss: 0.6863 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 31707/35720 Training loss: 0.6863 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31708/35720 Training loss: 0.6863 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 31709/35720 Training loss: 0.6863 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 31710/35720 Training loss: 0.6863 0.2189 sec/batch\n",
      "Epoch 18/20  Iteration 31711/35720 Training loss: 0.6863 0.2202 sec/batch\n",
      "Epoch 18/20  Iteration 31712/35720 Training loss: 0.6863 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 31713/35720 Training loss: 0.6863 0.2776 sec/batch\n",
      "Epoch 18/20  Iteration 31714/35720 Training loss: 0.6864 0.2233 sec/batch\n",
      "Epoch 18/20  Iteration 31715/35720 Training loss: 0.6864 0.2142 sec/batch\n",
      "Epoch 18/20  Iteration 31716/35720 Training loss: 0.6864 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 31717/35720 Training loss: 0.6863 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31718/35720 Training loss: 0.6863 0.2123 sec/batch\n",
      "Epoch 18/20  Iteration 31719/35720 Training loss: 0.6863 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 31720/35720 Training loss: 0.6863 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31721/35720 Training loss: 0.6863 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 31722/35720 Training loss: 0.6862 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 31723/35720 Training loss: 0.6862 0.2227 sec/batch\n",
      "Epoch 18/20  Iteration 31724/35720 Training loss: 0.6862 0.2119 sec/batch\n",
      "Epoch 18/20  Iteration 31725/35720 Training loss: 0.6862 0.2252 sec/batch\n",
      "Epoch 18/20  Iteration 31726/35720 Training loss: 0.6862 0.2125 sec/batch\n",
      "Epoch 18/20  Iteration 31727/35720 Training loss: 0.6861 0.2253 sec/batch\n",
      "Epoch 18/20  Iteration 31728/35720 Training loss: 0.6861 0.2101 sec/batch\n",
      "Epoch 18/20  Iteration 31729/35720 Training loss: 0.6861 0.2137 sec/batch\n",
      "Epoch 18/20  Iteration 31730/35720 Training loss: 0.6861 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 31731/35720 Training loss: 0.6861 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 31732/35720 Training loss: 0.6861 0.2476 sec/batch\n",
      "Epoch 18/20  Iteration 31733/35720 Training loss: 0.6861 0.2132 sec/batch\n",
      "Epoch 18/20  Iteration 31734/35720 Training loss: 0.6861 0.2055 sec/batch\n",
      "Epoch 18/20  Iteration 31735/35720 Training loss: 0.6861 0.2169 sec/batch\n",
      "Epoch 18/20  Iteration 31736/35720 Training loss: 0.6860 0.2179 sec/batch\n",
      "Epoch 18/20  Iteration 31737/35720 Training loss: 0.6860 0.2120 sec/batch\n",
      "Epoch 18/20  Iteration 31738/35720 Training loss: 0.6860 0.2279 sec/batch\n",
      "Epoch 18/20  Iteration 31739/35720 Training loss: 0.6860 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31740/35720 Training loss: 0.6860 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 31741/35720 Training loss: 0.6860 0.2164 sec/batch\n",
      "Epoch 18/20  Iteration 31742/35720 Training loss: 0.6860 0.2178 sec/batch\n",
      "Epoch 18/20  Iteration 31743/35720 Training loss: 0.6860 0.2174 sec/batch\n",
      "Epoch 18/20  Iteration 31744/35720 Training loss: 0.6860 0.2194 sec/batch\n",
      "Epoch 18/20  Iteration 31745/35720 Training loss: 0.6860 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 31746/35720 Training loss: 0.6860 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 31747/35720 Training loss: 0.6860 0.2409 sec/batch\n",
      "Epoch 18/20  Iteration 31748/35720 Training loss: 0.6860 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31749/35720 Training loss: 0.6860 0.2280 sec/batch\n",
      "Epoch 18/20  Iteration 31750/35720 Training loss: 0.6860 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31751/35720 Training loss: 0.6861 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 31752/35720 Training loss: 0.6860 0.2176 sec/batch\n",
      "Epoch 18/20  Iteration 31753/35720 Training loss: 0.6860 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 31754/35720 Training loss: 0.6860 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31755/35720 Training loss: 0.6859 0.2236 sec/batch\n",
      "Epoch 18/20  Iteration 31756/35720 Training loss: 0.6859 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31757/35720 Training loss: 0.6859 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 31758/35720 Training loss: 0.6858 0.2326 sec/batch\n",
      "Epoch 18/20  Iteration 31759/35720 Training loss: 0.6858 0.2230 sec/batch\n",
      "Epoch 18/20  Iteration 31760/35720 Training loss: 0.6858 0.2275 sec/batch\n",
      "Epoch 18/20  Iteration 31761/35720 Training loss: 0.6857 0.2118 sec/batch\n",
      "Epoch 18/20  Iteration 31762/35720 Training loss: 0.6857 0.2052 sec/batch\n",
      "Epoch 18/20  Iteration 31763/35720 Training loss: 0.6857 0.2192 sec/batch\n",
      "Epoch 18/20  Iteration 31764/35720 Training loss: 0.6856 0.2075 sec/batch\n",
      "Epoch 18/20  Iteration 31765/35720 Training loss: 0.6856 0.2181 sec/batch\n",
      "Epoch 18/20  Iteration 31766/35720 Training loss: 0.6856 0.2314 sec/batch\n",
      "Epoch 18/20  Iteration 31767/35720 Training loss: 0.6856 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 31768/35720 Training loss: 0.6855 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 31769/35720 Training loss: 0.6855 0.2399 sec/batch\n",
      "Epoch 18/20  Iteration 31770/35720 Training loss: 0.6855 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31771/35720 Training loss: 0.6855 0.2189 sec/batch\n",
      "Epoch 18/20  Iteration 31772/35720 Training loss: 0.6855 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31773/35720 Training loss: 0.6855 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31774/35720 Training loss: 0.6855 0.2256 sec/batch\n",
      "Epoch 18/20  Iteration 31775/35720 Training loss: 0.6855 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 31776/35720 Training loss: 0.6854 0.2114 sec/batch\n",
      "Epoch 18/20  Iteration 31777/35720 Training loss: 0.6854 0.2163 sec/batch\n",
      "Epoch 18/20  Iteration 31778/35720 Training loss: 0.6854 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 31779/35720 Training loss: 0.6854 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31780/35720 Training loss: 0.6854 0.2225 sec/batch\n",
      "Epoch 18/20  Iteration 31781/35720 Training loss: 0.6854 0.2182 sec/batch\n",
      "Epoch 18/20  Iteration 31782/35720 Training loss: 0.6854 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 31783/35720 Training loss: 0.6854 0.2203 sec/batch\n",
      "Epoch 18/20  Iteration 31784/35720 Training loss: 0.6854 0.2141 sec/batch\n",
      "Epoch 18/20  Iteration 31785/35720 Training loss: 0.6854 0.2216 sec/batch\n",
      "Epoch 18/20  Iteration 31786/35720 Training loss: 0.6854 0.2261 sec/batch\n",
      "Epoch 18/20  Iteration 31787/35720 Training loss: 0.6854 0.2159 sec/batch\n",
      "Epoch 18/20  Iteration 31788/35720 Training loss: 0.6854 0.2373 sec/batch\n",
      "Epoch 18/20  Iteration 31789/35720 Training loss: 0.6854 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 31790/35720 Training loss: 0.6854 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 31791/35720 Training loss: 0.6854 0.2284 sec/batch\n",
      "Epoch 18/20  Iteration 31792/35720 Training loss: 0.6854 0.2139 sec/batch\n",
      "Epoch 18/20  Iteration 31793/35720 Training loss: 0.6854 0.2256 sec/batch\n",
      "Epoch 18/20  Iteration 31794/35720 Training loss: 0.6854 0.2053 sec/batch\n",
      "Epoch 18/20  Iteration 31795/35720 Training loss: 0.6854 0.2225 sec/batch\n",
      "Epoch 18/20  Iteration 31796/35720 Training loss: 0.6854 0.2389 sec/batch\n",
      "Epoch 18/20  Iteration 31797/35720 Training loss: 0.6854 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 31798/35720 Training loss: 0.6853 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 31799/35720 Training loss: 0.6853 0.2175 sec/batch\n",
      "Epoch 18/20  Iteration 31800/35720 Training loss: 0.6853 0.2086 sec/batch\n",
      "Validation loss: 1.63036 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 31801/35720 Training loss: 0.6857 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31802/35720 Training loss: 0.6857 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 31803/35720 Training loss: 0.6858 0.2324 sec/batch\n",
      "Epoch 18/20  Iteration 31804/35720 Training loss: 0.6858 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 31805/35720 Training loss: 0.6857 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 31806/35720 Training loss: 0.6857 0.2125 sec/batch\n",
      "Epoch 18/20  Iteration 31807/35720 Training loss: 0.6857 0.2282 sec/batch\n",
      "Epoch 18/20  Iteration 31808/35720 Training loss: 0.6858 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 31809/35720 Training loss: 0.6857 0.2107 sec/batch\n",
      "Epoch 18/20  Iteration 31810/35720 Training loss: 0.6857 0.2107 sec/batch\n",
      "Epoch 18/20  Iteration 31811/35720 Training loss: 0.6857 0.2288 sec/batch\n",
      "Epoch 18/20  Iteration 31812/35720 Training loss: 0.6857 0.2186 sec/batch\n",
      "Epoch 18/20  Iteration 31813/35720 Training loss: 0.6856 0.2293 sec/batch\n",
      "Epoch 18/20  Iteration 31814/35720 Training loss: 0.6857 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 31815/35720 Training loss: 0.6857 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31816/35720 Training loss: 0.6857 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 31817/35720 Training loss: 0.6856 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 31818/35720 Training loss: 0.6857 0.2289 sec/batch\n",
      "Epoch 18/20  Iteration 31819/35720 Training loss: 0.6857 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 31820/35720 Training loss: 0.6857 0.2198 sec/batch\n",
      "Epoch 18/20  Iteration 31821/35720 Training loss: 0.6857 0.2267 sec/batch\n",
      "Epoch 18/20  Iteration 31822/35720 Training loss: 0.6857 0.2175 sec/batch\n",
      "Epoch 18/20  Iteration 31823/35720 Training loss: 0.6857 0.2225 sec/batch\n",
      "Epoch 18/20  Iteration 31824/35720 Training loss: 0.6858 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 31825/35720 Training loss: 0.6858 0.2219 sec/batch\n",
      "Epoch 18/20  Iteration 31826/35720 Training loss: 0.6858 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 31827/35720 Training loss: 0.6858 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31828/35720 Training loss: 0.6858 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31829/35720 Training loss: 0.6858 0.2338 sec/batch\n",
      "Epoch 18/20  Iteration 31830/35720 Training loss: 0.6858 0.2249 sec/batch\n",
      "Epoch 18/20  Iteration 31831/35720 Training loss: 0.6857 0.2265 sec/batch\n",
      "Epoch 18/20  Iteration 31832/35720 Training loss: 0.6857 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31833/35720 Training loss: 0.6857 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 31834/35720 Training loss: 0.6857 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 31835/35720 Training loss: 0.6857 0.2314 sec/batch\n",
      "Epoch 18/20  Iteration 31836/35720 Training loss: 0.6857 0.2386 sec/batch\n",
      "Epoch 18/20  Iteration 31837/35720 Training loss: 0.6856 0.2161 sec/batch\n",
      "Epoch 18/20  Iteration 31838/35720 Training loss: 0.6856 0.2168 sec/batch\n",
      "Epoch 18/20  Iteration 31839/35720 Training loss: 0.6855 0.2079 sec/batch\n",
      "Epoch 18/20  Iteration 31840/35720 Training loss: 0.6855 0.2203 sec/batch\n",
      "Epoch 18/20  Iteration 31841/35720 Training loss: 0.6855 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 31842/35720 Training loss: 0.6855 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 31843/35720 Training loss: 0.6855 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 31844/35720 Training loss: 0.6854 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 31845/35720 Training loss: 0.6854 0.2122 sec/batch\n",
      "Epoch 18/20  Iteration 31846/35720 Training loss: 0.6853 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 31847/35720 Training loss: 0.6853 0.2175 sec/batch\n",
      "Epoch 18/20  Iteration 31848/35720 Training loss: 0.6853 0.2201 sec/batch\n",
      "Epoch 18/20  Iteration 31849/35720 Training loss: 0.6853 0.2164 sec/batch\n",
      "Epoch 18/20  Iteration 31850/35720 Training loss: 0.6853 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 31851/35720 Training loss: 0.6853 0.2052 sec/batch\n",
      "Epoch 18/20  Iteration 31852/35720 Training loss: 0.6853 0.2103 sec/batch\n",
      "Epoch 18/20  Iteration 31853/35720 Training loss: 0.6853 0.2217 sec/batch\n",
      "Epoch 18/20  Iteration 31854/35720 Training loss: 0.6852 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 31855/35720 Training loss: 0.6852 0.2059 sec/batch\n",
      "Epoch 18/20  Iteration 31856/35720 Training loss: 0.6852 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 31857/35720 Training loss: 0.6852 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 31858/35720 Training loss: 0.6852 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 31859/35720 Training loss: 0.6852 0.2284 sec/batch\n",
      "Epoch 18/20  Iteration 31860/35720 Training loss: 0.6851 0.2229 sec/batch\n",
      "Epoch 18/20  Iteration 31861/35720 Training loss: 0.6851 0.2159 sec/batch\n",
      "Epoch 18/20  Iteration 31862/35720 Training loss: 0.6851 0.2170 sec/batch\n",
      "Epoch 18/20  Iteration 31863/35720 Training loss: 0.6850 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31864/35720 Training loss: 0.6850 0.2166 sec/batch\n",
      "Epoch 18/20  Iteration 31865/35720 Training loss: 0.6850 0.2202 sec/batch\n",
      "Epoch 18/20  Iteration 31866/35720 Training loss: 0.6850 0.2068 sec/batch\n",
      "Epoch 18/20  Iteration 31867/35720 Training loss: 0.6850 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 31868/35720 Training loss: 0.6850 0.2250 sec/batch\n",
      "Epoch 18/20  Iteration 31869/35720 Training loss: 0.6850 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 31870/35720 Training loss: 0.6850 0.2267 sec/batch\n",
      "Epoch 18/20  Iteration 31871/35720 Training loss: 0.6850 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 31872/35720 Training loss: 0.6849 0.2148 sec/batch\n",
      "Epoch 18/20  Iteration 31873/35720 Training loss: 0.6850 0.2227 sec/batch\n",
      "Epoch 18/20  Iteration 31874/35720 Training loss: 0.6849 0.2177 sec/batch\n",
      "Epoch 18/20  Iteration 31875/35720 Training loss: 0.6849 0.2161 sec/batch\n",
      "Epoch 18/20  Iteration 31876/35720 Training loss: 0.6849 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 31877/35720 Training loss: 0.6849 0.2052 sec/batch\n",
      "Epoch 18/20  Iteration 31878/35720 Training loss: 0.6849 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 31879/35720 Training loss: 0.6849 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 31880/35720 Training loss: 0.6849 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 31881/35720 Training loss: 0.6849 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 31882/35720 Training loss: 0.6849 0.2215 sec/batch\n",
      "Epoch 18/20  Iteration 31883/35720 Training loss: 0.6849 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31884/35720 Training loss: 0.6849 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31885/35720 Training loss: 0.6849 0.2210 sec/batch\n",
      "Epoch 18/20  Iteration 31886/35720 Training loss: 0.6849 0.2182 sec/batch\n",
      "Epoch 18/20  Iteration 31887/35720 Training loss: 0.6849 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 31888/35720 Training loss: 0.6849 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 31889/35720 Training loss: 0.6849 0.2134 sec/batch\n",
      "Epoch 18/20  Iteration 31890/35720 Training loss: 0.6849 0.2066 sec/batch\n",
      "Epoch 18/20  Iteration 31891/35720 Training loss: 0.6849 0.2152 sec/batch\n",
      "Epoch 18/20  Iteration 31892/35720 Training loss: 0.6850 0.2333 sec/batch\n",
      "Epoch 18/20  Iteration 31893/35720 Training loss: 0.6850 0.2114 sec/batch\n",
      "Epoch 18/20  Iteration 31894/35720 Training loss: 0.6850 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 31895/35720 Training loss: 0.6850 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 31896/35720 Training loss: 0.6850 0.2374 sec/batch\n",
      "Epoch 18/20  Iteration 31897/35720 Training loss: 0.6849 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 31898/35720 Training loss: 0.6849 0.2209 sec/batch\n",
      "Epoch 18/20  Iteration 31899/35720 Training loss: 0.6849 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 31900/35720 Training loss: 0.6849 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31901/35720 Training loss: 0.6849 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 31902/35720 Training loss: 0.6848 0.2161 sec/batch\n",
      "Epoch 18/20  Iteration 31903/35720 Training loss: 0.6848 0.2241 sec/batch\n",
      "Epoch 18/20  Iteration 31904/35720 Training loss: 0.6848 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31905/35720 Training loss: 0.6848 0.2077 sec/batch\n",
      "Epoch 18/20  Iteration 31906/35720 Training loss: 0.6848 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31907/35720 Training loss: 0.6847 0.2352 sec/batch\n",
      "Epoch 18/20  Iteration 31908/35720 Training loss: 0.6847 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31909/35720 Training loss: 0.6847 0.2227 sec/batch\n",
      "Epoch 18/20  Iteration 31910/35720 Training loss: 0.6847 0.2118 sec/batch\n",
      "Epoch 18/20  Iteration 31911/35720 Training loss: 0.6847 0.2128 sec/batch\n",
      "Epoch 18/20  Iteration 31912/35720 Training loss: 0.6847 0.2230 sec/batch\n",
      "Epoch 18/20  Iteration 31913/35720 Training loss: 0.6847 0.2106 sec/batch\n",
      "Epoch 18/20  Iteration 31914/35720 Training loss: 0.6847 0.2142 sec/batch\n",
      "Epoch 18/20  Iteration 31915/35720 Training loss: 0.6847 0.2254 sec/batch\n",
      "Epoch 18/20  Iteration 31916/35720 Training loss: 0.6847 0.2072 sec/batch\n",
      "Epoch 18/20  Iteration 31917/35720 Training loss: 0.6846 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31918/35720 Training loss: 0.6846 0.2421 sec/batch\n",
      "Epoch 18/20  Iteration 31919/35720 Training loss: 0.6846 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31920/35720 Training loss: 0.6846 0.2263 sec/batch\n",
      "Epoch 18/20  Iteration 31921/35720 Training loss: 0.6845 0.2076 sec/batch\n",
      "Epoch 18/20  Iteration 31922/35720 Training loss: 0.6845 0.2065 sec/batch\n",
      "Epoch 18/20  Iteration 31923/35720 Training loss: 0.6845 0.2214 sec/batch\n",
      "Epoch 18/20  Iteration 31924/35720 Training loss: 0.6845 0.2180 sec/batch\n",
      "Epoch 18/20  Iteration 31925/35720 Training loss: 0.6845 0.2208 sec/batch\n",
      "Epoch 18/20  Iteration 31926/35720 Training loss: 0.6845 0.2102 sec/batch\n",
      "Epoch 18/20  Iteration 31927/35720 Training loss: 0.6845 0.2278 sec/batch\n",
      "Epoch 18/20  Iteration 31928/35720 Training loss: 0.6844 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 31929/35720 Training loss: 0.6844 0.2235 sec/batch\n",
      "Epoch 18/20  Iteration 31930/35720 Training loss: 0.6844 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 31931/35720 Training loss: 0.6844 0.2266 sec/batch\n",
      "Epoch 18/20  Iteration 31932/35720 Training loss: 0.6844 0.2082 sec/batch\n",
      "Epoch 18/20  Iteration 31933/35720 Training loss: 0.6844 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 31934/35720 Training loss: 0.6844 0.2159 sec/batch\n",
      "Epoch 18/20  Iteration 31935/35720 Training loss: 0.6843 0.2170 sec/batch\n",
      "Epoch 18/20  Iteration 31936/35720 Training loss: 0.6843 0.2887 sec/batch\n",
      "Epoch 18/20  Iteration 31937/35720 Training loss: 0.6843 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 31938/35720 Training loss: 0.6843 0.2185 sec/batch\n",
      "Epoch 18/20  Iteration 31939/35720 Training loss: 0.6842 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 31940/35720 Training loss: 0.6842 0.2252 sec/batch\n",
      "Epoch 18/20  Iteration 31941/35720 Training loss: 0.6843 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31942/35720 Training loss: 0.6842 0.2140 sec/batch\n",
      "Epoch 18/20  Iteration 31943/35720 Training loss: 0.6843 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 31944/35720 Training loss: 0.6843 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31945/35720 Training loss: 0.6843 0.2055 sec/batch\n",
      "Epoch 18/20  Iteration 31946/35720 Training loss: 0.6842 0.2130 sec/batch\n",
      "Epoch 18/20  Iteration 31947/35720 Training loss: 0.6842 0.2167 sec/batch\n",
      "Epoch 18/20  Iteration 31948/35720 Training loss: 0.6842 0.2239 sec/batch\n",
      "Epoch 18/20  Iteration 31949/35720 Training loss: 0.6842 0.2183 sec/batch\n",
      "Epoch 18/20  Iteration 31950/35720 Training loss: 0.6841 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 31951/35720 Training loss: 0.6841 0.2283 sec/batch\n",
      "Epoch 18/20  Iteration 31952/35720 Training loss: 0.6841 0.2118 sec/batch\n",
      "Epoch 18/20  Iteration 31953/35720 Training loss: 0.6840 0.2202 sec/batch\n",
      "Epoch 18/20  Iteration 31954/35720 Training loss: 0.6840 0.2106 sec/batch\n",
      "Epoch 18/20  Iteration 31955/35720 Training loss: 0.6840 0.2060 sec/batch\n",
      "Epoch 18/20  Iteration 31956/35720 Training loss: 0.6840 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 31957/35720 Training loss: 0.6839 0.2199 sec/batch\n",
      "Epoch 18/20  Iteration 31958/35720 Training loss: 0.6839 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 31959/35720 Training loss: 0.6839 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 31960/35720 Training loss: 0.6838 0.2075 sec/batch\n",
      "Epoch 18/20  Iteration 31961/35720 Training loss: 0.6838 0.2090 sec/batch\n",
      "Epoch 18/20  Iteration 31962/35720 Training loss: 0.6838 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31963/35720 Training loss: 0.6838 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 31964/35720 Training loss: 0.6837 0.2162 sec/batch\n",
      "Epoch 18/20  Iteration 31965/35720 Training loss: 0.6837 0.2114 sec/batch\n",
      "Epoch 18/20  Iteration 31966/35720 Training loss: 0.6837 0.2123 sec/batch\n",
      "Epoch 18/20  Iteration 31967/35720 Training loss: 0.6837 0.2113 sec/batch\n",
      "Epoch 18/20  Iteration 31968/35720 Training loss: 0.6837 0.2259 sec/batch\n",
      "Epoch 18/20  Iteration 31969/35720 Training loss: 0.6836 0.2102 sec/batch\n",
      "Epoch 18/20  Iteration 31970/35720 Training loss: 0.6836 0.2243 sec/batch\n",
      "Epoch 18/20  Iteration 31971/35720 Training loss: 0.6836 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 31972/35720 Training loss: 0.6835 0.2079 sec/batch\n",
      "Epoch 18/20  Iteration 31973/35720 Training loss: 0.6835 0.2379 sec/batch\n",
      "Epoch 18/20  Iteration 31974/35720 Training loss: 0.6835 0.2183 sec/batch\n",
      "Epoch 18/20  Iteration 31975/35720 Training loss: 0.6835 0.2294 sec/batch\n",
      "Epoch 18/20  Iteration 31976/35720 Training loss: 0.6835 0.2057 sec/batch\n",
      "Epoch 18/20  Iteration 31977/35720 Training loss: 0.6835 0.2073 sec/batch\n",
      "Epoch 18/20  Iteration 31978/35720 Training loss: 0.6835 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 31979/35720 Training loss: 0.6835 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 31980/35720 Training loss: 0.6835 0.2092 sec/batch\n",
      "Epoch 18/20  Iteration 31981/35720 Training loss: 0.6835 0.2272 sec/batch\n",
      "Epoch 18/20  Iteration 31982/35720 Training loss: 0.6835 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 31983/35720 Training loss: 0.6834 0.2061 sec/batch\n",
      "Epoch 18/20  Iteration 31984/35720 Training loss: 0.6834 0.2206 sec/batch\n",
      "Epoch 18/20  Iteration 31985/35720 Training loss: 0.6834 0.2193 sec/batch\n",
      "Epoch 18/20  Iteration 31986/35720 Training loss: 0.6834 0.2200 sec/batch\n",
      "Epoch 18/20  Iteration 31987/35720 Training loss: 0.6834 0.2107 sec/batch\n",
      "Epoch 18/20  Iteration 31988/35720 Training loss: 0.6834 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 31989/35720 Training loss: 0.6834 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 31990/35720 Training loss: 0.6834 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31991/35720 Training loss: 0.6834 0.2233 sec/batch\n",
      "Epoch 18/20  Iteration 31992/35720 Training loss: 0.6834 0.2690 sec/batch\n",
      "Epoch 18/20  Iteration 31993/35720 Training loss: 0.6834 0.2053 sec/batch\n",
      "Epoch 18/20  Iteration 31994/35720 Training loss: 0.6833 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 31995/35720 Training loss: 0.6833 0.2268 sec/batch\n",
      "Epoch 18/20  Iteration 31996/35720 Training loss: 0.6834 0.2115 sec/batch\n",
      "Epoch 18/20  Iteration 31997/35720 Training loss: 0.6833 0.2310 sec/batch\n",
      "Epoch 18/20  Iteration 31998/35720 Training loss: 0.6834 0.2221 sec/batch\n",
      "Epoch 18/20  Iteration 31999/35720 Training loss: 0.6833 0.2076 sec/batch\n",
      "Epoch 18/20  Iteration 32000/35720 Training loss: 0.6833 0.2121 sec/batch\n",
      "Validation loss: 1.64697 Saving checkpoint!\n",
      "Epoch 18/20  Iteration 32001/35720 Training loss: 0.6836 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 32002/35720 Training loss: 0.6835 0.2192 sec/batch\n",
      "Epoch 18/20  Iteration 32003/35720 Training loss: 0.6835 0.2144 sec/batch\n",
      "Epoch 18/20  Iteration 32004/35720 Training loss: 0.6836 0.2107 sec/batch\n",
      "Epoch 18/20  Iteration 32005/35720 Training loss: 0.6835 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 32006/35720 Training loss: 0.6835 0.2249 sec/batch\n",
      "Epoch 18/20  Iteration 32007/35720 Training loss: 0.6835 0.2198 sec/batch\n",
      "Epoch 18/20  Iteration 32008/35720 Training loss: 0.6835 0.2371 sec/batch\n",
      "Epoch 18/20  Iteration 32009/35720 Training loss: 0.6835 0.2063 sec/batch\n",
      "Epoch 18/20  Iteration 32010/35720 Training loss: 0.6835 0.2093 sec/batch\n",
      "Epoch 18/20  Iteration 32011/35720 Training loss: 0.6835 0.2190 sec/batch\n",
      "Epoch 18/20  Iteration 32012/35720 Training loss: 0.6835 0.2329 sec/batch\n",
      "Epoch 18/20  Iteration 32013/35720 Training loss: 0.6835 0.2116 sec/batch\n",
      "Epoch 18/20  Iteration 32014/35720 Training loss: 0.6835 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 32015/35720 Training loss: 0.6835 0.2111 sec/batch\n",
      "Epoch 18/20  Iteration 32016/35720 Training loss: 0.6835 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 32017/35720 Training loss: 0.6835 0.2126 sec/batch\n",
      "Epoch 18/20  Iteration 32018/35720 Training loss: 0.6835 0.2085 sec/batch\n",
      "Epoch 18/20  Iteration 32019/35720 Training loss: 0.6836 0.2195 sec/batch\n",
      "Epoch 18/20  Iteration 32020/35720 Training loss: 0.6836 0.2222 sec/batch\n",
      "Epoch 18/20  Iteration 32021/35720 Training loss: 0.6835 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 32022/35720 Training loss: 0.6835 0.2155 sec/batch\n",
      "Epoch 18/20  Iteration 32023/35720 Training loss: 0.6835 0.2172 sec/batch\n",
      "Epoch 18/20  Iteration 32024/35720 Training loss: 0.6835 0.2129 sec/batch\n",
      "Epoch 18/20  Iteration 32025/35720 Training loss: 0.6835 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 32026/35720 Training loss: 0.6835 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 32027/35720 Training loss: 0.6835 0.2257 sec/batch\n",
      "Epoch 18/20  Iteration 32028/35720 Training loss: 0.6835 0.2143 sec/batch\n",
      "Epoch 18/20  Iteration 32029/35720 Training loss: 0.6835 0.2188 sec/batch\n",
      "Epoch 18/20  Iteration 32030/35720 Training loss: 0.6835 0.2213 sec/batch\n",
      "Epoch 18/20  Iteration 32031/35720 Training loss: 0.6836 0.2117 sec/batch\n",
      "Epoch 18/20  Iteration 32032/35720 Training loss: 0.6836 0.2091 sec/batch\n",
      "Epoch 18/20  Iteration 32033/35720 Training loss: 0.6836 0.2102 sec/batch\n",
      "Epoch 18/20  Iteration 32034/35720 Training loss: 0.6836 0.2359 sec/batch\n",
      "Epoch 18/20  Iteration 32035/35720 Training loss: 0.6836 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 32036/35720 Training loss: 0.6837 0.2118 sec/batch\n",
      "Epoch 18/20  Iteration 32037/35720 Training loss: 0.6836 0.2203 sec/batch\n",
      "Epoch 18/20  Iteration 32038/35720 Training loss: 0.6836 0.2145 sec/batch\n",
      "Epoch 18/20  Iteration 32039/35720 Training loss: 0.6836 0.2199 sec/batch\n",
      "Epoch 18/20  Iteration 32040/35720 Training loss: 0.6836 0.2203 sec/batch\n",
      "Epoch 18/20  Iteration 32041/35720 Training loss: 0.6836 0.2164 sec/batch\n",
      "Epoch 18/20  Iteration 32042/35720 Training loss: 0.6835 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 32043/35720 Training loss: 0.6835 0.2069 sec/batch\n",
      "Epoch 18/20  Iteration 32044/35720 Training loss: 0.6834 0.2087 sec/batch\n",
      "Epoch 18/20  Iteration 32045/35720 Training loss: 0.6834 0.2168 sec/batch\n",
      "Epoch 18/20  Iteration 32046/35720 Training loss: 0.6835 0.2224 sec/batch\n",
      "Epoch 18/20  Iteration 32047/35720 Training loss: 0.6835 0.2186 sec/batch\n",
      "Epoch 18/20  Iteration 32048/35720 Training loss: 0.6835 0.2228 sec/batch\n",
      "Epoch 18/20  Iteration 32049/35720 Training loss: 0.6835 0.2095 sec/batch\n",
      "Epoch 18/20  Iteration 32050/35720 Training loss: 0.6835 0.2296 sec/batch\n",
      "Epoch 18/20  Iteration 32051/35720 Training loss: 0.6835 0.2084 sec/batch\n",
      "Epoch 18/20  Iteration 32052/35720 Training loss: 0.6834 0.2131 sec/batch\n",
      "Epoch 18/20  Iteration 32053/35720 Training loss: 0.6834 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 32054/35720 Training loss: 0.6834 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 32055/35720 Training loss: 0.6834 0.2138 sec/batch\n",
      "Epoch 18/20  Iteration 32056/35720 Training loss: 0.6834 0.2067 sec/batch\n",
      "Epoch 18/20  Iteration 32057/35720 Training loss: 0.6834 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 32058/35720 Training loss: 0.6834 0.2245 sec/batch\n",
      "Epoch 18/20  Iteration 32059/35720 Training loss: 0.6834 0.2369 sec/batch\n",
      "Epoch 18/20  Iteration 32060/35720 Training loss: 0.6834 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 32061/35720 Training loss: 0.6834 0.2183 sec/batch\n",
      "Epoch 18/20  Iteration 32062/35720 Training loss: 0.6834 0.2178 sec/batch\n",
      "Epoch 18/20  Iteration 32063/35720 Training loss: 0.6834 0.2149 sec/batch\n",
      "Epoch 18/20  Iteration 32064/35720 Training loss: 0.6834 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 32065/35720 Training loss: 0.6833 0.2100 sec/batch\n",
      "Epoch 18/20  Iteration 32066/35720 Training loss: 0.6833 0.2081 sec/batch\n",
      "Epoch 18/20  Iteration 32067/35720 Training loss: 0.6833 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 32068/35720 Training loss: 0.6833 0.2119 sec/batch\n",
      "Epoch 18/20  Iteration 32069/35720 Training loss: 0.6833 0.2198 sec/batch\n",
      "Epoch 18/20  Iteration 32070/35720 Training loss: 0.6833 0.2191 sec/batch\n",
      "Epoch 18/20  Iteration 32071/35720 Training loss: 0.6833 0.2125 sec/batch\n",
      "Epoch 18/20  Iteration 32072/35720 Training loss: 0.6833 0.2099 sec/batch\n",
      "Epoch 18/20  Iteration 32073/35720 Training loss: 0.6834 0.2277 sec/batch\n",
      "Epoch 18/20  Iteration 32074/35720 Training loss: 0.6834 0.2134 sec/batch\n",
      "Epoch 18/20  Iteration 32075/35720 Training loss: 0.6834 0.2152 sec/batch\n",
      "Epoch 18/20  Iteration 32076/35720 Training loss: 0.6834 0.2070 sec/batch\n",
      "Epoch 18/20  Iteration 32077/35720 Training loss: 0.6834 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 32078/35720 Training loss: 0.6834 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 32079/35720 Training loss: 0.6833 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 32080/35720 Training loss: 0.6833 0.2327 sec/batch\n",
      "Epoch 18/20  Iteration 32081/35720 Training loss: 0.6833 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 32082/35720 Training loss: 0.6833 0.2094 sec/batch\n",
      "Epoch 18/20  Iteration 32083/35720 Training loss: 0.6833 0.2144 sec/batch\n",
      "Epoch 18/20  Iteration 32084/35720 Training loss: 0.6833 0.2357 sec/batch\n",
      "Epoch 18/20  Iteration 32085/35720 Training loss: 0.6833 0.2224 sec/batch\n",
      "Epoch 18/20  Iteration 32086/35720 Training loss: 0.6833 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 32087/35720 Training loss: 0.6833 0.2057 sec/batch\n",
      "Epoch 18/20  Iteration 32088/35720 Training loss: 0.6833 0.2124 sec/batch\n",
      "Epoch 18/20  Iteration 32089/35720 Training loss: 0.6833 0.2244 sec/batch\n",
      "Epoch 18/20  Iteration 32090/35720 Training loss: 0.6833 0.2110 sec/batch\n",
      "Epoch 18/20  Iteration 32091/35720 Training loss: 0.6833 0.2278 sec/batch\n",
      "Epoch 18/20  Iteration 32092/35720 Training loss: 0.6833 0.2220 sec/batch\n",
      "Epoch 18/20  Iteration 32093/35720 Training loss: 0.6833 0.2056 sec/batch\n",
      "Epoch 18/20  Iteration 32094/35720 Training loss: 0.6833 0.2171 sec/batch\n",
      "Epoch 18/20  Iteration 32095/35720 Training loss: 0.6833 0.2269 sec/batch\n",
      "Epoch 18/20  Iteration 32096/35720 Training loss: 0.6833 0.2119 sec/batch\n",
      "Epoch 18/20  Iteration 32097/35720 Training loss: 0.6833 0.2316 sec/batch\n",
      "Epoch 18/20  Iteration 32098/35720 Training loss: 0.6833 0.2136 sec/batch\n",
      "Epoch 18/20  Iteration 32099/35720 Training loss: 0.6834 0.2089 sec/batch\n",
      "Epoch 18/20  Iteration 32100/35720 Training loss: 0.6833 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 32101/35720 Training loss: 0.6834 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 32102/35720 Training loss: 0.6833 0.2112 sec/batch\n",
      "Epoch 18/20  Iteration 32103/35720 Training loss: 0.6833 0.2157 sec/batch\n",
      "Epoch 18/20  Iteration 32104/35720 Training loss: 0.6833 0.2097 sec/batch\n",
      "Epoch 18/20  Iteration 32105/35720 Training loss: 0.6833 0.2142 sec/batch\n",
      "Epoch 18/20  Iteration 32106/35720 Training loss: 0.6833 0.2257 sec/batch\n",
      "Epoch 18/20  Iteration 32107/35720 Training loss: 0.6833 0.2147 sec/batch\n",
      "Epoch 18/20  Iteration 32108/35720 Training loss: 0.6834 0.2243 sec/batch\n",
      "Epoch 18/20  Iteration 32109/35720 Training loss: 0.6834 0.2156 sec/batch\n",
      "Epoch 18/20  Iteration 32110/35720 Training loss: 0.6834 0.2058 sec/batch\n",
      "Epoch 18/20  Iteration 32111/35720 Training loss: 0.6834 0.2325 sec/batch\n",
      "Epoch 18/20  Iteration 32112/35720 Training loss: 0.6834 0.2088 sec/batch\n",
      "Epoch 18/20  Iteration 32113/35720 Training loss: 0.6834 0.2283 sec/batch\n",
      "Epoch 18/20  Iteration 32114/35720 Training loss: 0.6834 0.2048 sec/batch\n",
      "Epoch 18/20  Iteration 32115/35720 Training loss: 0.6834 0.2130 sec/batch\n",
      "Epoch 18/20  Iteration 32116/35720 Training loss: 0.6834 0.2124 sec/batch\n",
      "Epoch 18/20  Iteration 32117/35720 Training loss: 0.6835 0.2169 sec/batch\n",
      "Epoch 18/20  Iteration 32118/35720 Training loss: 0.6835 0.2154 sec/batch\n",
      "Epoch 18/20  Iteration 32119/35720 Training loss: 0.6835 0.2248 sec/batch\n",
      "Epoch 18/20  Iteration 32120/35720 Training loss: 0.6835 0.2062 sec/batch\n",
      "Epoch 18/20  Iteration 32121/35720 Training loss: 0.6835 0.2200 sec/batch\n",
      "Epoch 18/20  Iteration 32122/35720 Training loss: 0.6835 0.2105 sec/batch\n",
      "Epoch 18/20  Iteration 32123/35720 Training loss: 0.6835 0.2169 sec/batch\n",
      "Epoch 18/20  Iteration 32124/35720 Training loss: 0.6835 0.2299 sec/batch\n",
      "Epoch 18/20  Iteration 32125/35720 Training loss: 0.6835 0.2127 sec/batch\n",
      "Epoch 18/20  Iteration 32126/35720 Training loss: 0.6835 0.2064 sec/batch\n",
      "Epoch 18/20  Iteration 32127/35720 Training loss: 0.6835 0.2098 sec/batch\n",
      "Epoch 18/20  Iteration 32128/35720 Training loss: 0.6835 0.2200 sec/batch\n",
      "Epoch 18/20  Iteration 32129/35720 Training loss: 0.6835 0.2100 sec/batch\n",
      "Epoch 18/20  Iteration 32130/35720 Training loss: 0.6835 0.2281 sec/batch\n",
      "Epoch 18/20  Iteration 32131/35720 Training loss: 0.6835 0.2108 sec/batch\n",
      "Epoch 18/20  Iteration 32132/35720 Training loss: 0.6835 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 32133/35720 Training loss: 0.6835 0.2083 sec/batch\n",
      "Epoch 18/20  Iteration 32134/35720 Training loss: 0.6835 0.2214 sec/batch\n",
      "Epoch 18/20  Iteration 32135/35720 Training loss: 0.6835 0.2299 sec/batch\n",
      "Epoch 18/20  Iteration 32136/35720 Training loss: 0.6834 0.2279 sec/batch\n",
      "Epoch 18/20  Iteration 32137/35720 Training loss: 0.6834 0.2056 sec/batch\n",
      "Epoch 18/20  Iteration 32138/35720 Training loss: 0.6834 0.2141 sec/batch\n",
      "Epoch 18/20  Iteration 32139/35720 Training loss: 0.6834 0.2160 sec/batch\n",
      "Epoch 18/20  Iteration 32140/35720 Training loss: 0.6834 0.2109 sec/batch\n",
      "Epoch 18/20  Iteration 32141/35720 Training loss: 0.6834 0.2238 sec/batch\n",
      "Epoch 18/20  Iteration 32142/35720 Training loss: 0.6833 0.2104 sec/batch\n",
      "Epoch 18/20  Iteration 32143/35720 Training loss: 0.6833 0.2149 sec/batch\n",
      "Epoch 18/20  Iteration 32144/35720 Training loss: 0.6833 0.2086 sec/batch\n",
      "Epoch 18/20  Iteration 32145/35720 Training loss: 0.6833 0.2150 sec/batch\n",
      "Epoch 18/20  Iteration 32146/35720 Training loss: 0.6833 0.2133 sec/batch\n",
      "Epoch 18/20  Iteration 32147/35720 Training loss: 0.6833 0.2267 sec/batch\n",
      "Epoch 18/20  Iteration 32148/35720 Training loss: 0.6833 0.2077 sec/batch\n",
      "Epoch 19/20  Iteration 32149/35720 Training loss: 0.6886 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 32150/35720 Training loss: 0.6956 0.2214 sec/batch\n",
      "Epoch 19/20  Iteration 32151/35720 Training loss: 0.6945 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 32152/35720 Training loss: 0.6860 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 32153/35720 Training loss: 0.6924 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 32154/35720 Training loss: 0.6861 0.2246 sec/batch\n",
      "Epoch 19/20  Iteration 32155/35720 Training loss: 0.6860 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32156/35720 Training loss: 0.6795 0.2221 sec/batch\n",
      "Epoch 19/20  Iteration 32157/35720 Training loss: 0.6739 0.2190 sec/batch\n",
      "Epoch 19/20  Iteration 32158/35720 Training loss: 0.6749 0.2209 sec/batch\n",
      "Epoch 19/20  Iteration 32159/35720 Training loss: 0.6767 0.2188 sec/batch\n",
      "Epoch 19/20  Iteration 32160/35720 Training loss: 0.6721 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32161/35720 Training loss: 0.6721 0.2285 sec/batch\n",
      "Epoch 19/20  Iteration 32162/35720 Training loss: 0.6740 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 32163/35720 Training loss: 0.6755 0.2295 sec/batch\n",
      "Epoch 19/20  Iteration 32164/35720 Training loss: 0.6755 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32165/35720 Training loss: 0.6755 0.2079 sec/batch\n",
      "Epoch 19/20  Iteration 32166/35720 Training loss: 0.6726 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32167/35720 Training loss: 0.6687 0.2197 sec/batch\n",
      "Epoch 19/20  Iteration 32168/35720 Training loss: 0.6704 0.2185 sec/batch\n",
      "Epoch 19/20  Iteration 32169/35720 Training loss: 0.6718 0.2239 sec/batch\n",
      "Epoch 19/20  Iteration 32170/35720 Training loss: 0.6690 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 32171/35720 Training loss: 0.6692 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 32172/35720 Training loss: 0.6707 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 32173/35720 Training loss: 0.6722 0.2250 sec/batch\n",
      "Epoch 19/20  Iteration 32174/35720 Training loss: 0.6728 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 32175/35720 Training loss: 0.6749 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 32176/35720 Training loss: 0.6759 0.2298 sec/batch\n",
      "Epoch 19/20  Iteration 32177/35720 Training loss: 0.6742 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 32178/35720 Training loss: 0.6752 0.2153 sec/batch\n",
      "Epoch 19/20  Iteration 32179/35720 Training loss: 0.6764 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 32180/35720 Training loss: 0.6760 0.2339 sec/batch\n",
      "Epoch 19/20  Iteration 32181/35720 Training loss: 0.6775 0.2184 sec/batch\n",
      "Epoch 19/20  Iteration 32182/35720 Training loss: 0.6789 0.2183 sec/batch\n",
      "Epoch 19/20  Iteration 32183/35720 Training loss: 0.6817 0.2188 sec/batch\n",
      "Epoch 19/20  Iteration 32184/35720 Training loss: 0.6819 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 32185/35720 Training loss: 0.6825 0.2530 sec/batch\n",
      "Epoch 19/20  Iteration 32186/35720 Training loss: 0.6818 0.2162 sec/batch\n",
      "Epoch 19/20  Iteration 32187/35720 Training loss: 0.6819 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 32188/35720 Training loss: 0.6833 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 32189/35720 Training loss: 0.6831 0.2227 sec/batch\n",
      "Epoch 19/20  Iteration 32190/35720 Training loss: 0.6829 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 32191/35720 Training loss: 0.6811 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 32192/35720 Training loss: 0.6798 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 32193/35720 Training loss: 0.6801 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 32194/35720 Training loss: 0.6793 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 32195/35720 Training loss: 0.6786 0.2155 sec/batch\n",
      "Epoch 19/20  Iteration 32196/35720 Training loss: 0.6781 0.2156 sec/batch\n",
      "Epoch 19/20  Iteration 32197/35720 Training loss: 0.6776 0.2138 sec/batch\n",
      "Epoch 19/20  Iteration 32198/35720 Training loss: 0.6766 0.2147 sec/batch\n",
      "Epoch 19/20  Iteration 32199/35720 Training loss: 0.6771 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 32200/35720 Training loss: 0.6777 0.2159 sec/batch\n",
      "Validation loss: 1.63718 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 32201/35720 Training loss: 0.6854 0.2123 sec/batch\n",
      "Epoch 19/20  Iteration 32202/35720 Training loss: 0.6840 0.2074 sec/batch\n",
      "Epoch 19/20  Iteration 32203/35720 Training loss: 0.6825 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32204/35720 Training loss: 0.6820 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32205/35720 Training loss: 0.6818 0.2272 sec/batch\n",
      "Epoch 19/20  Iteration 32206/35720 Training loss: 0.6812 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 32207/35720 Training loss: 0.6807 0.2202 sec/batch\n",
      "Epoch 19/20  Iteration 32208/35720 Training loss: 0.6801 0.2110 sec/batch\n",
      "Epoch 19/20  Iteration 32209/35720 Training loss: 0.6797 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 32210/35720 Training loss: 0.6785 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 32211/35720 Training loss: 0.6790 0.2261 sec/batch\n",
      "Epoch 19/20  Iteration 32212/35720 Training loss: 0.6794 0.2282 sec/batch\n",
      "Epoch 19/20  Iteration 32213/35720 Training loss: 0.6798 0.2251 sec/batch\n",
      "Epoch 19/20  Iteration 32214/35720 Training loss: 0.6797 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 32215/35720 Training loss: 0.6789 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 32216/35720 Training loss: 0.6780 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 32217/35720 Training loss: 0.6784 0.2111 sec/batch\n",
      "Epoch 19/20  Iteration 32218/35720 Training loss: 0.6782 0.2128 sec/batch\n",
      "Epoch 19/20  Iteration 32219/35720 Training loss: 0.6787 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32220/35720 Training loss: 0.6790 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 32221/35720 Training loss: 0.6791 0.2198 sec/batch\n",
      "Epoch 19/20  Iteration 32222/35720 Training loss: 0.6788 0.2275 sec/batch\n",
      "Epoch 19/20  Iteration 32223/35720 Training loss: 0.6784 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32224/35720 Training loss: 0.6783 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 32225/35720 Training loss: 0.6778 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 32226/35720 Training loss: 0.6785 0.2124 sec/batch\n",
      "Epoch 19/20  Iteration 32227/35720 Training loss: 0.6783 0.2150 sec/batch\n",
      "Epoch 19/20  Iteration 32228/35720 Training loss: 0.6793 0.2149 sec/batch\n",
      "Epoch 19/20  Iteration 32229/35720 Training loss: 0.6796 0.2185 sec/batch\n",
      "Epoch 19/20  Iteration 32230/35720 Training loss: 0.6797 0.2253 sec/batch\n",
      "Epoch 19/20  Iteration 32231/35720 Training loss: 0.6799 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 32232/35720 Training loss: 0.6800 0.2119 sec/batch\n",
      "Epoch 19/20  Iteration 32233/35720 Training loss: 0.6799 0.2272 sec/batch\n",
      "Epoch 19/20  Iteration 32234/35720 Training loss: 0.6800 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 32235/35720 Training loss: 0.6797 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 32236/35720 Training loss: 0.6797 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32237/35720 Training loss: 0.6791 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32238/35720 Training loss: 0.6786 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32239/35720 Training loss: 0.6789 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 32240/35720 Training loss: 0.6787 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32241/35720 Training loss: 0.6792 0.2147 sec/batch\n",
      "Epoch 19/20  Iteration 32242/35720 Training loss: 0.6799 0.2172 sec/batch\n",
      "Epoch 19/20  Iteration 32243/35720 Training loss: 0.6797 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 32244/35720 Training loss: 0.6796 0.2148 sec/batch\n",
      "Epoch 19/20  Iteration 32245/35720 Training loss: 0.6799 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32246/35720 Training loss: 0.6800 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 32247/35720 Training loss: 0.6799 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32248/35720 Training loss: 0.6795 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 32249/35720 Training loss: 0.6790 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32250/35720 Training loss: 0.6791 0.2326 sec/batch\n",
      "Epoch 19/20  Iteration 32251/35720 Training loss: 0.6785 0.2312 sec/batch\n",
      "Epoch 19/20  Iteration 32252/35720 Training loss: 0.6781 0.2280 sec/batch\n",
      "Epoch 19/20  Iteration 32253/35720 Training loss: 0.6777 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 32254/35720 Training loss: 0.6774 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 32255/35720 Training loss: 0.6773 0.2266 sec/batch\n",
      "Epoch 19/20  Iteration 32256/35720 Training loss: 0.6774 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32257/35720 Training loss: 0.6776 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 32258/35720 Training loss: 0.6774 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 32259/35720 Training loss: 0.6776 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 32260/35720 Training loss: 0.6777 0.2123 sec/batch\n",
      "Epoch 19/20  Iteration 32261/35720 Training loss: 0.6778 0.2288 sec/batch\n",
      "Epoch 19/20  Iteration 32262/35720 Training loss: 0.6776 0.2172 sec/batch\n",
      "Epoch 19/20  Iteration 32263/35720 Training loss: 0.6776 0.2284 sec/batch\n",
      "Epoch 19/20  Iteration 32264/35720 Training loss: 0.6776 0.2075 sec/batch\n",
      "Epoch 19/20  Iteration 32265/35720 Training loss: 0.6774 0.2210 sec/batch\n",
      "Epoch 19/20  Iteration 32266/35720 Training loss: 0.6774 0.2260 sec/batch\n",
      "Epoch 19/20  Iteration 32267/35720 Training loss: 0.6774 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32268/35720 Training loss: 0.6780 0.2232 sec/batch\n",
      "Epoch 19/20  Iteration 32269/35720 Training loss: 0.6781 0.2056 sec/batch\n",
      "Epoch 19/20  Iteration 32270/35720 Training loss: 0.6776 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 32271/35720 Training loss: 0.6775 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 32272/35720 Training loss: 0.6777 0.2277 sec/batch\n",
      "Epoch 19/20  Iteration 32273/35720 Training loss: 0.6773 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 32274/35720 Training loss: 0.6774 0.2265 sec/batch\n",
      "Epoch 19/20  Iteration 32275/35720 Training loss: 0.6776 0.2198 sec/batch\n",
      "Epoch 19/20  Iteration 32276/35720 Training loss: 0.6775 0.2080 sec/batch\n",
      "Epoch 19/20  Iteration 32277/35720 Training loss: 0.6772 0.2215 sec/batch\n",
      "Epoch 19/20  Iteration 32278/35720 Training loss: 0.6775 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 32279/35720 Training loss: 0.6774 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 32280/35720 Training loss: 0.6771 0.2057 sec/batch\n",
      "Epoch 19/20  Iteration 32281/35720 Training loss: 0.6771 0.2206 sec/batch\n",
      "Epoch 19/20  Iteration 32282/35720 Training loss: 0.6771 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 32283/35720 Training loss: 0.6769 0.2212 sec/batch\n",
      "Epoch 19/20  Iteration 32284/35720 Training loss: 0.6765 0.2299 sec/batch\n",
      "Epoch 19/20  Iteration 32285/35720 Training loss: 0.6768 0.2187 sec/batch\n",
      "Epoch 19/20  Iteration 32286/35720 Training loss: 0.6769 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 32287/35720 Training loss: 0.6771 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 32288/35720 Training loss: 0.6773 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 32289/35720 Training loss: 0.6771 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 32290/35720 Training loss: 0.6769 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32291/35720 Training loss: 0.6766 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 32292/35720 Training loss: 0.6761 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 32293/35720 Training loss: 0.6762 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32294/35720 Training loss: 0.6765 0.2176 sec/batch\n",
      "Epoch 19/20  Iteration 32295/35720 Training loss: 0.6764 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 32296/35720 Training loss: 0.6763 0.2239 sec/batch\n",
      "Epoch 19/20  Iteration 32297/35720 Training loss: 0.6763 0.2226 sec/batch\n",
      "Epoch 19/20  Iteration 32298/35720 Training loss: 0.6758 0.2121 sec/batch\n",
      "Epoch 19/20  Iteration 32299/35720 Training loss: 0.6758 0.2138 sec/batch\n",
      "Epoch 19/20  Iteration 32300/35720 Training loss: 0.6759 0.2157 sec/batch\n",
      "Epoch 19/20  Iteration 32301/35720 Training loss: 0.6759 0.2109 sec/batch\n",
      "Epoch 19/20  Iteration 32302/35720 Training loss: 0.6760 0.2134 sec/batch\n",
      "Epoch 19/20  Iteration 32303/35720 Training loss: 0.6761 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 32304/35720 Training loss: 0.6763 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 32305/35720 Training loss: 0.6763 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 32306/35720 Training loss: 0.6766 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 32307/35720 Training loss: 0.6763 0.2277 sec/batch\n",
      "Epoch 19/20  Iteration 32308/35720 Training loss: 0.6762 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 32309/35720 Training loss: 0.6760 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32310/35720 Training loss: 0.6761 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32311/35720 Training loss: 0.6760 0.2156 sec/batch\n",
      "Epoch 19/20  Iteration 32312/35720 Training loss: 0.6761 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 32313/35720 Training loss: 0.6763 0.2229 sec/batch\n",
      "Epoch 19/20  Iteration 32314/35720 Training loss: 0.6762 0.2049 sec/batch\n",
      "Epoch 19/20  Iteration 32315/35720 Training loss: 0.6761 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 32316/35720 Training loss: 0.6763 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 32317/35720 Training loss: 0.6766 0.2191 sec/batch\n",
      "Epoch 19/20  Iteration 32318/35720 Training loss: 0.6769 0.2208 sec/batch\n",
      "Epoch 19/20  Iteration 32319/35720 Training loss: 0.6773 0.2223 sec/batch\n",
      "Epoch 19/20  Iteration 32320/35720 Training loss: 0.6778 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32321/35720 Training loss: 0.6781 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 32322/35720 Training loss: 0.6785 0.2324 sec/batch\n",
      "Epoch 19/20  Iteration 32323/35720 Training loss: 0.6786 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 32324/35720 Training loss: 0.6787 0.2255 sec/batch\n",
      "Epoch 19/20  Iteration 32325/35720 Training loss: 0.6788 0.2058 sec/batch\n",
      "Epoch 19/20  Iteration 32326/35720 Training loss: 0.6786 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 32327/35720 Training loss: 0.6785 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 32328/35720 Training loss: 0.6782 0.2124 sec/batch\n",
      "Epoch 19/20  Iteration 32329/35720 Training loss: 0.6782 0.2143 sec/batch\n",
      "Epoch 19/20  Iteration 32330/35720 Training loss: 0.6781 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 32331/35720 Training loss: 0.6783 0.2184 sec/batch\n",
      "Epoch 19/20  Iteration 32332/35720 Training loss: 0.6785 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 32333/35720 Training loss: 0.6783 0.2348 sec/batch\n",
      "Epoch 19/20  Iteration 32334/35720 Training loss: 0.6783 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32335/35720 Training loss: 0.6782 0.2283 sec/batch\n",
      "Epoch 19/20  Iteration 32336/35720 Training loss: 0.6783 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 32337/35720 Training loss: 0.6784 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 32338/35720 Training loss: 0.6782 0.2076 sec/batch\n",
      "Epoch 19/20  Iteration 32339/35720 Training loss: 0.6784 0.2258 sec/batch\n",
      "Epoch 19/20  Iteration 32340/35720 Training loss: 0.6786 0.2138 sec/batch\n",
      "Epoch 19/20  Iteration 32341/35720 Training loss: 0.6788 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 32342/35720 Training loss: 0.6788 0.2291 sec/batch\n",
      "Epoch 19/20  Iteration 32343/35720 Training loss: 0.6788 0.2166 sec/batch\n",
      "Epoch 19/20  Iteration 32344/35720 Training loss: 0.6789 0.2398 sec/batch\n",
      "Epoch 19/20  Iteration 32345/35720 Training loss: 0.6787 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32346/35720 Training loss: 0.6786 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 32347/35720 Training loss: 0.6787 0.2160 sec/batch\n",
      "Epoch 19/20  Iteration 32348/35720 Training loss: 0.6789 0.2123 sec/batch\n",
      "Epoch 19/20  Iteration 32349/35720 Training loss: 0.6789 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 32350/35720 Training loss: 0.6790 0.2195 sec/batch\n",
      "Epoch 19/20  Iteration 32351/35720 Training loss: 0.6792 0.2176 sec/batch\n",
      "Epoch 19/20  Iteration 32352/35720 Training loss: 0.6792 0.2172 sec/batch\n",
      "Epoch 19/20  Iteration 32353/35720 Training loss: 0.6793 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 32354/35720 Training loss: 0.6793 0.2159 sec/batch\n",
      "Epoch 19/20  Iteration 32355/35720 Training loss: 0.6796 0.2362 sec/batch\n",
      "Epoch 19/20  Iteration 32356/35720 Training loss: 0.6799 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 32357/35720 Training loss: 0.6803 0.2287 sec/batch\n",
      "Epoch 19/20  Iteration 32358/35720 Training loss: 0.6802 0.2196 sec/batch\n",
      "Epoch 19/20  Iteration 32359/35720 Training loss: 0.6803 0.2110 sec/batch\n",
      "Epoch 19/20  Iteration 32360/35720 Training loss: 0.6803 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 32361/35720 Training loss: 0.6802 0.2285 sec/batch\n",
      "Epoch 19/20  Iteration 32362/35720 Training loss: 0.6801 0.2178 sec/batch\n",
      "Epoch 19/20  Iteration 32363/35720 Training loss: 0.6801 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 32364/35720 Training loss: 0.6802 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32365/35720 Training loss: 0.6801 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 32366/35720 Training loss: 0.6802 0.2237 sec/batch\n",
      "Epoch 19/20  Iteration 32367/35720 Training loss: 0.6802 0.2102 sec/batch\n",
      "Epoch 19/20  Iteration 32368/35720 Training loss: 0.6803 0.2332 sec/batch\n",
      "Epoch 19/20  Iteration 32369/35720 Training loss: 0.6803 0.2055 sec/batch\n",
      "Epoch 19/20  Iteration 32370/35720 Training loss: 0.6803 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 32371/35720 Training loss: 0.6805 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 32372/35720 Training loss: 0.6805 0.2269 sec/batch\n",
      "Epoch 19/20  Iteration 32373/35720 Training loss: 0.6807 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 32374/35720 Training loss: 0.6808 0.2252 sec/batch\n",
      "Epoch 19/20  Iteration 32375/35720 Training loss: 0.6806 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 32376/35720 Training loss: 0.6805 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 32377/35720 Training loss: 0.6802 0.2233 sec/batch\n",
      "Epoch 19/20  Iteration 32378/35720 Training loss: 0.6803 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32379/35720 Training loss: 0.6807 0.2200 sec/batch\n",
      "Epoch 19/20  Iteration 32380/35720 Training loss: 0.6808 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 32381/35720 Training loss: 0.6808 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 32382/35720 Training loss: 0.6808 0.2696 sec/batch\n",
      "Epoch 19/20  Iteration 32383/35720 Training loss: 0.6808 0.2252 sec/batch\n",
      "Epoch 19/20  Iteration 32384/35720 Training loss: 0.6808 0.2157 sec/batch\n",
      "Epoch 19/20  Iteration 32385/35720 Training loss: 0.6809 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 32386/35720 Training loss: 0.6808 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32387/35720 Training loss: 0.6809 0.2200 sec/batch\n",
      "Epoch 19/20  Iteration 32388/35720 Training loss: 0.6809 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 32389/35720 Training loss: 0.6808 0.2192 sec/batch\n",
      "Epoch 19/20  Iteration 32390/35720 Training loss: 0.6807 0.2213 sec/batch\n",
      "Epoch 19/20  Iteration 32391/35720 Training loss: 0.6808 0.2059 sec/batch\n",
      "Epoch 19/20  Iteration 32392/35720 Training loss: 0.6806 0.2051 sec/batch\n",
      "Epoch 19/20  Iteration 32393/35720 Training loss: 0.6803 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 32394/35720 Training loss: 0.6804 0.2256 sec/batch\n",
      "Epoch 19/20  Iteration 32395/35720 Training loss: 0.6805 0.2197 sec/batch\n",
      "Epoch 19/20  Iteration 32396/35720 Training loss: 0.6805 0.2253 sec/batch\n",
      "Epoch 19/20  Iteration 32397/35720 Training loss: 0.6804 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 32398/35720 Training loss: 0.6803 0.2153 sec/batch\n",
      "Epoch 19/20  Iteration 32399/35720 Training loss: 0.6803 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 32400/35720 Training loss: 0.6803 0.2147 sec/batch\n",
      "Validation loss: 1.65538 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 32401/35720 Training loss: 0.6817 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 32402/35720 Training loss: 0.6818 0.2058 sec/batch\n",
      "Epoch 19/20  Iteration 32403/35720 Training loss: 0.6818 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 32404/35720 Training loss: 0.6818 0.2208 sec/batch\n",
      "Epoch 19/20  Iteration 32405/35720 Training loss: 0.6818 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 32406/35720 Training loss: 0.6817 0.2276 sec/batch\n",
      "Epoch 19/20  Iteration 32407/35720 Training loss: 0.6819 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 32408/35720 Training loss: 0.6819 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 32409/35720 Training loss: 0.6818 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 32410/35720 Training loss: 0.6817 0.2366 sec/batch\n",
      "Epoch 19/20  Iteration 32411/35720 Training loss: 0.6815 0.2197 sec/batch\n",
      "Epoch 19/20  Iteration 32412/35720 Training loss: 0.6815 0.2183 sec/batch\n",
      "Epoch 19/20  Iteration 32413/35720 Training loss: 0.6815 0.2057 sec/batch\n",
      "Epoch 19/20  Iteration 32414/35720 Training loss: 0.6817 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 32415/35720 Training loss: 0.6816 0.2171 sec/batch\n",
      "Epoch 19/20  Iteration 32416/35720 Training loss: 0.6815 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 32417/35720 Training loss: 0.6814 0.2159 sec/batch\n",
      "Epoch 19/20  Iteration 32418/35720 Training loss: 0.6811 0.2138 sec/batch\n",
      "Epoch 19/20  Iteration 32419/35720 Training loss: 0.6809 0.2308 sec/batch\n",
      "Epoch 19/20  Iteration 32420/35720 Training loss: 0.6809 0.2131 sec/batch\n",
      "Epoch 19/20  Iteration 32421/35720 Training loss: 0.6808 0.2257 sec/batch\n",
      "Epoch 19/20  Iteration 32422/35720 Training loss: 0.6808 0.2121 sec/batch\n",
      "Epoch 19/20  Iteration 32423/35720 Training loss: 0.6808 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 32424/35720 Training loss: 0.6807 0.2209 sec/batch\n",
      "Epoch 19/20  Iteration 32425/35720 Training loss: 0.6805 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 32426/35720 Training loss: 0.6803 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 32427/35720 Training loss: 0.6801 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 32428/35720 Training loss: 0.6801 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 32429/35720 Training loss: 0.6801 0.2234 sec/batch\n",
      "Epoch 19/20  Iteration 32430/35720 Training loss: 0.6800 0.2274 sec/batch\n",
      "Epoch 19/20  Iteration 32431/35720 Training loss: 0.6798 0.2148 sec/batch\n",
      "Epoch 19/20  Iteration 32432/35720 Training loss: 0.6797 0.2250 sec/batch\n",
      "Epoch 19/20  Iteration 32433/35720 Training loss: 0.6798 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 32434/35720 Training loss: 0.6797 0.2294 sec/batch\n",
      "Epoch 19/20  Iteration 32435/35720 Training loss: 0.6795 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32436/35720 Training loss: 0.6796 0.2259 sec/batch\n",
      "Epoch 19/20  Iteration 32437/35720 Training loss: 0.6797 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 32438/35720 Training loss: 0.6798 0.2302 sec/batch\n",
      "Epoch 19/20  Iteration 32439/35720 Training loss: 0.6797 0.2218 sec/batch\n",
      "Epoch 19/20  Iteration 32440/35720 Training loss: 0.6798 0.2265 sec/batch\n",
      "Epoch 19/20  Iteration 32441/35720 Training loss: 0.6797 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 32442/35720 Training loss: 0.6797 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 32443/35720 Training loss: 0.6799 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 32444/35720 Training loss: 0.6799 0.2118 sec/batch\n",
      "Epoch 19/20  Iteration 32445/35720 Training loss: 0.6798 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 32446/35720 Training loss: 0.6799 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 32447/35720 Training loss: 0.6797 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 32448/35720 Training loss: 0.6797 0.2257 sec/batch\n",
      "Epoch 19/20  Iteration 32449/35720 Training loss: 0.6797 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 32450/35720 Training loss: 0.6795 0.2290 sec/batch\n",
      "Epoch 19/20  Iteration 32451/35720 Training loss: 0.6796 0.2074 sec/batch\n",
      "Epoch 19/20  Iteration 32452/35720 Training loss: 0.6797 0.2058 sec/batch\n",
      "Epoch 19/20  Iteration 32453/35720 Training loss: 0.6795 0.2142 sec/batch\n",
      "Epoch 19/20  Iteration 32454/35720 Training loss: 0.6793 0.2177 sec/batch\n",
      "Epoch 19/20  Iteration 32455/35720 Training loss: 0.6792 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32456/35720 Training loss: 0.6793 0.2260 sec/batch\n",
      "Epoch 19/20  Iteration 32457/35720 Training loss: 0.6792 0.2074 sec/batch\n",
      "Epoch 19/20  Iteration 32458/35720 Training loss: 0.6792 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 32459/35720 Training loss: 0.6790 0.2191 sec/batch\n",
      "Epoch 19/20  Iteration 32460/35720 Training loss: 0.6789 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 32461/35720 Training loss: 0.6788 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32462/35720 Training loss: 0.6787 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 32463/35720 Training loss: 0.6788 0.2204 sec/batch\n",
      "Epoch 19/20  Iteration 32464/35720 Training loss: 0.6789 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 32465/35720 Training loss: 0.6788 0.2213 sec/batch\n",
      "Epoch 19/20  Iteration 32466/35720 Training loss: 0.6788 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 32467/35720 Training loss: 0.6789 0.2212 sec/batch\n",
      "Epoch 19/20  Iteration 32468/35720 Training loss: 0.6789 0.2079 sec/batch\n",
      "Epoch 19/20  Iteration 32469/35720 Training loss: 0.6790 0.2077 sec/batch\n",
      "Epoch 19/20  Iteration 32470/35720 Training loss: 0.6790 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 32471/35720 Training loss: 0.6791 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 32472/35720 Training loss: 0.6792 0.2215 sec/batch\n",
      "Epoch 19/20  Iteration 32473/35720 Training loss: 0.6790 0.2276 sec/batch\n",
      "Epoch 19/20  Iteration 32474/35720 Training loss: 0.6792 0.2102 sec/batch\n",
      "Epoch 19/20  Iteration 32475/35720 Training loss: 0.6792 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 32476/35720 Training loss: 0.6790 0.2152 sec/batch\n",
      "Epoch 19/20  Iteration 32477/35720 Training loss: 0.6790 0.2171 sec/batch\n",
      "Epoch 19/20  Iteration 32478/35720 Training loss: 0.6791 0.2149 sec/batch\n",
      "Epoch 19/20  Iteration 32479/35720 Training loss: 0.6790 0.2134 sec/batch\n",
      "Epoch 19/20  Iteration 32480/35720 Training loss: 0.6791 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 32481/35720 Training loss: 0.6790 0.2160 sec/batch\n",
      "Epoch 19/20  Iteration 32482/35720 Training loss: 0.6791 0.2168 sec/batch\n",
      "Epoch 19/20  Iteration 32483/35720 Training loss: 0.6790 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 32484/35720 Training loss: 0.6788 0.2186 sec/batch\n",
      "Epoch 19/20  Iteration 32485/35720 Training loss: 0.6788 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32486/35720 Training loss: 0.6786 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32487/35720 Training loss: 0.6785 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 32488/35720 Training loss: 0.6784 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 32489/35720 Training loss: 0.6783 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 32490/35720 Training loss: 0.6782 0.2227 sec/batch\n",
      "Epoch 19/20  Iteration 32491/35720 Training loss: 0.6783 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32492/35720 Training loss: 0.6784 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32493/35720 Training loss: 0.6782 0.2409 sec/batch\n",
      "Epoch 19/20  Iteration 32494/35720 Training loss: 0.6784 0.2157 sec/batch\n",
      "Epoch 19/20  Iteration 32495/35720 Training loss: 0.6785 0.2196 sec/batch\n",
      "Epoch 19/20  Iteration 32496/35720 Training loss: 0.6785 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32497/35720 Training loss: 0.6784 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32498/35720 Training loss: 0.6785 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 32499/35720 Training loss: 0.6785 0.2218 sec/batch\n",
      "Epoch 19/20  Iteration 32500/35720 Training loss: 0.6783 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 32501/35720 Training loss: 0.6784 0.2223 sec/batch\n",
      "Epoch 19/20  Iteration 32502/35720 Training loss: 0.6784 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32503/35720 Training loss: 0.6785 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32504/35720 Training loss: 0.6785 0.2368 sec/batch\n",
      "Epoch 19/20  Iteration 32505/35720 Training loss: 0.6786 0.2153 sec/batch\n",
      "Epoch 19/20  Iteration 32506/35720 Training loss: 0.6786 0.2170 sec/batch\n",
      "Epoch 19/20  Iteration 32507/35720 Training loss: 0.6786 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 32508/35720 Training loss: 0.6786 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32509/35720 Training loss: 0.6785 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 32510/35720 Training loss: 0.6786 0.2196 sec/batch\n",
      "Epoch 19/20  Iteration 32511/35720 Training loss: 0.6786 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32512/35720 Training loss: 0.6786 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 32513/35720 Training loss: 0.6786 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32514/35720 Training loss: 0.6785 0.2142 sec/batch\n",
      "Epoch 19/20  Iteration 32515/35720 Training loss: 0.6786 0.2155 sec/batch\n",
      "Epoch 19/20  Iteration 32516/35720 Training loss: 0.6786 0.2175 sec/batch\n",
      "Epoch 19/20  Iteration 32517/35720 Training loss: 0.6785 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32518/35720 Training loss: 0.6783 0.2159 sec/batch\n",
      "Epoch 19/20  Iteration 32519/35720 Training loss: 0.6783 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 32520/35720 Training loss: 0.6783 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 32521/35720 Training loss: 0.6782 0.2245 sec/batch\n",
      "Epoch 19/20  Iteration 32522/35720 Training loss: 0.6782 0.2128 sec/batch\n",
      "Epoch 19/20  Iteration 32523/35720 Training loss: 0.6781 0.2155 sec/batch\n",
      "Epoch 19/20  Iteration 32524/35720 Training loss: 0.6781 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 32525/35720 Training loss: 0.6782 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32526/35720 Training loss: 0.6782 0.2185 sec/batch\n",
      "Epoch 19/20  Iteration 32527/35720 Training loss: 0.6782 0.2204 sec/batch\n",
      "Epoch 19/20  Iteration 32528/35720 Training loss: 0.6781 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 32529/35720 Training loss: 0.6780 0.2310 sec/batch\n",
      "Epoch 19/20  Iteration 32530/35720 Training loss: 0.6779 0.2210 sec/batch\n",
      "Epoch 19/20  Iteration 32531/35720 Training loss: 0.6779 0.2097 sec/batch\n",
      "Epoch 19/20  Iteration 32532/35720 Training loss: 0.6779 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 32533/35720 Training loss: 0.6780 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32534/35720 Training loss: 0.6779 0.2183 sec/batch\n",
      "Epoch 19/20  Iteration 32535/35720 Training loss: 0.6779 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 32536/35720 Training loss: 0.6779 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32537/35720 Training loss: 0.6779 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 32538/35720 Training loss: 0.6779 0.2272 sec/batch\n",
      "Epoch 19/20  Iteration 32539/35720 Training loss: 0.6779 0.2202 sec/batch\n",
      "Epoch 19/20  Iteration 32540/35720 Training loss: 0.6778 0.2236 sec/batch\n",
      "Epoch 19/20  Iteration 32541/35720 Training loss: 0.6778 0.2058 sec/batch\n",
      "Epoch 19/20  Iteration 32542/35720 Training loss: 0.6776 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32543/35720 Training loss: 0.6776 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 32544/35720 Training loss: 0.6775 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 32545/35720 Training loss: 0.6776 0.2079 sec/batch\n",
      "Epoch 19/20  Iteration 32546/35720 Training loss: 0.6774 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32547/35720 Training loss: 0.6773 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 32548/35720 Training loss: 0.6772 0.2122 sec/batch\n",
      "Epoch 19/20  Iteration 32549/35720 Training loss: 0.6772 0.2204 sec/batch\n",
      "Epoch 19/20  Iteration 32550/35720 Training loss: 0.6773 0.2180 sec/batch\n",
      "Epoch 19/20  Iteration 32551/35720 Training loss: 0.6773 0.2293 sec/batch\n",
      "Epoch 19/20  Iteration 32552/35720 Training loss: 0.6773 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 32553/35720 Training loss: 0.6772 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 32554/35720 Training loss: 0.6772 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32555/35720 Training loss: 0.6772 0.2241 sec/batch\n",
      "Epoch 19/20  Iteration 32556/35720 Training loss: 0.6772 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 32557/35720 Training loss: 0.6771 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 32558/35720 Training loss: 0.6770 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 32559/35720 Training loss: 0.6771 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32560/35720 Training loss: 0.6770 0.2205 sec/batch\n",
      "Epoch 19/20  Iteration 32561/35720 Training loss: 0.6768 0.2207 sec/batch\n",
      "Epoch 19/20  Iteration 32562/35720 Training loss: 0.6769 0.2247 sec/batch\n",
      "Epoch 19/20  Iteration 32563/35720 Training loss: 0.6768 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32564/35720 Training loss: 0.6768 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32565/35720 Training loss: 0.6767 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32566/35720 Training loss: 0.6766 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 32567/35720 Training loss: 0.6766 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 32568/35720 Training loss: 0.6766 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 32569/35720 Training loss: 0.6766 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32570/35720 Training loss: 0.6766 0.2055 sec/batch\n",
      "Epoch 19/20  Iteration 32571/35720 Training loss: 0.6766 0.2102 sec/batch\n",
      "Epoch 19/20  Iteration 32572/35720 Training loss: 0.6766 0.2196 sec/batch\n",
      "Epoch 19/20  Iteration 32573/35720 Training loss: 0.6766 0.2254 sec/batch\n",
      "Epoch 19/20  Iteration 32574/35720 Training loss: 0.6764 0.2261 sec/batch\n",
      "Epoch 19/20  Iteration 32575/35720 Training loss: 0.6763 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 32576/35720 Training loss: 0.6763 0.2119 sec/batch\n",
      "Epoch 19/20  Iteration 32577/35720 Training loss: 0.6762 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 32578/35720 Training loss: 0.6763 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 32579/35720 Training loss: 0.6764 0.2278 sec/batch\n",
      "Epoch 19/20  Iteration 32580/35720 Training loss: 0.6763 0.2124 sec/batch\n",
      "Epoch 19/20  Iteration 32581/35720 Training loss: 0.6764 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 32582/35720 Training loss: 0.6764 0.2200 sec/batch\n",
      "Epoch 19/20  Iteration 32583/35720 Training loss: 0.6766 0.2171 sec/batch\n",
      "Epoch 19/20  Iteration 32584/35720 Training loss: 0.6765 0.2178 sec/batch\n",
      "Epoch 19/20  Iteration 32585/35720 Training loss: 0.6767 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 32586/35720 Training loss: 0.6768 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 32587/35720 Training loss: 0.6769 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 32588/35720 Training loss: 0.6770 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 32589/35720 Training loss: 0.6770 0.2185 sec/batch\n",
      "Epoch 19/20  Iteration 32590/35720 Training loss: 0.6770 0.2309 sec/batch\n",
      "Epoch 19/20  Iteration 32591/35720 Training loss: 0.6770 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 32592/35720 Training loss: 0.6770 0.2175 sec/batch\n",
      "Epoch 19/20  Iteration 32593/35720 Training loss: 0.6771 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32594/35720 Training loss: 0.6772 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 32595/35720 Training loss: 0.6774 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 32596/35720 Training loss: 0.6775 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 32597/35720 Training loss: 0.6776 0.2111 sec/batch\n",
      "Epoch 19/20  Iteration 32598/35720 Training loss: 0.6776 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 32599/35720 Training loss: 0.6775 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 32600/35720 Training loss: 0.6774 0.2112 sec/batch\n",
      "Validation loss: 1.65487 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 32601/35720 Training loss: 0.6784 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 32602/35720 Training loss: 0.6785 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 32603/35720 Training loss: 0.6786 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 32604/35720 Training loss: 0.6788 0.2178 sec/batch\n",
      "Epoch 19/20  Iteration 32605/35720 Training loss: 0.6788 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 32606/35720 Training loss: 0.6788 0.2142 sec/batch\n",
      "Epoch 19/20  Iteration 32607/35720 Training loss: 0.6787 0.2171 sec/batch\n",
      "Epoch 19/20  Iteration 32608/35720 Training loss: 0.6785 0.2170 sec/batch\n",
      "Epoch 19/20  Iteration 32609/35720 Training loss: 0.6786 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 32610/35720 Training loss: 0.6787 0.2195 sec/batch\n",
      "Epoch 19/20  Iteration 32611/35720 Training loss: 0.6788 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 32612/35720 Training loss: 0.6788 0.2215 sec/batch\n",
      "Epoch 19/20  Iteration 32613/35720 Training loss: 0.6788 0.2188 sec/batch\n",
      "Epoch 19/20  Iteration 32614/35720 Training loss: 0.6788 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 32615/35720 Training loss: 0.6789 0.2143 sec/batch\n",
      "Epoch 19/20  Iteration 32616/35720 Training loss: 0.6789 0.2248 sec/batch\n",
      "Epoch 19/20  Iteration 32617/35720 Training loss: 0.6790 0.2273 sec/batch\n",
      "Epoch 19/20  Iteration 32618/35720 Training loss: 0.6790 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 32619/35720 Training loss: 0.6790 0.2148 sec/batch\n",
      "Epoch 19/20  Iteration 32620/35720 Training loss: 0.6789 0.2299 sec/batch\n",
      "Epoch 19/20  Iteration 32621/35720 Training loss: 0.6790 0.2284 sec/batch\n",
      "Epoch 19/20  Iteration 32622/35720 Training loss: 0.6789 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 32623/35720 Training loss: 0.6790 0.2210 sec/batch\n",
      "Epoch 19/20  Iteration 32624/35720 Training loss: 0.6791 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 32625/35720 Training loss: 0.6791 0.2080 sec/batch\n",
      "Epoch 19/20  Iteration 32626/35720 Training loss: 0.6790 0.2215 sec/batch\n",
      "Epoch 19/20  Iteration 32627/35720 Training loss: 0.6791 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 32628/35720 Training loss: 0.6790 0.2213 sec/batch\n",
      "Epoch 19/20  Iteration 32629/35720 Training loss: 0.6790 0.2160 sec/batch\n",
      "Epoch 19/20  Iteration 32630/35720 Training loss: 0.6790 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 32631/35720 Training loss: 0.6791 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 32632/35720 Training loss: 0.6790 0.2146 sec/batch\n",
      "Epoch 19/20  Iteration 32633/35720 Training loss: 0.6791 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 32634/35720 Training loss: 0.6791 0.2160 sec/batch\n",
      "Epoch 19/20  Iteration 32635/35720 Training loss: 0.6791 0.2122 sec/batch\n",
      "Epoch 19/20  Iteration 32636/35720 Training loss: 0.6790 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 32637/35720 Training loss: 0.6790 0.2219 sec/batch\n",
      "Epoch 19/20  Iteration 32638/35720 Training loss: 0.6790 0.2220 sec/batch\n",
      "Epoch 19/20  Iteration 32639/35720 Training loss: 0.6790 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 32640/35720 Training loss: 0.6790 0.2206 sec/batch\n",
      "Epoch 19/20  Iteration 32641/35720 Training loss: 0.6790 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 32642/35720 Training loss: 0.6789 0.2201 sec/batch\n",
      "Epoch 19/20  Iteration 32643/35720 Training loss: 0.6789 0.2184 sec/batch\n",
      "Epoch 19/20  Iteration 32644/35720 Training loss: 0.6788 0.2119 sec/batch\n",
      "Epoch 19/20  Iteration 32645/35720 Training loss: 0.6789 0.2176 sec/batch\n",
      "Epoch 19/20  Iteration 32646/35720 Training loss: 0.6789 0.2097 sec/batch\n",
      "Epoch 19/20  Iteration 32647/35720 Training loss: 0.6789 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 32648/35720 Training loss: 0.6789 0.2082 sec/batch\n",
      "Epoch 19/20  Iteration 32649/35720 Training loss: 0.6787 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32650/35720 Training loss: 0.6787 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 32651/35720 Training loss: 0.6786 0.2187 sec/batch\n",
      "Epoch 19/20  Iteration 32652/35720 Training loss: 0.6785 0.2196 sec/batch\n",
      "Epoch 19/20  Iteration 32653/35720 Training loss: 0.6785 0.2190 sec/batch\n",
      "Epoch 19/20  Iteration 32654/35720 Training loss: 0.6786 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 32655/35720 Training loss: 0.6785 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 32656/35720 Training loss: 0.6785 0.2210 sec/batch\n",
      "Epoch 19/20  Iteration 32657/35720 Training loss: 0.6786 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 32658/35720 Training loss: 0.6786 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 32659/35720 Training loss: 0.6785 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 32660/35720 Training loss: 0.6785 0.2174 sec/batch\n",
      "Epoch 19/20  Iteration 32661/35720 Training loss: 0.6785 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 32662/35720 Training loss: 0.6785 0.2343 sec/batch\n",
      "Epoch 19/20  Iteration 32663/35720 Training loss: 0.6785 0.2306 sec/batch\n",
      "Epoch 19/20  Iteration 32664/35720 Training loss: 0.6786 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32665/35720 Training loss: 0.6787 0.2269 sec/batch\n",
      "Epoch 19/20  Iteration 32666/35720 Training loss: 0.6786 0.2211 sec/batch\n",
      "Epoch 19/20  Iteration 32667/35720 Training loss: 0.6786 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 32668/35720 Training loss: 0.6787 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 32669/35720 Training loss: 0.6787 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32670/35720 Training loss: 0.6786 0.2119 sec/batch\n",
      "Epoch 19/20  Iteration 32671/35720 Training loss: 0.6786 0.2256 sec/batch\n",
      "Epoch 19/20  Iteration 32672/35720 Training loss: 0.6785 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 32673/35720 Training loss: 0.6785 0.2209 sec/batch\n",
      "Epoch 19/20  Iteration 32674/35720 Training loss: 0.6784 0.2279 sec/batch\n",
      "Epoch 19/20  Iteration 32675/35720 Training loss: 0.6785 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32676/35720 Training loss: 0.6785 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32677/35720 Training loss: 0.6785 0.2186 sec/batch\n",
      "Epoch 19/20  Iteration 32678/35720 Training loss: 0.6784 0.2224 sec/batch\n",
      "Epoch 19/20  Iteration 32679/35720 Training loss: 0.6785 0.2217 sec/batch\n",
      "Epoch 19/20  Iteration 32680/35720 Training loss: 0.6785 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 32681/35720 Training loss: 0.6785 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 32682/35720 Training loss: 0.6785 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 32683/35720 Training loss: 0.6784 0.2212 sec/batch\n",
      "Epoch 19/20  Iteration 32684/35720 Training loss: 0.6783 0.2192 sec/batch\n",
      "Epoch 19/20  Iteration 32685/35720 Training loss: 0.6783 0.2271 sec/batch\n",
      "Epoch 19/20  Iteration 32686/35720 Training loss: 0.6782 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 32687/35720 Training loss: 0.6782 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 32688/35720 Training loss: 0.6781 0.2242 sec/batch\n",
      "Epoch 19/20  Iteration 32689/35720 Training loss: 0.6781 0.2253 sec/batch\n",
      "Epoch 19/20  Iteration 32690/35720 Training loss: 0.6780 0.2056 sec/batch\n",
      "Epoch 19/20  Iteration 32691/35720 Training loss: 0.6780 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 32692/35720 Training loss: 0.6780 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 32693/35720 Training loss: 0.6781 0.2198 sec/batch\n",
      "Epoch 19/20  Iteration 32694/35720 Training loss: 0.6782 0.2287 sec/batch\n",
      "Epoch 19/20  Iteration 32695/35720 Training loss: 0.6782 0.2369 sec/batch\n",
      "Epoch 19/20  Iteration 32696/35720 Training loss: 0.6782 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32697/35720 Training loss: 0.6782 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 32698/35720 Training loss: 0.6782 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32699/35720 Training loss: 0.6782 0.2186 sec/batch\n",
      "Epoch 19/20  Iteration 32700/35720 Training loss: 0.6781 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 32701/35720 Training loss: 0.6780 0.2175 sec/batch\n",
      "Epoch 19/20  Iteration 32702/35720 Training loss: 0.6780 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32703/35720 Training loss: 0.6781 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32704/35720 Training loss: 0.6782 0.2299 sec/batch\n",
      "Epoch 19/20  Iteration 32705/35720 Training loss: 0.6781 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 32706/35720 Training loss: 0.6782 0.2161 sec/batch\n",
      "Epoch 19/20  Iteration 32707/35720 Training loss: 0.6782 0.2296 sec/batch\n",
      "Epoch 19/20  Iteration 32708/35720 Training loss: 0.6781 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 32709/35720 Training loss: 0.6782 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 32710/35720 Training loss: 0.6781 0.2237 sec/batch\n",
      "Epoch 19/20  Iteration 32711/35720 Training loss: 0.6780 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 32712/35720 Training loss: 0.6780 0.2265 sec/batch\n",
      "Epoch 19/20  Iteration 32713/35720 Training loss: 0.6779 0.2184 sec/batch\n",
      "Epoch 19/20  Iteration 32714/35720 Training loss: 0.6780 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 32715/35720 Training loss: 0.6780 0.2198 sec/batch\n",
      "Epoch 19/20  Iteration 32716/35720 Training loss: 0.6779 0.2185 sec/batch\n",
      "Epoch 19/20  Iteration 32717/35720 Training loss: 0.6778 0.2191 sec/batch\n",
      "Epoch 19/20  Iteration 32718/35720 Training loss: 0.6777 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 32719/35720 Training loss: 0.6777 0.2058 sec/batch\n",
      "Epoch 19/20  Iteration 32720/35720 Training loss: 0.6777 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32721/35720 Training loss: 0.6778 0.2425 sec/batch\n",
      "Epoch 19/20  Iteration 32722/35720 Training loss: 0.6779 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32723/35720 Training loss: 0.6779 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 32724/35720 Training loss: 0.6779 0.2124 sec/batch\n",
      "Epoch 19/20  Iteration 32725/35720 Training loss: 0.6780 0.2231 sec/batch\n",
      "Epoch 19/20  Iteration 32726/35720 Training loss: 0.6780 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 32727/35720 Training loss: 0.6779 0.2304 sec/batch\n",
      "Epoch 19/20  Iteration 32728/35720 Training loss: 0.6779 0.2247 sec/batch\n",
      "Epoch 19/20  Iteration 32729/35720 Training loss: 0.6780 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 32730/35720 Training loss: 0.6780 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32731/35720 Training loss: 0.6780 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 32732/35720 Training loss: 0.6780 0.2217 sec/batch\n",
      "Epoch 19/20  Iteration 32733/35720 Training loss: 0.6780 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 32734/35720 Training loss: 0.6779 0.2170 sec/batch\n",
      "Epoch 19/20  Iteration 32735/35720 Training loss: 0.6779 0.2059 sec/batch\n",
      "Epoch 19/20  Iteration 32736/35720 Training loss: 0.6778 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32737/35720 Training loss: 0.6777 0.2340 sec/batch\n",
      "Epoch 19/20  Iteration 32738/35720 Training loss: 0.6777 0.2216 sec/batch\n",
      "Epoch 19/20  Iteration 32739/35720 Training loss: 0.6777 0.2204 sec/batch\n",
      "Epoch 19/20  Iteration 32740/35720 Training loss: 0.6778 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 32741/35720 Training loss: 0.6778 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32742/35720 Training loss: 0.6778 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 32743/35720 Training loss: 0.6777 0.2357 sec/batch\n",
      "Epoch 19/20  Iteration 32744/35720 Training loss: 0.6777 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 32745/35720 Training loss: 0.6776 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 32746/35720 Training loss: 0.6776 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 32747/35720 Training loss: 0.6775 0.2210 sec/batch\n",
      "Epoch 19/20  Iteration 32748/35720 Training loss: 0.6774 0.2178 sec/batch\n",
      "Epoch 19/20  Iteration 32749/35720 Training loss: 0.6773 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 32750/35720 Training loss: 0.6772 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32751/35720 Training loss: 0.6772 0.2157 sec/batch\n",
      "Epoch 19/20  Iteration 32752/35720 Training loss: 0.6771 0.2227 sec/batch\n",
      "Epoch 19/20  Iteration 32753/35720 Training loss: 0.6771 0.2160 sec/batch\n",
      "Epoch 19/20  Iteration 32754/35720 Training loss: 0.6771 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 32755/35720 Training loss: 0.6771 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 32756/35720 Training loss: 0.6770 0.2230 sec/batch\n",
      "Epoch 19/20  Iteration 32757/35720 Training loss: 0.6771 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32758/35720 Training loss: 0.6770 0.2058 sec/batch\n",
      "Epoch 19/20  Iteration 32759/35720 Training loss: 0.6770 0.2194 sec/batch\n",
      "Epoch 19/20  Iteration 32760/35720 Training loss: 0.6769 0.2342 sec/batch\n",
      "Epoch 19/20  Iteration 32761/35720 Training loss: 0.6769 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 32762/35720 Training loss: 0.6769 0.2262 sec/batch\n",
      "Epoch 19/20  Iteration 32763/35720 Training loss: 0.6769 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 32764/35720 Training loss: 0.6768 0.2185 sec/batch\n",
      "Epoch 19/20  Iteration 32765/35720 Training loss: 0.6767 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 32766/35720 Training loss: 0.6767 0.2189 sec/batch\n",
      "Epoch 19/20  Iteration 32767/35720 Training loss: 0.6767 0.2255 sec/batch\n",
      "Epoch 19/20  Iteration 32768/35720 Training loss: 0.6766 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32769/35720 Training loss: 0.6766 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 32770/35720 Training loss: 0.6766 0.2186 sec/batch\n",
      "Epoch 19/20  Iteration 32771/35720 Training loss: 0.6766 0.2161 sec/batch\n",
      "Epoch 19/20  Iteration 32772/35720 Training loss: 0.6765 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 32773/35720 Training loss: 0.6765 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 32774/35720 Training loss: 0.6765 0.2050 sec/batch\n",
      "Epoch 19/20  Iteration 32775/35720 Training loss: 0.6766 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32776/35720 Training loss: 0.6766 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 32777/35720 Training loss: 0.6766 0.2142 sec/batch\n",
      "Epoch 19/20  Iteration 32778/35720 Training loss: 0.6766 0.2057 sec/batch\n",
      "Epoch 19/20  Iteration 32779/35720 Training loss: 0.6766 0.2157 sec/batch\n",
      "Epoch 19/20  Iteration 32780/35720 Training loss: 0.6765 0.2222 sec/batch\n",
      "Epoch 19/20  Iteration 32781/35720 Training loss: 0.6765 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 32782/35720 Training loss: 0.6765 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 32783/35720 Training loss: 0.6764 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 32784/35720 Training loss: 0.6764 0.2293 sec/batch\n",
      "Epoch 19/20  Iteration 32785/35720 Training loss: 0.6764 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32786/35720 Training loss: 0.6764 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32787/35720 Training loss: 0.6764 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 32788/35720 Training loss: 0.6764 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 32789/35720 Training loss: 0.6765 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 32790/35720 Training loss: 0.6766 0.2172 sec/batch\n",
      "Epoch 19/20  Iteration 32791/35720 Training loss: 0.6766 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 32792/35720 Training loss: 0.6765 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32793/35720 Training loss: 0.6765 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 32794/35720 Training loss: 0.6765 0.2306 sec/batch\n",
      "Epoch 19/20  Iteration 32795/35720 Training loss: 0.6764 0.2121 sec/batch\n",
      "Epoch 19/20  Iteration 32796/35720 Training loss: 0.6764 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32797/35720 Training loss: 0.6763 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 32798/35720 Training loss: 0.6762 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 32799/35720 Training loss: 0.6762 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 32800/35720 Training loss: 0.6762 0.2093 sec/batch\n",
      "Validation loss: 1.64602 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 32801/35720 Training loss: 0.6769 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 32802/35720 Training loss: 0.6769 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32803/35720 Training loss: 0.6770 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 32804/35720 Training loss: 0.6770 0.2277 sec/batch\n",
      "Epoch 19/20  Iteration 32805/35720 Training loss: 0.6771 0.2213 sec/batch\n",
      "Epoch 19/20  Iteration 32806/35720 Training loss: 0.6771 0.2241 sec/batch\n",
      "Epoch 19/20  Iteration 32807/35720 Training loss: 0.6771 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32808/35720 Training loss: 0.6772 0.2156 sec/batch\n",
      "Epoch 19/20  Iteration 32809/35720 Training loss: 0.6772 0.2078 sec/batch\n",
      "Epoch 19/20  Iteration 32810/35720 Training loss: 0.6773 0.2234 sec/batch\n",
      "Epoch 19/20  Iteration 32811/35720 Training loss: 0.6772 0.2255 sec/batch\n",
      "Epoch 19/20  Iteration 32812/35720 Training loss: 0.6772 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 32813/35720 Training loss: 0.6773 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32814/35720 Training loss: 0.6773 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 32815/35720 Training loss: 0.6774 0.2152 sec/batch\n",
      "Epoch 19/20  Iteration 32816/35720 Training loss: 0.6773 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 32817/35720 Training loss: 0.6773 0.2205 sec/batch\n",
      "Epoch 19/20  Iteration 32818/35720 Training loss: 0.6774 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32819/35720 Training loss: 0.6772 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 32820/35720 Training loss: 0.6772 0.2286 sec/batch\n",
      "Epoch 19/20  Iteration 32821/35720 Training loss: 0.6773 0.2254 sec/batch\n",
      "Epoch 19/20  Iteration 32822/35720 Training loss: 0.6772 0.2189 sec/batch\n",
      "Epoch 19/20  Iteration 32823/35720 Training loss: 0.6772 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 32824/35720 Training loss: 0.6772 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32825/35720 Training loss: 0.6773 0.2231 sec/batch\n",
      "Epoch 19/20  Iteration 32826/35720 Training loss: 0.6773 0.2150 sec/batch\n",
      "Epoch 19/20  Iteration 32827/35720 Training loss: 0.6773 0.2189 sec/batch\n",
      "Epoch 19/20  Iteration 32828/35720 Training loss: 0.6772 0.2212 sec/batch\n",
      "Epoch 19/20  Iteration 32829/35720 Training loss: 0.6772 0.2131 sec/batch\n",
      "Epoch 19/20  Iteration 32830/35720 Training loss: 0.6772 0.2074 sec/batch\n",
      "Epoch 19/20  Iteration 32831/35720 Training loss: 0.6772 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 32832/35720 Training loss: 0.6772 0.2192 sec/batch\n",
      "Epoch 19/20  Iteration 32833/35720 Training loss: 0.6772 0.2180 sec/batch\n",
      "Epoch 19/20  Iteration 32834/35720 Training loss: 0.6771 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 32835/35720 Training loss: 0.6771 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 32836/35720 Training loss: 0.6771 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 32837/35720 Training loss: 0.6771 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 32838/35720 Training loss: 0.6771 0.2181 sec/batch\n",
      "Epoch 19/20  Iteration 32839/35720 Training loss: 0.6771 0.2308 sec/batch\n",
      "Epoch 19/20  Iteration 32840/35720 Training loss: 0.6772 0.2161 sec/batch\n",
      "Epoch 19/20  Iteration 32841/35720 Training loss: 0.6773 0.2059 sec/batch\n",
      "Epoch 19/20  Iteration 32842/35720 Training loss: 0.6773 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 32843/35720 Training loss: 0.6773 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 32844/35720 Training loss: 0.6772 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 32845/35720 Training loss: 0.6773 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 32846/35720 Training loss: 0.6773 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 32847/35720 Training loss: 0.6772 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32848/35720 Training loss: 0.6772 0.2334 sec/batch\n",
      "Epoch 19/20  Iteration 32849/35720 Training loss: 0.6771 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 32850/35720 Training loss: 0.6772 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 32851/35720 Training loss: 0.6771 0.2082 sec/batch\n",
      "Epoch 19/20  Iteration 32852/35720 Training loss: 0.6771 0.2411 sec/batch\n",
      "Epoch 19/20  Iteration 32853/35720 Training loss: 0.6771 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 32854/35720 Training loss: 0.6771 0.2242 sec/batch\n",
      "Epoch 19/20  Iteration 32855/35720 Training loss: 0.6772 0.2248 sec/batch\n",
      "Epoch 19/20  Iteration 32856/35720 Training loss: 0.6772 0.2055 sec/batch\n",
      "Epoch 19/20  Iteration 32857/35720 Training loss: 0.6773 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 32858/35720 Training loss: 0.6773 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 32859/35720 Training loss: 0.6774 0.2345 sec/batch\n",
      "Epoch 19/20  Iteration 32860/35720 Training loss: 0.6774 0.2228 sec/batch\n",
      "Epoch 19/20  Iteration 32861/35720 Training loss: 0.6774 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 32862/35720 Training loss: 0.6774 0.2110 sec/batch\n",
      "Epoch 19/20  Iteration 32863/35720 Training loss: 0.6774 0.2118 sec/batch\n",
      "Epoch 19/20  Iteration 32864/35720 Training loss: 0.6774 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32865/35720 Training loss: 0.6775 0.2181 sec/batch\n",
      "Epoch 19/20  Iteration 32866/35720 Training loss: 0.6775 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32867/35720 Training loss: 0.6775 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 32868/35720 Training loss: 0.6775 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 32869/35720 Training loss: 0.6775 0.2203 sec/batch\n",
      "Epoch 19/20  Iteration 32870/35720 Training loss: 0.6776 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 32871/35720 Training loss: 0.6777 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 32872/35720 Training loss: 0.6776 0.2322 sec/batch\n",
      "Epoch 19/20  Iteration 32873/35720 Training loss: 0.6775 0.2184 sec/batch\n",
      "Epoch 19/20  Iteration 32874/35720 Training loss: 0.6775 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32875/35720 Training loss: 0.6777 0.2131 sec/batch\n",
      "Epoch 19/20  Iteration 32876/35720 Training loss: 0.6777 0.2232 sec/batch\n",
      "Epoch 19/20  Iteration 32877/35720 Training loss: 0.6778 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32878/35720 Training loss: 0.6778 0.2248 sec/batch\n",
      "Epoch 19/20  Iteration 32879/35720 Training loss: 0.6778 0.2057 sec/batch\n",
      "Epoch 19/20  Iteration 32880/35720 Training loss: 0.6778 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 32881/35720 Training loss: 0.6777 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 32882/35720 Training loss: 0.6777 0.2307 sec/batch\n",
      "Epoch 19/20  Iteration 32883/35720 Training loss: 0.6777 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32884/35720 Training loss: 0.6777 0.2256 sec/batch\n",
      "Epoch 19/20  Iteration 32885/35720 Training loss: 0.6777 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 32886/35720 Training loss: 0.6777 0.2097 sec/batch\n",
      "Epoch 19/20  Iteration 32887/35720 Training loss: 0.6778 0.2152 sec/batch\n",
      "Epoch 19/20  Iteration 32888/35720 Training loss: 0.6778 0.2128 sec/batch\n",
      "Epoch 19/20  Iteration 32889/35720 Training loss: 0.6778 0.2304 sec/batch\n",
      "Epoch 19/20  Iteration 32890/35720 Training loss: 0.6778 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32891/35720 Training loss: 0.6778 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 32892/35720 Training loss: 0.6777 0.2192 sec/batch\n",
      "Epoch 19/20  Iteration 32893/35720 Training loss: 0.6777 0.2201 sec/batch\n",
      "Epoch 19/20  Iteration 32894/35720 Training loss: 0.6777 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 32895/35720 Training loss: 0.6777 0.2187 sec/batch\n",
      "Epoch 19/20  Iteration 32896/35720 Training loss: 0.6776 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 32897/35720 Training loss: 0.6776 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 32898/35720 Training loss: 0.6776 0.2210 sec/batch\n",
      "Epoch 19/20  Iteration 32899/35720 Training loss: 0.6776 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 32900/35720 Training loss: 0.6777 0.2161 sec/batch\n",
      "Epoch 19/20  Iteration 32901/35720 Training loss: 0.6776 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32902/35720 Training loss: 0.6775 0.2072 sec/batch\n",
      "Epoch 19/20  Iteration 32903/35720 Training loss: 0.6774 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32904/35720 Training loss: 0.6774 0.2146 sec/batch\n",
      "Epoch 19/20  Iteration 32905/35720 Training loss: 0.6774 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 32906/35720 Training loss: 0.6774 0.2289 sec/batch\n",
      "Epoch 19/20  Iteration 32907/35720 Training loss: 0.6774 0.2187 sec/batch\n",
      "Epoch 19/20  Iteration 32908/35720 Training loss: 0.6774 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32909/35720 Training loss: 0.6774 0.2156 sec/batch\n",
      "Epoch 19/20  Iteration 32910/35720 Training loss: 0.6774 0.2082 sec/batch\n",
      "Epoch 19/20  Iteration 32911/35720 Training loss: 0.6774 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 32912/35720 Training loss: 0.6774 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 32913/35720 Training loss: 0.6774 0.2056 sec/batch\n",
      "Epoch 19/20  Iteration 32914/35720 Training loss: 0.6774 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 32915/35720 Training loss: 0.6773 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 32916/35720 Training loss: 0.6773 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 32917/35720 Training loss: 0.6774 0.2396 sec/batch\n",
      "Epoch 19/20  Iteration 32918/35720 Training loss: 0.6774 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32919/35720 Training loss: 0.6775 0.2205 sec/batch\n",
      "Epoch 19/20  Iteration 32920/35720 Training loss: 0.6775 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 32921/35720 Training loss: 0.6775 0.2428 sec/batch\n",
      "Epoch 19/20  Iteration 32922/35720 Training loss: 0.6774 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 32923/35720 Training loss: 0.6774 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 32924/35720 Training loss: 0.6773 0.2232 sec/batch\n",
      "Epoch 19/20  Iteration 32925/35720 Training loss: 0.6773 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 32926/35720 Training loss: 0.6773 0.2134 sec/batch\n",
      "Epoch 19/20  Iteration 32927/35720 Training loss: 0.6774 0.2153 sec/batch\n",
      "Epoch 19/20  Iteration 32928/35720 Training loss: 0.6773 0.2350 sec/batch\n",
      "Epoch 19/20  Iteration 32929/35720 Training loss: 0.6773 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 32930/35720 Training loss: 0.6773 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32931/35720 Training loss: 0.6773 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 32932/35720 Training loss: 0.6773 0.2369 sec/batch\n",
      "Epoch 19/20  Iteration 32933/35720 Training loss: 0.6773 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 32934/35720 Training loss: 0.6772 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 32935/35720 Training loss: 0.6773 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 32936/35720 Training loss: 0.6773 0.2152 sec/batch\n",
      "Epoch 19/20  Iteration 32937/35720 Training loss: 0.6773 0.2167 sec/batch\n",
      "Epoch 19/20  Iteration 32938/35720 Training loss: 0.6773 0.2357 sec/batch\n",
      "Epoch 19/20  Iteration 32939/35720 Training loss: 0.6773 0.2167 sec/batch\n",
      "Epoch 19/20  Iteration 32940/35720 Training loss: 0.6773 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 32941/35720 Training loss: 0.6773 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 32942/35720 Training loss: 0.6773 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 32943/35720 Training loss: 0.6773 0.2162 sec/batch\n",
      "Epoch 19/20  Iteration 32944/35720 Training loss: 0.6773 0.2231 sec/batch\n",
      "Epoch 19/20  Iteration 32945/35720 Training loss: 0.6773 0.2072 sec/batch\n",
      "Epoch 19/20  Iteration 32946/35720 Training loss: 0.6773 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 32947/35720 Training loss: 0.6773 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 32948/35720 Training loss: 0.6774 0.2155 sec/batch\n",
      "Epoch 19/20  Iteration 32949/35720 Training loss: 0.6774 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32950/35720 Training loss: 0.6775 0.2257 sec/batch\n",
      "Epoch 19/20  Iteration 32951/35720 Training loss: 0.6774 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 32952/35720 Training loss: 0.6775 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32953/35720 Training loss: 0.6776 0.2110 sec/batch\n",
      "Epoch 19/20  Iteration 32954/35720 Training loss: 0.6775 0.2131 sec/batch\n",
      "Epoch 19/20  Iteration 32955/35720 Training loss: 0.6776 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32956/35720 Training loss: 0.6775 0.2233 sec/batch\n",
      "Epoch 19/20  Iteration 32957/35720 Training loss: 0.6775 0.2147 sec/batch\n",
      "Epoch 19/20  Iteration 32958/35720 Training loss: 0.6776 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 32959/35720 Training loss: 0.6776 0.2210 sec/batch\n",
      "Epoch 19/20  Iteration 32960/35720 Training loss: 0.6776 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 32961/35720 Training loss: 0.6776 0.2254 sec/batch\n",
      "Epoch 19/20  Iteration 32962/35720 Training loss: 0.6777 0.2186 sec/batch\n",
      "Epoch 19/20  Iteration 32963/35720 Training loss: 0.6777 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 32964/35720 Training loss: 0.6777 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 32965/35720 Training loss: 0.6777 0.2162 sec/batch\n",
      "Epoch 19/20  Iteration 32966/35720 Training loss: 0.6777 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 32967/35720 Training loss: 0.6777 0.2161 sec/batch\n",
      "Epoch 19/20  Iteration 32968/35720 Training loss: 0.6777 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32969/35720 Training loss: 0.6777 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 32970/35720 Training loss: 0.6777 0.2305 sec/batch\n",
      "Epoch 19/20  Iteration 32971/35720 Training loss: 0.6776 0.2273 sec/batch\n",
      "Epoch 19/20  Iteration 32972/35720 Training loss: 0.6776 0.2226 sec/batch\n",
      "Epoch 19/20  Iteration 32973/35720 Training loss: 0.6775 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 32974/35720 Training loss: 0.6775 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 32975/35720 Training loss: 0.6775 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 32976/35720 Training loss: 0.6774 0.2146 sec/batch\n",
      "Epoch 19/20  Iteration 32977/35720 Training loss: 0.6774 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32978/35720 Training loss: 0.6774 0.2183 sec/batch\n",
      "Epoch 19/20  Iteration 32979/35720 Training loss: 0.6774 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 32980/35720 Training loss: 0.6774 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 32981/35720 Training loss: 0.6775 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 32982/35720 Training loss: 0.6775 0.2189 sec/batch\n",
      "Epoch 19/20  Iteration 32983/35720 Training loss: 0.6775 0.2252 sec/batch\n",
      "Epoch 19/20  Iteration 32984/35720 Training loss: 0.6775 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32985/35720 Training loss: 0.6775 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 32986/35720 Training loss: 0.6775 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 32987/35720 Training loss: 0.6775 0.2243 sec/batch\n",
      "Epoch 19/20  Iteration 32988/35720 Training loss: 0.6775 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 32989/35720 Training loss: 0.6776 0.2244 sec/batch\n",
      "Epoch 19/20  Iteration 32990/35720 Training loss: 0.6775 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 32991/35720 Training loss: 0.6775 0.2053 sec/batch\n",
      "Epoch 19/20  Iteration 32992/35720 Training loss: 0.6776 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 32993/35720 Training loss: 0.6777 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 32994/35720 Training loss: 0.6777 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 32995/35720 Training loss: 0.6777 0.2212 sec/batch\n",
      "Epoch 19/20  Iteration 32996/35720 Training loss: 0.6777 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 32997/35720 Training loss: 0.6776 0.2074 sec/batch\n",
      "Epoch 19/20  Iteration 32998/35720 Training loss: 0.6777 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 32999/35720 Training loss: 0.6777 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 33000/35720 Training loss: 0.6777 0.2097 sec/batch\n",
      "Validation loss: 1.63953 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 33001/35720 Training loss: 0.6783 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33002/35720 Training loss: 0.6783 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 33003/35720 Training loss: 0.6784 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 33004/35720 Training loss: 0.6784 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 33005/35720 Training loss: 0.6783 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33006/35720 Training loss: 0.6784 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 33007/35720 Training loss: 0.6783 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 33008/35720 Training loss: 0.6783 0.2143 sec/batch\n",
      "Epoch 19/20  Iteration 33009/35720 Training loss: 0.6783 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33010/35720 Training loss: 0.6783 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33011/35720 Training loss: 0.6783 0.2201 sec/batch\n",
      "Epoch 19/20  Iteration 33012/35720 Training loss: 0.6782 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 33013/35720 Training loss: 0.6782 0.2175 sec/batch\n",
      "Epoch 19/20  Iteration 33014/35720 Training loss: 0.6782 0.2183 sec/batch\n",
      "Epoch 19/20  Iteration 33015/35720 Training loss: 0.6783 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 33016/35720 Training loss: 0.6782 0.2344 sec/batch\n",
      "Epoch 19/20  Iteration 33017/35720 Training loss: 0.6782 0.2215 sec/batch\n",
      "Epoch 19/20  Iteration 33018/35720 Training loss: 0.6782 0.2111 sec/batch\n",
      "Epoch 19/20  Iteration 33019/35720 Training loss: 0.6782 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 33020/35720 Training loss: 0.6782 0.2255 sec/batch\n",
      "Epoch 19/20  Iteration 33021/35720 Training loss: 0.6781 0.2299 sec/batch\n",
      "Epoch 19/20  Iteration 33022/35720 Training loss: 0.6781 0.2245 sec/batch\n",
      "Epoch 19/20  Iteration 33023/35720 Training loss: 0.6781 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 33024/35720 Training loss: 0.6781 0.2131 sec/batch\n",
      "Epoch 19/20  Iteration 33025/35720 Training loss: 0.6781 0.2176 sec/batch\n",
      "Epoch 19/20  Iteration 33026/35720 Training loss: 0.6781 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33027/35720 Training loss: 0.6781 0.2194 sec/batch\n",
      "Epoch 19/20  Iteration 33028/35720 Training loss: 0.6780 0.2118 sec/batch\n",
      "Epoch 19/20  Iteration 33029/35720 Training loss: 0.6780 0.2197 sec/batch\n",
      "Epoch 19/20  Iteration 33030/35720 Training loss: 0.6779 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 33031/35720 Training loss: 0.6780 0.2254 sec/batch\n",
      "Epoch 19/20  Iteration 33032/35720 Training loss: 0.6779 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 33033/35720 Training loss: 0.6779 0.2157 sec/batch\n",
      "Epoch 19/20  Iteration 33034/35720 Training loss: 0.6778 0.2188 sec/batch\n",
      "Epoch 19/20  Iteration 33035/35720 Training loss: 0.6778 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33036/35720 Training loss: 0.6778 0.2170 sec/batch\n",
      "Epoch 19/20  Iteration 33037/35720 Training loss: 0.6778 0.2134 sec/batch\n",
      "Epoch 19/20  Iteration 33038/35720 Training loss: 0.6777 0.2076 sec/batch\n",
      "Epoch 19/20  Iteration 33039/35720 Training loss: 0.6777 0.2219 sec/batch\n",
      "Epoch 19/20  Iteration 33040/35720 Training loss: 0.6777 0.2153 sec/batch\n",
      "Epoch 19/20  Iteration 33041/35720 Training loss: 0.6776 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33042/35720 Training loss: 0.6776 0.2167 sec/batch\n",
      "Epoch 19/20  Iteration 33043/35720 Training loss: 0.6776 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 33044/35720 Training loss: 0.6776 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33045/35720 Training loss: 0.6776 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 33046/35720 Training loss: 0.6775 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33047/35720 Training loss: 0.6774 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 33048/35720 Training loss: 0.6774 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 33049/35720 Training loss: 0.6774 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33050/35720 Training loss: 0.6773 0.2263 sec/batch\n",
      "Epoch 19/20  Iteration 33051/35720 Training loss: 0.6773 0.2236 sec/batch\n",
      "Epoch 19/20  Iteration 33052/35720 Training loss: 0.6773 0.2157 sec/batch\n",
      "Epoch 19/20  Iteration 33053/35720 Training loss: 0.6773 0.2210 sec/batch\n",
      "Epoch 19/20  Iteration 33054/35720 Training loss: 0.6773 0.2187 sec/batch\n",
      "Epoch 19/20  Iteration 33055/35720 Training loss: 0.6772 0.2201 sec/batch\n",
      "Epoch 19/20  Iteration 33056/35720 Training loss: 0.6772 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 33057/35720 Training loss: 0.6772 0.2118 sec/batch\n",
      "Epoch 19/20  Iteration 33058/35720 Training loss: 0.6772 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33059/35720 Training loss: 0.6772 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 33060/35720 Training loss: 0.6772 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33061/35720 Training loss: 0.6771 0.2192 sec/batch\n",
      "Epoch 19/20  Iteration 33062/35720 Training loss: 0.6771 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 33063/35720 Training loss: 0.6772 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 33064/35720 Training loss: 0.6772 0.2268 sec/batch\n",
      "Epoch 19/20  Iteration 33065/35720 Training loss: 0.6772 0.2310 sec/batch\n",
      "Epoch 19/20  Iteration 33066/35720 Training loss: 0.6772 0.2156 sec/batch\n",
      "Epoch 19/20  Iteration 33067/35720 Training loss: 0.6771 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33068/35720 Training loss: 0.6771 0.2073 sec/batch\n",
      "Epoch 19/20  Iteration 33069/35720 Training loss: 0.6772 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33070/35720 Training loss: 0.6771 0.2209 sec/batch\n",
      "Epoch 19/20  Iteration 33071/35720 Training loss: 0.6771 0.2082 sec/batch\n",
      "Epoch 19/20  Iteration 33072/35720 Training loss: 0.6772 0.2272 sec/batch\n",
      "Epoch 19/20  Iteration 33073/35720 Training loss: 0.6772 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 33074/35720 Training loss: 0.6772 0.2171 sec/batch\n",
      "Epoch 19/20  Iteration 33075/35720 Training loss: 0.6772 0.2612 sec/batch\n",
      "Epoch 19/20  Iteration 33076/35720 Training loss: 0.6771 0.2332 sec/batch\n",
      "Epoch 19/20  Iteration 33077/35720 Training loss: 0.6772 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33078/35720 Training loss: 0.6771 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 33079/35720 Training loss: 0.6771 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 33080/35720 Training loss: 0.6771 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33081/35720 Training loss: 0.6771 0.2217 sec/batch\n",
      "Epoch 19/20  Iteration 33082/35720 Training loss: 0.6771 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 33083/35720 Training loss: 0.6770 0.2246 sec/batch\n",
      "Epoch 19/20  Iteration 33084/35720 Training loss: 0.6769 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33085/35720 Training loss: 0.6769 0.2059 sec/batch\n",
      "Epoch 19/20  Iteration 33086/35720 Training loss: 0.6768 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33087/35720 Training loss: 0.6768 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33088/35720 Training loss: 0.6767 0.2160 sec/batch\n",
      "Epoch 19/20  Iteration 33089/35720 Training loss: 0.6767 0.2185 sec/batch\n",
      "Epoch 19/20  Iteration 33090/35720 Training loss: 0.6767 0.2204 sec/batch\n",
      "Epoch 19/20  Iteration 33091/35720 Training loss: 0.6767 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 33092/35720 Training loss: 0.6767 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33093/35720 Training loss: 0.6767 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33094/35720 Training loss: 0.6767 0.2213 sec/batch\n",
      "Epoch 19/20  Iteration 33095/35720 Training loss: 0.6767 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 33096/35720 Training loss: 0.6766 0.2076 sec/batch\n",
      "Epoch 19/20  Iteration 33097/35720 Training loss: 0.6766 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 33098/35720 Training loss: 0.6766 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 33099/35720 Training loss: 0.6765 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33100/35720 Training loss: 0.6765 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 33101/35720 Training loss: 0.6765 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33102/35720 Training loss: 0.6765 0.2123 sec/batch\n",
      "Epoch 19/20  Iteration 33103/35720 Training loss: 0.6764 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 33104/35720 Training loss: 0.6764 0.2331 sec/batch\n",
      "Epoch 19/20  Iteration 33105/35720 Training loss: 0.6764 0.2266 sec/batch\n",
      "Epoch 19/20  Iteration 33106/35720 Training loss: 0.6764 0.2075 sec/batch\n",
      "Epoch 19/20  Iteration 33107/35720 Training loss: 0.6764 0.2056 sec/batch\n",
      "Epoch 19/20  Iteration 33108/35720 Training loss: 0.6763 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33109/35720 Training loss: 0.6763 0.2324 sec/batch\n",
      "Epoch 19/20  Iteration 33110/35720 Training loss: 0.6763 0.2206 sec/batch\n",
      "Epoch 19/20  Iteration 33111/35720 Training loss: 0.6762 0.2205 sec/batch\n",
      "Epoch 19/20  Iteration 33112/35720 Training loss: 0.6762 0.2110 sec/batch\n",
      "Epoch 19/20  Iteration 33113/35720 Training loss: 0.6762 0.2177 sec/batch\n",
      "Epoch 19/20  Iteration 33114/35720 Training loss: 0.6762 0.2191 sec/batch\n",
      "Epoch 19/20  Iteration 33115/35720 Training loss: 0.6762 0.2161 sec/batch\n",
      "Epoch 19/20  Iteration 33116/35720 Training loss: 0.6761 0.2273 sec/batch\n",
      "Epoch 19/20  Iteration 33117/35720 Training loss: 0.6762 0.2170 sec/batch\n",
      "Epoch 19/20  Iteration 33118/35720 Training loss: 0.6763 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 33119/35720 Training loss: 0.6762 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33120/35720 Training loss: 0.6761 0.2253 sec/batch\n",
      "Epoch 19/20  Iteration 33121/35720 Training loss: 0.6761 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33122/35720 Training loss: 0.6760 0.2284 sec/batch\n",
      "Epoch 19/20  Iteration 33123/35720 Training loss: 0.6760 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33124/35720 Training loss: 0.6760 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33125/35720 Training loss: 0.6760 0.2079 sec/batch\n",
      "Epoch 19/20  Iteration 33126/35720 Training loss: 0.6760 0.2176 sec/batch\n",
      "Epoch 19/20  Iteration 33127/35720 Training loss: 0.6760 0.2174 sec/batch\n",
      "Epoch 19/20  Iteration 33128/35720 Training loss: 0.6760 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33129/35720 Training loss: 0.6759 0.2055 sec/batch\n",
      "Epoch 19/20  Iteration 33130/35720 Training loss: 0.6759 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 33131/35720 Training loss: 0.6759 0.2272 sec/batch\n",
      "Epoch 19/20  Iteration 33132/35720 Training loss: 0.6758 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 33133/35720 Training loss: 0.6758 0.2236 sec/batch\n",
      "Epoch 19/20  Iteration 33134/35720 Training loss: 0.6757 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 33135/35720 Training loss: 0.6757 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 33136/35720 Training loss: 0.6757 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 33137/35720 Training loss: 0.6757 0.2265 sec/batch\n",
      "Epoch 19/20  Iteration 33138/35720 Training loss: 0.6756 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 33139/35720 Training loss: 0.6756 0.2444 sec/batch\n",
      "Epoch 19/20  Iteration 33140/35720 Training loss: 0.6756 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33141/35720 Training loss: 0.6756 0.2082 sec/batch\n",
      "Epoch 19/20  Iteration 33142/35720 Training loss: 0.6756 0.2253 sec/batch\n",
      "Epoch 19/20  Iteration 33143/35720 Training loss: 0.6756 0.2128 sec/batch\n",
      "Epoch 19/20  Iteration 33144/35720 Training loss: 0.6756 0.2077 sec/batch\n",
      "Epoch 19/20  Iteration 33145/35720 Training loss: 0.6756 0.2287 sec/batch\n",
      "Epoch 19/20  Iteration 33146/35720 Training loss: 0.6756 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33147/35720 Training loss: 0.6756 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33148/35720 Training loss: 0.6755 0.2228 sec/batch\n",
      "Epoch 19/20  Iteration 33149/35720 Training loss: 0.6755 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 33150/35720 Training loss: 0.6754 0.2162 sec/batch\n",
      "Epoch 19/20  Iteration 33151/35720 Training loss: 0.6753 0.2111 sec/batch\n",
      "Epoch 19/20  Iteration 33152/35720 Training loss: 0.6753 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33153/35720 Training loss: 0.6752 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 33154/35720 Training loss: 0.6752 0.2257 sec/batch\n",
      "Epoch 19/20  Iteration 33155/35720 Training loss: 0.6751 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 33156/35720 Training loss: 0.6751 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 33157/35720 Training loss: 0.6750 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33158/35720 Training loss: 0.6750 0.2197 sec/batch\n",
      "Epoch 19/20  Iteration 33159/35720 Training loss: 0.6750 0.2171 sec/batch\n",
      "Epoch 19/20  Iteration 33160/35720 Training loss: 0.6750 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33161/35720 Training loss: 0.6749 0.2296 sec/batch\n",
      "Epoch 19/20  Iteration 33162/35720 Training loss: 0.6749 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 33163/35720 Training loss: 0.6749 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33164/35720 Training loss: 0.6749 0.2213 sec/batch\n",
      "Epoch 19/20  Iteration 33165/35720 Training loss: 0.6749 0.2200 sec/batch\n",
      "Epoch 19/20  Iteration 33166/35720 Training loss: 0.6749 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33167/35720 Training loss: 0.6748 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 33168/35720 Training loss: 0.6748 0.2124 sec/batch\n",
      "Epoch 19/20  Iteration 33169/35720 Training loss: 0.6747 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33170/35720 Training loss: 0.6748 0.2475 sec/batch\n",
      "Epoch 19/20  Iteration 33171/35720 Training loss: 0.6748 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33172/35720 Training loss: 0.6748 0.2226 sec/batch\n",
      "Epoch 19/20  Iteration 33173/35720 Training loss: 0.6748 0.2249 sec/batch\n",
      "Epoch 19/20  Iteration 33174/35720 Training loss: 0.6748 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 33175/35720 Training loss: 0.6747 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 33176/35720 Training loss: 0.6748 0.2226 sec/batch\n",
      "Epoch 19/20  Iteration 33177/35720 Training loss: 0.6747 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33178/35720 Training loss: 0.6747 0.2076 sec/batch\n",
      "Epoch 19/20  Iteration 33179/35720 Training loss: 0.6747 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 33180/35720 Training loss: 0.6747 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 33181/35720 Training loss: 0.6746 0.2217 sec/batch\n",
      "Epoch 19/20  Iteration 33182/35720 Training loss: 0.6746 0.2351 sec/batch\n",
      "Epoch 19/20  Iteration 33183/35720 Training loss: 0.6747 0.2118 sec/batch\n",
      "Epoch 19/20  Iteration 33184/35720 Training loss: 0.6747 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 33185/35720 Training loss: 0.6748 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 33186/35720 Training loss: 0.6748 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 33187/35720 Training loss: 0.6748 0.2153 sec/batch\n",
      "Epoch 19/20  Iteration 33188/35720 Training loss: 0.6748 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 33189/35720 Training loss: 0.6748 0.2192 sec/batch\n",
      "Epoch 19/20  Iteration 33190/35720 Training loss: 0.6748 0.2220 sec/batch\n",
      "Epoch 19/20  Iteration 33191/35720 Training loss: 0.6747 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33192/35720 Training loss: 0.6748 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 33193/35720 Training loss: 0.6747 0.2138 sec/batch\n",
      "Epoch 19/20  Iteration 33194/35720 Training loss: 0.6748 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 33195/35720 Training loss: 0.6748 0.2261 sec/batch\n",
      "Epoch 19/20  Iteration 33196/35720 Training loss: 0.6748 0.2180 sec/batch\n",
      "Epoch 19/20  Iteration 33197/35720 Training loss: 0.6748 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 33198/35720 Training loss: 0.6748 0.2259 sec/batch\n",
      "Epoch 19/20  Iteration 33199/35720 Training loss: 0.6748 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 33200/35720 Training loss: 0.6748 0.2254 sec/batch\n",
      "Validation loss: 1.66893 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 33201/35720 Training loss: 0.6754 0.2111 sec/batch\n",
      "Epoch 19/20  Iteration 33202/35720 Training loss: 0.6755 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 33203/35720 Training loss: 0.6756 0.2244 sec/batch\n",
      "Epoch 19/20  Iteration 33204/35720 Training loss: 0.6756 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 33205/35720 Training loss: 0.6756 0.2262 sec/batch\n",
      "Epoch 19/20  Iteration 33206/35720 Training loss: 0.6756 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 33207/35720 Training loss: 0.6756 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33208/35720 Training loss: 0.6756 0.2316 sec/batch\n",
      "Epoch 19/20  Iteration 33209/35720 Training loss: 0.6756 0.2146 sec/batch\n",
      "Epoch 19/20  Iteration 33210/35720 Training loss: 0.6756 0.2251 sec/batch\n",
      "Epoch 19/20  Iteration 33211/35720 Training loss: 0.6757 0.2146 sec/batch\n",
      "Epoch 19/20  Iteration 33212/35720 Training loss: 0.6756 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 33213/35720 Training loss: 0.6756 0.2186 sec/batch\n",
      "Epoch 19/20  Iteration 33214/35720 Training loss: 0.6756 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 33215/35720 Training loss: 0.6756 0.2134 sec/batch\n",
      "Epoch 19/20  Iteration 33216/35720 Training loss: 0.6756 0.2415 sec/batch\n",
      "Epoch 19/20  Iteration 33217/35720 Training loss: 0.6756 0.2392 sec/batch\n",
      "Epoch 19/20  Iteration 33218/35720 Training loss: 0.6757 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33219/35720 Training loss: 0.6756 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 33220/35720 Training loss: 0.6756 0.2242 sec/batch\n",
      "Epoch 19/20  Iteration 33221/35720 Training loss: 0.6756 0.2174 sec/batch\n",
      "Epoch 19/20  Iteration 33222/35720 Training loss: 0.6756 0.2143 sec/batch\n",
      "Epoch 19/20  Iteration 33223/35720 Training loss: 0.6756 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 33224/35720 Training loss: 0.6756 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33225/35720 Training loss: 0.6756 0.2296 sec/batch\n",
      "Epoch 19/20  Iteration 33226/35720 Training loss: 0.6756 0.2073 sec/batch\n",
      "Epoch 19/20  Iteration 33227/35720 Training loss: 0.6756 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33228/35720 Training loss: 0.6757 0.2072 sec/batch\n",
      "Epoch 19/20  Iteration 33229/35720 Training loss: 0.6756 0.2123 sec/batch\n",
      "Epoch 19/20  Iteration 33230/35720 Training loss: 0.6756 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 33231/35720 Training loss: 0.6756 0.2359 sec/batch\n",
      "Epoch 19/20  Iteration 33232/35720 Training loss: 0.6756 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33233/35720 Training loss: 0.6756 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33234/35720 Training loss: 0.6756 0.2051 sec/batch\n",
      "Epoch 19/20  Iteration 33235/35720 Training loss: 0.6756 0.2123 sec/batch\n",
      "Epoch 19/20  Iteration 33236/35720 Training loss: 0.6757 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33237/35720 Training loss: 0.6756 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33238/35720 Training loss: 0.6757 0.2336 sec/batch\n",
      "Epoch 19/20  Iteration 33239/35720 Training loss: 0.6757 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33240/35720 Training loss: 0.6757 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 33241/35720 Training loss: 0.6756 0.2147 sec/batch\n",
      "Epoch 19/20  Iteration 33242/35720 Training loss: 0.6756 0.2215 sec/batch\n",
      "Epoch 19/20  Iteration 33243/35720 Training loss: 0.6757 0.2189 sec/batch\n",
      "Epoch 19/20  Iteration 33244/35720 Training loss: 0.6757 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33245/35720 Training loss: 0.6756 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33246/35720 Training loss: 0.6756 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 33247/35720 Training loss: 0.6756 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33248/35720 Training loss: 0.6756 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 33249/35720 Training loss: 0.6757 0.2174 sec/batch\n",
      "Epoch 19/20  Iteration 33250/35720 Training loss: 0.6757 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33251/35720 Training loss: 0.6757 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33252/35720 Training loss: 0.6757 0.2313 sec/batch\n",
      "Epoch 19/20  Iteration 33253/35720 Training loss: 0.6757 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 33254/35720 Training loss: 0.6756 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 33255/35720 Training loss: 0.6757 0.2297 sec/batch\n",
      "Epoch 19/20  Iteration 33256/35720 Training loss: 0.6757 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33257/35720 Training loss: 0.6757 0.2097 sec/batch\n",
      "Epoch 19/20  Iteration 33258/35720 Training loss: 0.6757 0.2162 sec/batch\n",
      "Epoch 19/20  Iteration 33259/35720 Training loss: 0.6757 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 33260/35720 Training loss: 0.6756 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 33261/35720 Training loss: 0.6756 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33262/35720 Training loss: 0.6756 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 33263/35720 Training loss: 0.6756 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 33264/35720 Training loss: 0.6756 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33265/35720 Training loss: 0.6755 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 33266/35720 Training loss: 0.6755 0.2324 sec/batch\n",
      "Epoch 19/20  Iteration 33267/35720 Training loss: 0.6755 0.2365 sec/batch\n",
      "Epoch 19/20  Iteration 33268/35720 Training loss: 0.6755 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33269/35720 Training loss: 0.6755 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 33270/35720 Training loss: 0.6755 0.2181 sec/batch\n",
      "Epoch 19/20  Iteration 33271/35720 Training loss: 0.6755 0.2174 sec/batch\n",
      "Epoch 19/20  Iteration 33272/35720 Training loss: 0.6755 0.2171 sec/batch\n",
      "Epoch 19/20  Iteration 33273/35720 Training loss: 0.6755 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33274/35720 Training loss: 0.6755 0.2097 sec/batch\n",
      "Epoch 19/20  Iteration 33275/35720 Training loss: 0.6755 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33276/35720 Training loss: 0.6755 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 33277/35720 Training loss: 0.6754 0.2244 sec/batch\n",
      "Epoch 19/20  Iteration 33278/35720 Training loss: 0.6754 0.2079 sec/batch\n",
      "Epoch 19/20  Iteration 33279/35720 Training loss: 0.6754 0.2256 sec/batch\n",
      "Epoch 19/20  Iteration 33280/35720 Training loss: 0.6753 0.2191 sec/batch\n",
      "Epoch 19/20  Iteration 33281/35720 Training loss: 0.6753 0.2246 sec/batch\n",
      "Epoch 19/20  Iteration 33282/35720 Training loss: 0.6753 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 33283/35720 Training loss: 0.6753 0.2148 sec/batch\n",
      "Epoch 19/20  Iteration 33284/35720 Training loss: 0.6752 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33285/35720 Training loss: 0.6752 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33286/35720 Training loss: 0.6752 0.2229 sec/batch\n",
      "Epoch 19/20  Iteration 33287/35720 Training loss: 0.6751 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33288/35720 Training loss: 0.6751 0.2203 sec/batch\n",
      "Epoch 19/20  Iteration 33289/35720 Training loss: 0.6751 0.2364 sec/batch\n",
      "Epoch 19/20  Iteration 33290/35720 Training loss: 0.6751 0.2202 sec/batch\n",
      "Epoch 19/20  Iteration 33291/35720 Training loss: 0.6750 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 33292/35720 Training loss: 0.6750 0.2173 sec/batch\n",
      "Epoch 19/20  Iteration 33293/35720 Training loss: 0.6749 0.2181 sec/batch\n",
      "Epoch 19/20  Iteration 33294/35720 Training loss: 0.6749 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 33295/35720 Training loss: 0.6749 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 33296/35720 Training loss: 0.6748 0.2172 sec/batch\n",
      "Epoch 19/20  Iteration 33297/35720 Training loss: 0.6749 0.2180 sec/batch\n",
      "Epoch 19/20  Iteration 33298/35720 Training loss: 0.6748 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33299/35720 Training loss: 0.6749 0.2147 sec/batch\n",
      "Epoch 19/20  Iteration 33300/35720 Training loss: 0.6749 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33301/35720 Training loss: 0.6749 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 33302/35720 Training loss: 0.6749 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 33303/35720 Training loss: 0.6749 0.2275 sec/batch\n",
      "Epoch 19/20  Iteration 33304/35720 Training loss: 0.6749 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 33305/35720 Training loss: 0.6749 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33306/35720 Training loss: 0.6749 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 33307/35720 Training loss: 0.6749 0.2121 sec/batch\n",
      "Epoch 19/20  Iteration 33308/35720 Training loss: 0.6749 0.2235 sec/batch\n",
      "Epoch 19/20  Iteration 33309/35720 Training loss: 0.6748 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 33310/35720 Training loss: 0.6748 0.2160 sec/batch\n",
      "Epoch 19/20  Iteration 33311/35720 Training loss: 0.6749 0.2075 sec/batch\n",
      "Epoch 19/20  Iteration 33312/35720 Training loss: 0.6749 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 33313/35720 Training loss: 0.6748 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33314/35720 Training loss: 0.6748 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 33315/35720 Training loss: 0.6748 0.2351 sec/batch\n",
      "Epoch 19/20  Iteration 33316/35720 Training loss: 0.6748 0.2250 sec/batch\n",
      "Epoch 19/20  Iteration 33317/35720 Training loss: 0.6748 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 33318/35720 Training loss: 0.6748 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 33319/35720 Training loss: 0.6748 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 33320/35720 Training loss: 0.6748 0.2296 sec/batch\n",
      "Epoch 19/20  Iteration 33321/35720 Training loss: 0.6748 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 33322/35720 Training loss: 0.6748 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 33323/35720 Training loss: 0.6749 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 33324/35720 Training loss: 0.6749 0.2157 sec/batch\n",
      "Epoch 19/20  Iteration 33325/35720 Training loss: 0.6749 0.2171 sec/batch\n",
      "Epoch 19/20  Iteration 33326/35720 Training loss: 0.6749 0.2265 sec/batch\n",
      "Epoch 19/20  Iteration 33327/35720 Training loss: 0.6749 0.2269 sec/batch\n",
      "Epoch 19/20  Iteration 33328/35720 Training loss: 0.6749 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33329/35720 Training loss: 0.6750 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 33330/35720 Training loss: 0.6750 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 33331/35720 Training loss: 0.6750 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 33332/35720 Training loss: 0.6749 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33333/35720 Training loss: 0.6749 0.2271 sec/batch\n",
      "Epoch 19/20  Iteration 33334/35720 Training loss: 0.6749 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 33335/35720 Training loss: 0.6749 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 33336/35720 Training loss: 0.6749 0.2266 sec/batch\n",
      "Epoch 19/20  Iteration 33337/35720 Training loss: 0.6749 0.2252 sec/batch\n",
      "Epoch 19/20  Iteration 33338/35720 Training loss: 0.6749 0.2193 sec/batch\n",
      "Epoch 19/20  Iteration 33339/35720 Training loss: 0.6749 0.2161 sec/batch\n",
      "Epoch 19/20  Iteration 33340/35720 Training loss: 0.6750 0.2055 sec/batch\n",
      "Epoch 19/20  Iteration 33341/35720 Training loss: 0.6750 0.2217 sec/batch\n",
      "Epoch 19/20  Iteration 33342/35720 Training loss: 0.6750 0.2241 sec/batch\n",
      "Epoch 19/20  Iteration 33343/35720 Training loss: 0.6750 0.2121 sec/batch\n",
      "Epoch 19/20  Iteration 33344/35720 Training loss: 0.6749 0.2231 sec/batch\n",
      "Epoch 19/20  Iteration 33345/35720 Training loss: 0.6749 0.2074 sec/batch\n",
      "Epoch 19/20  Iteration 33346/35720 Training loss: 0.6750 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 33347/35720 Training loss: 0.6749 0.2289 sec/batch\n",
      "Epoch 19/20  Iteration 33348/35720 Training loss: 0.6749 0.2246 sec/batch\n",
      "Epoch 19/20  Iteration 33349/35720 Training loss: 0.6749 0.2219 sec/batch\n",
      "Epoch 19/20  Iteration 33350/35720 Training loss: 0.6748 0.2102 sec/batch\n",
      "Epoch 19/20  Iteration 33351/35720 Training loss: 0.6748 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33352/35720 Training loss: 0.6747 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33353/35720 Training loss: 0.6747 0.2224 sec/batch\n",
      "Epoch 19/20  Iteration 33354/35720 Training loss: 0.6747 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 33355/35720 Training loss: 0.6747 0.2208 sec/batch\n",
      "Epoch 19/20  Iteration 33356/35720 Training loss: 0.6747 0.2119 sec/batch\n",
      "Epoch 19/20  Iteration 33357/35720 Training loss: 0.6747 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33358/35720 Training loss: 0.6747 0.2285 sec/batch\n",
      "Epoch 19/20  Iteration 33359/35720 Training loss: 0.6747 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33360/35720 Training loss: 0.6747 0.2358 sec/batch\n",
      "Epoch 19/20  Iteration 33361/35720 Training loss: 0.6747 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 33362/35720 Training loss: 0.6747 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33363/35720 Training loss: 0.6747 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 33364/35720 Training loss: 0.6746 0.2268 sec/batch\n",
      "Epoch 19/20  Iteration 33365/35720 Training loss: 0.6746 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33366/35720 Training loss: 0.6746 0.2236 sec/batch\n",
      "Epoch 19/20  Iteration 33367/35720 Training loss: 0.6746 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33368/35720 Training loss: 0.6746 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 33369/35720 Training loss: 0.6746 0.2109 sec/batch\n",
      "Epoch 19/20  Iteration 33370/35720 Training loss: 0.6746 0.2345 sec/batch\n",
      "Epoch 19/20  Iteration 33371/35720 Training loss: 0.6745 0.2273 sec/batch\n",
      "Epoch 19/20  Iteration 33372/35720 Training loss: 0.6745 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33373/35720 Training loss: 0.6745 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33374/35720 Training loss: 0.6745 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 33375/35720 Training loss: 0.6745 0.2242 sec/batch\n",
      "Epoch 19/20  Iteration 33376/35720 Training loss: 0.6745 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33377/35720 Training loss: 0.6745 0.2214 sec/batch\n",
      "Epoch 19/20  Iteration 33378/35720 Training loss: 0.6745 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 33379/35720 Training loss: 0.6745 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 33380/35720 Training loss: 0.6745 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33381/35720 Training loss: 0.6745 0.2257 sec/batch\n",
      "Epoch 19/20  Iteration 33382/35720 Training loss: 0.6745 0.2052 sec/batch\n",
      "Epoch 19/20  Iteration 33383/35720 Training loss: 0.6744 0.2289 sec/batch\n",
      "Epoch 19/20  Iteration 33384/35720 Training loss: 0.6744 0.2199 sec/batch\n",
      "Epoch 19/20  Iteration 33385/35720 Training loss: 0.6744 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 33386/35720 Training loss: 0.6743 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 33387/35720 Training loss: 0.6743 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 33388/35720 Training loss: 0.6742 0.2282 sec/batch\n",
      "Epoch 19/20  Iteration 33389/35720 Training loss: 0.6742 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 33390/35720 Training loss: 0.6742 0.2054 sec/batch\n",
      "Epoch 19/20  Iteration 33391/35720 Training loss: 0.6742 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33392/35720 Training loss: 0.6741 0.2193 sec/batch\n",
      "Epoch 19/20  Iteration 33393/35720 Training loss: 0.6741 0.2272 sec/batch\n",
      "Epoch 19/20  Iteration 33394/35720 Training loss: 0.6741 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 33395/35720 Training loss: 0.6740 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 33396/35720 Training loss: 0.6740 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33397/35720 Training loss: 0.6740 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 33398/35720 Training loss: 0.6739 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 33399/35720 Training loss: 0.6739 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 33400/35720 Training loss: 0.6739 0.2062 sec/batch\n",
      "Validation loss: 1.66547 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 33401/35720 Training loss: 0.6741 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 33402/35720 Training loss: 0.6741 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 33403/35720 Training loss: 0.6741 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 33404/35720 Training loss: 0.6741 0.2155 sec/batch\n",
      "Epoch 19/20  Iteration 33405/35720 Training loss: 0.6741 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 33406/35720 Training loss: 0.6741 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33407/35720 Training loss: 0.6740 0.2224 sec/batch\n",
      "Epoch 19/20  Iteration 33408/35720 Training loss: 0.6741 0.2177 sec/batch\n",
      "Epoch 19/20  Iteration 33409/35720 Training loss: 0.6740 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33410/35720 Training loss: 0.6740 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 33411/35720 Training loss: 0.6739 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 33412/35720 Training loss: 0.6739 0.2346 sec/batch\n",
      "Epoch 19/20  Iteration 33413/35720 Training loss: 0.6739 0.2239 sec/batch\n",
      "Epoch 19/20  Iteration 33414/35720 Training loss: 0.6739 0.2180 sec/batch\n",
      "Epoch 19/20  Iteration 33415/35720 Training loss: 0.6738 0.2307 sec/batch\n",
      "Epoch 19/20  Iteration 33416/35720 Training loss: 0.6738 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 33417/35720 Training loss: 0.6738 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33418/35720 Training loss: 0.6738 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 33419/35720 Training loss: 0.6738 0.2287 sec/batch\n",
      "Epoch 19/20  Iteration 33420/35720 Training loss: 0.6738 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 33421/35720 Training loss: 0.6738 0.2192 sec/batch\n",
      "Epoch 19/20  Iteration 33422/35720 Training loss: 0.6738 0.2183 sec/batch\n",
      "Epoch 19/20  Iteration 33423/35720 Training loss: 0.6738 0.2102 sec/batch\n",
      "Epoch 19/20  Iteration 33424/35720 Training loss: 0.6737 0.2279 sec/batch\n",
      "Epoch 19/20  Iteration 33425/35720 Training loss: 0.6737 0.2080 sec/batch\n",
      "Epoch 19/20  Iteration 33426/35720 Training loss: 0.6736 0.2318 sec/batch\n",
      "Epoch 19/20  Iteration 33427/35720 Training loss: 0.6736 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 33428/35720 Training loss: 0.6736 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33429/35720 Training loss: 0.6736 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 33430/35720 Training loss: 0.6736 0.2180 sec/batch\n",
      "Epoch 19/20  Iteration 33431/35720 Training loss: 0.6735 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33432/35720 Training loss: 0.6735 0.2278 sec/batch\n",
      "Epoch 19/20  Iteration 33433/35720 Training loss: 0.6735 0.2131 sec/batch\n",
      "Epoch 19/20  Iteration 33434/35720 Training loss: 0.6735 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33435/35720 Training loss: 0.6735 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33436/35720 Training loss: 0.6735 0.2291 sec/batch\n",
      "Epoch 19/20  Iteration 33437/35720 Training loss: 0.6734 0.2273 sec/batch\n",
      "Epoch 19/20  Iteration 33438/35720 Training loss: 0.6735 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33439/35720 Training loss: 0.6734 0.2053 sec/batch\n",
      "Epoch 19/20  Iteration 33440/35720 Training loss: 0.6734 0.2173 sec/batch\n",
      "Epoch 19/20  Iteration 33441/35720 Training loss: 0.6734 0.2170 sec/batch\n",
      "Epoch 19/20  Iteration 33442/35720 Training loss: 0.6734 0.2262 sec/batch\n",
      "Epoch 19/20  Iteration 33443/35720 Training loss: 0.6734 0.2244 sec/batch\n",
      "Epoch 19/20  Iteration 33444/35720 Training loss: 0.6733 0.2072 sec/batch\n",
      "Epoch 19/20  Iteration 33445/35720 Training loss: 0.6734 0.2080 sec/batch\n",
      "Epoch 19/20  Iteration 33446/35720 Training loss: 0.6734 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 33447/35720 Training loss: 0.6733 0.2273 sec/batch\n",
      "Epoch 19/20  Iteration 33448/35720 Training loss: 0.6733 0.2388 sec/batch\n",
      "Epoch 19/20  Iteration 33449/35720 Training loss: 0.6733 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 33450/35720 Training loss: 0.6733 0.2075 sec/batch\n",
      "Epoch 19/20  Iteration 33451/35720 Training loss: 0.6732 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 33452/35720 Training loss: 0.6732 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33453/35720 Training loss: 0.6732 0.2184 sec/batch\n",
      "Epoch 19/20  Iteration 33454/35720 Training loss: 0.6732 0.2146 sec/batch\n",
      "Epoch 19/20  Iteration 33455/35720 Training loss: 0.6732 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33456/35720 Training loss: 0.6731 0.2072 sec/batch\n",
      "Epoch 19/20  Iteration 33457/35720 Training loss: 0.6731 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 33458/35720 Training loss: 0.6731 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 33459/35720 Training loss: 0.6731 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 33460/35720 Training loss: 0.6731 0.2288 sec/batch\n",
      "Epoch 19/20  Iteration 33461/35720 Training loss: 0.6731 0.2162 sec/batch\n",
      "Epoch 19/20  Iteration 33462/35720 Training loss: 0.6731 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 33463/35720 Training loss: 0.6731 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 33464/35720 Training loss: 0.6731 0.2217 sec/batch\n",
      "Epoch 19/20  Iteration 33465/35720 Training loss: 0.6730 0.2180 sec/batch\n",
      "Epoch 19/20  Iteration 33466/35720 Training loss: 0.6730 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 33467/35720 Training loss: 0.6730 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 33468/35720 Training loss: 0.6730 0.2181 sec/batch\n",
      "Epoch 19/20  Iteration 33469/35720 Training loss: 0.6730 0.2274 sec/batch\n",
      "Epoch 19/20  Iteration 33470/35720 Training loss: 0.6730 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 33471/35720 Training loss: 0.6730 0.2180 sec/batch\n",
      "Epoch 19/20  Iteration 33472/35720 Training loss: 0.6730 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 33473/35720 Training loss: 0.6730 0.2111 sec/batch\n",
      "Epoch 19/20  Iteration 33474/35720 Training loss: 0.6730 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33475/35720 Training loss: 0.6730 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33476/35720 Training loss: 0.6730 0.2057 sec/batch\n",
      "Epoch 19/20  Iteration 33477/35720 Training loss: 0.6730 0.2123 sec/batch\n",
      "Epoch 19/20  Iteration 33478/35720 Training loss: 0.6729 0.2153 sec/batch\n",
      "Epoch 19/20  Iteration 33479/35720 Training loss: 0.6729 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 33480/35720 Training loss: 0.6729 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33481/35720 Training loss: 0.6730 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 33482/35720 Training loss: 0.6730 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 33483/35720 Training loss: 0.6730 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 33484/35720 Training loss: 0.6730 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 33485/35720 Training loss: 0.6731 0.2143 sec/batch\n",
      "Epoch 19/20  Iteration 33486/35720 Training loss: 0.6731 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 33487/35720 Training loss: 0.6731 0.2110 sec/batch\n",
      "Epoch 19/20  Iteration 33488/35720 Training loss: 0.6731 0.2172 sec/batch\n",
      "Epoch 19/20  Iteration 33489/35720 Training loss: 0.6731 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 33490/35720 Training loss: 0.6731 0.2276 sec/batch\n",
      "Epoch 19/20  Iteration 33491/35720 Training loss: 0.6730 0.2156 sec/batch\n",
      "Epoch 19/20  Iteration 33492/35720 Training loss: 0.6730 0.2205 sec/batch\n",
      "Epoch 19/20  Iteration 33493/35720 Training loss: 0.6730 0.2217 sec/batch\n",
      "Epoch 19/20  Iteration 33494/35720 Training loss: 0.6730 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33495/35720 Training loss: 0.6730 0.2118 sec/batch\n",
      "Epoch 19/20  Iteration 33496/35720 Training loss: 0.6730 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 33497/35720 Training loss: 0.6730 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 33498/35720 Training loss: 0.6730 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 33499/35720 Training loss: 0.6731 0.2206 sec/batch\n",
      "Epoch 19/20  Iteration 33500/35720 Training loss: 0.6731 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33501/35720 Training loss: 0.6731 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33502/35720 Training loss: 0.6731 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 33503/35720 Training loss: 0.6731 0.2600 sec/batch\n",
      "Epoch 19/20  Iteration 33504/35720 Training loss: 0.6731 0.2250 sec/batch\n",
      "Epoch 19/20  Iteration 33505/35720 Training loss: 0.6731 0.2131 sec/batch\n",
      "Epoch 19/20  Iteration 33506/35720 Training loss: 0.6731 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33507/35720 Training loss: 0.6730 0.2159 sec/batch\n",
      "Epoch 19/20  Iteration 33508/35720 Training loss: 0.6730 0.2238 sec/batch\n",
      "Epoch 19/20  Iteration 33509/35720 Training loss: 0.6730 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33510/35720 Training loss: 0.6729 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33511/35720 Training loss: 0.6729 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33512/35720 Training loss: 0.6729 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33513/35720 Training loss: 0.6729 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 33514/35720 Training loss: 0.6729 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33515/35720 Training loss: 0.6729 0.2254 sec/batch\n",
      "Epoch 19/20  Iteration 33516/35720 Training loss: 0.6729 0.2191 sec/batch\n",
      "Epoch 19/20  Iteration 33517/35720 Training loss: 0.6729 0.2072 sec/batch\n",
      "Epoch 19/20  Iteration 33518/35720 Training loss: 0.6729 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 33519/35720 Training loss: 0.6729 0.2243 sec/batch\n",
      "Epoch 19/20  Iteration 33520/35720 Training loss: 0.6729 0.2170 sec/batch\n",
      "Epoch 19/20  Iteration 33521/35720 Training loss: 0.6729 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33522/35720 Training loss: 0.6729 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33523/35720 Training loss: 0.6729 0.2072 sec/batch\n",
      "Epoch 19/20  Iteration 33524/35720 Training loss: 0.6729 0.2193 sec/batch\n",
      "Epoch 19/20  Iteration 33525/35720 Training loss: 0.6728 0.2172 sec/batch\n",
      "Epoch 19/20  Iteration 33526/35720 Training loss: 0.6729 0.2273 sec/batch\n",
      "Epoch 19/20  Iteration 33527/35720 Training loss: 0.6729 0.2253 sec/batch\n",
      "Epoch 19/20  Iteration 33528/35720 Training loss: 0.6729 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33529/35720 Training loss: 0.6729 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33530/35720 Training loss: 0.6729 0.2283 sec/batch\n",
      "Epoch 19/20  Iteration 33531/35720 Training loss: 0.6729 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 33532/35720 Training loss: 0.6728 0.2264 sec/batch\n",
      "Epoch 19/20  Iteration 33533/35720 Training loss: 0.6729 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 33534/35720 Training loss: 0.6729 0.2097 sec/batch\n",
      "Epoch 19/20  Iteration 33535/35720 Training loss: 0.6729 0.2167 sec/batch\n",
      "Epoch 19/20  Iteration 33536/35720 Training loss: 0.6729 0.2214 sec/batch\n",
      "Epoch 19/20  Iteration 33537/35720 Training loss: 0.6729 0.2104 sec/batch\n",
      "Epoch 19/20  Iteration 33538/35720 Training loss: 0.6729 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 33539/35720 Training loss: 0.6729 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 33540/35720 Training loss: 0.6728 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33541/35720 Training loss: 0.6728 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 33542/35720 Training loss: 0.6727 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33543/35720 Training loss: 0.6727 0.2246 sec/batch\n",
      "Epoch 19/20  Iteration 33544/35720 Training loss: 0.6727 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 33545/35720 Training loss: 0.6727 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33546/35720 Training loss: 0.6727 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 33547/35720 Training loss: 0.6726 0.2184 sec/batch\n",
      "Epoch 19/20  Iteration 33548/35720 Training loss: 0.6726 0.2092 sec/batch\n",
      "Epoch 19/20  Iteration 33549/35720 Training loss: 0.6726 0.2241 sec/batch\n",
      "Epoch 19/20  Iteration 33550/35720 Training loss: 0.6725 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33551/35720 Training loss: 0.6726 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33552/35720 Training loss: 0.6726 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33553/35720 Training loss: 0.6725 0.2118 sec/batch\n",
      "Epoch 19/20  Iteration 33554/35720 Training loss: 0.6725 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 33555/35720 Training loss: 0.6725 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33556/35720 Training loss: 0.6725 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 33557/35720 Training loss: 0.6725 0.2143 sec/batch\n",
      "Epoch 19/20  Iteration 33558/35720 Training loss: 0.6725 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 33559/35720 Training loss: 0.6725 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33560/35720 Training loss: 0.6725 0.2196 sec/batch\n",
      "Epoch 19/20  Iteration 33561/35720 Training loss: 0.6724 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 33562/35720 Training loss: 0.6724 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 33563/35720 Training loss: 0.6724 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 33564/35720 Training loss: 0.6724 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33565/35720 Training loss: 0.6724 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33566/35720 Training loss: 0.6724 0.2155 sec/batch\n",
      "Epoch 19/20  Iteration 33567/35720 Training loss: 0.6724 0.2204 sec/batch\n",
      "Epoch 19/20  Iteration 33568/35720 Training loss: 0.6724 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 33569/35720 Training loss: 0.6724 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 33570/35720 Training loss: 0.6724 0.2148 sec/batch\n",
      "Epoch 19/20  Iteration 33571/35720 Training loss: 0.6724 0.2166 sec/batch\n",
      "Epoch 19/20  Iteration 33572/35720 Training loss: 0.6724 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 33573/35720 Training loss: 0.6724 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 33574/35720 Training loss: 0.6724 0.2107 sec/batch\n",
      "Epoch 19/20  Iteration 33575/35720 Training loss: 0.6724 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 33576/35720 Training loss: 0.6724 0.2150 sec/batch\n",
      "Epoch 19/20  Iteration 33577/35720 Training loss: 0.6724 0.2357 sec/batch\n",
      "Epoch 19/20  Iteration 33578/35720 Training loss: 0.6724 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 33579/35720 Training loss: 0.6724 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33580/35720 Training loss: 0.6724 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 33581/35720 Training loss: 0.6724 0.2225 sec/batch\n",
      "Epoch 19/20  Iteration 33582/35720 Training loss: 0.6724 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 33583/35720 Training loss: 0.6724 0.2178 sec/batch\n",
      "Epoch 19/20  Iteration 33584/35720 Training loss: 0.6723 0.2060 sec/batch\n",
      "Epoch 19/20  Iteration 33585/35720 Training loss: 0.6723 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 33586/35720 Training loss: 0.6724 0.2156 sec/batch\n",
      "Epoch 19/20  Iteration 33587/35720 Training loss: 0.6724 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33588/35720 Training loss: 0.6724 0.2177 sec/batch\n",
      "Epoch 19/20  Iteration 33589/35720 Training loss: 0.6724 0.2216 sec/batch\n",
      "Epoch 19/20  Iteration 33590/35720 Training loss: 0.6724 0.2214 sec/batch\n",
      "Epoch 19/20  Iteration 33591/35720 Training loss: 0.6724 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 33592/35720 Training loss: 0.6724 0.2251 sec/batch\n",
      "Epoch 19/20  Iteration 33593/35720 Training loss: 0.6724 0.2238 sec/batch\n",
      "Epoch 19/20  Iteration 33594/35720 Training loss: 0.6724 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 33595/35720 Training loss: 0.6724 0.2059 sec/batch\n",
      "Epoch 19/20  Iteration 33596/35720 Training loss: 0.6724 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 33597/35720 Training loss: 0.6724 0.2200 sec/batch\n",
      "Epoch 19/20  Iteration 33598/35720 Training loss: 0.6724 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33599/35720 Training loss: 0.6724 0.2342 sec/batch\n",
      "Epoch 19/20  Iteration 33600/35720 Training loss: 0.6724 0.2072 sec/batch\n",
      "Validation loss: 1.64214 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 33601/35720 Training loss: 0.6728 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 33602/35720 Training loss: 0.6728 0.2058 sec/batch\n",
      "Epoch 19/20  Iteration 33603/35720 Training loss: 0.6728 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33604/35720 Training loss: 0.6728 0.2239 sec/batch\n",
      "Epoch 19/20  Iteration 33605/35720 Training loss: 0.6729 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 33606/35720 Training loss: 0.6729 0.2068 sec/batch\n",
      "Epoch 19/20  Iteration 33607/35720 Training loss: 0.6729 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33608/35720 Training loss: 0.6729 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33609/35720 Training loss: 0.6730 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 33610/35720 Training loss: 0.6730 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33611/35720 Training loss: 0.6730 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 33612/35720 Training loss: 0.6730 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 33613/35720 Training loss: 0.6730 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33614/35720 Training loss: 0.6729 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 33615/35720 Training loss: 0.6730 0.2190 sec/batch\n",
      "Epoch 19/20  Iteration 33616/35720 Training loss: 0.6730 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33617/35720 Training loss: 0.6730 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33618/35720 Training loss: 0.6730 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 33619/35720 Training loss: 0.6729 0.2144 sec/batch\n",
      "Epoch 19/20  Iteration 33620/35720 Training loss: 0.6729 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33621/35720 Training loss: 0.6729 0.2138 sec/batch\n",
      "Epoch 19/20  Iteration 33622/35720 Training loss: 0.6729 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33623/35720 Training loss: 0.6728 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 33624/35720 Training loss: 0.6728 0.2188 sec/batch\n",
      "Epoch 19/20  Iteration 33625/35720 Training loss: 0.6728 0.2220 sec/batch\n",
      "Epoch 19/20  Iteration 33626/35720 Training loss: 0.6727 0.2123 sec/batch\n",
      "Epoch 19/20  Iteration 33627/35720 Training loss: 0.6727 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 33628/35720 Training loss: 0.6727 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33629/35720 Training loss: 0.6727 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 33630/35720 Training loss: 0.6727 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 33631/35720 Training loss: 0.6727 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33632/35720 Training loss: 0.6727 0.2268 sec/batch\n",
      "Epoch 19/20  Iteration 33633/35720 Training loss: 0.6726 0.2122 sec/batch\n",
      "Epoch 19/20  Iteration 33634/35720 Training loss: 0.6726 0.2057 sec/batch\n",
      "Epoch 19/20  Iteration 33635/35720 Training loss: 0.6726 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 33636/35720 Training loss: 0.6726 0.2274 sec/batch\n",
      "Epoch 19/20  Iteration 33637/35720 Training loss: 0.6726 0.2142 sec/batch\n",
      "Epoch 19/20  Iteration 33638/35720 Training loss: 0.6726 0.2239 sec/batch\n",
      "Epoch 19/20  Iteration 33639/35720 Training loss: 0.6726 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 33640/35720 Training loss: 0.6725 0.2111 sec/batch\n",
      "Epoch 19/20  Iteration 33641/35720 Training loss: 0.6725 0.2242 sec/batch\n",
      "Epoch 19/20  Iteration 33642/35720 Training loss: 0.6725 0.2169 sec/batch\n",
      "Epoch 19/20  Iteration 33643/35720 Training loss: 0.6725 0.2197 sec/batch\n",
      "Epoch 19/20  Iteration 33644/35720 Training loss: 0.6725 0.2134 sec/batch\n",
      "Epoch 19/20  Iteration 33645/35720 Training loss: 0.6725 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 33646/35720 Training loss: 0.6724 0.2078 sec/batch\n",
      "Epoch 19/20  Iteration 33647/35720 Training loss: 0.6724 0.2262 sec/batch\n",
      "Epoch 19/20  Iteration 33648/35720 Training loss: 0.6724 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 33649/35720 Training loss: 0.6724 0.2255 sec/batch\n",
      "Epoch 19/20  Iteration 33650/35720 Training loss: 0.6723 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 33651/35720 Training loss: 0.6723 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33652/35720 Training loss: 0.6723 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33653/35720 Training loss: 0.6723 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33654/35720 Training loss: 0.6724 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33655/35720 Training loss: 0.6724 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 33656/35720 Training loss: 0.6724 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33657/35720 Training loss: 0.6724 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33658/35720 Training loss: 0.6723 0.2156 sec/batch\n",
      "Epoch 19/20  Iteration 33659/35720 Training loss: 0.6724 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33660/35720 Training loss: 0.6723 0.2133 sec/batch\n",
      "Epoch 19/20  Iteration 33661/35720 Training loss: 0.6723 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33662/35720 Training loss: 0.6723 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 33663/35720 Training loss: 0.6723 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 33664/35720 Training loss: 0.6723 0.2153 sec/batch\n",
      "Epoch 19/20  Iteration 33665/35720 Training loss: 0.6723 0.2134 sec/batch\n",
      "Epoch 19/20  Iteration 33666/35720 Training loss: 0.6723 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 33667/35720 Training loss: 0.6723 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33668/35720 Training loss: 0.6723 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 33669/35720 Training loss: 0.6723 0.2083 sec/batch\n",
      "Epoch 19/20  Iteration 33670/35720 Training loss: 0.6723 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 33671/35720 Training loss: 0.6723 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33672/35720 Training loss: 0.6724 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 33673/35720 Training loss: 0.6724 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 33674/35720 Training loss: 0.6723 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33675/35720 Training loss: 0.6723 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 33676/35720 Training loss: 0.6723 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 33677/35720 Training loss: 0.6724 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33678/35720 Training loss: 0.6724 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 33679/35720 Training loss: 0.6724 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 33680/35720 Training loss: 0.6724 0.2216 sec/batch\n",
      "Epoch 19/20  Iteration 33681/35720 Training loss: 0.6724 0.2264 sec/batch\n",
      "Epoch 19/20  Iteration 33682/35720 Training loss: 0.6724 0.2143 sec/batch\n",
      "Epoch 19/20  Iteration 33683/35720 Training loss: 0.6724 0.2258 sec/batch\n",
      "Epoch 19/20  Iteration 33684/35720 Training loss: 0.6724 0.2063 sec/batch\n",
      "Epoch 19/20  Iteration 33685/35720 Training loss: 0.6723 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 33686/35720 Training loss: 0.6723 0.3039 sec/batch\n",
      "Epoch 19/20  Iteration 33687/35720 Training loss: 0.6723 0.2086 sec/batch\n",
      "Epoch 19/20  Iteration 33688/35720 Training loss: 0.6722 0.2293 sec/batch\n",
      "Epoch 19/20  Iteration 33689/35720 Training loss: 0.6722 0.2175 sec/batch\n",
      "Epoch 19/20  Iteration 33690/35720 Training loss: 0.6722 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 33691/35720 Training loss: 0.6722 0.2150 sec/batch\n",
      "Epoch 19/20  Iteration 33692/35720 Training loss: 0.6722 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33693/35720 Training loss: 0.6722 0.2313 sec/batch\n",
      "Epoch 19/20  Iteration 33694/35720 Training loss: 0.6722 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33695/35720 Training loss: 0.6722 0.2073 sec/batch\n",
      "Epoch 19/20  Iteration 33696/35720 Training loss: 0.6722 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 33697/35720 Training loss: 0.6722 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 33698/35720 Training loss: 0.6722 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 33699/35720 Training loss: 0.6722 0.2135 sec/batch\n",
      "Epoch 19/20  Iteration 33700/35720 Training loss: 0.6721 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 33701/35720 Training loss: 0.6721 0.2134 sec/batch\n",
      "Epoch 19/20  Iteration 33702/35720 Training loss: 0.6721 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33703/35720 Training loss: 0.6721 0.2200 sec/batch\n",
      "Epoch 19/20  Iteration 33704/35720 Training loss: 0.6721 0.2118 sec/batch\n",
      "Epoch 19/20  Iteration 33705/35720 Training loss: 0.6721 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33706/35720 Training loss: 0.6720 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 33707/35720 Training loss: 0.6720 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33708/35720 Training loss: 0.6720 0.2088 sec/batch\n",
      "Epoch 19/20  Iteration 33709/35720 Training loss: 0.6720 0.2194 sec/batch\n",
      "Epoch 19/20  Iteration 33710/35720 Training loss: 0.6719 0.2214 sec/batch\n",
      "Epoch 19/20  Iteration 33711/35720 Training loss: 0.6719 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 33712/35720 Training loss: 0.6719 0.2205 sec/batch\n",
      "Epoch 19/20  Iteration 33713/35720 Training loss: 0.6719 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33714/35720 Training loss: 0.6719 0.2228 sec/batch\n",
      "Epoch 19/20  Iteration 33715/35720 Training loss: 0.6718 0.2267 sec/batch\n",
      "Epoch 19/20  Iteration 33716/35720 Training loss: 0.6719 0.2248 sec/batch\n",
      "Epoch 19/20  Iteration 33717/35720 Training loss: 0.6719 0.2188 sec/batch\n",
      "Epoch 19/20  Iteration 33718/35720 Training loss: 0.6719 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33719/35720 Training loss: 0.6718 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33720/35720 Training loss: 0.6718 0.2184 sec/batch\n",
      "Epoch 19/20  Iteration 33721/35720 Training loss: 0.6718 0.2049 sec/batch\n",
      "Epoch 19/20  Iteration 33722/35720 Training loss: 0.6718 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 33723/35720 Training loss: 0.6718 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33724/35720 Training loss: 0.6717 0.2090 sec/batch\n",
      "Epoch 19/20  Iteration 33725/35720 Training loss: 0.6717 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33726/35720 Training loss: 0.6717 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33727/35720 Training loss: 0.6717 0.2177 sec/batch\n",
      "Epoch 19/20  Iteration 33728/35720 Training loss: 0.6717 0.2064 sec/batch\n",
      "Epoch 19/20  Iteration 33729/35720 Training loss: 0.6717 0.2071 sec/batch\n",
      "Epoch 19/20  Iteration 33730/35720 Training loss: 0.6717 0.2094 sec/batch\n",
      "Epoch 19/20  Iteration 33731/35720 Training loss: 0.6717 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 33732/35720 Training loss: 0.6716 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33733/35720 Training loss: 0.6716 0.2211 sec/batch\n",
      "Epoch 19/20  Iteration 33734/35720 Training loss: 0.6716 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33735/35720 Training loss: 0.6716 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33736/35720 Training loss: 0.6715 0.2058 sec/batch\n",
      "Epoch 19/20  Iteration 33737/35720 Training loss: 0.6715 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 33738/35720 Training loss: 0.6715 0.2274 sec/batch\n",
      "Epoch 19/20  Iteration 33739/35720 Training loss: 0.6715 0.2218 sec/batch\n",
      "Epoch 19/20  Iteration 33740/35720 Training loss: 0.6715 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33741/35720 Training loss: 0.6715 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 33742/35720 Training loss: 0.6714 0.2274 sec/batch\n",
      "Epoch 19/20  Iteration 33743/35720 Training loss: 0.6714 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 33744/35720 Training loss: 0.6714 0.2148 sec/batch\n",
      "Epoch 19/20  Iteration 33745/35720 Training loss: 0.6713 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 33746/35720 Training loss: 0.6713 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 33747/35720 Training loss: 0.6712 0.2140 sec/batch\n",
      "Epoch 19/20  Iteration 33748/35720 Training loss: 0.6713 0.2297 sec/batch\n",
      "Epoch 19/20  Iteration 33749/35720 Training loss: 0.6712 0.2185 sec/batch\n",
      "Epoch 19/20  Iteration 33750/35720 Training loss: 0.6712 0.2149 sec/batch\n",
      "Epoch 19/20  Iteration 33751/35720 Training loss: 0.6712 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 33752/35720 Training loss: 0.6712 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 33753/35720 Training loss: 0.6711 0.2150 sec/batch\n",
      "Epoch 19/20  Iteration 33754/35720 Training loss: 0.6711 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 33755/35720 Training loss: 0.6711 0.2143 sec/batch\n",
      "Epoch 19/20  Iteration 33756/35720 Training loss: 0.6711 0.2106 sec/batch\n",
      "Epoch 19/20  Iteration 33757/35720 Training loss: 0.6711 0.2204 sec/batch\n",
      "Epoch 19/20  Iteration 33758/35720 Training loss: 0.6710 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 33759/35720 Training loss: 0.6710 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 33760/35720 Training loss: 0.6710 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 33761/35720 Training loss: 0.6710 0.2270 sec/batch\n",
      "Epoch 19/20  Iteration 33762/35720 Training loss: 0.6710 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33763/35720 Training loss: 0.6710 0.2121 sec/batch\n",
      "Epoch 19/20  Iteration 33764/35720 Training loss: 0.6710 0.2077 sec/batch\n",
      "Epoch 19/20  Iteration 33765/35720 Training loss: 0.6710 0.2175 sec/batch\n",
      "Epoch 19/20  Iteration 33766/35720 Training loss: 0.6710 0.2326 sec/batch\n",
      "Epoch 19/20  Iteration 33767/35720 Training loss: 0.6710 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33768/35720 Training loss: 0.6710 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 33769/35720 Training loss: 0.6710 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 33770/35720 Training loss: 0.6710 0.2152 sec/batch\n",
      "Epoch 19/20  Iteration 33771/35720 Training loss: 0.6710 0.2257 sec/batch\n",
      "Epoch 19/20  Iteration 33772/35720 Training loss: 0.6709 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33773/35720 Training loss: 0.6710 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 33774/35720 Training loss: 0.6710 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 33775/35720 Training loss: 0.6710 0.2128 sec/batch\n",
      "Epoch 19/20  Iteration 33776/35720 Training loss: 0.6710 0.2254 sec/batch\n",
      "Epoch 19/20  Iteration 33777/35720 Training loss: 0.6709 0.2267 sec/batch\n",
      "Epoch 19/20  Iteration 33778/35720 Training loss: 0.6710 0.2074 sec/batch\n",
      "Epoch 19/20  Iteration 33779/35720 Training loss: 0.6710 0.2124 sec/batch\n",
      "Epoch 19/20  Iteration 33780/35720 Training loss: 0.6709 0.2121 sec/batch\n",
      "Epoch 19/20  Iteration 33781/35720 Training loss: 0.6709 0.2082 sec/batch\n",
      "Epoch 19/20  Iteration 33782/35720 Training loss: 0.6709 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 33783/35720 Training loss: 0.6709 0.2258 sec/batch\n",
      "Epoch 19/20  Iteration 33784/35720 Training loss: 0.6709 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33785/35720 Training loss: 0.6709 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33786/35720 Training loss: 0.6709 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 33787/35720 Training loss: 0.6709 0.2274 sec/batch\n",
      "Epoch 19/20  Iteration 33788/35720 Training loss: 0.6709 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 33789/35720 Training loss: 0.6709 0.2165 sec/batch\n",
      "Epoch 19/20  Iteration 33790/35720 Training loss: 0.6709 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 33791/35720 Training loss: 0.6709 0.2149 sec/batch\n",
      "Epoch 19/20  Iteration 33792/35720 Training loss: 0.6709 0.2108 sec/batch\n",
      "Epoch 19/20  Iteration 33793/35720 Training loss: 0.6709 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 33794/35720 Training loss: 0.6709 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33795/35720 Training loss: 0.6709 0.2163 sec/batch\n",
      "Epoch 19/20  Iteration 33796/35720 Training loss: 0.6709 0.2121 sec/batch\n",
      "Epoch 19/20  Iteration 33797/35720 Training loss: 0.6709 0.2221 sec/batch\n",
      "Epoch 19/20  Iteration 33798/35720 Training loss: 0.6709 0.2267 sec/batch\n",
      "Epoch 19/20  Iteration 33799/35720 Training loss: 0.6709 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33800/35720 Training loss: 0.6709 0.2157 sec/batch\n",
      "Validation loss: 1.6707 Saving checkpoint!\n",
      "Epoch 19/20  Iteration 33801/35720 Training loss: 0.6712 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33802/35720 Training loss: 0.6712 0.2097 sec/batch\n",
      "Epoch 19/20  Iteration 33803/35720 Training loss: 0.6711 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 33804/35720 Training loss: 0.6712 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33805/35720 Training loss: 0.6712 0.2209 sec/batch\n",
      "Epoch 19/20  Iteration 33806/35720 Training loss: 0.6712 0.2190 sec/batch\n",
      "Epoch 19/20  Iteration 33807/35720 Training loss: 0.6712 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 33808/35720 Training loss: 0.6711 0.2280 sec/batch\n",
      "Epoch 19/20  Iteration 33809/35720 Training loss: 0.6712 0.2245 sec/batch\n",
      "Epoch 19/20  Iteration 33810/35720 Training loss: 0.6712 0.2238 sec/batch\n",
      "Epoch 19/20  Iteration 33811/35720 Training loss: 0.6712 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 33812/35720 Training loss: 0.6711 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 33813/35720 Training loss: 0.6712 0.2161 sec/batch\n",
      "Epoch 19/20  Iteration 33814/35720 Training loss: 0.6712 0.2146 sec/batch\n",
      "Epoch 19/20  Iteration 33815/35720 Training loss: 0.6712 0.2093 sec/batch\n",
      "Epoch 19/20  Iteration 33816/35720 Training loss: 0.6712 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33817/35720 Training loss: 0.6712 0.2057 sec/batch\n",
      "Epoch 19/20  Iteration 33818/35720 Training loss: 0.6712 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33819/35720 Training loss: 0.6712 0.2176 sec/batch\n",
      "Epoch 19/20  Iteration 33820/35720 Training loss: 0.6712 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33821/35720 Training loss: 0.6712 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 33822/35720 Training loss: 0.6713 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 33823/35720 Training loss: 0.6712 0.2152 sec/batch\n",
      "Epoch 19/20  Iteration 33824/35720 Training loss: 0.6712 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 33825/35720 Training loss: 0.6712 0.2190 sec/batch\n",
      "Epoch 19/20  Iteration 33826/35720 Training loss: 0.6712 0.2097 sec/batch\n",
      "Epoch 19/20  Iteration 33827/35720 Training loss: 0.6712 0.2152 sec/batch\n",
      "Epoch 19/20  Iteration 33828/35720 Training loss: 0.6712 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 33829/35720 Training loss: 0.6712 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33830/35720 Training loss: 0.6711 0.2089 sec/batch\n",
      "Epoch 19/20  Iteration 33831/35720 Training loss: 0.6711 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 33832/35720 Training loss: 0.6712 0.2114 sec/batch\n",
      "Epoch 19/20  Iteration 33833/35720 Training loss: 0.6711 0.2164 sec/batch\n",
      "Epoch 19/20  Iteration 33834/35720 Training loss: 0.6712 0.2220 sec/batch\n",
      "Epoch 19/20  Iteration 33835/35720 Training loss: 0.6712 0.2062 sec/batch\n",
      "Epoch 19/20  Iteration 33836/35720 Training loss: 0.6711 0.2182 sec/batch\n",
      "Epoch 19/20  Iteration 33837/35720 Training loss: 0.6711 0.2100 sec/batch\n",
      "Epoch 19/20  Iteration 33838/35720 Training loss: 0.6711 0.2128 sec/batch\n",
      "Epoch 19/20  Iteration 33839/35720 Training loss: 0.6711 0.2154 sec/batch\n",
      "Epoch 19/20  Iteration 33840/35720 Training loss: 0.6711 0.2125 sec/batch\n",
      "Epoch 19/20  Iteration 33841/35720 Training loss: 0.6710 0.2128 sec/batch\n",
      "Epoch 19/20  Iteration 33842/35720 Training loss: 0.6711 0.2117 sec/batch\n",
      "Epoch 19/20  Iteration 33843/35720 Training loss: 0.6710 0.2145 sec/batch\n",
      "Epoch 19/20  Iteration 33844/35720 Training loss: 0.6710 0.2179 sec/batch\n",
      "Epoch 19/20  Iteration 33845/35720 Training loss: 0.6710 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 33846/35720 Training loss: 0.6710 0.2056 sec/batch\n",
      "Epoch 19/20  Iteration 33847/35720 Training loss: 0.6710 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 33848/35720 Training loss: 0.6710 0.2243 sec/batch\n",
      "Epoch 19/20  Iteration 33849/35720 Training loss: 0.6710 0.2112 sec/batch\n",
      "Epoch 19/20  Iteration 33850/35720 Training loss: 0.6710 0.2181 sec/batch\n",
      "Epoch 19/20  Iteration 33851/35720 Training loss: 0.6710 0.2166 sec/batch\n",
      "Epoch 19/20  Iteration 33852/35720 Training loss: 0.6710 0.2091 sec/batch\n",
      "Epoch 19/20  Iteration 33853/35720 Training loss: 0.6710 0.2267 sec/batch\n",
      "Epoch 19/20  Iteration 33854/35720 Training loss: 0.6710 0.2178 sec/batch\n",
      "Epoch 19/20  Iteration 33855/35720 Training loss: 0.6709 0.2173 sec/batch\n",
      "Epoch 19/20  Iteration 33856/35720 Training loss: 0.6710 0.2056 sec/batch\n",
      "Epoch 19/20  Iteration 33857/35720 Training loss: 0.6710 0.2101 sec/batch\n",
      "Epoch 19/20  Iteration 33858/35720 Training loss: 0.6710 0.2127 sec/batch\n",
      "Epoch 19/20  Iteration 33859/35720 Training loss: 0.6710 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 33860/35720 Training loss: 0.6710 0.2129 sec/batch\n",
      "Epoch 19/20  Iteration 33861/35720 Training loss: 0.6711 0.2162 sec/batch\n",
      "Epoch 19/20  Iteration 33862/35720 Training loss: 0.6710 0.2120 sec/batch\n",
      "Epoch 19/20  Iteration 33863/35720 Training loss: 0.6710 0.2065 sec/batch\n",
      "Epoch 19/20  Iteration 33864/35720 Training loss: 0.6710 0.2115 sec/batch\n",
      "Epoch 19/20  Iteration 33865/35720 Training loss: 0.6710 0.2189 sec/batch\n",
      "Epoch 19/20  Iteration 33866/35720 Training loss: 0.6710 0.2201 sec/batch\n",
      "Epoch 19/20  Iteration 33867/35720 Training loss: 0.6710 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33868/35720 Training loss: 0.6710 0.2066 sec/batch\n",
      "Epoch 19/20  Iteration 33869/35720 Training loss: 0.6710 0.2132 sec/batch\n",
      "Epoch 19/20  Iteration 33870/35720 Training loss: 0.6710 0.2151 sec/batch\n",
      "Epoch 19/20  Iteration 33871/35720 Training loss: 0.6710 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 33872/35720 Training loss: 0.6710 0.2268 sec/batch\n",
      "Epoch 19/20  Iteration 33873/35720 Training loss: 0.6710 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33874/35720 Training loss: 0.6710 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33875/35720 Training loss: 0.6710 0.2081 sec/batch\n",
      "Epoch 19/20  Iteration 33876/35720 Training loss: 0.6710 0.2220 sec/batch\n",
      "Epoch 19/20  Iteration 33877/35720 Training loss: 0.6710 0.2276 sec/batch\n",
      "Epoch 19/20  Iteration 33878/35720 Training loss: 0.6710 0.2251 sec/batch\n",
      "Epoch 19/20  Iteration 33879/35720 Training loss: 0.6710 0.2070 sec/batch\n",
      "Epoch 19/20  Iteration 33880/35720 Training loss: 0.6710 0.2111 sec/batch\n",
      "Epoch 19/20  Iteration 33881/35720 Training loss: 0.6710 0.2270 sec/batch\n",
      "Epoch 19/20  Iteration 33882/35720 Training loss: 0.6710 0.2095 sec/batch\n",
      "Epoch 19/20  Iteration 33883/35720 Training loss: 0.6710 0.2240 sec/batch\n",
      "Epoch 19/20  Iteration 33884/35720 Training loss: 0.6710 0.2220 sec/batch\n",
      "Epoch 19/20  Iteration 33885/35720 Training loss: 0.6711 0.2162 sec/batch\n",
      "Epoch 19/20  Iteration 33886/35720 Training loss: 0.6711 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 33887/35720 Training loss: 0.6711 0.2159 sec/batch\n",
      "Epoch 19/20  Iteration 33888/35720 Training loss: 0.6711 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 33889/35720 Training loss: 0.6711 0.2241 sec/batch\n",
      "Epoch 19/20  Iteration 33890/35720 Training loss: 0.6710 0.2116 sec/batch\n",
      "Epoch 19/20  Iteration 33891/35720 Training loss: 0.6710 0.2212 sec/batch\n",
      "Epoch 19/20  Iteration 33892/35720 Training loss: 0.6710 0.2236 sec/batch\n",
      "Epoch 19/20  Iteration 33893/35720 Training loss: 0.6710 0.2136 sec/batch\n",
      "Epoch 19/20  Iteration 33894/35720 Training loss: 0.6711 0.2146 sec/batch\n",
      "Epoch 19/20  Iteration 33895/35720 Training loss: 0.6711 0.2102 sec/batch\n",
      "Epoch 19/20  Iteration 33896/35720 Training loss: 0.6711 0.2069 sec/batch\n",
      "Epoch 19/20  Iteration 33897/35720 Training loss: 0.6711 0.2087 sec/batch\n",
      "Epoch 19/20  Iteration 33898/35720 Training loss: 0.6711 0.2141 sec/batch\n",
      "Epoch 19/20  Iteration 33899/35720 Training loss: 0.6711 0.2258 sec/batch\n",
      "Epoch 19/20  Iteration 33900/35720 Training loss: 0.6711 0.2142 sec/batch\n",
      "Epoch 19/20  Iteration 33901/35720 Training loss: 0.6711 0.2098 sec/batch\n",
      "Epoch 19/20  Iteration 33902/35720 Training loss: 0.6711 0.2078 sec/batch\n",
      "Epoch 19/20  Iteration 33903/35720 Training loss: 0.6711 0.2084 sec/batch\n",
      "Epoch 19/20  Iteration 33904/35720 Training loss: 0.6712 0.2261 sec/batch\n",
      "Epoch 19/20  Iteration 33905/35720 Training loss: 0.6712 0.2152 sec/batch\n",
      "Epoch 19/20  Iteration 33906/35720 Training loss: 0.6712 0.2126 sec/batch\n",
      "Epoch 19/20  Iteration 33907/35720 Training loss: 0.6712 0.2189 sec/batch\n",
      "Epoch 19/20  Iteration 33908/35720 Training loss: 0.6712 0.2513 sec/batch\n",
      "Epoch 19/20  Iteration 33909/35720 Training loss: 0.6712 0.2304 sec/batch\n",
      "Epoch 19/20  Iteration 33910/35720 Training loss: 0.6711 0.2113 sec/batch\n",
      "Epoch 19/20  Iteration 33911/35720 Training loss: 0.6711 0.2190 sec/batch\n",
      "Epoch 19/20  Iteration 33912/35720 Training loss: 0.6711 0.2139 sec/batch\n",
      "Epoch 19/20  Iteration 33913/35720 Training loss: 0.6711 0.2059 sec/batch\n",
      "Epoch 19/20  Iteration 33914/35720 Training loss: 0.6711 0.2085 sec/batch\n",
      "Epoch 19/20  Iteration 33915/35720 Training loss: 0.6711 0.2137 sec/batch\n",
      "Epoch 19/20  Iteration 33916/35720 Training loss: 0.6712 0.2103 sec/batch\n",
      "Epoch 19/20  Iteration 33917/35720 Training loss: 0.6712 0.2158 sec/batch\n",
      "Epoch 19/20  Iteration 33918/35720 Training loss: 0.6712 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 33919/35720 Training loss: 0.6712 0.2105 sec/batch\n",
      "Epoch 19/20  Iteration 33920/35720 Training loss: 0.6712 0.2230 sec/batch\n",
      "Epoch 19/20  Iteration 33921/35720 Training loss: 0.6711 0.2218 sec/batch\n",
      "Epoch 19/20  Iteration 33922/35720 Training loss: 0.6711 0.2226 sec/batch\n",
      "Epoch 19/20  Iteration 33923/35720 Training loss: 0.6711 0.2061 sec/batch\n",
      "Epoch 19/20  Iteration 33924/35720 Training loss: 0.6711 0.2096 sec/batch\n",
      "Epoch 19/20  Iteration 33925/35720 Training loss: 0.6711 0.2130 sec/batch\n",
      "Epoch 19/20  Iteration 33926/35720 Training loss: 0.6711 0.2200 sec/batch\n",
      "Epoch 19/20  Iteration 33927/35720 Training loss: 0.6711 0.2082 sec/batch\n",
      "Epoch 19/20  Iteration 33928/35720 Training loss: 0.6711 0.2147 sec/batch\n",
      "Epoch 19/20  Iteration 33929/35720 Training loss: 0.6710 0.2067 sec/batch\n",
      "Epoch 19/20  Iteration 33930/35720 Training loss: 0.6710 0.2099 sec/batch\n",
      "Epoch 19/20  Iteration 33931/35720 Training loss: 0.6710 0.2196 sec/batch\n",
      "Epoch 19/20  Iteration 33932/35720 Training loss: 0.6710 0.2176 sec/batch\n",
      "Epoch 19/20  Iteration 33933/35720 Training loss: 0.6710 0.2172 sec/batch\n",
      "Epoch 19/20  Iteration 33934/35720 Training loss: 0.6710 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 33935/35720 Training loss: 0.6976 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 33936/35720 Training loss: 0.6808 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 33937/35720 Training loss: 0.6779 0.2285 sec/batch\n",
      "Epoch 20/20  Iteration 33938/35720 Training loss: 0.6772 0.2287 sec/batch\n",
      "Epoch 20/20  Iteration 33939/35720 Training loss: 0.6839 0.2273 sec/batch\n",
      "Epoch 20/20  Iteration 33940/35720 Training loss: 0.6726 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 33941/35720 Training loss: 0.6730 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 33942/35720 Training loss: 0.6659 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 33943/35720 Training loss: 0.6617 0.2190 sec/batch\n",
      "Epoch 20/20  Iteration 33944/35720 Training loss: 0.6650 0.2200 sec/batch\n",
      "Epoch 20/20  Iteration 33945/35720 Training loss: 0.6667 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 33946/35720 Training loss: 0.6617 0.2594 sec/batch\n",
      "Epoch 20/20  Iteration 33947/35720 Training loss: 0.6625 0.2314 sec/batch\n",
      "Epoch 20/20  Iteration 33948/35720 Training loss: 0.6644 0.2356 sec/batch\n",
      "Epoch 20/20  Iteration 33949/35720 Training loss: 0.6639 0.2238 sec/batch\n",
      "Epoch 20/20  Iteration 33950/35720 Training loss: 0.6627 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 33951/35720 Training loss: 0.6658 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 33952/35720 Training loss: 0.6617 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 33953/35720 Training loss: 0.6588 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 33954/35720 Training loss: 0.6598 0.2104 sec/batch\n",
      "Epoch 20/20  Iteration 33955/35720 Training loss: 0.6608 0.2184 sec/batch\n",
      "Epoch 20/20  Iteration 33956/35720 Training loss: 0.6577 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 33957/35720 Training loss: 0.6576 0.2242 sec/batch\n",
      "Epoch 20/20  Iteration 33958/35720 Training loss: 0.6588 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 33959/35720 Training loss: 0.6603 0.2053 sec/batch\n",
      "Epoch 20/20  Iteration 33960/35720 Training loss: 0.6608 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 33961/35720 Training loss: 0.6612 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 33962/35720 Training loss: 0.6625 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 33963/35720 Training loss: 0.6616 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 33964/35720 Training loss: 0.6630 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 33965/35720 Training loss: 0.6641 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 33966/35720 Training loss: 0.6640 0.2078 sec/batch\n",
      "Epoch 20/20  Iteration 33967/35720 Training loss: 0.6659 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 33968/35720 Training loss: 0.6672 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 33969/35720 Training loss: 0.6693 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 33970/35720 Training loss: 0.6685 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 33971/35720 Training loss: 0.6693 0.2129 sec/batch\n",
      "Epoch 20/20  Iteration 33972/35720 Training loss: 0.6686 0.2225 sec/batch\n",
      "Epoch 20/20  Iteration 33973/35720 Training loss: 0.6680 0.2165 sec/batch\n",
      "Epoch 20/20  Iteration 33974/35720 Training loss: 0.6689 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 33975/35720 Training loss: 0.6683 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 33976/35720 Training loss: 0.6681 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 33977/35720 Training loss: 0.6667 0.2114 sec/batch\n",
      "Epoch 20/20  Iteration 33978/35720 Training loss: 0.6659 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 33979/35720 Training loss: 0.6658 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 33980/35720 Training loss: 0.6654 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 33981/35720 Training loss: 0.6640 0.2172 sec/batch\n",
      "Epoch 20/20  Iteration 33982/35720 Training loss: 0.6637 0.2155 sec/batch\n",
      "Epoch 20/20  Iteration 33983/35720 Training loss: 0.6622 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 33984/35720 Training loss: 0.6614 0.2188 sec/batch\n",
      "Epoch 20/20  Iteration 33985/35720 Training loss: 0.6617 0.2254 sec/batch\n",
      "Epoch 20/20  Iteration 33986/35720 Training loss: 0.6623 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 33987/35720 Training loss: 0.6625 0.2222 sec/batch\n",
      "Epoch 20/20  Iteration 33988/35720 Training loss: 0.6610 0.2077 sec/batch\n",
      "Epoch 20/20  Iteration 33989/35720 Training loss: 0.6598 0.2175 sec/batch\n",
      "Epoch 20/20  Iteration 33990/35720 Training loss: 0.6599 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 33991/35720 Training loss: 0.6600 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 33992/35720 Training loss: 0.6592 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 33993/35720 Training loss: 0.6586 0.2266 sec/batch\n",
      "Epoch 20/20  Iteration 33994/35720 Training loss: 0.6578 0.2236 sec/batch\n",
      "Epoch 20/20  Iteration 33995/35720 Training loss: 0.6577 0.2075 sec/batch\n",
      "Epoch 20/20  Iteration 33996/35720 Training loss: 0.6565 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 33997/35720 Training loss: 0.6576 0.2076 sec/batch\n",
      "Epoch 20/20  Iteration 33998/35720 Training loss: 0.6579 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 33999/35720 Training loss: 0.6584 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 34000/35720 Training loss: 0.6582 0.2070 sec/batch\n",
      "Validation loss: 1.6619 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 34001/35720 Training loss: 0.6637 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34002/35720 Training loss: 0.6631 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 34003/35720 Training loss: 0.6641 0.2056 sec/batch\n",
      "Epoch 20/20  Iteration 34004/35720 Training loss: 0.6640 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 34005/35720 Training loss: 0.6648 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 34006/35720 Training loss: 0.6652 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34007/35720 Training loss: 0.6655 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 34008/35720 Training loss: 0.6655 0.2107 sec/batch\n",
      "Epoch 20/20  Iteration 34009/35720 Training loss: 0.6648 0.2240 sec/batch\n",
      "Epoch 20/20  Iteration 34010/35720 Training loss: 0.6647 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 34011/35720 Training loss: 0.6645 0.2299 sec/batch\n",
      "Epoch 20/20  Iteration 34012/35720 Training loss: 0.6651 0.2057 sec/batch\n",
      "Epoch 20/20  Iteration 34013/35720 Training loss: 0.6650 0.2255 sec/batch\n",
      "Epoch 20/20  Iteration 34014/35720 Training loss: 0.6656 0.2258 sec/batch\n",
      "Epoch 20/20  Iteration 34015/35720 Training loss: 0.6660 0.2280 sec/batch\n",
      "Epoch 20/20  Iteration 34016/35720 Training loss: 0.6661 0.2192 sec/batch\n",
      "Epoch 20/20  Iteration 34017/35720 Training loss: 0.6663 0.2075 sec/batch\n",
      "Epoch 20/20  Iteration 34018/35720 Training loss: 0.6663 0.2057 sec/batch\n",
      "Epoch 20/20  Iteration 34019/35720 Training loss: 0.6660 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34020/35720 Training loss: 0.6662 0.2184 sec/batch\n",
      "Epoch 20/20  Iteration 34021/35720 Training loss: 0.6663 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 34022/35720 Training loss: 0.6662 0.2103 sec/batch\n",
      "Epoch 20/20  Iteration 34023/35720 Training loss: 0.6653 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 34024/35720 Training loss: 0.6650 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 34025/35720 Training loss: 0.6653 0.2128 sec/batch\n",
      "Epoch 20/20  Iteration 34026/35720 Training loss: 0.6648 0.2166 sec/batch\n",
      "Epoch 20/20  Iteration 34027/35720 Training loss: 0.6652 0.2142 sec/batch\n",
      "Epoch 20/20  Iteration 34028/35720 Training loss: 0.6659 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 34029/35720 Training loss: 0.6655 0.2213 sec/batch\n",
      "Epoch 20/20  Iteration 34030/35720 Training loss: 0.6651 0.2154 sec/batch\n",
      "Epoch 20/20  Iteration 34031/35720 Training loss: 0.6653 0.2217 sec/batch\n",
      "Epoch 20/20  Iteration 34032/35720 Training loss: 0.6654 0.2275 sec/batch\n",
      "Epoch 20/20  Iteration 34033/35720 Training loss: 0.6654 0.2140 sec/batch\n",
      "Epoch 20/20  Iteration 34034/35720 Training loss: 0.6647 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34035/35720 Training loss: 0.6644 0.2053 sec/batch\n",
      "Epoch 20/20  Iteration 34036/35720 Training loss: 0.6644 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34037/35720 Training loss: 0.6640 0.2247 sec/batch\n",
      "Epoch 20/20  Iteration 34038/35720 Training loss: 0.6638 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 34039/35720 Training loss: 0.6638 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 34040/35720 Training loss: 0.6635 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 34041/35720 Training loss: 0.6633 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 34042/35720 Training loss: 0.6634 0.2163 sec/batch\n",
      "Epoch 20/20  Iteration 34043/35720 Training loss: 0.6636 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 34044/35720 Training loss: 0.6635 0.2337 sec/batch\n",
      "Epoch 20/20  Iteration 34045/35720 Training loss: 0.6635 0.2234 sec/batch\n",
      "Epoch 20/20  Iteration 34046/35720 Training loss: 0.6636 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34047/35720 Training loss: 0.6637 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 34048/35720 Training loss: 0.6639 0.2255 sec/batch\n",
      "Epoch 20/20  Iteration 34049/35720 Training loss: 0.6640 0.2235 sec/batch\n",
      "Epoch 20/20  Iteration 34050/35720 Training loss: 0.6639 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 34051/35720 Training loss: 0.6638 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34052/35720 Training loss: 0.6639 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34053/35720 Training loss: 0.6637 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 34054/35720 Training loss: 0.6641 0.2184 sec/batch\n",
      "Epoch 20/20  Iteration 34055/35720 Training loss: 0.6641 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 34056/35720 Training loss: 0.6635 0.2235 sec/batch\n",
      "Epoch 20/20  Iteration 34057/35720 Training loss: 0.6635 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34058/35720 Training loss: 0.6637 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 34059/35720 Training loss: 0.6636 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 34060/35720 Training loss: 0.6636 0.2207 sec/batch\n",
      "Epoch 20/20  Iteration 34061/35720 Training loss: 0.6640 0.2274 sec/batch\n",
      "Epoch 20/20  Iteration 34062/35720 Training loss: 0.6639 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 34063/35720 Training loss: 0.6636 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 34064/35720 Training loss: 0.6639 0.2291 sec/batch\n",
      "Epoch 20/20  Iteration 34065/35720 Training loss: 0.6636 0.2249 sec/batch\n",
      "Epoch 20/20  Iteration 34066/35720 Training loss: 0.6634 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 34067/35720 Training loss: 0.6633 0.2113 sec/batch\n",
      "Epoch 20/20  Iteration 34068/35720 Training loss: 0.6634 0.2110 sec/batch\n",
      "Epoch 20/20  Iteration 34069/35720 Training loss: 0.6632 0.2120 sec/batch\n",
      "Epoch 20/20  Iteration 34070/35720 Training loss: 0.6629 0.2127 sec/batch\n",
      "Epoch 20/20  Iteration 34071/35720 Training loss: 0.6632 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 34072/35720 Training loss: 0.6635 0.2178 sec/batch\n",
      "Epoch 20/20  Iteration 34073/35720 Training loss: 0.6636 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 34074/35720 Training loss: 0.6638 0.2302 sec/batch\n",
      "Epoch 20/20  Iteration 34075/35720 Training loss: 0.6637 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34076/35720 Training loss: 0.6634 0.2340 sec/batch\n",
      "Epoch 20/20  Iteration 34077/35720 Training loss: 0.6631 0.2107 sec/batch\n",
      "Epoch 20/20  Iteration 34078/35720 Training loss: 0.6627 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 34079/35720 Training loss: 0.6628 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 34080/35720 Training loss: 0.6629 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 34081/35720 Training loss: 0.6626 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 34082/35720 Training loss: 0.6625 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 34083/35720 Training loss: 0.6626 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 34084/35720 Training loss: 0.6621 0.2056 sec/batch\n",
      "Epoch 20/20  Iteration 34085/35720 Training loss: 0.6620 0.2126 sec/batch\n",
      "Epoch 20/20  Iteration 34086/35720 Training loss: 0.6620 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 34087/35720 Training loss: 0.6622 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34088/35720 Training loss: 0.6623 0.2185 sec/batch\n",
      "Epoch 20/20  Iteration 34089/35720 Training loss: 0.6623 0.2231 sec/batch\n",
      "Epoch 20/20  Iteration 34090/35720 Training loss: 0.6626 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34091/35720 Training loss: 0.6627 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 34092/35720 Training loss: 0.6629 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 34093/35720 Training loss: 0.6627 0.2263 sec/batch\n",
      "Epoch 20/20  Iteration 34094/35720 Training loss: 0.6626 0.2367 sec/batch\n",
      "Epoch 20/20  Iteration 34095/35720 Training loss: 0.6623 0.2303 sec/batch\n",
      "Epoch 20/20  Iteration 34096/35720 Training loss: 0.6624 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 34097/35720 Training loss: 0.6625 0.2376 sec/batch\n",
      "Epoch 20/20  Iteration 34098/35720 Training loss: 0.6624 0.2176 sec/batch\n",
      "Epoch 20/20  Iteration 34099/35720 Training loss: 0.6624 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 34100/35720 Training loss: 0.6624 0.2173 sec/batch\n",
      "Epoch 20/20  Iteration 34101/35720 Training loss: 0.6624 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 34102/35720 Training loss: 0.6626 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 34103/35720 Training loss: 0.6629 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 34104/35720 Training loss: 0.6630 0.2154 sec/batch\n",
      "Epoch 20/20  Iteration 34105/35720 Training loss: 0.6633 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 34106/35720 Training loss: 0.6637 0.2132 sec/batch\n",
      "Epoch 20/20  Iteration 34107/35720 Training loss: 0.6639 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 34108/35720 Training loss: 0.6642 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34109/35720 Training loss: 0.6645 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 34110/35720 Training loss: 0.6645 0.2226 sec/batch\n",
      "Epoch 20/20  Iteration 34111/35720 Training loss: 0.6647 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 34112/35720 Training loss: 0.6645 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 34113/35720 Training loss: 0.6646 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 34114/35720 Training loss: 0.6643 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 34115/35720 Training loss: 0.6644 0.2264 sec/batch\n",
      "Epoch 20/20  Iteration 34116/35720 Training loss: 0.6643 0.2126 sec/batch\n",
      "Epoch 20/20  Iteration 34117/35720 Training loss: 0.6644 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 34118/35720 Training loss: 0.6644 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 34119/35720 Training loss: 0.6644 0.2132 sec/batch\n",
      "Epoch 20/20  Iteration 34120/35720 Training loss: 0.6644 0.2230 sec/batch\n",
      "Epoch 20/20  Iteration 34121/35720 Training loss: 0.6643 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 34122/35720 Training loss: 0.6644 0.2245 sec/batch\n",
      "Epoch 20/20  Iteration 34123/35720 Training loss: 0.6645 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 34124/35720 Training loss: 0.6644 0.2076 sec/batch\n",
      "Epoch 20/20  Iteration 34125/35720 Training loss: 0.6644 0.2080 sec/batch\n",
      "Epoch 20/20  Iteration 34126/35720 Training loss: 0.6645 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 34127/35720 Training loss: 0.6646 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 34128/35720 Training loss: 0.6647 0.2190 sec/batch\n",
      "Epoch 20/20  Iteration 34129/35720 Training loss: 0.6646 0.2122 sec/batch\n",
      "Epoch 20/20  Iteration 34130/35720 Training loss: 0.6648 0.2051 sec/batch\n",
      "Epoch 20/20  Iteration 34131/35720 Training loss: 0.6646 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34132/35720 Training loss: 0.6646 0.3916 sec/batch\n",
      "Epoch 20/20  Iteration 34133/35720 Training loss: 0.6647 0.2514 sec/batch\n",
      "Epoch 20/20  Iteration 34134/35720 Training loss: 0.6649 0.2202 sec/batch\n",
      "Epoch 20/20  Iteration 34135/35720 Training loss: 0.6648 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 34136/35720 Training loss: 0.6648 0.2293 sec/batch\n",
      "Epoch 20/20  Iteration 34137/35720 Training loss: 0.6651 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 34138/35720 Training loss: 0.6650 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 34139/35720 Training loss: 0.6651 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 34140/35720 Training loss: 0.6650 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34141/35720 Training loss: 0.6653 0.2132 sec/batch\n",
      "Epoch 20/20  Iteration 34142/35720 Training loss: 0.6656 0.2230 sec/batch\n",
      "Epoch 20/20  Iteration 34143/35720 Training loss: 0.6660 0.2275 sec/batch\n",
      "Epoch 20/20  Iteration 34144/35720 Training loss: 0.6659 0.2148 sec/batch\n",
      "Epoch 20/20  Iteration 34145/35720 Training loss: 0.6662 0.2236 sec/batch\n",
      "Epoch 20/20  Iteration 34146/35720 Training loss: 0.6661 0.2207 sec/batch\n",
      "Epoch 20/20  Iteration 34147/35720 Training loss: 0.6661 0.2193 sec/batch\n",
      "Epoch 20/20  Iteration 34148/35720 Training loss: 0.6661 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 34149/35720 Training loss: 0.6661 0.2633 sec/batch\n",
      "Epoch 20/20  Iteration 34150/35720 Training loss: 0.6662 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34151/35720 Training loss: 0.6662 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34152/35720 Training loss: 0.6662 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 34153/35720 Training loss: 0.6663 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 34154/35720 Training loss: 0.6663 0.2223 sec/batch\n",
      "Epoch 20/20  Iteration 34155/35720 Training loss: 0.6664 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 34156/35720 Training loss: 0.6663 0.2051 sec/batch\n",
      "Epoch 20/20  Iteration 34157/35720 Training loss: 0.6666 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 34158/35720 Training loss: 0.6667 0.2137 sec/batch\n",
      "Epoch 20/20  Iteration 34159/35720 Training loss: 0.6668 0.2200 sec/batch\n",
      "Epoch 20/20  Iteration 34160/35720 Training loss: 0.6669 0.2131 sec/batch\n",
      "Epoch 20/20  Iteration 34161/35720 Training loss: 0.6668 0.2151 sec/batch\n",
      "Epoch 20/20  Iteration 34162/35720 Training loss: 0.6668 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 34163/35720 Training loss: 0.6666 0.2111 sec/batch\n",
      "Epoch 20/20  Iteration 34164/35720 Training loss: 0.6666 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 34165/35720 Training loss: 0.6669 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 34166/35720 Training loss: 0.6670 0.2271 sec/batch\n",
      "Epoch 20/20  Iteration 34167/35720 Training loss: 0.6669 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34168/35720 Training loss: 0.6668 0.2075 sec/batch\n",
      "Epoch 20/20  Iteration 34169/35720 Training loss: 0.6669 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34170/35720 Training loss: 0.6669 0.2403 sec/batch\n",
      "Epoch 20/20  Iteration 34171/35720 Training loss: 0.6671 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 34172/35720 Training loss: 0.6669 0.2266 sec/batch\n",
      "Epoch 20/20  Iteration 34173/35720 Training loss: 0.6669 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 34174/35720 Training loss: 0.6669 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34175/35720 Training loss: 0.6669 0.2204 sec/batch\n",
      "Epoch 20/20  Iteration 34176/35720 Training loss: 0.6670 0.2258 sec/batch\n",
      "Epoch 20/20  Iteration 34177/35720 Training loss: 0.6670 0.2129 sec/batch\n",
      "Epoch 20/20  Iteration 34178/35720 Training loss: 0.6667 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34179/35720 Training loss: 0.6665 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 34180/35720 Training loss: 0.6666 0.2290 sec/batch\n",
      "Epoch 20/20  Iteration 34181/35720 Training loss: 0.6666 0.2246 sec/batch\n",
      "Epoch 20/20  Iteration 34182/35720 Training loss: 0.6665 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34183/35720 Training loss: 0.6664 0.2148 sec/batch\n",
      "Epoch 20/20  Iteration 34184/35720 Training loss: 0.6662 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34185/35720 Training loss: 0.6662 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 34186/35720 Training loss: 0.6662 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34187/35720 Training loss: 0.6660 0.2162 sec/batch\n",
      "Epoch 20/20  Iteration 34188/35720 Training loss: 0.6661 0.2186 sec/batch\n",
      "Epoch 20/20  Iteration 34189/35720 Training loss: 0.6664 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34190/35720 Training loss: 0.6664 0.2057 sec/batch\n",
      "Epoch 20/20  Iteration 34191/35720 Training loss: 0.6664 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 34192/35720 Training loss: 0.6663 0.2244 sec/batch\n",
      "Epoch 20/20  Iteration 34193/35720 Training loss: 0.6666 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 34194/35720 Training loss: 0.6666 0.2281 sec/batch\n",
      "Epoch 20/20  Iteration 34195/35720 Training loss: 0.6666 0.2259 sec/batch\n",
      "Epoch 20/20  Iteration 34196/35720 Training loss: 0.6665 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 34197/35720 Training loss: 0.6665 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34198/35720 Training loss: 0.6665 0.2224 sec/batch\n",
      "Epoch 20/20  Iteration 34199/35720 Training loss: 0.6664 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 34200/35720 Training loss: 0.6667 0.2241 sec/batch\n",
      "Validation loss: 1.68811 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 34201/35720 Training loss: 0.6679 0.2078 sec/batch\n",
      "Epoch 20/20  Iteration 34202/35720 Training loss: 0.6680 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 34203/35720 Training loss: 0.6681 0.2311 sec/batch\n",
      "Epoch 20/20  Iteration 34204/35720 Training loss: 0.6678 0.2189 sec/batch\n",
      "Epoch 20/20  Iteration 34205/35720 Training loss: 0.6676 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 34206/35720 Training loss: 0.6675 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 34207/35720 Training loss: 0.6674 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 34208/35720 Training loss: 0.6674 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 34209/35720 Training loss: 0.6673 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34210/35720 Training loss: 0.6672 0.2072 sec/batch\n",
      "Epoch 20/20  Iteration 34211/35720 Training loss: 0.6670 0.2058 sec/batch\n",
      "Epoch 20/20  Iteration 34212/35720 Training loss: 0.6669 0.2111 sec/batch\n",
      "Epoch 20/20  Iteration 34213/35720 Training loss: 0.6667 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 34214/35720 Training loss: 0.6667 0.2241 sec/batch\n",
      "Epoch 20/20  Iteration 34215/35720 Training loss: 0.6666 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 34216/35720 Training loss: 0.6664 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 34217/35720 Training loss: 0.6663 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 34218/35720 Training loss: 0.6662 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 34219/35720 Training loss: 0.6663 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34220/35720 Training loss: 0.6663 0.2135 sec/batch\n",
      "Epoch 20/20  Iteration 34221/35720 Training loss: 0.6661 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 34222/35720 Training loss: 0.6662 0.2167 sec/batch\n",
      "Epoch 20/20  Iteration 34223/35720 Training loss: 0.6663 0.2255 sec/batch\n",
      "Epoch 20/20  Iteration 34224/35720 Training loss: 0.6662 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34225/35720 Training loss: 0.6661 0.2255 sec/batch\n",
      "Epoch 20/20  Iteration 34226/35720 Training loss: 0.6662 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34227/35720 Training loss: 0.6660 0.2155 sec/batch\n",
      "Epoch 20/20  Iteration 34228/35720 Training loss: 0.6660 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34229/35720 Training loss: 0.6662 0.2079 sec/batch\n",
      "Epoch 20/20  Iteration 34230/35720 Training loss: 0.6662 0.2134 sec/batch\n",
      "Epoch 20/20  Iteration 34231/35720 Training loss: 0.6660 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 34232/35720 Training loss: 0.6662 0.2375 sec/batch\n",
      "Epoch 20/20  Iteration 34233/35720 Training loss: 0.6660 0.2189 sec/batch\n",
      "Epoch 20/20  Iteration 34234/35720 Training loss: 0.6661 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 34235/35720 Training loss: 0.6660 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 34236/35720 Training loss: 0.6658 0.2280 sec/batch\n",
      "Epoch 20/20  Iteration 34237/35720 Training loss: 0.6659 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 34238/35720 Training loss: 0.6660 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 34239/35720 Training loss: 0.6660 0.2076 sec/batch\n",
      "Epoch 20/20  Iteration 34240/35720 Training loss: 0.6657 0.2125 sec/batch\n",
      "Epoch 20/20  Iteration 34241/35720 Training loss: 0.6656 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 34242/35720 Training loss: 0.6657 0.2111 sec/batch\n",
      "Epoch 20/20  Iteration 34243/35720 Training loss: 0.6656 0.2206 sec/batch\n",
      "Epoch 20/20  Iteration 34244/35720 Training loss: 0.6656 0.2222 sec/batch\n",
      "Epoch 20/20  Iteration 34245/35720 Training loss: 0.6655 0.2234 sec/batch\n",
      "Epoch 20/20  Iteration 34246/35720 Training loss: 0.6654 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34247/35720 Training loss: 0.6653 0.2129 sec/batch\n",
      "Epoch 20/20  Iteration 34248/35720 Training loss: 0.6652 0.2320 sec/batch\n",
      "Epoch 20/20  Iteration 34249/35720 Training loss: 0.6652 0.2251 sec/batch\n",
      "Epoch 20/20  Iteration 34250/35720 Training loss: 0.6653 0.2142 sec/batch\n",
      "Epoch 20/20  Iteration 34251/35720 Training loss: 0.6651 0.2113 sec/batch\n",
      "Epoch 20/20  Iteration 34252/35720 Training loss: 0.6651 0.2209 sec/batch\n",
      "Epoch 20/20  Iteration 34253/35720 Training loss: 0.6652 0.2361 sec/batch\n",
      "Epoch 20/20  Iteration 34254/35720 Training loss: 0.6652 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34255/35720 Training loss: 0.6654 0.2258 sec/batch\n",
      "Epoch 20/20  Iteration 34256/35720 Training loss: 0.6655 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 34257/35720 Training loss: 0.6655 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 34258/35720 Training loss: 0.6655 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34259/35720 Training loss: 0.6653 0.2125 sec/batch\n",
      "Epoch 20/20  Iteration 34260/35720 Training loss: 0.6654 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 34261/35720 Training loss: 0.6654 0.2368 sec/batch\n",
      "Epoch 20/20  Iteration 34262/35720 Training loss: 0.6653 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34263/35720 Training loss: 0.6652 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 34264/35720 Training loss: 0.6652 0.2204 sec/batch\n",
      "Epoch 20/20  Iteration 34265/35720 Training loss: 0.6652 0.2234 sec/batch\n",
      "Epoch 20/20  Iteration 34266/35720 Training loss: 0.6653 0.2206 sec/batch\n",
      "Epoch 20/20  Iteration 34267/35720 Training loss: 0.6651 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34268/35720 Training loss: 0.6651 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34269/35720 Training loss: 0.6651 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 34270/35720 Training loss: 0.6649 0.2225 sec/batch\n",
      "Epoch 20/20  Iteration 34271/35720 Training loss: 0.6647 0.2248 sec/batch\n",
      "Epoch 20/20  Iteration 34272/35720 Training loss: 0.6646 0.2058 sec/batch\n",
      "Epoch 20/20  Iteration 34273/35720 Training loss: 0.6645 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34274/35720 Training loss: 0.6645 0.2291 sec/batch\n",
      "Epoch 20/20  Iteration 34275/35720 Training loss: 0.6644 0.2182 sec/batch\n",
      "Epoch 20/20  Iteration 34276/35720 Training loss: 0.6644 0.2333 sec/batch\n",
      "Epoch 20/20  Iteration 34277/35720 Training loss: 0.6645 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 34278/35720 Training loss: 0.6645 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 34279/35720 Training loss: 0.6643 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34280/35720 Training loss: 0.6644 0.2128 sec/batch\n",
      "Epoch 20/20  Iteration 34281/35720 Training loss: 0.6644 0.2137 sec/batch\n",
      "Epoch 20/20  Iteration 34282/35720 Training loss: 0.6644 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34283/35720 Training loss: 0.6644 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 34284/35720 Training loss: 0.6645 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 34285/35720 Training loss: 0.6644 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 34286/35720 Training loss: 0.6643 0.2311 sec/batch\n",
      "Epoch 20/20  Iteration 34287/35720 Training loss: 0.6643 0.2218 sec/batch\n",
      "Epoch 20/20  Iteration 34288/35720 Training loss: 0.6644 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 34289/35720 Training loss: 0.6645 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 34290/35720 Training loss: 0.6645 0.2113 sec/batch\n",
      "Epoch 20/20  Iteration 34291/35720 Training loss: 0.6646 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 34292/35720 Training loss: 0.6647 0.2264 sec/batch\n",
      "Epoch 20/20  Iteration 34293/35720 Training loss: 0.6646 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 34294/35720 Training loss: 0.6645 0.2131 sec/batch\n",
      "Epoch 20/20  Iteration 34295/35720 Training loss: 0.6645 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 34296/35720 Training loss: 0.6645 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34297/35720 Training loss: 0.6645 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 34298/35720 Training loss: 0.6644 0.2199 sec/batch\n",
      "Epoch 20/20  Iteration 34299/35720 Training loss: 0.6644 0.2135 sec/batch\n",
      "Epoch 20/20  Iteration 34300/35720 Training loss: 0.6644 0.2236 sec/batch\n",
      "Epoch 20/20  Iteration 34301/35720 Training loss: 0.6645 0.2191 sec/batch\n",
      "Epoch 20/20  Iteration 34302/35720 Training loss: 0.6643 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34303/35720 Training loss: 0.6643 0.2205 sec/batch\n",
      "Epoch 20/20  Iteration 34304/35720 Training loss: 0.6642 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 34305/35720 Training loss: 0.6642 0.2191 sec/batch\n",
      "Epoch 20/20  Iteration 34306/35720 Training loss: 0.6642 0.2056 sec/batch\n",
      "Epoch 20/20  Iteration 34307/35720 Training loss: 0.6642 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 34308/35720 Training loss: 0.6641 0.2175 sec/batch\n",
      "Epoch 20/20  Iteration 34309/35720 Training loss: 0.6641 0.2242 sec/batch\n",
      "Epoch 20/20  Iteration 34310/35720 Training loss: 0.6640 0.2132 sec/batch\n",
      "Epoch 20/20  Iteration 34311/35720 Training loss: 0.6641 0.2207 sec/batch\n",
      "Epoch 20/20  Iteration 34312/35720 Training loss: 0.6641 0.2229 sec/batch\n",
      "Epoch 20/20  Iteration 34313/35720 Training loss: 0.6639 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 34314/35720 Training loss: 0.6639 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 34315/35720 Training loss: 0.6638 0.2131 sec/batch\n",
      "Epoch 20/20  Iteration 34316/35720 Training loss: 0.6637 0.2270 sec/batch\n",
      "Epoch 20/20  Iteration 34317/35720 Training loss: 0.6636 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 34318/35720 Training loss: 0.6637 0.2056 sec/batch\n",
      "Epoch 20/20  Iteration 34319/35720 Training loss: 0.6637 0.2103 sec/batch\n",
      "Epoch 20/20  Iteration 34320/35720 Training loss: 0.6637 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 34321/35720 Training loss: 0.6637 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 34322/35720 Training loss: 0.6637 0.2186 sec/batch\n",
      "Epoch 20/20  Iteration 34323/35720 Training loss: 0.6637 0.2251 sec/batch\n",
      "Epoch 20/20  Iteration 34324/35720 Training loss: 0.6637 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34325/35720 Training loss: 0.6636 0.2185 sec/batch\n",
      "Epoch 20/20  Iteration 34326/35720 Training loss: 0.6634 0.2127 sec/batch\n",
      "Epoch 20/20  Iteration 34327/35720 Training loss: 0.6635 0.2283 sec/batch\n",
      "Epoch 20/20  Iteration 34328/35720 Training loss: 0.6633 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 34329/35720 Training loss: 0.6634 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34330/35720 Training loss: 0.6634 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34331/35720 Training loss: 0.6634 0.2264 sec/batch\n",
      "Epoch 20/20  Iteration 34332/35720 Training loss: 0.6633 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34333/35720 Training loss: 0.6633 0.2299 sec/batch\n",
      "Epoch 20/20  Iteration 34334/35720 Training loss: 0.6633 0.2046 sec/batch\n",
      "Epoch 20/20  Iteration 34335/35720 Training loss: 0.6633 0.2224 sec/batch\n",
      "Epoch 20/20  Iteration 34336/35720 Training loss: 0.6633 0.2249 sec/batch\n",
      "Epoch 20/20  Iteration 34337/35720 Training loss: 0.6633 0.2218 sec/batch\n",
      "Epoch 20/20  Iteration 34338/35720 Training loss: 0.6633 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 34339/35720 Training loss: 0.6632 0.2076 sec/batch\n",
      "Epoch 20/20  Iteration 34340/35720 Training loss: 0.6632 0.2216 sec/batch\n",
      "Epoch 20/20  Iteration 34341/35720 Training loss: 0.6631 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34342/35720 Training loss: 0.6632 0.2261 sec/batch\n",
      "Epoch 20/20  Iteration 34343/35720 Training loss: 0.6631 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34344/35720 Training loss: 0.6629 0.2237 sec/batch\n",
      "Epoch 20/20  Iteration 34345/35720 Training loss: 0.6630 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 34346/35720 Training loss: 0.6629 0.2110 sec/batch\n",
      "Epoch 20/20  Iteration 34347/35720 Training loss: 0.6628 0.2231 sec/batch\n",
      "Epoch 20/20  Iteration 34348/35720 Training loss: 0.6627 0.2197 sec/batch\n",
      "Epoch 20/20  Iteration 34349/35720 Training loss: 0.6626 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 34350/35720 Training loss: 0.6626 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34351/35720 Training loss: 0.6625 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34352/35720 Training loss: 0.6624 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34353/35720 Training loss: 0.6624 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 34354/35720 Training loss: 0.6624 0.2130 sec/batch\n",
      "Epoch 20/20  Iteration 34355/35720 Training loss: 0.6624 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 34356/35720 Training loss: 0.6624 0.2178 sec/batch\n",
      "Epoch 20/20  Iteration 34357/35720 Training loss: 0.6624 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 34358/35720 Training loss: 0.6625 0.2216 sec/batch\n",
      "Epoch 20/20  Iteration 34359/35720 Training loss: 0.6625 0.2176 sec/batch\n",
      "Epoch 20/20  Iteration 34360/35720 Training loss: 0.6624 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34361/35720 Training loss: 0.6622 0.2165 sec/batch\n",
      "Epoch 20/20  Iteration 34362/35720 Training loss: 0.6622 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34363/35720 Training loss: 0.6622 0.2132 sec/batch\n",
      "Epoch 20/20  Iteration 34364/35720 Training loss: 0.6622 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 34365/35720 Training loss: 0.6623 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 34366/35720 Training loss: 0.6622 0.2487 sec/batch\n",
      "Epoch 20/20  Iteration 34367/35720 Training loss: 0.6623 0.2126 sec/batch\n",
      "Epoch 20/20  Iteration 34368/35720 Training loss: 0.6623 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 34369/35720 Training loss: 0.6624 0.2291 sec/batch\n",
      "Epoch 20/20  Iteration 34370/35720 Training loss: 0.6624 0.2214 sec/batch\n",
      "Epoch 20/20  Iteration 34371/35720 Training loss: 0.6626 0.2210 sec/batch\n",
      "Epoch 20/20  Iteration 34372/35720 Training loss: 0.6626 0.2237 sec/batch\n",
      "Epoch 20/20  Iteration 34373/35720 Training loss: 0.6627 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 34374/35720 Training loss: 0.6627 0.2199 sec/batch\n",
      "Epoch 20/20  Iteration 34375/35720 Training loss: 0.6627 0.2257 sec/batch\n",
      "Epoch 20/20  Iteration 34376/35720 Training loss: 0.6628 0.2151 sec/batch\n",
      "Epoch 20/20  Iteration 34377/35720 Training loss: 0.6627 0.2148 sec/batch\n",
      "Epoch 20/20  Iteration 34378/35720 Training loss: 0.6627 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 34379/35720 Training loss: 0.6627 0.2126 sec/batch\n",
      "Epoch 20/20  Iteration 34380/35720 Training loss: 0.6628 0.2205 sec/batch\n",
      "Epoch 20/20  Iteration 34381/35720 Training loss: 0.6629 0.2221 sec/batch\n",
      "Epoch 20/20  Iteration 34382/35720 Training loss: 0.6630 0.2168 sec/batch\n",
      "Epoch 20/20  Iteration 34383/35720 Training loss: 0.6631 0.2303 sec/batch\n",
      "Epoch 20/20  Iteration 34384/35720 Training loss: 0.6630 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34385/35720 Training loss: 0.6629 0.2107 sec/batch\n",
      "Epoch 20/20  Iteration 34386/35720 Training loss: 0.6628 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 34387/35720 Training loss: 0.6629 0.2080 sec/batch\n",
      "Epoch 20/20  Iteration 34388/35720 Training loss: 0.6629 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 34389/35720 Training loss: 0.6630 0.2154 sec/batch\n",
      "Epoch 20/20  Iteration 34390/35720 Training loss: 0.6631 0.2170 sec/batch\n",
      "Epoch 20/20  Iteration 34391/35720 Training loss: 0.6632 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 34392/35720 Training loss: 0.6632 0.2212 sec/batch\n",
      "Epoch 20/20  Iteration 34393/35720 Training loss: 0.6630 0.2218 sec/batch\n",
      "Epoch 20/20  Iteration 34394/35720 Training loss: 0.6629 0.2179 sec/batch\n",
      "Epoch 20/20  Iteration 34395/35720 Training loss: 0.6630 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34396/35720 Training loss: 0.6631 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 34397/35720 Training loss: 0.6633 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 34398/35720 Training loss: 0.6633 0.2217 sec/batch\n",
      "Epoch 20/20  Iteration 34399/35720 Training loss: 0.6633 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 34400/35720 Training loss: 0.6633 0.2258 sec/batch\n",
      "Validation loss: 1.66917 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 34401/35720 Training loss: 0.6641 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 34402/35720 Training loss: 0.6641 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34403/35720 Training loss: 0.6641 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 34404/35720 Training loss: 0.6641 0.2128 sec/batch\n",
      "Epoch 20/20  Iteration 34405/35720 Training loss: 0.6641 0.2140 sec/batch\n",
      "Epoch 20/20  Iteration 34406/35720 Training loss: 0.6640 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 34407/35720 Training loss: 0.6640 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 34408/35720 Training loss: 0.6639 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 34409/35720 Training loss: 0.6640 0.2170 sec/batch\n",
      "Epoch 20/20  Iteration 34410/35720 Training loss: 0.6640 0.2199 sec/batch\n",
      "Epoch 20/20  Iteration 34411/35720 Training loss: 0.6640 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 34412/35720 Training loss: 0.6640 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 34413/35720 Training loss: 0.6640 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 34414/35720 Training loss: 0.6640 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 34415/35720 Training loss: 0.6639 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 34416/35720 Training loss: 0.6639 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 34417/35720 Training loss: 0.6639 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34418/35720 Training loss: 0.6640 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34419/35720 Training loss: 0.6640 0.2188 sec/batch\n",
      "Epoch 20/20  Iteration 34420/35720 Training loss: 0.6640 0.2185 sec/batch\n",
      "Epoch 20/20  Iteration 34421/35720 Training loss: 0.6640 0.2205 sec/batch\n",
      "Epoch 20/20  Iteration 34422/35720 Training loss: 0.6640 0.2167 sec/batch\n",
      "Epoch 20/20  Iteration 34423/35720 Training loss: 0.6640 0.2058 sec/batch\n",
      "Epoch 20/20  Iteration 34424/35720 Training loss: 0.6639 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34425/35720 Training loss: 0.6639 0.2173 sec/batch\n",
      "Epoch 20/20  Iteration 34426/35720 Training loss: 0.6640 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 34427/35720 Training loss: 0.6640 0.2183 sec/batch\n",
      "Epoch 20/20  Iteration 34428/35720 Training loss: 0.6639 0.2244 sec/batch\n",
      "Epoch 20/20  Iteration 34429/35720 Training loss: 0.6639 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 34430/35720 Training loss: 0.6638 0.2188 sec/batch\n",
      "Epoch 20/20  Iteration 34431/35720 Training loss: 0.6639 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 34432/35720 Training loss: 0.6640 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 34433/35720 Training loss: 0.6639 0.2224 sec/batch\n",
      "Epoch 20/20  Iteration 34434/35720 Training loss: 0.6640 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 34435/35720 Training loss: 0.6639 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34436/35720 Training loss: 0.6639 0.2142 sec/batch\n",
      "Epoch 20/20  Iteration 34437/35720 Training loss: 0.6638 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 34438/35720 Training loss: 0.6637 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 34439/35720 Training loss: 0.6637 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 34440/35720 Training loss: 0.6636 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 34441/35720 Training loss: 0.6636 0.2075 sec/batch\n",
      "Epoch 20/20  Iteration 34442/35720 Training loss: 0.6636 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 34443/35720 Training loss: 0.6637 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 34444/35720 Training loss: 0.6637 0.2289 sec/batch\n",
      "Epoch 20/20  Iteration 34445/35720 Training loss: 0.6636 0.2254 sec/batch\n",
      "Epoch 20/20  Iteration 34446/35720 Training loss: 0.6637 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 34447/35720 Training loss: 0.6636 0.2104 sec/batch\n",
      "Epoch 20/20  Iteration 34448/35720 Training loss: 0.6636 0.2248 sec/batch\n",
      "Epoch 20/20  Iteration 34449/35720 Training loss: 0.6637 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 34450/35720 Training loss: 0.6637 0.2216 sec/batch\n",
      "Epoch 20/20  Iteration 34451/35720 Training loss: 0.6637 0.2058 sec/batch\n",
      "Epoch 20/20  Iteration 34452/35720 Training loss: 0.6637 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 34453/35720 Training loss: 0.6637 0.2401 sec/batch\n",
      "Epoch 20/20  Iteration 34454/35720 Training loss: 0.6638 0.2373 sec/batch\n",
      "Epoch 20/20  Iteration 34455/35720 Training loss: 0.6637 0.2273 sec/batch\n",
      "Epoch 20/20  Iteration 34456/35720 Training loss: 0.6637 0.2165 sec/batch\n",
      "Epoch 20/20  Iteration 34457/35720 Training loss: 0.6636 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34458/35720 Training loss: 0.6635 0.2281 sec/batch\n",
      "Epoch 20/20  Iteration 34459/35720 Training loss: 0.6635 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34460/35720 Training loss: 0.6635 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 34461/35720 Training loss: 0.6635 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34462/35720 Training loss: 0.6635 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34463/35720 Training loss: 0.6636 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 34464/35720 Training loss: 0.6635 0.2274 sec/batch\n",
      "Epoch 20/20  Iteration 34465/35720 Training loss: 0.6636 0.2258 sec/batch\n",
      "Epoch 20/20  Iteration 34466/35720 Training loss: 0.6636 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34467/35720 Training loss: 0.6636 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 34468/35720 Training loss: 0.6636 0.2103 sec/batch\n",
      "Epoch 20/20  Iteration 34469/35720 Training loss: 0.6635 0.2197 sec/batch\n",
      "Epoch 20/20  Iteration 34470/35720 Training loss: 0.6635 0.2172 sec/batch\n",
      "Epoch 20/20  Iteration 34471/35720 Training loss: 0.6634 0.2372 sec/batch\n",
      "Epoch 20/20  Iteration 34472/35720 Training loss: 0.6634 0.2058 sec/batch\n",
      "Epoch 20/20  Iteration 34473/35720 Training loss: 0.6634 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 34474/35720 Training loss: 0.6634 0.2187 sec/batch\n",
      "Epoch 20/20  Iteration 34475/35720 Training loss: 0.6633 0.2619 sec/batch\n",
      "Epoch 20/20  Iteration 34476/35720 Training loss: 0.6633 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 34477/35720 Training loss: 0.6633 0.2284 sec/batch\n",
      "Epoch 20/20  Iteration 34478/35720 Training loss: 0.6632 0.2165 sec/batch\n",
      "Epoch 20/20  Iteration 34479/35720 Training loss: 0.6633 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34480/35720 Training loss: 0.6634 0.2298 sec/batch\n",
      "Epoch 20/20  Iteration 34481/35720 Training loss: 0.6634 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 34482/35720 Training loss: 0.6634 0.2249 sec/batch\n",
      "Epoch 20/20  Iteration 34483/35720 Training loss: 0.6634 0.2057 sec/batch\n",
      "Epoch 20/20  Iteration 34484/35720 Training loss: 0.6635 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34485/35720 Training loss: 0.6635 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 34486/35720 Training loss: 0.6635 0.2394 sec/batch\n",
      "Epoch 20/20  Iteration 34487/35720 Training loss: 0.6635 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 34488/35720 Training loss: 0.6635 0.2353 sec/batch\n",
      "Epoch 20/20  Iteration 34489/35720 Training loss: 0.6635 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 34490/35720 Training loss: 0.6636 0.2244 sec/batch\n",
      "Epoch 20/20  Iteration 34491/35720 Training loss: 0.6635 0.2206 sec/batch\n",
      "Epoch 20/20  Iteration 34492/35720 Training loss: 0.6636 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 34493/35720 Training loss: 0.6636 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 34494/35720 Training loss: 0.6635 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34495/35720 Training loss: 0.6636 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 34496/35720 Training loss: 0.6635 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 34497/35720 Training loss: 0.6635 0.2292 sec/batch\n",
      "Epoch 20/20  Iteration 34498/35720 Training loss: 0.6635 0.2414 sec/batch\n",
      "Epoch 20/20  Iteration 34499/35720 Training loss: 0.6634 0.2234 sec/batch\n",
      "Epoch 20/20  Iteration 34500/35720 Training loss: 0.6635 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 34501/35720 Training loss: 0.6635 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34502/35720 Training loss: 0.6634 0.2271 sec/batch\n",
      "Epoch 20/20  Iteration 34503/35720 Training loss: 0.6633 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 34504/35720 Training loss: 0.6633 0.2358 sec/batch\n",
      "Epoch 20/20  Iteration 34505/35720 Training loss: 0.6633 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 34506/35720 Training loss: 0.6633 0.2110 sec/batch\n",
      "Epoch 20/20  Iteration 34507/35720 Training loss: 0.6634 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 34508/35720 Training loss: 0.6634 0.2275 sec/batch\n",
      "Epoch 20/20  Iteration 34509/35720 Training loss: 0.6634 0.2295 sec/batch\n",
      "Epoch 20/20  Iteration 34510/35720 Training loss: 0.6635 0.2054 sec/batch\n",
      "Epoch 20/20  Iteration 34511/35720 Training loss: 0.6635 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 34512/35720 Training loss: 0.6635 0.2303 sec/batch\n",
      "Epoch 20/20  Iteration 34513/35720 Training loss: 0.6634 0.2296 sec/batch\n",
      "Epoch 20/20  Iteration 34514/35720 Training loss: 0.6634 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34515/35720 Training loss: 0.6635 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 34516/35720 Training loss: 0.6635 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34517/35720 Training loss: 0.6635 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34518/35720 Training loss: 0.6635 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34519/35720 Training loss: 0.6634 0.2265 sec/batch\n",
      "Epoch 20/20  Iteration 34520/35720 Training loss: 0.6633 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 34521/35720 Training loss: 0.6633 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 34522/35720 Training loss: 0.6633 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 34523/35720 Training loss: 0.6631 0.2241 sec/batch\n",
      "Epoch 20/20  Iteration 34524/35720 Training loss: 0.6631 0.2411 sec/batch\n",
      "Epoch 20/20  Iteration 34525/35720 Training loss: 0.6632 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 34526/35720 Training loss: 0.6632 0.2425 sec/batch\n",
      "Epoch 20/20  Iteration 34527/35720 Training loss: 0.6632 0.2054 sec/batch\n",
      "Epoch 20/20  Iteration 34528/35720 Training loss: 0.6632 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 34529/35720 Training loss: 0.6631 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 34530/35720 Training loss: 0.6631 0.2393 sec/batch\n",
      "Epoch 20/20  Iteration 34531/35720 Training loss: 0.6631 0.2223 sec/batch\n",
      "Epoch 20/20  Iteration 34532/35720 Training loss: 0.6631 0.2188 sec/batch\n",
      "Epoch 20/20  Iteration 34533/35720 Training loss: 0.6630 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 34534/35720 Training loss: 0.6628 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 34535/35720 Training loss: 0.6628 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34536/35720 Training loss: 0.6627 0.2125 sec/batch\n",
      "Epoch 20/20  Iteration 34537/35720 Training loss: 0.6627 0.2306 sec/batch\n",
      "Epoch 20/20  Iteration 34538/35720 Training loss: 0.6626 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34539/35720 Training loss: 0.6627 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 34540/35720 Training loss: 0.6627 0.2113 sec/batch\n",
      "Epoch 20/20  Iteration 34541/35720 Training loss: 0.6626 0.2246 sec/batch\n",
      "Epoch 20/20  Iteration 34542/35720 Training loss: 0.6626 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34543/35720 Training loss: 0.6627 0.2207 sec/batch\n",
      "Epoch 20/20  Iteration 34544/35720 Training loss: 0.6626 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34545/35720 Training loss: 0.6625 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34546/35720 Training loss: 0.6624 0.2319 sec/batch\n",
      "Epoch 20/20  Iteration 34547/35720 Training loss: 0.6624 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 34548/35720 Training loss: 0.6624 0.2157 sec/batch\n",
      "Epoch 20/20  Iteration 34549/35720 Training loss: 0.6624 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 34550/35720 Training loss: 0.6623 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34551/35720 Training loss: 0.6623 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 34552/35720 Training loss: 0.6622 0.2291 sec/batch\n",
      "Epoch 20/20  Iteration 34553/35720 Training loss: 0.6621 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 34554/35720 Training loss: 0.6621 0.2272 sec/batch\n",
      "Epoch 20/20  Iteration 34555/35720 Training loss: 0.6621 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34556/35720 Training loss: 0.6621 0.2055 sec/batch\n",
      "Epoch 20/20  Iteration 34557/35720 Training loss: 0.6621 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 34558/35720 Training loss: 0.6620 0.2335 sec/batch\n",
      "Epoch 20/20  Iteration 34559/35720 Training loss: 0.6620 0.2261 sec/batch\n",
      "Epoch 20/20  Iteration 34560/35720 Training loss: 0.6620 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 34561/35720 Training loss: 0.6620 0.2080 sec/batch\n",
      "Epoch 20/20  Iteration 34562/35720 Training loss: 0.6620 0.2132 sec/batch\n",
      "Epoch 20/20  Iteration 34563/35720 Training loss: 0.6620 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 34564/35720 Training loss: 0.6620 0.2166 sec/batch\n",
      "Epoch 20/20  Iteration 34565/35720 Training loss: 0.6621 0.2217 sec/batch\n",
      "Epoch 20/20  Iteration 34566/35720 Training loss: 0.6620 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 34567/35720 Training loss: 0.6619 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 34568/35720 Training loss: 0.6619 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34569/35720 Training loss: 0.6619 0.2199 sec/batch\n",
      "Epoch 20/20  Iteration 34570/35720 Training loss: 0.6620 0.2548 sec/batch\n",
      "Epoch 20/20  Iteration 34571/35720 Training loss: 0.6620 0.2276 sec/batch\n",
      "Epoch 20/20  Iteration 34572/35720 Training loss: 0.6620 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34573/35720 Training loss: 0.6620 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34574/35720 Training loss: 0.6620 0.2167 sec/batch\n",
      "Epoch 20/20  Iteration 34575/35720 Training loss: 0.6620 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 34576/35720 Training loss: 0.6621 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 34577/35720 Training loss: 0.6622 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34578/35720 Training loss: 0.6621 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 34579/35720 Training loss: 0.6621 0.3210 sec/batch\n",
      "Epoch 20/20  Iteration 34580/35720 Training loss: 0.6621 0.3306 sec/batch\n",
      "Epoch 20/20  Iteration 34581/35720 Training loss: 0.6620 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 34582/35720 Training loss: 0.6620 0.2055 sec/batch\n",
      "Epoch 20/20  Iteration 34583/35720 Training loss: 0.6620 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 34584/35720 Training loss: 0.6619 0.2169 sec/batch\n",
      "Epoch 20/20  Iteration 34585/35720 Training loss: 0.6619 0.2228 sec/batch\n",
      "Epoch 20/20  Iteration 34586/35720 Training loss: 0.6618 0.2214 sec/batch\n",
      "Epoch 20/20  Iteration 34587/35720 Training loss: 0.6619 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 34588/35720 Training loss: 0.6619 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34589/35720 Training loss: 0.6620 0.2143 sec/batch\n",
      "Epoch 20/20  Iteration 34590/35720 Training loss: 0.6620 0.2244 sec/batch\n",
      "Epoch 20/20  Iteration 34591/35720 Training loss: 0.6621 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 34592/35720 Training loss: 0.6622 0.2150 sec/batch\n",
      "Epoch 20/20  Iteration 34593/35720 Training loss: 0.6622 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34594/35720 Training loss: 0.6622 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 34595/35720 Training loss: 0.6622 0.2332 sec/batch\n",
      "Epoch 20/20  Iteration 34596/35720 Training loss: 0.6623 0.2186 sec/batch\n",
      "Epoch 20/20  Iteration 34597/35720 Training loss: 0.6623 0.2297 sec/batch\n",
      "Epoch 20/20  Iteration 34598/35720 Training loss: 0.6623 0.2122 sec/batch\n",
      "Epoch 20/20  Iteration 34599/35720 Training loss: 0.6623 0.2167 sec/batch\n",
      "Epoch 20/20  Iteration 34600/35720 Training loss: 0.6624 0.2202 sec/batch\n",
      "Validation loss: 1.65832 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 34601/35720 Training loss: 0.6632 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 34602/35720 Training loss: 0.6632 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 34603/35720 Training loss: 0.6632 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 34604/35720 Training loss: 0.6632 0.2130 sec/batch\n",
      "Epoch 20/20  Iteration 34605/35720 Training loss: 0.6631 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 34606/35720 Training loss: 0.6631 0.2256 sec/batch\n",
      "Epoch 20/20  Iteration 34607/35720 Training loss: 0.6632 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 34608/35720 Training loss: 0.6631 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34609/35720 Training loss: 0.6631 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34610/35720 Training loss: 0.6631 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34611/35720 Training loss: 0.6631 0.2310 sec/batch\n",
      "Epoch 20/20  Iteration 34612/35720 Training loss: 0.6631 0.2114 sec/batch\n",
      "Epoch 20/20  Iteration 34613/35720 Training loss: 0.6632 0.2190 sec/batch\n",
      "Epoch 20/20  Iteration 34614/35720 Training loss: 0.6631 0.2266 sec/batch\n",
      "Epoch 20/20  Iteration 34615/35720 Training loss: 0.6631 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34616/35720 Training loss: 0.6631 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34617/35720 Training loss: 0.6631 0.2286 sec/batch\n",
      "Epoch 20/20  Iteration 34618/35720 Training loss: 0.6631 0.2134 sec/batch\n",
      "Epoch 20/20  Iteration 34619/35720 Training loss: 0.6631 0.2181 sec/batch\n",
      "Epoch 20/20  Iteration 34620/35720 Training loss: 0.6630 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34621/35720 Training loss: 0.6631 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 34622/35720 Training loss: 0.6630 0.2269 sec/batch\n",
      "Epoch 20/20  Iteration 34623/35720 Training loss: 0.6630 0.2274 sec/batch\n",
      "Epoch 20/20  Iteration 34624/35720 Training loss: 0.6630 0.2253 sec/batch\n",
      "Epoch 20/20  Iteration 34625/35720 Training loss: 0.6631 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34626/35720 Training loss: 0.6631 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34627/35720 Training loss: 0.6632 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34628/35720 Training loss: 0.6632 0.2352 sec/batch\n",
      "Epoch 20/20  Iteration 34629/35720 Training loss: 0.6633 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 34630/35720 Training loss: 0.6632 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 34631/35720 Training loss: 0.6633 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 34632/35720 Training loss: 0.6632 0.2303 sec/batch\n",
      "Epoch 20/20  Iteration 34633/35720 Training loss: 0.6632 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 34634/35720 Training loss: 0.6632 0.2205 sec/batch\n",
      "Epoch 20/20  Iteration 34635/35720 Training loss: 0.6631 0.2277 sec/batch\n",
      "Epoch 20/20  Iteration 34636/35720 Training loss: 0.6632 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34637/35720 Training loss: 0.6631 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34638/35720 Training loss: 0.6631 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 34639/35720 Training loss: 0.6631 0.2169 sec/batch\n",
      "Epoch 20/20  Iteration 34640/35720 Training loss: 0.6631 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 34641/35720 Training loss: 0.6632 0.2365 sec/batch\n",
      "Epoch 20/20  Iteration 34642/35720 Training loss: 0.6632 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34643/35720 Training loss: 0.6633 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 34644/35720 Training loss: 0.6633 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34645/35720 Training loss: 0.6634 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 34646/35720 Training loss: 0.6634 0.2271 sec/batch\n",
      "Epoch 20/20  Iteration 34647/35720 Training loss: 0.6634 0.2299 sec/batch\n",
      "Epoch 20/20  Iteration 34648/35720 Training loss: 0.6634 0.2111 sec/batch\n",
      "Epoch 20/20  Iteration 34649/35720 Training loss: 0.6634 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34650/35720 Training loss: 0.6634 0.2168 sec/batch\n",
      "Epoch 20/20  Iteration 34651/35720 Training loss: 0.6635 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34652/35720 Training loss: 0.6635 0.2220 sec/batch\n",
      "Epoch 20/20  Iteration 34653/35720 Training loss: 0.6634 0.2058 sec/batch\n",
      "Epoch 20/20  Iteration 34654/35720 Training loss: 0.6634 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 34655/35720 Training loss: 0.6635 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 34656/35720 Training loss: 0.6635 0.2314 sec/batch\n",
      "Epoch 20/20  Iteration 34657/35720 Training loss: 0.6636 0.2211 sec/batch\n",
      "Epoch 20/20  Iteration 34658/35720 Training loss: 0.6635 0.2190 sec/batch\n",
      "Epoch 20/20  Iteration 34659/35720 Training loss: 0.6635 0.2080 sec/batch\n",
      "Epoch 20/20  Iteration 34660/35720 Training loss: 0.6635 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 34661/35720 Training loss: 0.6636 0.2175 sec/batch\n",
      "Epoch 20/20  Iteration 34662/35720 Training loss: 0.6636 0.2151 sec/batch\n",
      "Epoch 20/20  Iteration 34663/35720 Training loss: 0.6637 0.2285 sec/batch\n",
      "Epoch 20/20  Iteration 34664/35720 Training loss: 0.6638 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34665/35720 Training loss: 0.6637 0.2055 sec/batch\n",
      "Epoch 20/20  Iteration 34666/35720 Training loss: 0.6637 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 34667/35720 Training loss: 0.6637 0.2425 sec/batch\n",
      "Epoch 20/20  Iteration 34668/35720 Training loss: 0.6637 0.2077 sec/batch\n",
      "Epoch 20/20  Iteration 34669/35720 Training loss: 0.6636 0.2207 sec/batch\n",
      "Epoch 20/20  Iteration 34670/35720 Training loss: 0.6637 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 34671/35720 Training loss: 0.6637 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 34672/35720 Training loss: 0.6636 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 34673/35720 Training loss: 0.6636 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 34674/35720 Training loss: 0.6637 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 34675/35720 Training loss: 0.6637 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 34676/35720 Training loss: 0.6637 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34677/35720 Training loss: 0.6637 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 34678/35720 Training loss: 0.6636 0.2532 sec/batch\n",
      "Epoch 20/20  Iteration 34679/35720 Training loss: 0.6636 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34680/35720 Training loss: 0.6636 0.2299 sec/batch\n",
      "Epoch 20/20  Iteration 34681/35720 Training loss: 0.6636 0.2179 sec/batch\n",
      "Epoch 20/20  Iteration 34682/35720 Training loss: 0.6635 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34683/35720 Training loss: 0.6635 0.2280 sec/batch\n",
      "Epoch 20/20  Iteration 34684/35720 Training loss: 0.6635 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34685/35720 Training loss: 0.6634 0.2222 sec/batch\n",
      "Epoch 20/20  Iteration 34686/35720 Training loss: 0.6635 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 34687/35720 Training loss: 0.6634 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34688/35720 Training loss: 0.6633 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 34689/35720 Training loss: 0.6633 0.2251 sec/batch\n",
      "Epoch 20/20  Iteration 34690/35720 Training loss: 0.6633 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 34691/35720 Training loss: 0.6632 0.2279 sec/batch\n",
      "Epoch 20/20  Iteration 34692/35720 Training loss: 0.6632 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34693/35720 Training loss: 0.6632 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34694/35720 Training loss: 0.6632 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 34695/35720 Training loss: 0.6632 0.2130 sec/batch\n",
      "Epoch 20/20  Iteration 34696/35720 Training loss: 0.6632 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 34697/35720 Training loss: 0.6632 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34698/35720 Training loss: 0.6632 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34699/35720 Training loss: 0.6631 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34700/35720 Training loss: 0.6631 0.2230 sec/batch\n",
      "Epoch 20/20  Iteration 34701/35720 Training loss: 0.6631 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 34702/35720 Training loss: 0.6631 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 34703/35720 Training loss: 0.6632 0.2110 sec/batch\n",
      "Epoch 20/20  Iteration 34704/35720 Training loss: 0.6632 0.2221 sec/batch\n",
      "Epoch 20/20  Iteration 34705/35720 Training loss: 0.6632 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34706/35720 Training loss: 0.6633 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 34707/35720 Training loss: 0.6632 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34708/35720 Training loss: 0.6632 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 34709/35720 Training loss: 0.6632 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 34710/35720 Training loss: 0.6631 0.2150 sec/batch\n",
      "Epoch 20/20  Iteration 34711/35720 Training loss: 0.6631 0.2337 sec/batch\n",
      "Epoch 20/20  Iteration 34712/35720 Training loss: 0.6630 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 34713/35720 Training loss: 0.6630 0.2201 sec/batch\n",
      "Epoch 20/20  Iteration 34714/35720 Training loss: 0.6630 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34715/35720 Training loss: 0.6630 0.2114 sec/batch\n",
      "Epoch 20/20  Iteration 34716/35720 Training loss: 0.6630 0.2208 sec/batch\n",
      "Epoch 20/20  Iteration 34717/35720 Training loss: 0.6630 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 34718/35720 Training loss: 0.6630 0.2169 sec/batch\n",
      "Epoch 20/20  Iteration 34719/35720 Training loss: 0.6630 0.2264 sec/batch\n",
      "Epoch 20/20  Iteration 34720/35720 Training loss: 0.6630 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34721/35720 Training loss: 0.6630 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 34722/35720 Training loss: 0.6630 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34723/35720 Training loss: 0.6631 0.2203 sec/batch\n",
      "Epoch 20/20  Iteration 34724/35720 Training loss: 0.6631 0.2418 sec/batch\n",
      "Epoch 20/20  Iteration 34725/35720 Training loss: 0.6631 0.2209 sec/batch\n",
      "Epoch 20/20  Iteration 34726/35720 Training loss: 0.6631 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34727/35720 Training loss: 0.6631 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 34728/35720 Training loss: 0.6630 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 34729/35720 Training loss: 0.6631 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34730/35720 Training loss: 0.6631 0.2228 sec/batch\n",
      "Epoch 20/20  Iteration 34731/35720 Training loss: 0.6631 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34732/35720 Training loss: 0.6630 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 34733/35720 Training loss: 0.6630 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34734/35720 Training loss: 0.6631 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 34735/35720 Training loss: 0.6631 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 34736/35720 Training loss: 0.6631 0.2364 sec/batch\n",
      "Epoch 20/20  Iteration 34737/35720 Training loss: 0.6631 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 34738/35720 Training loss: 0.6632 0.2132 sec/batch\n",
      "Epoch 20/20  Iteration 34739/35720 Training loss: 0.6633 0.2181 sec/batch\n",
      "Epoch 20/20  Iteration 34740/35720 Training loss: 0.6633 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 34741/35720 Training loss: 0.6633 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 34742/35720 Training loss: 0.6633 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 34743/35720 Training loss: 0.6633 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 34744/35720 Training loss: 0.6634 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34745/35720 Training loss: 0.6634 0.2295 sec/batch\n",
      "Epoch 20/20  Iteration 34746/35720 Training loss: 0.6634 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34747/35720 Training loss: 0.6634 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 34748/35720 Training loss: 0.6634 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34749/35720 Training loss: 0.6635 0.2049 sec/batch\n",
      "Epoch 20/20  Iteration 34750/35720 Training loss: 0.6635 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34751/35720 Training loss: 0.6635 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 34752/35720 Training loss: 0.6635 0.2237 sec/batch\n",
      "Epoch 20/20  Iteration 34753/35720 Training loss: 0.6635 0.2178 sec/batch\n",
      "Epoch 20/20  Iteration 34754/35720 Training loss: 0.6635 0.2137 sec/batch\n",
      "Epoch 20/20  Iteration 34755/35720 Training loss: 0.6635 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34756/35720 Training loss: 0.6635 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 34757/35720 Training loss: 0.6634 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 34758/35720 Training loss: 0.6634 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 34759/35720 Training loss: 0.6633 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 34760/35720 Training loss: 0.6633 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34761/35720 Training loss: 0.6632 0.2255 sec/batch\n",
      "Epoch 20/20  Iteration 34762/35720 Training loss: 0.6632 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 34763/35720 Training loss: 0.6632 0.2314 sec/batch\n",
      "Epoch 20/20  Iteration 34764/35720 Training loss: 0.6632 0.2187 sec/batch\n",
      "Epoch 20/20  Iteration 34765/35720 Training loss: 0.6631 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 34766/35720 Training loss: 0.6632 0.2161 sec/batch\n",
      "Epoch 20/20  Iteration 34767/35720 Training loss: 0.6632 0.2122 sec/batch\n",
      "Epoch 20/20  Iteration 34768/35720 Training loss: 0.6632 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 34769/35720 Training loss: 0.6632 0.2213 sec/batch\n",
      "Epoch 20/20  Iteration 34770/35720 Training loss: 0.6632 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 34771/35720 Training loss: 0.6631 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 34772/35720 Training loss: 0.6631 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 34773/35720 Training loss: 0.6632 0.2207 sec/batch\n",
      "Epoch 20/20  Iteration 34774/35720 Training loss: 0.6632 0.2163 sec/batch\n",
      "Epoch 20/20  Iteration 34775/35720 Training loss: 0.6632 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 34776/35720 Training loss: 0.6632 0.2266 sec/batch\n",
      "Epoch 20/20  Iteration 34777/35720 Training loss: 0.6631 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 34778/35720 Training loss: 0.6632 0.2225 sec/batch\n",
      "Epoch 20/20  Iteration 34779/35720 Training loss: 0.6632 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34780/35720 Training loss: 0.6632 0.2149 sec/batch\n",
      "Epoch 20/20  Iteration 34781/35720 Training loss: 0.6633 0.2052 sec/batch\n",
      "Epoch 20/20  Iteration 34782/35720 Training loss: 0.6633 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 34783/35720 Training loss: 0.6632 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34784/35720 Training loss: 0.6633 0.2227 sec/batch\n",
      "Epoch 20/20  Iteration 34785/35720 Training loss: 0.6632 0.2227 sec/batch\n",
      "Epoch 20/20  Iteration 34786/35720 Training loss: 0.6632 0.2203 sec/batch\n",
      "Epoch 20/20  Iteration 34787/35720 Training loss: 0.6633 0.2201 sec/batch\n",
      "Epoch 20/20  Iteration 34788/35720 Training loss: 0.6632 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 34789/35720 Training loss: 0.6633 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 34790/35720 Training loss: 0.6632 0.2176 sec/batch\n",
      "Epoch 20/20  Iteration 34791/35720 Training loss: 0.6632 0.2283 sec/batch\n",
      "Epoch 20/20  Iteration 34792/35720 Training loss: 0.6632 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 34793/35720 Training loss: 0.6631 0.2111 sec/batch\n",
      "Epoch 20/20  Iteration 34794/35720 Training loss: 0.6631 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 34795/35720 Training loss: 0.6631 0.2226 sec/batch\n",
      "Epoch 20/20  Iteration 34796/35720 Training loss: 0.6631 0.2314 sec/batch\n",
      "Epoch 20/20  Iteration 34797/35720 Training loss: 0.6631 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 34798/35720 Training loss: 0.6631 0.2209 sec/batch\n",
      "Epoch 20/20  Iteration 34799/35720 Training loss: 0.6630 0.2906 sec/batch\n",
      "Epoch 20/20  Iteration 34800/35720 Training loss: 0.6631 0.2385 sec/batch\n",
      "Validation loss: 1.67863 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 34801/35720 Training loss: 0.6635 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 34802/35720 Training loss: 0.6635 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 34803/35720 Training loss: 0.6635 0.2128 sec/batch\n",
      "Epoch 20/20  Iteration 34804/35720 Training loss: 0.6634 0.2157 sec/batch\n",
      "Epoch 20/20  Iteration 34805/35720 Training loss: 0.6634 0.2222 sec/batch\n",
      "Epoch 20/20  Iteration 34806/35720 Training loss: 0.6634 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 34807/35720 Training loss: 0.6634 0.2076 sec/batch\n",
      "Epoch 20/20  Iteration 34808/35720 Training loss: 0.6634 0.2052 sec/batch\n",
      "Epoch 20/20  Iteration 34809/35720 Training loss: 0.6633 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 34810/35720 Training loss: 0.6633 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34811/35720 Training loss: 0.6633 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 34812/35720 Training loss: 0.6633 0.2156 sec/batch\n",
      "Epoch 20/20  Iteration 34813/35720 Training loss: 0.6633 0.2164 sec/batch\n",
      "Epoch 20/20  Iteration 34814/35720 Training loss: 0.6632 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34815/35720 Training loss: 0.6631 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34816/35720 Training loss: 0.6631 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 34817/35720 Training loss: 0.6631 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34818/35720 Training loss: 0.6631 0.2135 sec/batch\n",
      "Epoch 20/20  Iteration 34819/35720 Training loss: 0.6631 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 34820/35720 Training loss: 0.6630 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 34821/35720 Training loss: 0.6630 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34822/35720 Training loss: 0.6630 0.2157 sec/batch\n",
      "Epoch 20/20  Iteration 34823/35720 Training loss: 0.6630 0.2218 sec/batch\n",
      "Epoch 20/20  Iteration 34824/35720 Training loss: 0.6629 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 34825/35720 Training loss: 0.6629 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34826/35720 Training loss: 0.6629 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34827/35720 Training loss: 0.6628 0.2148 sec/batch\n",
      "Epoch 20/20  Iteration 34828/35720 Training loss: 0.6627 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 34829/35720 Training loss: 0.6627 0.2132 sec/batch\n",
      "Epoch 20/20  Iteration 34830/35720 Training loss: 0.6627 0.2122 sec/batch\n",
      "Epoch 20/20  Iteration 34831/35720 Training loss: 0.6626 0.2163 sec/batch\n",
      "Epoch 20/20  Iteration 34832/35720 Training loss: 0.6626 0.2104 sec/batch\n",
      "Epoch 20/20  Iteration 34833/35720 Training loss: 0.6625 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 34834/35720 Training loss: 0.6625 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 34835/35720 Training loss: 0.6625 0.2248 sec/batch\n",
      "Epoch 20/20  Iteration 34836/35720 Training loss: 0.6624 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34837/35720 Training loss: 0.6624 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34838/35720 Training loss: 0.6624 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 34839/35720 Training loss: 0.6624 0.2122 sec/batch\n",
      "Epoch 20/20  Iteration 34840/35720 Training loss: 0.6624 0.2103 sec/batch\n",
      "Epoch 20/20  Iteration 34841/35720 Training loss: 0.6623 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 34842/35720 Training loss: 0.6623 0.2154 sec/batch\n",
      "Epoch 20/20  Iteration 34843/35720 Training loss: 0.6623 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 34844/35720 Training loss: 0.6623 0.2312 sec/batch\n",
      "Epoch 20/20  Iteration 34845/35720 Training loss: 0.6623 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 34846/35720 Training loss: 0.6623 0.2189 sec/batch\n",
      "Epoch 20/20  Iteration 34847/35720 Training loss: 0.6623 0.2259 sec/batch\n",
      "Epoch 20/20  Iteration 34848/35720 Training loss: 0.6623 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34849/35720 Training loss: 0.6623 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34850/35720 Training loss: 0.6624 0.2148 sec/batch\n",
      "Epoch 20/20  Iteration 34851/35720 Training loss: 0.6624 0.2134 sec/batch\n",
      "Epoch 20/20  Iteration 34852/35720 Training loss: 0.6623 0.2259 sec/batch\n",
      "Epoch 20/20  Iteration 34853/35720 Training loss: 0.6623 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 34854/35720 Training loss: 0.6623 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 34855/35720 Training loss: 0.6623 0.2196 sec/batch\n",
      "Epoch 20/20  Iteration 34856/35720 Training loss: 0.6622 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 34857/35720 Training loss: 0.6622 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34858/35720 Training loss: 0.6622 0.2201 sec/batch\n",
      "Epoch 20/20  Iteration 34859/35720 Training loss: 0.6622 0.2129 sec/batch\n",
      "Epoch 20/20  Iteration 34860/35720 Training loss: 0.6623 0.2154 sec/batch\n",
      "Epoch 20/20  Iteration 34861/35720 Training loss: 0.6623 0.2289 sec/batch\n",
      "Epoch 20/20  Iteration 34862/35720 Training loss: 0.6621 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 34863/35720 Training loss: 0.6621 0.2248 sec/batch\n",
      "Epoch 20/20  Iteration 34864/35720 Training loss: 0.6621 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 34865/35720 Training loss: 0.6622 0.2072 sec/batch\n",
      "Epoch 20/20  Iteration 34866/35720 Training loss: 0.6622 0.2126 sec/batch\n",
      "Epoch 20/20  Iteration 34867/35720 Training loss: 0.6621 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 34868/35720 Training loss: 0.6622 0.2156 sec/batch\n",
      "Epoch 20/20  Iteration 34869/35720 Training loss: 0.6621 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 34870/35720 Training loss: 0.6620 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34871/35720 Training loss: 0.6620 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 34872/35720 Training loss: 0.6619 0.2264 sec/batch\n",
      "Epoch 20/20  Iteration 34873/35720 Training loss: 0.6619 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 34874/35720 Training loss: 0.6618 0.2292 sec/batch\n",
      "Epoch 20/20  Iteration 34875/35720 Training loss: 0.6618 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 34876/35720 Training loss: 0.6617 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 34877/35720 Training loss: 0.6617 0.2281 sec/batch\n",
      "Epoch 20/20  Iteration 34878/35720 Training loss: 0.6618 0.2130 sec/batch\n",
      "Epoch 20/20  Iteration 34879/35720 Training loss: 0.6617 0.2173 sec/batch\n",
      "Epoch 20/20  Iteration 34880/35720 Training loss: 0.6618 0.2173 sec/batch\n",
      "Epoch 20/20  Iteration 34881/35720 Training loss: 0.6618 0.2107 sec/batch\n",
      "Epoch 20/20  Iteration 34882/35720 Training loss: 0.6617 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 34883/35720 Training loss: 0.6617 0.2222 sec/batch\n",
      "Epoch 20/20  Iteration 34884/35720 Training loss: 0.6617 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 34885/35720 Training loss: 0.6617 0.2258 sec/batch\n",
      "Epoch 20/20  Iteration 34886/35720 Training loss: 0.6616 0.2055 sec/batch\n",
      "Epoch 20/20  Iteration 34887/35720 Training loss: 0.6616 0.2127 sec/batch\n",
      "Epoch 20/20  Iteration 34888/35720 Training loss: 0.6616 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34889/35720 Training loss: 0.6616 0.2187 sec/batch\n",
      "Epoch 20/20  Iteration 34890/35720 Training loss: 0.6616 0.2120 sec/batch\n",
      "Epoch 20/20  Iteration 34891/35720 Training loss: 0.6616 0.2155 sec/batch\n",
      "Epoch 20/20  Iteration 34892/35720 Training loss: 0.6616 0.2205 sec/batch\n",
      "Epoch 20/20  Iteration 34893/35720 Training loss: 0.6615 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 34894/35720 Training loss: 0.6615 0.2072 sec/batch\n",
      "Epoch 20/20  Iteration 34895/35720 Training loss: 0.6614 0.2162 sec/batch\n",
      "Epoch 20/20  Iteration 34896/35720 Training loss: 0.6614 0.2139 sec/batch\n",
      "Epoch 20/20  Iteration 34897/35720 Training loss: 0.6613 0.2079 sec/batch\n",
      "Epoch 20/20  Iteration 34898/35720 Training loss: 0.6613 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 34899/35720 Training loss: 0.6613 0.2161 sec/batch\n",
      "Epoch 20/20  Iteration 34900/35720 Training loss: 0.6613 0.2224 sec/batch\n",
      "Epoch 20/20  Iteration 34901/35720 Training loss: 0.6612 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 34902/35720 Training loss: 0.6612 0.2161 sec/batch\n",
      "Epoch 20/20  Iteration 34903/35720 Training loss: 0.6612 0.2142 sec/batch\n",
      "Epoch 20/20  Iteration 34904/35720 Training loss: 0.6613 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34905/35720 Training loss: 0.6613 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 34906/35720 Training loss: 0.6612 0.2246 sec/batch\n",
      "Epoch 20/20  Iteration 34907/35720 Training loss: 0.6612 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 34908/35720 Training loss: 0.6612 0.2156 sec/batch\n",
      "Epoch 20/20  Iteration 34909/35720 Training loss: 0.6611 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34910/35720 Training loss: 0.6611 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 34911/35720 Training loss: 0.6612 0.2187 sec/batch\n",
      "Epoch 20/20  Iteration 34912/35720 Training loss: 0.6612 0.2120 sec/batch\n",
      "Epoch 20/20  Iteration 34913/35720 Training loss: 0.6611 0.2211 sec/batch\n",
      "Epoch 20/20  Iteration 34914/35720 Training loss: 0.6611 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34915/35720 Training loss: 0.6610 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 34916/35720 Training loss: 0.6610 0.2120 sec/batch\n",
      "Epoch 20/20  Iteration 34917/35720 Training loss: 0.6610 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 34918/35720 Training loss: 0.6609 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 34919/35720 Training loss: 0.6610 0.2255 sec/batch\n",
      "Epoch 20/20  Iteration 34920/35720 Training loss: 0.6609 0.2078 sec/batch\n",
      "Epoch 20/20  Iteration 34921/35720 Training loss: 0.6609 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 34922/35720 Training loss: 0.6608 0.2079 sec/batch\n",
      "Epoch 20/20  Iteration 34923/35720 Training loss: 0.6608 0.2266 sec/batch\n",
      "Epoch 20/20  Iteration 34924/35720 Training loss: 0.6608 0.2200 sec/batch\n",
      "Epoch 20/20  Iteration 34925/35720 Training loss: 0.6608 0.2079 sec/batch\n",
      "Epoch 20/20  Iteration 34926/35720 Training loss: 0.6608 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 34927/35720 Training loss: 0.6608 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34928/35720 Training loss: 0.6608 0.2327 sec/batch\n",
      "Epoch 20/20  Iteration 34929/35720 Training loss: 0.6608 0.2246 sec/batch\n",
      "Epoch 20/20  Iteration 34930/35720 Training loss: 0.6608 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 34931/35720 Training loss: 0.6608 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 34932/35720 Training loss: 0.6609 0.2167 sec/batch\n",
      "Epoch 20/20  Iteration 34933/35720 Training loss: 0.6609 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 34934/35720 Training loss: 0.6608 0.2243 sec/batch\n",
      "Epoch 20/20  Iteration 34935/35720 Training loss: 0.6608 0.2209 sec/batch\n",
      "Epoch 20/20  Iteration 34936/35720 Training loss: 0.6607 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34937/35720 Training loss: 0.6606 0.2185 sec/batch\n",
      "Epoch 20/20  Iteration 34938/35720 Training loss: 0.6606 0.2162 sec/batch\n",
      "Epoch 20/20  Iteration 34939/35720 Training loss: 0.6605 0.2172 sec/batch\n",
      "Epoch 20/20  Iteration 34940/35720 Training loss: 0.6605 0.2220 sec/batch\n",
      "Epoch 20/20  Iteration 34941/35720 Training loss: 0.6605 0.2170 sec/batch\n",
      "Epoch 20/20  Iteration 34942/35720 Training loss: 0.6605 0.2058 sec/batch\n",
      "Epoch 20/20  Iteration 34943/35720 Training loss: 0.6604 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 34944/35720 Training loss: 0.6603 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 34945/35720 Training loss: 0.6603 0.2264 sec/batch\n",
      "Epoch 20/20  Iteration 34946/35720 Training loss: 0.6603 0.2201 sec/batch\n",
      "Epoch 20/20  Iteration 34947/35720 Training loss: 0.6603 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34948/35720 Training loss: 0.6603 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34949/35720 Training loss: 0.6603 0.2079 sec/batch\n",
      "Epoch 20/20  Iteration 34950/35720 Training loss: 0.6603 0.2265 sec/batch\n",
      "Epoch 20/20  Iteration 34951/35720 Training loss: 0.6603 0.2113 sec/batch\n",
      "Epoch 20/20  Iteration 34952/35720 Training loss: 0.6603 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 34953/35720 Training loss: 0.6602 0.2114 sec/batch\n",
      "Epoch 20/20  Iteration 34954/35720 Training loss: 0.6602 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 34955/35720 Training loss: 0.6602 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 34956/35720 Training loss: 0.6602 0.2242 sec/batch\n",
      "Epoch 20/20  Iteration 34957/35720 Training loss: 0.6603 0.2175 sec/batch\n",
      "Epoch 20/20  Iteration 34958/35720 Training loss: 0.6603 0.2256 sec/batch\n",
      "Epoch 20/20  Iteration 34959/35720 Training loss: 0.6603 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34960/35720 Training loss: 0.6603 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34961/35720 Training loss: 0.6603 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 34962/35720 Training loss: 0.6603 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 34963/35720 Training loss: 0.6602 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 34964/35720 Training loss: 0.6602 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 34965/35720 Training loss: 0.6602 0.2176 sec/batch\n",
      "Epoch 20/20  Iteration 34966/35720 Training loss: 0.6602 0.2364 sec/batch\n",
      "Epoch 20/20  Iteration 34967/35720 Training loss: 0.6602 0.2230 sec/batch\n",
      "Epoch 20/20  Iteration 34968/35720 Training loss: 0.6602 0.2137 sec/batch\n",
      "Epoch 20/20  Iteration 34969/35720 Training loss: 0.6602 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 34970/35720 Training loss: 0.6602 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 34971/35720 Training loss: 0.6603 0.2075 sec/batch\n",
      "Epoch 20/20  Iteration 34972/35720 Training loss: 0.6603 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 34973/35720 Training loss: 0.6603 0.2154 sec/batch\n",
      "Epoch 20/20  Iteration 34974/35720 Training loss: 0.6603 0.2272 sec/batch\n",
      "Epoch 20/20  Iteration 34975/35720 Training loss: 0.6603 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 34976/35720 Training loss: 0.6603 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 34977/35720 Training loss: 0.6602 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 34978/35720 Training loss: 0.6603 0.2297 sec/batch\n",
      "Epoch 20/20  Iteration 34979/35720 Training loss: 0.6603 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 34980/35720 Training loss: 0.6603 0.2157 sec/batch\n",
      "Epoch 20/20  Iteration 34981/35720 Training loss: 0.6603 0.2054 sec/batch\n",
      "Epoch 20/20  Iteration 34982/35720 Training loss: 0.6603 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 34983/35720 Training loss: 0.6602 0.2251 sec/batch\n",
      "Epoch 20/20  Iteration 34984/35720 Training loss: 0.6603 0.2258 sec/batch\n",
      "Epoch 20/20  Iteration 34985/35720 Training loss: 0.6603 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 34986/35720 Training loss: 0.6603 0.2187 sec/batch\n",
      "Epoch 20/20  Iteration 34987/35720 Training loss: 0.6604 0.2079 sec/batch\n",
      "Epoch 20/20  Iteration 34988/35720 Training loss: 0.6604 0.2110 sec/batch\n",
      "Epoch 20/20  Iteration 34989/35720 Training loss: 0.6605 0.2305 sec/batch\n",
      "Epoch 20/20  Iteration 34990/35720 Training loss: 0.6605 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 34991/35720 Training loss: 0.6605 0.2162 sec/batch\n",
      "Epoch 20/20  Iteration 34992/35720 Training loss: 0.6605 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 34993/35720 Training loss: 0.6606 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 34994/35720 Training loss: 0.6605 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 34995/35720 Training loss: 0.6605 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 34996/35720 Training loss: 0.6605 0.2120 sec/batch\n",
      "Epoch 20/20  Iteration 34997/35720 Training loss: 0.6606 0.2134 sec/batch\n",
      "Epoch 20/20  Iteration 34998/35720 Training loss: 0.6606 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 34999/35720 Training loss: 0.6606 0.2170 sec/batch\n",
      "Epoch 20/20  Iteration 35000/35720 Training loss: 0.6606 0.2219 sec/batch\n",
      "Validation loss: 1.67923 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 35001/35720 Training loss: 0.6609 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 35002/35720 Training loss: 0.6610 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 35003/35720 Training loss: 0.6610 0.2072 sec/batch\n",
      "Epoch 20/20  Iteration 35004/35720 Training loss: 0.6610 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35005/35720 Training loss: 0.6610 0.2143 sec/batch\n",
      "Epoch 20/20  Iteration 35006/35720 Training loss: 0.6610 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 35007/35720 Training loss: 0.6610 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 35008/35720 Training loss: 0.6610 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35009/35720 Training loss: 0.6610 0.2181 sec/batch\n",
      "Epoch 20/20  Iteration 35010/35720 Training loss: 0.6610 0.2207 sec/batch\n",
      "Epoch 20/20  Iteration 35011/35720 Training loss: 0.6610 0.2164 sec/batch\n",
      "Epoch 20/20  Iteration 35012/35720 Training loss: 0.6610 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 35013/35720 Training loss: 0.6611 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 35014/35720 Training loss: 0.6611 0.2197 sec/batch\n",
      "Epoch 20/20  Iteration 35015/35720 Training loss: 0.6610 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 35016/35720 Training loss: 0.6610 0.2237 sec/batch\n",
      "Epoch 20/20  Iteration 35017/35720 Training loss: 0.6611 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 35018/35720 Training loss: 0.6611 0.2157 sec/batch\n",
      "Epoch 20/20  Iteration 35019/35720 Training loss: 0.6611 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 35020/35720 Training loss: 0.6611 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 35021/35720 Training loss: 0.6611 0.2135 sec/batch\n",
      "Epoch 20/20  Iteration 35022/35720 Training loss: 0.6611 0.2288 sec/batch\n",
      "Epoch 20/20  Iteration 35023/35720 Training loss: 0.6611 0.2243 sec/batch\n",
      "Epoch 20/20  Iteration 35024/35720 Training loss: 0.6612 0.2151 sec/batch\n",
      "Epoch 20/20  Iteration 35025/35720 Training loss: 0.6612 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35026/35720 Training loss: 0.6612 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35027/35720 Training loss: 0.6612 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 35028/35720 Training loss: 0.6612 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35029/35720 Training loss: 0.6612 0.2143 sec/batch\n",
      "Epoch 20/20  Iteration 35030/35720 Training loss: 0.6612 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 35031/35720 Training loss: 0.6611 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 35032/35720 Training loss: 0.6611 0.2251 sec/batch\n",
      "Epoch 20/20  Iteration 35033/35720 Training loss: 0.6612 0.2222 sec/batch\n",
      "Epoch 20/20  Iteration 35034/35720 Training loss: 0.6611 0.2188 sec/batch\n",
      "Epoch 20/20  Iteration 35035/35720 Training loss: 0.6612 0.2182 sec/batch\n",
      "Epoch 20/20  Iteration 35036/35720 Training loss: 0.6612 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 35037/35720 Training loss: 0.6612 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 35038/35720 Training loss: 0.6612 0.2186 sec/batch\n",
      "Epoch 20/20  Iteration 35039/35720 Training loss: 0.6612 0.2126 sec/batch\n",
      "Epoch 20/20  Iteration 35040/35720 Training loss: 0.6612 0.2176 sec/batch\n",
      "Epoch 20/20  Iteration 35041/35720 Training loss: 0.6612 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 35042/35720 Training loss: 0.6612 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 35043/35720 Training loss: 0.6612 0.2214 sec/batch\n",
      "Epoch 20/20  Iteration 35044/35720 Training loss: 0.6612 0.2149 sec/batch\n",
      "Epoch 20/20  Iteration 35045/35720 Training loss: 0.6612 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 35046/35720 Training loss: 0.6612 0.2259 sec/batch\n",
      "Epoch 20/20  Iteration 35047/35720 Training loss: 0.6611 0.2355 sec/batch\n",
      "Epoch 20/20  Iteration 35048/35720 Training loss: 0.6612 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 35049/35720 Training loss: 0.6612 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 35050/35720 Training loss: 0.6612 0.2193 sec/batch\n",
      "Epoch 20/20  Iteration 35051/35720 Training loss: 0.6612 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 35052/35720 Training loss: 0.6611 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35053/35720 Training loss: 0.6611 0.2103 sec/batch\n",
      "Epoch 20/20  Iteration 35054/35720 Training loss: 0.6612 0.2249 sec/batch\n",
      "Epoch 20/20  Iteration 35055/35720 Training loss: 0.6611 0.2142 sec/batch\n",
      "Epoch 20/20  Iteration 35056/35720 Training loss: 0.6612 0.2403 sec/batch\n",
      "Epoch 20/20  Iteration 35057/35720 Training loss: 0.6612 0.2191 sec/batch\n",
      "Epoch 20/20  Iteration 35058/35720 Training loss: 0.6612 0.2055 sec/batch\n",
      "Epoch 20/20  Iteration 35059/35720 Training loss: 0.6612 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 35060/35720 Training loss: 0.6612 0.2110 sec/batch\n",
      "Epoch 20/20  Iteration 35061/35720 Training loss: 0.6612 0.2183 sec/batch\n",
      "Epoch 20/20  Iteration 35062/35720 Training loss: 0.6612 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 35063/35720 Training loss: 0.6612 0.2299 sec/batch\n",
      "Epoch 20/20  Iteration 35064/35720 Training loss: 0.6612 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 35065/35720 Training loss: 0.6611 0.2142 sec/batch\n",
      "Epoch 20/20  Iteration 35066/35720 Training loss: 0.6611 0.2304 sec/batch\n",
      "Epoch 20/20  Iteration 35067/35720 Training loss: 0.6610 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 35068/35720 Training loss: 0.6610 0.2230 sec/batch\n",
      "Epoch 20/20  Iteration 35069/35720 Training loss: 0.6610 0.2313 sec/batch\n",
      "Epoch 20/20  Iteration 35070/35720 Training loss: 0.6609 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 35071/35720 Training loss: 0.6609 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 35072/35720 Training loss: 0.6609 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 35073/35720 Training loss: 0.6608 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 35074/35720 Training loss: 0.6608 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35075/35720 Training loss: 0.6608 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 35076/35720 Training loss: 0.6608 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 35077/35720 Training loss: 0.6608 0.2057 sec/batch\n",
      "Epoch 20/20  Iteration 35078/35720 Training loss: 0.6608 0.2128 sec/batch\n",
      "Epoch 20/20  Iteration 35079/35720 Training loss: 0.6608 0.2191 sec/batch\n",
      "Epoch 20/20  Iteration 35080/35720 Training loss: 0.6607 0.2056 sec/batch\n",
      "Epoch 20/20  Iteration 35081/35720 Training loss: 0.6607 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 35082/35720 Training loss: 0.6607 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 35083/35720 Training loss: 0.6607 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 35084/35720 Training loss: 0.6607 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 35085/35720 Training loss: 0.6607 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 35086/35720 Training loss: 0.6607 0.2075 sec/batch\n",
      "Epoch 20/20  Iteration 35087/35720 Training loss: 0.6607 0.2076 sec/batch\n",
      "Epoch 20/20  Iteration 35088/35720 Training loss: 0.6607 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 35089/35720 Training loss: 0.6607 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 35090/35720 Training loss: 0.6607 0.2211 sec/batch\n",
      "Epoch 20/20  Iteration 35091/35720 Training loss: 0.6607 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 35092/35720 Training loss: 0.6607 0.2166 sec/batch\n",
      "Epoch 20/20  Iteration 35093/35720 Training loss: 0.6607 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 35094/35720 Training loss: 0.6607 0.2298 sec/batch\n",
      "Epoch 20/20  Iteration 35095/35720 Training loss: 0.6607 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 35096/35720 Training loss: 0.6607 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 35097/35720 Training loss: 0.6607 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35098/35720 Training loss: 0.6608 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35099/35720 Training loss: 0.6607 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 35100/35720 Training loss: 0.6607 0.2274 sec/batch\n",
      "Epoch 20/20  Iteration 35101/35720 Training loss: 0.6607 0.2127 sec/batch\n",
      "Epoch 20/20  Iteration 35102/35720 Training loss: 0.6607 0.2254 sec/batch\n",
      "Epoch 20/20  Iteration 35103/35720 Training loss: 0.6607 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 35104/35720 Training loss: 0.6607 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 35105/35720 Training loss: 0.6607 0.2397 sec/batch\n",
      "Epoch 20/20  Iteration 35106/35720 Training loss: 0.6608 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 35107/35720 Training loss: 0.6608 0.2252 sec/batch\n",
      "Epoch 20/20  Iteration 35108/35720 Training loss: 0.6608 0.2156 sec/batch\n",
      "Epoch 20/20  Iteration 35109/35720 Training loss: 0.6608 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 35110/35720 Training loss: 0.6608 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 35111/35720 Training loss: 0.6608 0.2140 sec/batch\n",
      "Epoch 20/20  Iteration 35112/35720 Training loss: 0.6609 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 35113/35720 Training loss: 0.6609 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 35114/35720 Training loss: 0.6609 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 35115/35720 Training loss: 0.6609 0.2052 sec/batch\n",
      "Epoch 20/20  Iteration 35116/35720 Training loss: 0.6609 0.2164 sec/batch\n",
      "Epoch 20/20  Iteration 35117/35720 Training loss: 0.6609 0.2220 sec/batch\n",
      "Epoch 20/20  Iteration 35118/35720 Training loss: 0.6608 0.2281 sec/batch\n",
      "Epoch 20/20  Iteration 35119/35720 Training loss: 0.6608 0.2184 sec/batch\n",
      "Epoch 20/20  Iteration 35120/35720 Training loss: 0.6608 0.2226 sec/batch\n",
      "Epoch 20/20  Iteration 35121/35720 Training loss: 0.6608 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 35122/35720 Training loss: 0.6608 0.2150 sec/batch\n",
      "Epoch 20/20  Iteration 35123/35720 Training loss: 0.6608 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 35124/35720 Training loss: 0.6608 0.2302 sec/batch\n",
      "Epoch 20/20  Iteration 35125/35720 Training loss: 0.6608 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 35126/35720 Training loss: 0.6608 0.2131 sec/batch\n",
      "Epoch 20/20  Iteration 35127/35720 Training loss: 0.6608 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 35128/35720 Training loss: 0.6608 0.2323 sec/batch\n",
      "Epoch 20/20  Iteration 35129/35720 Training loss: 0.6608 0.2181 sec/batch\n",
      "Epoch 20/20  Iteration 35130/35720 Training loss: 0.6608 0.2225 sec/batch\n",
      "Epoch 20/20  Iteration 35131/35720 Training loss: 0.6608 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 35132/35720 Training loss: 0.6608 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 35133/35720 Training loss: 0.6608 0.2056 sec/batch\n",
      "Epoch 20/20  Iteration 35134/35720 Training loss: 0.6608 0.2130 sec/batch\n",
      "Epoch 20/20  Iteration 35135/35720 Training loss: 0.6608 0.2283 sec/batch\n",
      "Epoch 20/20  Iteration 35136/35720 Training loss: 0.6607 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 35137/35720 Training loss: 0.6607 0.2183 sec/batch\n",
      "Epoch 20/20  Iteration 35138/35720 Training loss: 0.6606 0.2164 sec/batch\n",
      "Epoch 20/20  Iteration 35139/35720 Training loss: 0.6606 0.2316 sec/batch\n",
      "Epoch 20/20  Iteration 35140/35720 Training loss: 0.6606 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 35141/35720 Training loss: 0.6606 0.2293 sec/batch\n",
      "Epoch 20/20  Iteration 35142/35720 Training loss: 0.6606 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35143/35720 Training loss: 0.6606 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 35144/35720 Training loss: 0.6606 0.2297 sec/batch\n",
      "Epoch 20/20  Iteration 35145/35720 Training loss: 0.6606 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 35146/35720 Training loss: 0.6606 0.2372 sec/batch\n",
      "Epoch 20/20  Iteration 35147/35720 Training loss: 0.6605 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 35148/35720 Training loss: 0.6606 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35149/35720 Training loss: 0.6605 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 35150/35720 Training loss: 0.6605 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 35151/35720 Training loss: 0.6605 0.2253 sec/batch\n",
      "Epoch 20/20  Iteration 35152/35720 Training loss: 0.6605 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 35153/35720 Training loss: 0.6604 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 35154/35720 Training loss: 0.6604 0.2148 sec/batch\n",
      "Epoch 20/20  Iteration 35155/35720 Training loss: 0.6604 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 35156/35720 Training loss: 0.6604 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35157/35720 Training loss: 0.6604 0.2329 sec/batch\n",
      "Epoch 20/20  Iteration 35158/35720 Training loss: 0.6604 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35159/35720 Training loss: 0.6603 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 35160/35720 Training loss: 0.6603 0.2155 sec/batch\n",
      "Epoch 20/20  Iteration 35161/35720 Training loss: 0.6603 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 35162/35720 Training loss: 0.6603 0.2189 sec/batch\n",
      "Epoch 20/20  Iteration 35163/35720 Training loss: 0.6603 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 35164/35720 Training loss: 0.6603 0.2168 sec/batch\n",
      "Epoch 20/20  Iteration 35165/35720 Training loss: 0.6603 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 35166/35720 Training loss: 0.6603 0.2431 sec/batch\n",
      "Epoch 20/20  Iteration 35167/35720 Training loss: 0.6603 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 35168/35720 Training loss: 0.6603 0.2243 sec/batch\n",
      "Epoch 20/20  Iteration 35169/35720 Training loss: 0.6603 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 35170/35720 Training loss: 0.6603 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 35171/35720 Training loss: 0.6602 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 35172/35720 Training loss: 0.6602 0.2214 sec/batch\n",
      "Epoch 20/20  Iteration 35173/35720 Training loss: 0.6601 0.2143 sec/batch\n",
      "Epoch 20/20  Iteration 35174/35720 Training loss: 0.6601 0.2237 sec/batch\n",
      "Epoch 20/20  Iteration 35175/35720 Training loss: 0.6600 0.2183 sec/batch\n",
      "Epoch 20/20  Iteration 35176/35720 Training loss: 0.6600 0.2080 sec/batch\n",
      "Epoch 20/20  Iteration 35177/35720 Training loss: 0.6601 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 35178/35720 Training loss: 0.6600 0.2193 sec/batch\n",
      "Epoch 20/20  Iteration 35179/35720 Training loss: 0.6600 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 35180/35720 Training loss: 0.6599 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 35181/35720 Training loss: 0.6599 0.2111 sec/batch\n",
      "Epoch 20/20  Iteration 35182/35720 Training loss: 0.6599 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 35183/35720 Training loss: 0.6598 0.2129 sec/batch\n",
      "Epoch 20/20  Iteration 35184/35720 Training loss: 0.6598 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 35185/35720 Training loss: 0.6598 0.2189 sec/batch\n",
      "Epoch 20/20  Iteration 35186/35720 Training loss: 0.6598 0.2183 sec/batch\n",
      "Epoch 20/20  Iteration 35187/35720 Training loss: 0.6598 0.2197 sec/batch\n",
      "Epoch 20/20  Iteration 35188/35720 Training loss: 0.6598 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35189/35720 Training loss: 0.6598 0.2149 sec/batch\n",
      "Epoch 20/20  Iteration 35190/35720 Training loss: 0.6598 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35191/35720 Training loss: 0.6598 0.2111 sec/batch\n",
      "Epoch 20/20  Iteration 35192/35720 Training loss: 0.6597 0.2074 sec/batch\n",
      "Epoch 20/20  Iteration 35193/35720 Training loss: 0.6597 0.2137 sec/batch\n",
      "Epoch 20/20  Iteration 35194/35720 Training loss: 0.6597 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 35195/35720 Training loss: 0.6597 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 35196/35720 Training loss: 0.6597 0.2319 sec/batch\n",
      "Epoch 20/20  Iteration 35197/35720 Training loss: 0.6597 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 35198/35720 Training loss: 0.6597 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 35199/35720 Training loss: 0.6596 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 35200/35720 Training loss: 0.6596 0.2174 sec/batch\n",
      "Validation loss: 1.67582 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 35201/35720 Training loss: 0.6600 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 35202/35720 Training loss: 0.6600 0.2054 sec/batch\n",
      "Epoch 20/20  Iteration 35203/35720 Training loss: 0.6599 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 35204/35720 Training loss: 0.6600 0.2209 sec/batch\n",
      "Epoch 20/20  Iteration 35205/35720 Training loss: 0.6599 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 35206/35720 Training loss: 0.6599 0.2175 sec/batch\n",
      "Epoch 20/20  Iteration 35207/35720 Training loss: 0.6599 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 35208/35720 Training loss: 0.6599 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 35209/35720 Training loss: 0.6599 0.2125 sec/batch\n",
      "Epoch 20/20  Iteration 35210/35720 Training loss: 0.6599 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 35211/35720 Training loss: 0.6598 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 35212/35720 Training loss: 0.6598 0.2107 sec/batch\n",
      "Epoch 20/20  Iteration 35213/35720 Training loss: 0.6598 0.2135 sec/batch\n",
      "Epoch 20/20  Iteration 35214/35720 Training loss: 0.6597 0.2134 sec/batch\n",
      "Epoch 20/20  Iteration 35215/35720 Training loss: 0.6598 0.2234 sec/batch\n",
      "Epoch 20/20  Iteration 35216/35720 Training loss: 0.6598 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 35217/35720 Training loss: 0.6597 0.2267 sec/batch\n",
      "Epoch 20/20  Iteration 35218/35720 Training loss: 0.6597 0.2061 sec/batch\n",
      "Epoch 20/20  Iteration 35219/35720 Training loss: 0.6597 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 35220/35720 Training loss: 0.6597 0.2130 sec/batch\n",
      "Epoch 20/20  Iteration 35221/35720 Training loss: 0.6597 0.2308 sec/batch\n",
      "Epoch 20/20  Iteration 35222/35720 Training loss: 0.6597 0.2179 sec/batch\n",
      "Epoch 20/20  Iteration 35223/35720 Training loss: 0.6597 0.2143 sec/batch\n",
      "Epoch 20/20  Iteration 35224/35720 Training loss: 0.6597 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 35225/35720 Training loss: 0.6597 0.2051 sec/batch\n",
      "Epoch 20/20  Iteration 35226/35720 Training loss: 0.6596 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 35227/35720 Training loss: 0.6597 0.2224 sec/batch\n",
      "Epoch 20/20  Iteration 35228/35720 Training loss: 0.6597 0.2277 sec/batch\n",
      "Epoch 20/20  Iteration 35229/35720 Training loss: 0.6597 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35230/35720 Training loss: 0.6596 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 35231/35720 Training loss: 0.6597 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35232/35720 Training loss: 0.6596 0.2358 sec/batch\n",
      "Epoch 20/20  Iteration 35233/35720 Training loss: 0.6596 0.2211 sec/batch\n",
      "Epoch 20/20  Iteration 35234/35720 Training loss: 0.6596 0.2271 sec/batch\n",
      "Epoch 20/20  Iteration 35235/35720 Training loss: 0.6596 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 35236/35720 Training loss: 0.6596 0.2082 sec/batch\n",
      "Epoch 20/20  Iteration 35237/35720 Training loss: 0.6595 0.2166 sec/batch\n",
      "Epoch 20/20  Iteration 35238/35720 Training loss: 0.6595 0.2276 sec/batch\n",
      "Epoch 20/20  Iteration 35239/35720 Training loss: 0.6595 0.2116 sec/batch\n",
      "Epoch 20/20  Iteration 35240/35720 Training loss: 0.6595 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 35241/35720 Training loss: 0.6595 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35242/35720 Training loss: 0.6595 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 35243/35720 Training loss: 0.6594 0.2176 sec/batch\n",
      "Epoch 20/20  Iteration 35244/35720 Training loss: 0.6594 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 35245/35720 Training loss: 0.6594 0.2243 sec/batch\n",
      "Epoch 20/20  Iteration 35246/35720 Training loss: 0.6594 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 35247/35720 Training loss: 0.6594 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 35248/35720 Training loss: 0.6593 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 35249/35720 Training loss: 0.6593 0.2289 sec/batch\n",
      "Epoch 20/20  Iteration 35250/35720 Training loss: 0.6593 0.2179 sec/batch\n",
      "Epoch 20/20  Iteration 35251/35720 Training loss: 0.6593 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 35252/35720 Training loss: 0.6592 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35253/35720 Training loss: 0.6592 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 35254/35720 Training loss: 0.6592 0.2249 sec/batch\n",
      "Epoch 20/20  Iteration 35255/35720 Training loss: 0.6592 0.2164 sec/batch\n",
      "Epoch 20/20  Iteration 35256/35720 Training loss: 0.6592 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 35257/35720 Training loss: 0.6592 0.2155 sec/batch\n",
      "Epoch 20/20  Iteration 35258/35720 Training loss: 0.6592 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 35259/35720 Training loss: 0.6592 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 35260/35720 Training loss: 0.6592 0.2226 sec/batch\n",
      "Epoch 20/20  Iteration 35261/35720 Training loss: 0.6592 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 35262/35720 Training loss: 0.6592 0.2452 sec/batch\n",
      "Epoch 20/20  Iteration 35263/35720 Training loss: 0.6592 0.2120 sec/batch\n",
      "Epoch 20/20  Iteration 35264/35720 Training loss: 0.6592 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 35265/35720 Training loss: 0.6592 0.2336 sec/batch\n",
      "Epoch 20/20  Iteration 35266/35720 Training loss: 0.6592 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 35267/35720 Training loss: 0.6592 0.2238 sec/batch\n",
      "Epoch 20/20  Iteration 35268/35720 Training loss: 0.6592 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 35269/35720 Training loss: 0.6592 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 35270/35720 Training loss: 0.6592 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 35271/35720 Training loss: 0.6593 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 35272/35720 Training loss: 0.6593 0.2119 sec/batch\n",
      "Epoch 20/20  Iteration 35273/35720 Training loss: 0.6593 0.2386 sec/batch\n",
      "Epoch 20/20  Iteration 35274/35720 Training loss: 0.6593 0.2162 sec/batch\n",
      "Epoch 20/20  Iteration 35275/35720 Training loss: 0.6593 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 35276/35720 Training loss: 0.6593 0.2150 sec/batch\n",
      "Epoch 20/20  Iteration 35277/35720 Training loss: 0.6593 0.2150 sec/batch\n",
      "Epoch 20/20  Iteration 35278/35720 Training loss: 0.6592 0.2130 sec/batch\n",
      "Epoch 20/20  Iteration 35279/35720 Training loss: 0.6592 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35280/35720 Training loss: 0.6592 0.2190 sec/batch\n",
      "Epoch 20/20  Iteration 35281/35720 Training loss: 0.6592 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 35282/35720 Training loss: 0.6592 0.2134 sec/batch\n",
      "Epoch 20/20  Iteration 35283/35720 Training loss: 0.6592 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 35284/35720 Training loss: 0.6592 0.2274 sec/batch\n",
      "Epoch 20/20  Iteration 35285/35720 Training loss: 0.6592 0.2058 sec/batch\n",
      "Epoch 20/20  Iteration 35286/35720 Training loss: 0.6593 0.2104 sec/batch\n",
      "Epoch 20/20  Iteration 35287/35720 Training loss: 0.6593 0.2089 sec/batch\n",
      "Epoch 20/20  Iteration 35288/35720 Training loss: 0.6593 0.2170 sec/batch\n",
      "Epoch 20/20  Iteration 35289/35720 Training loss: 0.6592 0.2134 sec/batch\n",
      "Epoch 20/20  Iteration 35290/35720 Training loss: 0.6593 0.2278 sec/batch\n",
      "Epoch 20/20  Iteration 35291/35720 Training loss: 0.6593 0.2320 sec/batch\n",
      "Epoch 20/20  Iteration 35292/35720 Training loss: 0.6593 0.2173 sec/batch\n",
      "Epoch 20/20  Iteration 35293/35720 Training loss: 0.6592 0.2150 sec/batch\n",
      "Epoch 20/20  Iteration 35294/35720 Training loss: 0.6592 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35295/35720 Training loss: 0.6592 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 35296/35720 Training loss: 0.6592 0.2145 sec/batch\n",
      "Epoch 20/20  Iteration 35297/35720 Training loss: 0.6592 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 35298/35720 Training loss: 0.6592 0.2114 sec/batch\n",
      "Epoch 20/20  Iteration 35299/35720 Training loss: 0.6592 0.2258 sec/batch\n",
      "Epoch 20/20  Iteration 35300/35720 Training loss: 0.6592 0.2251 sec/batch\n",
      "Epoch 20/20  Iteration 35301/35720 Training loss: 0.6591 0.2202 sec/batch\n",
      "Epoch 20/20  Iteration 35302/35720 Training loss: 0.6591 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35303/35720 Training loss: 0.6591 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 35304/35720 Training loss: 0.6591 0.2208 sec/batch\n",
      "Epoch 20/20  Iteration 35305/35720 Training loss: 0.6591 0.2126 sec/batch\n",
      "Epoch 20/20  Iteration 35306/35720 Training loss: 0.6591 0.2334 sec/batch\n",
      "Epoch 20/20  Iteration 35307/35720 Training loss: 0.6591 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 35308/35720 Training loss: 0.6591 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35309/35720 Training loss: 0.6590 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35310/35720 Training loss: 0.6590 0.2148 sec/batch\n",
      "Epoch 20/20  Iteration 35311/35720 Training loss: 0.6590 0.2217 sec/batch\n",
      "Epoch 20/20  Iteration 35312/35720 Training loss: 0.6591 0.2051 sec/batch\n",
      "Epoch 20/20  Iteration 35313/35720 Training loss: 0.6591 0.2104 sec/batch\n",
      "Epoch 20/20  Iteration 35314/35720 Training loss: 0.6591 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 35315/35720 Training loss: 0.6591 0.2047 sec/batch\n",
      "Epoch 20/20  Iteration 35316/35720 Training loss: 0.6591 0.2257 sec/batch\n",
      "Epoch 20/20  Iteration 35317/35720 Training loss: 0.6591 0.2309 sec/batch\n",
      "Epoch 20/20  Iteration 35318/35720 Training loss: 0.6590 0.2306 sec/batch\n",
      "Epoch 20/20  Iteration 35319/35720 Training loss: 0.6590 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 35320/35720 Training loss: 0.6591 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 35321/35720 Training loss: 0.6591 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 35322/35720 Training loss: 0.6591 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 35323/35720 Training loss: 0.6591 0.2150 sec/batch\n",
      "Epoch 20/20  Iteration 35324/35720 Training loss: 0.6591 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35325/35720 Training loss: 0.6591 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 35326/35720 Training loss: 0.6590 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35327/35720 Training loss: 0.6590 0.2168 sec/batch\n",
      "Epoch 20/20  Iteration 35328/35720 Training loss: 0.6590 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 35329/35720 Training loss: 0.6590 0.2369 sec/batch\n",
      "Epoch 20/20  Iteration 35330/35720 Training loss: 0.6589 0.2178 sec/batch\n",
      "Epoch 20/20  Iteration 35331/35720 Training loss: 0.6589 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 35332/35720 Training loss: 0.6589 0.2175 sec/batch\n",
      "Epoch 20/20  Iteration 35333/35720 Training loss: 0.6588 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35334/35720 Training loss: 0.6588 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 35335/35720 Training loss: 0.6588 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35336/35720 Training loss: 0.6588 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 35337/35720 Training loss: 0.6588 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 35338/35720 Training loss: 0.6588 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 35339/35720 Training loss: 0.6587 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 35340/35720 Training loss: 0.6587 0.2247 sec/batch\n",
      "Epoch 20/20  Iteration 35341/35720 Training loss: 0.6587 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 35342/35720 Training loss: 0.6587 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 35343/35720 Training loss: 0.6588 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35344/35720 Training loss: 0.6588 0.2372 sec/batch\n",
      "Epoch 20/20  Iteration 35345/35720 Training loss: 0.6587 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 35346/35720 Training loss: 0.6587 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35347/35720 Training loss: 0.6587 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 35348/35720 Training loss: 0.6587 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35349/35720 Training loss: 0.6587 0.2243 sec/batch\n",
      "Epoch 20/20  Iteration 35350/35720 Training loss: 0.6587 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 35351/35720 Training loss: 0.6586 0.2172 sec/batch\n",
      "Epoch 20/20  Iteration 35352/35720 Training loss: 0.6587 0.2057 sec/batch\n",
      "Epoch 20/20  Iteration 35353/35720 Training loss: 0.6587 0.2078 sec/batch\n",
      "Epoch 20/20  Iteration 35354/35720 Training loss: 0.6587 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 35355/35720 Training loss: 0.6587 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 35356/35720 Training loss: 0.6587 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 35357/35720 Training loss: 0.6586 0.2296 sec/batch\n",
      "Epoch 20/20  Iteration 35358/35720 Training loss: 0.6586 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 35359/35720 Training loss: 0.6587 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 35360/35720 Training loss: 0.6587 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 35361/35720 Training loss: 0.6587 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35362/35720 Training loss: 0.6587 0.2161 sec/batch\n",
      "Epoch 20/20  Iteration 35363/35720 Training loss: 0.6587 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 35364/35720 Training loss: 0.6587 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35365/35720 Training loss: 0.6587 0.2129 sec/batch\n",
      "Epoch 20/20  Iteration 35366/35720 Training loss: 0.6587 0.2170 sec/batch\n",
      "Epoch 20/20  Iteration 35367/35720 Training loss: 0.6586 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 35368/35720 Training loss: 0.6586 0.2357 sec/batch\n",
      "Epoch 20/20  Iteration 35369/35720 Training loss: 0.6587 0.2194 sec/batch\n",
      "Epoch 20/20  Iteration 35370/35720 Training loss: 0.6586 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 35371/35720 Training loss: 0.6586 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 35372/35720 Training loss: 0.6586 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 35373/35720 Training loss: 0.6587 0.2388 sec/batch\n",
      "Epoch 20/20  Iteration 35374/35720 Training loss: 0.6587 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 35375/35720 Training loss: 0.6587 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 35376/35720 Training loss: 0.6587 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 35377/35720 Training loss: 0.6587 0.2201 sec/batch\n",
      "Epoch 20/20  Iteration 35378/35720 Training loss: 0.6587 0.2225 sec/batch\n",
      "Epoch 20/20  Iteration 35379/35720 Training loss: 0.6587 0.2198 sec/batch\n",
      "Epoch 20/20  Iteration 35380/35720 Training loss: 0.6587 0.2057 sec/batch\n",
      "Epoch 20/20  Iteration 35381/35720 Training loss: 0.6587 0.2181 sec/batch\n",
      "Epoch 20/20  Iteration 35382/35720 Training loss: 0.6588 0.2203 sec/batch\n",
      "Epoch 20/20  Iteration 35383/35720 Training loss: 0.6587 0.2271 sec/batch\n",
      "Epoch 20/20  Iteration 35384/35720 Training loss: 0.6587 0.2374 sec/batch\n",
      "Epoch 20/20  Iteration 35385/35720 Training loss: 0.6587 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35386/35720 Training loss: 0.6588 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 35387/35720 Training loss: 0.6588 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 35388/35720 Training loss: 0.6588 0.2246 sec/batch\n",
      "Epoch 20/20  Iteration 35389/35720 Training loss: 0.6588 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 35390/35720 Training loss: 0.6588 0.2374 sec/batch\n",
      "Epoch 20/20  Iteration 35391/35720 Training loss: 0.6588 0.2269 sec/batch\n",
      "Epoch 20/20  Iteration 35392/35720 Training loss: 0.6588 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 35393/35720 Training loss: 0.6588 0.2128 sec/batch\n",
      "Epoch 20/20  Iteration 35394/35720 Training loss: 0.6588 0.2233 sec/batch\n",
      "Epoch 20/20  Iteration 35395/35720 Training loss: 0.6589 0.2135 sec/batch\n",
      "Epoch 20/20  Iteration 35396/35720 Training loss: 0.6589 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 35397/35720 Training loss: 0.6589 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 35398/35720 Training loss: 0.6589 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 35399/35720 Training loss: 0.6589 0.2215 sec/batch\n",
      "Epoch 20/20  Iteration 35400/35720 Training loss: 0.6588 0.2094 sec/batch\n",
      "Validation loss: 1.67128 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 35401/35720 Training loss: 0.6592 0.2080 sec/batch\n",
      "Epoch 20/20  Iteration 35402/35720 Training loss: 0.6592 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 35403/35720 Training loss: 0.6592 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35404/35720 Training loss: 0.6592 0.2277 sec/batch\n",
      "Epoch 20/20  Iteration 35405/35720 Training loss: 0.6591 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 35406/35720 Training loss: 0.6591 0.2259 sec/batch\n",
      "Epoch 20/20  Iteration 35407/35720 Training loss: 0.6591 0.2493 sec/batch\n",
      "Epoch 20/20  Iteration 35408/35720 Training loss: 0.6591 0.2298 sec/batch\n",
      "Epoch 20/20  Iteration 35409/35720 Training loss: 0.6590 0.2481 sec/batch\n",
      "Epoch 20/20  Iteration 35410/35720 Training loss: 0.6590 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 35411/35720 Training loss: 0.6589 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 35412/35720 Training loss: 0.6589 0.2080 sec/batch\n",
      "Epoch 20/20  Iteration 35413/35720 Training loss: 0.6589 0.2234 sec/batch\n",
      "Epoch 20/20  Iteration 35414/35720 Training loss: 0.6589 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 35415/35720 Training loss: 0.6589 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 35416/35720 Training loss: 0.6589 0.2342 sec/batch\n",
      "Epoch 20/20  Iteration 35417/35720 Training loss: 0.6589 0.2113 sec/batch\n",
      "Epoch 20/20  Iteration 35418/35720 Training loss: 0.6588 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 35419/35720 Training loss: 0.6588 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 35420/35720 Training loss: 0.6588 0.2361 sec/batch\n",
      "Epoch 20/20  Iteration 35421/35720 Training loss: 0.6588 0.2213 sec/batch\n",
      "Epoch 20/20  Iteration 35422/35720 Training loss: 0.6587 0.2284 sec/batch\n",
      "Epoch 20/20  Iteration 35423/35720 Training loss: 0.6587 0.2201 sec/batch\n",
      "Epoch 20/20  Iteration 35424/35720 Training loss: 0.6587 0.2111 sec/batch\n",
      "Epoch 20/20  Iteration 35425/35720 Training loss: 0.6587 0.2224 sec/batch\n",
      "Epoch 20/20  Iteration 35426/35720 Training loss: 0.6586 0.2268 sec/batch\n",
      "Epoch 20/20  Iteration 35427/35720 Training loss: 0.6586 0.2228 sec/batch\n",
      "Epoch 20/20  Iteration 35428/35720 Training loss: 0.6586 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 35429/35720 Training loss: 0.6587 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 35430/35720 Training loss: 0.6586 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 35431/35720 Training loss: 0.6586 0.2242 sec/batch\n",
      "Epoch 20/20  Iteration 35432/35720 Training loss: 0.6586 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 35433/35720 Training loss: 0.6586 0.2246 sec/batch\n",
      "Epoch 20/20  Iteration 35434/35720 Training loss: 0.6585 0.2170 sec/batch\n",
      "Epoch 20/20  Iteration 35435/35720 Training loss: 0.6585 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 35436/35720 Training loss: 0.6585 0.2122 sec/batch\n",
      "Epoch 20/20  Iteration 35437/35720 Training loss: 0.6585 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 35438/35720 Training loss: 0.6585 0.2076 sec/batch\n",
      "Epoch 20/20  Iteration 35439/35720 Training loss: 0.6585 0.2129 sec/batch\n",
      "Epoch 20/20  Iteration 35440/35720 Training loss: 0.6585 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35441/35720 Training loss: 0.6585 0.2112 sec/batch\n",
      "Epoch 20/20  Iteration 35442/35720 Training loss: 0.6584 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 35443/35720 Training loss: 0.6585 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35444/35720 Training loss: 0.6584 0.2105 sec/batch\n",
      "Epoch 20/20  Iteration 35445/35720 Training loss: 0.6584 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 35446/35720 Training loss: 0.6584 0.2139 sec/batch\n",
      "Epoch 20/20  Iteration 35447/35720 Training loss: 0.6584 0.2210 sec/batch\n",
      "Epoch 20/20  Iteration 35448/35720 Training loss: 0.6584 0.2166 sec/batch\n",
      "Epoch 20/20  Iteration 35449/35720 Training loss: 0.6584 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 35450/35720 Training loss: 0.6584 0.2267 sec/batch\n",
      "Epoch 20/20  Iteration 35451/35720 Training loss: 0.6584 0.2054 sec/batch\n",
      "Epoch 20/20  Iteration 35452/35720 Training loss: 0.6584 0.2198 sec/batch\n",
      "Epoch 20/20  Iteration 35453/35720 Training loss: 0.6584 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 35454/35720 Training loss: 0.6584 0.2170 sec/batch\n",
      "Epoch 20/20  Iteration 35455/35720 Training loss: 0.6584 0.2223 sec/batch\n",
      "Epoch 20/20  Iteration 35456/35720 Training loss: 0.6584 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 35457/35720 Training loss: 0.6584 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 35458/35720 Training loss: 0.6584 0.2220 sec/batch\n",
      "Epoch 20/20  Iteration 35459/35720 Training loss: 0.6585 0.2151 sec/batch\n",
      "Epoch 20/20  Iteration 35460/35720 Training loss: 0.6585 0.2227 sec/batch\n",
      "Epoch 20/20  Iteration 35461/35720 Training loss: 0.6585 0.2319 sec/batch\n",
      "Epoch 20/20  Iteration 35462/35720 Training loss: 0.6584 0.2110 sec/batch\n",
      "Epoch 20/20  Iteration 35463/35720 Training loss: 0.6585 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 35464/35720 Training loss: 0.6585 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35465/35720 Training loss: 0.6585 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 35466/35720 Training loss: 0.6585 0.2203 sec/batch\n",
      "Epoch 20/20  Iteration 35467/35720 Training loss: 0.6585 0.2075 sec/batch\n",
      "Epoch 20/20  Iteration 35468/35720 Training loss: 0.6585 0.2109 sec/batch\n",
      "Epoch 20/20  Iteration 35469/35720 Training loss: 0.6585 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 35470/35720 Training loss: 0.6585 0.2431 sec/batch\n",
      "Epoch 20/20  Iteration 35471/35720 Training loss: 0.6585 0.2139 sec/batch\n",
      "Epoch 20/20  Iteration 35472/35720 Training loss: 0.6584 0.2408 sec/batch\n",
      "Epoch 20/20  Iteration 35473/35720 Training loss: 0.6584 0.2055 sec/batch\n",
      "Epoch 20/20  Iteration 35474/35720 Training loss: 0.6584 0.2056 sec/batch\n",
      "Epoch 20/20  Iteration 35475/35720 Training loss: 0.6583 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35476/35720 Training loss: 0.6584 0.2285 sec/batch\n",
      "Epoch 20/20  Iteration 35477/35720 Training loss: 0.6583 0.2214 sec/batch\n",
      "Epoch 20/20  Iteration 35478/35720 Training loss: 0.6583 0.2187 sec/batch\n",
      "Epoch 20/20  Iteration 35479/35720 Training loss: 0.6583 0.2220 sec/batch\n",
      "Epoch 20/20  Iteration 35480/35720 Training loss: 0.6583 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 35481/35720 Training loss: 0.6583 0.2258 sec/batch\n",
      "Epoch 20/20  Iteration 35482/35720 Training loss: 0.6583 0.2137 sec/batch\n",
      "Epoch 20/20  Iteration 35483/35720 Training loss: 0.6583 0.2160 sec/batch\n",
      "Epoch 20/20  Iteration 35484/35720 Training loss: 0.6583 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 35485/35720 Training loss: 0.6583 0.2227 sec/batch\n",
      "Epoch 20/20  Iteration 35486/35720 Training loss: 0.6583 0.2090 sec/batch\n",
      "Epoch 20/20  Iteration 35487/35720 Training loss: 0.6583 0.2162 sec/batch\n",
      "Epoch 20/20  Iteration 35488/35720 Training loss: 0.6583 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 35489/35720 Training loss: 0.6582 0.2255 sec/batch\n",
      "Epoch 20/20  Iteration 35490/35720 Training loss: 0.6582 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 35491/35720 Training loss: 0.6582 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 35492/35720 Training loss: 0.6581 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 35493/35720 Training loss: 0.6581 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 35494/35720 Training loss: 0.6581 0.2161 sec/batch\n",
      "Epoch 20/20  Iteration 35495/35720 Training loss: 0.6581 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 35496/35720 Training loss: 0.6580 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35497/35720 Training loss: 0.6581 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 35498/35720 Training loss: 0.6580 0.2151 sec/batch\n",
      "Epoch 20/20  Iteration 35499/35720 Training loss: 0.6580 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 35500/35720 Training loss: 0.6580 0.2243 sec/batch\n",
      "Epoch 20/20  Iteration 35501/35720 Training loss: 0.6580 0.2108 sec/batch\n",
      "Epoch 20/20  Iteration 35502/35720 Training loss: 0.6580 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 35503/35720 Training loss: 0.6580 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 35504/35720 Training loss: 0.6580 0.2144 sec/batch\n",
      "Epoch 20/20  Iteration 35505/35720 Training loss: 0.6580 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 35506/35720 Training loss: 0.6580 0.2220 sec/batch\n",
      "Epoch 20/20  Iteration 35507/35720 Training loss: 0.6579 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 35508/35720 Training loss: 0.6579 0.2220 sec/batch\n",
      "Epoch 20/20  Iteration 35509/35720 Training loss: 0.6579 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 35510/35720 Training loss: 0.6579 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 35511/35720 Training loss: 0.6579 0.2262 sec/batch\n",
      "Epoch 20/20  Iteration 35512/35720 Training loss: 0.6579 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 35513/35720 Training loss: 0.6579 0.2067 sec/batch\n",
      "Epoch 20/20  Iteration 35514/35720 Training loss: 0.6579 0.2136 sec/batch\n",
      "Epoch 20/20  Iteration 35515/35720 Training loss: 0.6579 0.2295 sec/batch\n",
      "Epoch 20/20  Iteration 35516/35720 Training loss: 0.6579 0.2287 sec/batch\n",
      "Epoch 20/20  Iteration 35517/35720 Training loss: 0.6579 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35518/35720 Training loss: 0.6578 0.2206 sec/batch\n",
      "Epoch 20/20  Iteration 35519/35720 Training loss: 0.6578 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 35520/35720 Training loss: 0.6578 0.2306 sec/batch\n",
      "Epoch 20/20  Iteration 35521/35720 Training loss: 0.6578 0.2154 sec/batch\n",
      "Epoch 20/20  Iteration 35522/35720 Training loss: 0.6578 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 35523/35720 Training loss: 0.6577 0.2182 sec/batch\n",
      "Epoch 20/20  Iteration 35524/35720 Training loss: 0.6577 0.2077 sec/batch\n",
      "Epoch 20/20  Iteration 35525/35720 Training loss: 0.6577 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 35526/35720 Training loss: 0.6577 0.2216 sec/batch\n",
      "Epoch 20/20  Iteration 35527/35720 Training loss: 0.6577 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 35528/35720 Training loss: 0.6577 0.2118 sec/batch\n",
      "Epoch 20/20  Iteration 35529/35720 Training loss: 0.6576 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35530/35720 Training loss: 0.6576 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35531/35720 Training loss: 0.6575 0.2250 sec/batch\n",
      "Epoch 20/20  Iteration 35532/35720 Training loss: 0.6575 0.2359 sec/batch\n",
      "Epoch 20/20  Iteration 35533/35720 Training loss: 0.6575 0.2226 sec/batch\n",
      "Epoch 20/20  Iteration 35534/35720 Training loss: 0.6575 0.2068 sec/batch\n",
      "Epoch 20/20  Iteration 35535/35720 Training loss: 0.6574 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 35536/35720 Training loss: 0.6574 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 35537/35720 Training loss: 0.6574 0.2304 sec/batch\n",
      "Epoch 20/20  Iteration 35538/35720 Training loss: 0.6574 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 35539/35720 Training loss: 0.6574 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 35540/35720 Training loss: 0.6573 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 35541/35720 Training loss: 0.6573 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35542/35720 Training loss: 0.6573 0.2154 sec/batch\n",
      "Epoch 20/20  Iteration 35543/35720 Training loss: 0.6573 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 35544/35720 Training loss: 0.6572 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 35545/35720 Training loss: 0.6572 0.2167 sec/batch\n",
      "Epoch 20/20  Iteration 35546/35720 Training loss: 0.6572 0.2053 sec/batch\n",
      "Epoch 20/20  Iteration 35547/35720 Training loss: 0.6572 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 35548/35720 Training loss: 0.6572 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 35549/35720 Training loss: 0.6572 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35550/35720 Training loss: 0.6572 0.2264 sec/batch\n",
      "Epoch 20/20  Iteration 35551/35720 Training loss: 0.6572 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 35552/35720 Training loss: 0.6572 0.2100 sec/batch\n",
      "Epoch 20/20  Iteration 35553/35720 Training loss: 0.6571 0.2106 sec/batch\n",
      "Epoch 20/20  Iteration 35554/35720 Training loss: 0.6571 0.2122 sec/batch\n",
      "Epoch 20/20  Iteration 35555/35720 Training loss: 0.6571 0.2265 sec/batch\n",
      "Epoch 20/20  Iteration 35556/35720 Training loss: 0.6572 0.2177 sec/batch\n",
      "Epoch 20/20  Iteration 35557/35720 Training loss: 0.6572 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35558/35720 Training loss: 0.6572 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35559/35720 Training loss: 0.6572 0.2252 sec/batch\n",
      "Epoch 20/20  Iteration 35560/35720 Training loss: 0.6572 0.2137 sec/batch\n",
      "Epoch 20/20  Iteration 35561/35720 Training loss: 0.6572 0.2218 sec/batch\n",
      "Epoch 20/20  Iteration 35562/35720 Training loss: 0.6572 0.2073 sec/batch\n",
      "Epoch 20/20  Iteration 35563/35720 Training loss: 0.6571 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 35564/35720 Training loss: 0.6572 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 35565/35720 Training loss: 0.6571 0.2188 sec/batch\n",
      "Epoch 20/20  Iteration 35566/35720 Training loss: 0.6571 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 35567/35720 Training loss: 0.6571 0.2269 sec/batch\n",
      "Epoch 20/20  Iteration 35568/35720 Training loss: 0.6571 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 35569/35720 Training loss: 0.6571 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 35570/35720 Training loss: 0.6571 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35571/35720 Training loss: 0.6571 0.2256 sec/batch\n",
      "Epoch 20/20  Iteration 35572/35720 Training loss: 0.6570 0.2208 sec/batch\n",
      "Epoch 20/20  Iteration 35573/35720 Training loss: 0.6571 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35574/35720 Training loss: 0.6571 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 35575/35720 Training loss: 0.6571 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 35576/35720 Training loss: 0.6571 0.2252 sec/batch\n",
      "Epoch 20/20  Iteration 35577/35720 Training loss: 0.6571 0.2166 sec/batch\n",
      "Epoch 20/20  Iteration 35578/35720 Training loss: 0.6571 0.2200 sec/batch\n",
      "Epoch 20/20  Iteration 35579/35720 Training loss: 0.6571 0.2117 sec/batch\n",
      "Epoch 20/20  Iteration 35580/35720 Training loss: 0.6571 0.2187 sec/batch\n",
      "Epoch 20/20  Iteration 35581/35720 Training loss: 0.6571 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 35582/35720 Training loss: 0.6571 0.2204 sec/batch\n",
      "Epoch 20/20  Iteration 35583/35720 Training loss: 0.6571 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 35584/35720 Training loss: 0.6571 0.2150 sec/batch\n",
      "Epoch 20/20  Iteration 35585/35720 Training loss: 0.6571 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35586/35720 Training loss: 0.6571 0.2097 sec/batch\n",
      "Epoch 20/20  Iteration 35587/35720 Training loss: 0.6571 0.2063 sec/batch\n",
      "Epoch 20/20  Iteration 35588/35720 Training loss: 0.6571 0.2125 sec/batch\n",
      "Epoch 20/20  Iteration 35589/35720 Training loss: 0.6571 0.2133 sec/batch\n",
      "Epoch 20/20  Iteration 35590/35720 Training loss: 0.6571 0.2302 sec/batch\n",
      "Epoch 20/20  Iteration 35591/35720 Training loss: 0.6571 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 35592/35720 Training loss: 0.6571 0.2131 sec/batch\n",
      "Epoch 20/20  Iteration 35593/35720 Training loss: 0.6570 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 35594/35720 Training loss: 0.6570 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 35595/35720 Training loss: 0.6570 0.2148 sec/batch\n",
      "Epoch 20/20  Iteration 35596/35720 Training loss: 0.6570 0.2098 sec/batch\n",
      "Epoch 20/20  Iteration 35597/35720 Training loss: 0.6570 0.2187 sec/batch\n",
      "Epoch 20/20  Iteration 35598/35720 Training loss: 0.6570 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 35599/35720 Training loss: 0.6570 0.2218 sec/batch\n",
      "Epoch 20/20  Iteration 35600/35720 Training loss: 0.6570 0.2305 sec/batch\n",
      "Validation loss: 1.68345 Saving checkpoint!\n",
      "Epoch 20/20  Iteration 35601/35720 Training loss: 0.6573 0.2093 sec/batch\n",
      "Epoch 20/20  Iteration 35602/35720 Training loss: 0.6574 0.2080 sec/batch\n",
      "Epoch 20/20  Iteration 35603/35720 Training loss: 0.6574 0.2181 sec/batch\n",
      "Epoch 20/20  Iteration 35604/35720 Training loss: 0.6574 0.2167 sec/batch\n",
      "Epoch 20/20  Iteration 35605/35720 Training loss: 0.6574 0.2158 sec/batch\n",
      "Epoch 20/20  Iteration 35606/35720 Training loss: 0.6574 0.2055 sec/batch\n",
      "Epoch 20/20  Iteration 35607/35720 Training loss: 0.6574 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 35608/35720 Training loss: 0.6575 0.2103 sec/batch\n",
      "Epoch 20/20  Iteration 35609/35720 Training loss: 0.6574 0.2159 sec/batch\n",
      "Epoch 20/20  Iteration 35610/35720 Training loss: 0.6574 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 35611/35720 Training loss: 0.6574 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 35612/35720 Training loss: 0.6574 0.2114 sec/batch\n",
      "Epoch 20/20  Iteration 35613/35720 Training loss: 0.6574 0.2236 sec/batch\n",
      "Epoch 20/20  Iteration 35614/35720 Training loss: 0.6574 0.2139 sec/batch\n",
      "Epoch 20/20  Iteration 35615/35720 Training loss: 0.6574 0.2123 sec/batch\n",
      "Epoch 20/20  Iteration 35616/35720 Training loss: 0.6573 0.2505 sec/batch\n",
      "Epoch 20/20  Iteration 35617/35720 Training loss: 0.6573 0.2060 sec/batch\n",
      "Epoch 20/20  Iteration 35618/35720 Training loss: 0.6574 0.2047 sec/batch\n",
      "Epoch 20/20  Iteration 35619/35720 Training loss: 0.6573 0.2092 sec/batch\n",
      "Epoch 20/20  Iteration 35620/35720 Training loss: 0.6574 0.2147 sec/batch\n",
      "Epoch 20/20  Iteration 35621/35720 Training loss: 0.6574 0.2088 sec/batch\n",
      "Epoch 20/20  Iteration 35622/35720 Training loss: 0.6574 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 35623/35720 Training loss: 0.6574 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 35624/35720 Training loss: 0.6574 0.2070 sec/batch\n",
      "Epoch 20/20  Iteration 35625/35720 Training loss: 0.6573 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 35626/35720 Training loss: 0.6573 0.2138 sec/batch\n",
      "Epoch 20/20  Iteration 35627/35720 Training loss: 0.6573 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35628/35720 Training loss: 0.6573 0.2345 sec/batch\n",
      "Epoch 20/20  Iteration 35629/35720 Training loss: 0.6573 0.2271 sec/batch\n",
      "Epoch 20/20  Iteration 35630/35720 Training loss: 0.6573 0.2289 sec/batch\n",
      "Epoch 20/20  Iteration 35631/35720 Training loss: 0.6573 0.2163 sec/batch\n",
      "Epoch 20/20  Iteration 35632/35720 Training loss: 0.6573 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35633/35720 Training loss: 0.6573 0.2206 sec/batch\n",
      "Epoch 20/20  Iteration 35634/35720 Training loss: 0.6573 0.2269 sec/batch\n",
      "Epoch 20/20  Iteration 35635/35720 Training loss: 0.6573 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 35636/35720 Training loss: 0.6572 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 35637/35720 Training loss: 0.6572 0.2199 sec/batch\n",
      "Epoch 20/20  Iteration 35638/35720 Training loss: 0.6572 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 35639/35720 Training loss: 0.6572 0.2114 sec/batch\n",
      "Epoch 20/20  Iteration 35640/35720 Training loss: 0.6572 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 35641/35720 Training loss: 0.6572 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 35642/35720 Training loss: 0.6572 0.2180 sec/batch\n",
      "Epoch 20/20  Iteration 35643/35720 Training loss: 0.6573 0.2152 sec/batch\n",
      "Epoch 20/20  Iteration 35644/35720 Training loss: 0.6573 0.2217 sec/batch\n",
      "Epoch 20/20  Iteration 35645/35720 Training loss: 0.6573 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35646/35720 Training loss: 0.6573 0.2062 sec/batch\n",
      "Epoch 20/20  Iteration 35647/35720 Training loss: 0.6573 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 35648/35720 Training loss: 0.6573 0.2276 sec/batch\n",
      "Epoch 20/20  Iteration 35649/35720 Training loss: 0.6573 0.2206 sec/batch\n",
      "Epoch 20/20  Iteration 35650/35720 Training loss: 0.6573 0.2232 sec/batch\n",
      "Epoch 20/20  Iteration 35651/35720 Training loss: 0.6573 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35652/35720 Training loss: 0.6573 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 35653/35720 Training loss: 0.6573 0.2057 sec/batch\n",
      "Epoch 20/20  Iteration 35654/35720 Training loss: 0.6573 0.2183 sec/batch\n",
      "Epoch 20/20  Iteration 35655/35720 Training loss: 0.6573 0.2176 sec/batch\n",
      "Epoch 20/20  Iteration 35656/35720 Training loss: 0.6573 0.2071 sec/batch\n",
      "Epoch 20/20  Iteration 35657/35720 Training loss: 0.6573 0.2069 sec/batch\n",
      "Epoch 20/20  Iteration 35658/35720 Training loss: 0.6573 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 35659/35720 Training loss: 0.6573 0.2183 sec/batch\n",
      "Epoch 20/20  Iteration 35660/35720 Training loss: 0.6573 0.2209 sec/batch\n",
      "Epoch 20/20  Iteration 35661/35720 Training loss: 0.6573 0.2235 sec/batch\n",
      "Epoch 20/20  Iteration 35662/35720 Training loss: 0.6573 0.2113 sec/batch\n",
      "Epoch 20/20  Iteration 35663/35720 Training loss: 0.6573 0.2077 sec/batch\n",
      "Epoch 20/20  Iteration 35664/35720 Training loss: 0.6573 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 35665/35720 Training loss: 0.6573 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 35666/35720 Training loss: 0.6574 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 35667/35720 Training loss: 0.6574 0.2146 sec/batch\n",
      "Epoch 20/20  Iteration 35668/35720 Training loss: 0.6574 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 35669/35720 Training loss: 0.6574 0.2084 sec/batch\n",
      "Epoch 20/20  Iteration 35670/35720 Training loss: 0.6574 0.2101 sec/batch\n",
      "Epoch 20/20  Iteration 35671/35720 Training loss: 0.6575 0.2157 sec/batch\n",
      "Epoch 20/20  Iteration 35672/35720 Training loss: 0.6574 0.2450 sec/batch\n",
      "Epoch 20/20  Iteration 35673/35720 Training loss: 0.6575 0.2182 sec/batch\n",
      "Epoch 20/20  Iteration 35674/35720 Training loss: 0.6574 0.2163 sec/batch\n",
      "Epoch 20/20  Iteration 35675/35720 Training loss: 0.6574 0.2141 sec/batch\n",
      "Epoch 20/20  Iteration 35676/35720 Training loss: 0.6574 0.2279 sec/batch\n",
      "Epoch 20/20  Iteration 35677/35720 Training loss: 0.6574 0.2099 sec/batch\n",
      "Epoch 20/20  Iteration 35678/35720 Training loss: 0.6574 0.2153 sec/batch\n",
      "Epoch 20/20  Iteration 35679/35720 Training loss: 0.6575 0.2059 sec/batch\n",
      "Epoch 20/20  Iteration 35680/35720 Training loss: 0.6575 0.2066 sec/batch\n",
      "Epoch 20/20  Iteration 35681/35720 Training loss: 0.6575 0.2102 sec/batch\n",
      "Epoch 20/20  Iteration 35682/35720 Training loss: 0.6575 0.2134 sec/batch\n",
      "Epoch 20/20  Iteration 35683/35720 Training loss: 0.6575 0.2362 sec/batch\n",
      "Epoch 20/20  Iteration 35684/35720 Training loss: 0.6575 0.2078 sec/batch\n",
      "Epoch 20/20  Iteration 35685/35720 Training loss: 0.6575 0.2083 sec/batch\n",
      "Epoch 20/20  Iteration 35686/35720 Training loss: 0.6575 0.2195 sec/batch\n",
      "Epoch 20/20  Iteration 35687/35720 Training loss: 0.6575 0.2231 sec/batch\n",
      "Epoch 20/20  Iteration 35688/35720 Training loss: 0.6575 0.2165 sec/batch\n",
      "Epoch 20/20  Iteration 35689/35720 Training loss: 0.6575 0.2311 sec/batch\n",
      "Epoch 20/20  Iteration 35690/35720 Training loss: 0.6576 0.2128 sec/batch\n",
      "Epoch 20/20  Iteration 35691/35720 Training loss: 0.6576 0.2085 sec/batch\n",
      "Epoch 20/20  Iteration 35692/35720 Training loss: 0.6575 0.2128 sec/batch\n",
      "Epoch 20/20  Iteration 35693/35720 Training loss: 0.6576 0.2287 sec/batch\n",
      "Epoch 20/20  Iteration 35694/35720 Training loss: 0.6576 0.2196 sec/batch\n",
      "Epoch 20/20  Iteration 35695/35720 Training loss: 0.6576 0.2076 sec/batch\n",
      "Epoch 20/20  Iteration 35696/35720 Training loss: 0.6576 0.2115 sec/batch\n",
      "Epoch 20/20  Iteration 35697/35720 Training loss: 0.6576 0.2192 sec/batch\n",
      "Epoch 20/20  Iteration 35698/35720 Training loss: 0.6576 0.2151 sec/batch\n",
      "Epoch 20/20  Iteration 35699/35720 Training loss: 0.6576 0.2086 sec/batch\n",
      "Epoch 20/20  Iteration 35700/35720 Training loss: 0.6575 0.2215 sec/batch\n",
      "Epoch 20/20  Iteration 35701/35720 Training loss: 0.6575 0.2091 sec/batch\n",
      "Epoch 20/20  Iteration 35702/35720 Training loss: 0.6576 0.2120 sec/batch\n",
      "Epoch 20/20  Iteration 35703/35720 Training loss: 0.6576 0.2114 sec/batch\n",
      "Epoch 20/20  Iteration 35704/35720 Training loss: 0.6576 0.2171 sec/batch\n",
      "Epoch 20/20  Iteration 35705/35720 Training loss: 0.6576 0.2081 sec/batch\n",
      "Epoch 20/20  Iteration 35706/35720 Training loss: 0.6575 0.2174 sec/batch\n",
      "Epoch 20/20  Iteration 35707/35720 Training loss: 0.6575 0.2124 sec/batch\n",
      "Epoch 20/20  Iteration 35708/35720 Training loss: 0.6575 0.2095 sec/batch\n",
      "Epoch 20/20  Iteration 35709/35720 Training loss: 0.6575 0.2429 sec/batch\n",
      "Epoch 20/20  Iteration 35710/35720 Training loss: 0.6575 0.2094 sec/batch\n",
      "Epoch 20/20  Iteration 35711/35720 Training loss: 0.6575 0.2121 sec/batch\n",
      "Epoch 20/20  Iteration 35712/35720 Training loss: 0.6575 0.2717 sec/batch\n",
      "Epoch 20/20  Iteration 35713/35720 Training loss: 0.6575 0.2065 sec/batch\n",
      "Epoch 20/20  Iteration 35714/35720 Training loss: 0.6574 0.2096 sec/batch\n",
      "Epoch 20/20  Iteration 35715/35720 Training loss: 0.6574 0.2398 sec/batch\n",
      "Epoch 20/20  Iteration 35716/35720 Training loss: 0.6574 0.2260 sec/batch\n",
      "Epoch 20/20  Iteration 35717/35720 Training loss: 0.6574 0.2087 sec/batch\n",
      "Epoch 20/20  Iteration 35718/35720 Training loss: 0.6574 0.2064 sec/batch\n",
      "Epoch 20/20  Iteration 35719/35720 Training loss: 0.6574 0.2127 sec/batch\n",
      "Epoch 20/20  Iteration 35720/35720 Training loss: 0.6574 0.2299 sec/batch\n",
      "Validation loss: 1.67012 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i35720_l512_v1.670.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16000_l512_v1.399.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16200_l512_v1.385.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16400_l512_v1.404.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16600_l512_v1.400.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i16800_l512_v1.388.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17000_l512_v1.411.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17200_l512_v1.402.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17400_l512_v1.407.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17600_l512_v1.423.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i17800_l512_v1.428.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18000_l512_v1.418.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18200_l512_v1.426.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18400_l512_v1.427.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18600_l512_v1.429.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i18800_l512_v1.423.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19000_l512_v1.425.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19200_l512_v1.434.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19400_l512_v1.436.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19600_l512_v1.443.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i19800_l512_v1.452.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20000_l512_v1.451.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20200_l512_v1.453.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20400_l512_v1.454.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20600_l512_v1.459.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i20800_l512_v1.456.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21000_l512_v1.460.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21200_l512_v1.468.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21400_l512_v1.463.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21600_l512_v1.479.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i21800_l512_v1.469.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22000_l512_v1.471.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22200_l512_v1.473.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22400_l512_v1.497.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22600_l512_v1.489.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i22800_l512_v1.490.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23000_l512_v1.503.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23200_l512_v1.491.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23400_l512_v1.504.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23600_l512_v1.504.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i23800_l512_v1.503.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24000_l512_v1.503.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24200_l512_v1.522.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24400_l512_v1.514.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24600_l512_v1.523.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i24800_l512_v1.527.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25000_l512_v1.524.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25200_l512_v1.536.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25400_l512_v1.543.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25600_l512_v1.538.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i25800_l512_v1.542.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26000_l512_v1.547.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26200_l512_v1.537.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26400_l512_v1.555.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26600_l512_v1.562.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i26800_l512_v1.551.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27000_l512_v1.566.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27200_l512_v1.565.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27400_l512_v1.559.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27600_l512_v1.566.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i27800_l512_v1.578.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28000_l512_v1.566.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28200_l512_v1.584.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28400_l512_v1.593.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28600_l512_v1.584.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i28800_l512_v1.605.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29000_l512_v1.597.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29200_l512_v1.589.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29400_l512_v1.592.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29600_l512_v1.607.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i29800_l512_v1.607.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30000_l512_v1.620.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30200_l512_v1.629.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30400_l512_v1.609.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30600_l512_v1.625.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i30800_l512_v1.628.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31000_l512_v1.618.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31200_l512_v1.606.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31400_l512_v1.632.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31600_l512_v1.641.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i31800_l512_v1.630.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32000_l512_v1.647.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32200_l512_v1.637.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32400_l512_v1.655.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32600_l512_v1.655.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i32800_l512_v1.646.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33000_l512_v1.640.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33200_l512_v1.669.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33400_l512_v1.665.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33600_l512_v1.642.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i33800_l512_v1.671.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34000_l512_v1.662.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34200_l512_v1.688.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34400_l512_v1.669.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34600_l512_v1.658.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i34800_l512_v1.679.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35000_l512_v1.679.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35200_l512_v1.676.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35400_l512_v1.671.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35600_l512_v1.683.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i35720_l512_v1.670.ckpt\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i35720_l512_v1.670.ckpt\n",
      "FardraiI and the\n",
      "next day to be where and institutions of the worries in the morning from\n",
      "the open chasb that had always had already with the tass of poverty\n",
      "in me of people in the air, in ready to begin telling Stepan Arkadyevitch\n",
      "at the races; but to this fearful movement interview with wime, that\n",
      "had been so cleverly and happy, and that he was to see which had been that\n",
      "of everymonot public opinion was not what, but they would be a growing\n",
      "heavenly flirting on.\n",
      "\n",
      "When the third day Sprenthy missham affectionations supporty over his\n",
      "man before the tiny which he liked to see him. But after a previous day\n",
      "and was taken off, the merest in the closest recognizing him with smiles\n",
      "that he had been felt by moneys as he had seen now, and half times in\n",
      "the same special row forests of the peasantry and his self-conformed expent\n",
      "with him, and with his characteristic step to move his self-confidence\n",
      "of his own, and he heard the counting house clerk.\n",
      "\n",
      "She knew and said to that subscing with review a qualttires, and sturbed his\n",
      "open field. He was not afraid of tempering any of himself, but at that\n",
      "moment, not to distincted men she arranged against me. And he had\n",
      "nothing to do with Vronsky's proposed, not to speak of it. She felt\n",
      "than the complexity it was that he wanted in his mind.\n",
      "\n",
      "\"You see what can always be the care! I know you when I strang up\n",
      "your horse, and always filled with him; and that I must marry, he had a\n",
      "visitors for me had to do,\" he thought. \"But that is trust, but in the\n",
      "middle its were disagreeably to be at home in the cart. Laska's\n",
      "wife, a take of drawerous order, some newspaper, and not one to know her as\n",
      "she added sending himself that she was more and more free the\n",
      "educated brother as a power, a man with a shirt. He did not know how to\n",
      "doubt the meeting was not when she had said about the terrar to back\n",
      "she found herself with a bright moments over the meeting. \"Wurt me so\n",
      "awful!\" Stepan Arkadyevitch answered coldly. \"With said the same,\" cried\n",
      "replies in a turb\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i35720_l512_v1.670.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i35720_l512_v1.670.ckpt\n",
      "Farth, while he\n",
      "does not think it. On the feeling, and to show men and means so triedered even\n",
      "of hisself.\n",
      "\n",
      "\"It's not though,\" said Alexey Alexandrovitch.\n",
      "\n",
      "Sergey Ivanovitch felt that the committee which she could, too, there\n",
      "was the count, the bailiff had an ecstasy of speaking in the air,\n",
      "order the world, which was that sort of himself and irritating the president\n",
      "was transformed, but a sign of stress one of the artilest, as it were\n",
      "apoled to all dutgs of skate.\n",
      "\n",
      "It was infend why there was a mistake as could in the side paction. It must be\n",
      "still all done of what sand all during in his own thing Iron wound that\n",
      "he carefully began to prove any complex it only flung and eager to be side to\n",
      "doubt. Levin, lean him that a man of the powerful source, he would have\n",
      "caused her. She turned the conversation with her things.\n",
      "\n",
      "\"Now masters, and there's a little right to make their position.\"\n",
      "\n",
      "\"Anna, it is not good for seciefie then for. When are you saying?\"\n",
      "\n",
      "\"Well, I don't said anything,\" answered Levin.\n",
      "\n",
      "\"You don't say so!\"\n",
      "\n",
      "\"Not love!\" Levin confined to assuge.\n",
      "\n",
      "She did not look at the meadow's way. It's lean quickly and also\" he\n",
      "repeated.\n",
      "\n",
      "Kitty snatched the countess and her slender in tightlits white bears of\n",
      "allet, she seemed so simply to make save him to a man without talking, of\n",
      "tea, but as she always doctor it always acquaintent with her.\n",
      "She recalled with tears of the men of Sergey Ivanovitch's eyes. She felt\n",
      "sorrowans, too, because in the service and the president, and then she\n",
      "smiled couthing, as though the same instant, so intended to his brother, but\n",
      "now they must tell whether as I can see her. He looked as intente-men I\n",
      "do. With him that you common being placed? What touch your daughter\n",
      "has occurred to me? We met any man's life whatever,\" he turned to\n",
      "a starched cud by his laughter, whispered only that still had a great\n",
      "deal of conversation to the beaten tree hand of beefude in a continual\n",
      "spiritual quality as a scarce altion of the marshal of the province.\n",
      "After a\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i35720_l512_v1.670.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i35720_l512_v1.670.ckpt\n",
      "The Amerin-ble the\n",
      "impressions of the morning. As in that case, he felt that the immortange\n",
      "always tried and take past his pockets, with a sudden and talking\n",
      "to the platform read it. And he felt that the complexing each otherse\n",
      "porter she had foreal its hind by such dusting into which he was complete\n",
      "controlless and accounts of the world, and that the master of the\n",
      "awful position with Anna Arkadyevna used to say to him something about so\n",
      "mangumed and important to himself, and even the fact that she was feeling the\n",
      "same intense for something. And all this tender or ten stop to the railway\n",
      "snalformence of the night ran down to him there to the message of his place\n",
      "with a friendly accidnation. With the president, this position, the slightest\n",
      "definite, angles, was that it was one of the province, but simply\n",
      "shaded acquaintances. And so immediately she might do not to do.\n",
      "\n",
      "Talking to himself his own past, and she rose too long age, as though\n",
      "to greet his speech, and that he had no time to come.\n",
      "\n",
      "As the cow on her ear had concealed at the cause of peasant as a narmor,\n",
      "and he saw after his opinion, that he could not help readined,\n",
      "they might begin.\n",
      "\n",
      "He was set afrect he did not care for her, and a splendid beyond\n",
      "the marshal of the most view, of which the possession ran about together that\n",
      "huss and a short storm of making up to her, and that he wanted with\n",
      "that in the hipsest he was happy, and had not the slightest deception. It\n",
      "would not be angle for him to deserve himself that he had highered so. Only\n",
      "solimeness and acquaintances: his feeling, try to drive in the middle of\n",
      "the carpenters, and were making peace on everything he thinked of it,\n",
      "that now of the same, have a departure and regretted it. And you've ced,\n",
      "took it to meeting you on that sobica lanch of things.\"\n",
      "\n",
      "\"Anyway!\" she went on, taking her arm in spring up her voice, \"and\n",
      "it's not the work,\" he said, smiling.\n",
      "\n",
      "\"I am doing afraid of himself, to my heart that you must all all do this\n",
      "will be. He's not my last coll\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i35720_l512_v1.670.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"The\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i35720_l512_v1.670.ckpt\n",
      "happy?\n",
      "He thought so much clearers, and what she had not been that seppt show\n",
      "it all in the sense that their former cause of the meetings of the alternal. He\n",
      "was confided by this topic of tention, and the female visitors\n",
      "were engrossed and he had a great sense of rivalry in the country, as the\n",
      "man's care of the peasantry had been bad for not build and a sense of inonisation,\n",
      "and a sense of terror and the necks, and forced, and as he always\n",
      "wroteed in with other machines from even before the meeting said, against him\n",
      "to be sent all to forget it all, and there were some hand by the\n",
      "aim of the meadow acred and thrown that men in the most surraund\n",
      "and the necks incident. She felt she was seeing a delightful summer at the\n",
      "elections.\n",
      "\n",
      "\"Anyoner returned, darling, sweet all her skirt. I liked you,\" he\n",
      "said all he felt afraid.\n",
      "\n",
      "She glanced at him as though he were as glad of an unmistakable\n",
      "scat..\n",
      "\n",
      "\"Will I hair hamp?\"\n",
      "\n",
      "\"Oh, you're ready too, but I suppose it we to be plined, if you want\n",
      "your head,\" as she assured her, \"it seemed to Levin if this was\n",
      "nothing; but this means, and I told him to my bedure. I should have to stop\n",
      "any mean for me. We too love it alone, and not a bird when my worr. I have tod!\n",
      "\n",
      "\n",
      "               love, she seems a capital feeling and all, and try to myself,\" he\n",
      "thought.\n",
      "\n",
      "She did not want to go away, all her life, turned up. Golenishtchev rossented\n",
      "and given up the money for Carlsbad to Tut be.\n",
      "\n",
      "\"One can memper either trusps as I am!... Only now don't touch and\n",
      "pleasen with withouts more. You try, when are today or take out of\n",
      "yourself,\" she said, turning to Stepan Arkadyevitch. \"Do you know all dones\n",
      "along, my fault?\" said Stepan Arkadyevitch. \"There, in this carriage than anyone\n",
      "who did not agree with its own explanation...\"\n",
      "\n",
      "\"It's not the same subject.\"\n",
      "\n",
      "\"No; do me darl now, I love; I could not say at the close peasants. They\n",
      "have thought of the pressy hands.\"\n",
      "\n",
      "\"I'm not a country life, when he's a sort of free time!\" he said, laying\n",
      "her head, \"and hates mo\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i35720_l512_v1.670.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"happy\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
